# Linear mixed-effects models

In this chapter, we will discuss how to deal with the non-independence of errors that can result from repeated-measures

## Non-independence in linear models

The General Linear Model we have considered thus far can be stated as follows:

$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
The assumptions of this model concern the residuals or errors $\epsilon_i$. The are assumed to be _independent and identically distributed_ (iid), following a Normal distribution with a mean of 0 and a standard deviation $\sigma_\epsilon$.

The assumption that the errors are independent means that knowing the value of the error for one case $i$ in the data does not give you any information determine the value of the error for another case $j$. More formally, we define independence in terms of a conditional probability. Remember when we discussed the rules of probability (Section \@ref(sec:02-rules-of-probability))? In particular, we used independence to state rule 7, which is a specific case of the multiplication rule (rule 6). We can define the distribution of the error for case $i$ as a conditional probability $p(\epsilon_i | X_{1,i}, \ldots, X_{m,i})$. Similarly, for case $j$, we can state the distribution of the error as a conditional probability $p(\epsilon_j | X_{1,j}, \ldots, X_{m,j})$. The errors are independent when 
$$p(\epsilon_j | \epsilon_i, X_{1,j}, \ldots, X_{m,j}) = p(\epsilon_j |  X_{1,j}, \ldots, X_{m,j})$$
i.e. the distribution of $\epsilon_j$, conditional upon knowing the value of the predictors and the error $\epsilon_i$ of another case $i$, is the same as the distribution conditional upon just knowing the value of the predictors. 

When might such independence _not_ hold? Let's for the moment go back to the simplest version of the GLM, the model with just an intercept that was the focus of Chapter \@ref(ch:simple-GLM):
\begin{equation}
Y_i = \beta_0 + \epsilon_i
(\#eq:simple-glm-ch09)
\end{equation}
Suppose $Y$ are again judgements of the height of Mount Everest, but that one of the labs in the ManyLabs study that collected the data, was based in Nepal. We might expect participants in Nepal to have a more accurate idea of the height of Mount Everest than participants in Poland (the country the participants were from in the subset of the data we considered in Chapter \@ref(ch:simple-GLM)). As a result, participants in Nepal would likely be less influenced by the low anchor than those with less knowledge of Mount Everest. If $\beta_0$ represents the mean judgement over all participants, then participants in Nepal would generally provide judgements which are higher than $\beta_0$ (i.e positive errors), whilst participants in Poland would generally provide judgements that are lower than $\beta_0$ (i.e. negative errors). If our model does not account for such grouping in the data, then the errors for cases from Nepal would be correlated, because all would tend to be positive. Similarly, the errors for cases from Poland would also be correlated, as all would tend to be negative. What this example points to is that the simple model does not account for the fact that participants from different countries might have a different average judgement. As such, the model is misspecified. We could of course alleviate this problem by including an effect for country, using a contrast-coding predictor $X_1$ to reflect the difference between participants from Nepal ($X_1 = \tfrac{1}{2}$) and Poland ($X_1 = -\tfrac{1}{2}$) in an expanded model
$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i$$
This model would allow for a different mean judgement for cases from Nepal ($\beta_0 + \tfrac{1}{2} \times \beta_1$) and cases from Poland ($\beta_0 - \tfrac{1}{2} \times \beta_1$). Doing so for all labs in the data quickly becomes cumbersome, however. In this study, there were `r library(sdamr); data("anchoring"); length(unique(anchoring$referrer))` labs involved, meaning we would need `r length(unique(anchoring$referrer)) - 1` contrast-coding predictors to reflect possible differences between them. Moreover, differences between the labs are not of primary interest here. The question we posed in Chapter \@ref(ch:simple-GLM) was whether people in general would, on average, give an accurate judgement of the hight of Mount Everest, not whether the country they reside in influences this.  Thus, country, if it has an effect, can be seen as a _nuisance effect_, something we have to consider for a valid model, but we would otherwise rather ignore. __Linear mixed-effects models__ allow you to solve this conundrum gracefully, providing a way for effects to differ between groupings in the data, without dramatically increasing the number of to-be-estimated parameters. They do so by treating such effects as random, rather than fixed, effects. The model then focuses on estimating the distribution of these random effects, rather than estimating each separately.

You can expect violations of the _iid_ assumption if data are collected from units of observations that are clustered in groups. A common example of this in psychology is when you repeatedly observe behaviour from the same person. Other examples of this are data from experiments collected in group settings, students within classrooms, or patients within hospitals. In such situations one would expect that observations within each cluster (i.e., a specific group, classroom, or hospital) are more similar to each other than observations across clusters. Unfortunately, compared to violations of other assumptions, such as the normality assumption or the assumption of variance homogeneity, standard statistical procedures are usually not robust to violations of the independence assumption [@kenny_consequences_1986;@judd_treating_2012]. They often lead to considerably increased Type I errors (i.e., false positives) and more generally can produce overconfident results (e.g., too narrow standard errors).

## Random intercept models

Let $Y_{i,j}$ denote the judgement of the height of Mount Everest for participant $i = 1,\ldots,n_j$ in group (lab) $j = 1, \ldots, n_g$. We will allow each group $j$ to have a different mean judgement. The first step to do so is to rewrite the simple model of Equation \@ref(eq:simple-glm-ch09) as
\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
(\#eq:random-intercept-model-level-1)
\end{equation}
For each observation $Y_{i,j}$, we thus assume the intercept $\beta_{0,j}$ which is different for each group $j$. In other respects, the model is exactly the same as the one of Equation \@ref(eq:simple-glm-ch09), and observations can deviate from the intercept (which represents the mean in group $j$) through the error term $\epsilon_{i,j}$, which is assumed Normal-distributed with a mean of 0 and constant (homogeneous) standard deviation $\sigma_\epsilon$. The second step is to define a model for the group-dependent intercepts:
\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
(\#eq:random-intercept-model-level-2)
\end{equation}
Let's pause for a moment and consider the Equation above in detail. You can think of $\beta_{0,j}$ as the dependent variable here. On the right hand side, there are two terms, and intercept $\beta_0$, and a residual term $\gamma_{0,j}$ which is assumed Normal-distributed with a mean of 0 and a constant (homogeneous) standard deviation $\sigma_{\gamma_0}$. Note that this model is structurally identical to that of Equation \@ref(eq:simple-glm-ch09). If you'd replace $\beta_{0,j}$ by $Y_i$, $\gamma_{0,j}$ by $\epsilon_i$, and $\sigma_{\gamma_0}$ by $\sigma_\epsilon$, you'd reproduce Equation \@ref(eq:simple-glm-ch09) exactly. We thus have a simple model for the parameter $\beta_{0,j}$ as an outcome. In such a __parameters as outcomes__ formulation of linear mixed-effects models, it is customary to call Equation \@ref(eq:random-intercept-model-level-1) a __level 1__ model. It is a model on the level of the observed values $Y_{i,j}$. Equation \@ref(eq:random-intercept-model-level-1) is then a __level 2__ model, a higher-order model which is on the level of the parameters of the model on the level below it (level 2). As we will see later on, further higher-order models can be specified, leading to so-called multilevel models. For present purposes, a level 2 model suffices.

The random-intercepts model defined by equations \@ref(eq:random-intercept-model-level-1) and \@ref(eq:random-intercept-model-level-2) has random elements ($\gamma_{0,j}$ and $\epsilon_{i,j}$), and non-random elements ($\beta_0$). Parameter $\beta_0$ is an (unknown) constant, and in the context of mixed-effects models is called a __fixed effect__. The $\gamma_{0,j}$ terms are called __random effects__. The reflect deviations between the means of the groups from $\beta_0$:
$$\gamma_{j,0} = \beta_{0,j} - \beta_0$$
This is similar to how the residuals $\epsilon_{i,j}$ reflect deviations of observations from the means $\beta_{0,i}$. In the study, differences between the labs (groups) are not of direct interest, and this is generally the case for random effects. As indicated earlier, they are usually nuisance factors, which we include in a model to make the model valid and precise, but we would otherwise like to ignore them. What is of interest in e.g. determining whether people's judgements on average are equal to the true height of Mount Everest is the fixed effect $\beta_0$. In our model with random intercepts $\beta_{0,j}$, this parameter equals the mean of these intercepts, because we can rewrite Equation \@ref(eq:random-intercept-model-level-2) in an equivalent form as
$$\beta_{0,j} \sim \mathbf{Normal}(\beta_0,\sigma_{\gamma_0})$$
A useful view of the model is then as a hierarchical model, as we have already done in our level-1 and level-2 formulation. This is depicted in Figure \@ref(fig:random-intercepts-plot-hierarchy). 

```{r random-intercepts-plot-hierarchy, fig.cap="Graphical depiction of the random-intercepts model. At the top you see the distribution of $\\beta_{0,j}$, which is a Normal distribution with mean $\\beta_0$ and standard deviation $\\sigma_{\\gamma_0}$. Each intercept $\\beta_{0,j}$ in the middle layer is a random sample from this distribution. The $\\beta_{0,j}$ terms are the means of the group-wise distributions from which the data points (lowest layer) are randomly samples. Each data point $Y_{i,j}$ is drawn from the group-wise distributions depicted on the muiddle layer, which are all Normal distributions with mean $\\beta_{0,j}$ and standard deviation $\\sigma_{\\gamma_0}$. The random effects $\\gamma_{0,j}$ (only one shown) are the deviations between the group-wise mean $\\beta_{0,j}$ and the overall mean $\\beta_0$. The errors $\\epsilon_{i,j}$ (only one shown) are the deviations between the observations $Y_{i,j}$ and the group-wise means $\\beta_{0,j}$."}
set.seed(872346)
beta0 <- 0
sigma_gamma <- 1
sigma_epsilon <- .4
beta0j <- c(-1.5,1,2)
nobs <- 5
y <- NULL
for(i in 1:length(beta0j)) {
  y <- c(y, rnorm(nobs,beta0j[i],sigma_epsilon))
}
p <- ggplot() + stat_function(fun=function(x) 2 + dnorm(x,mean=beta0,sd=sigma_gamma)) + geom_point(data=data.frame(x=beta0,y=rep(2,length(beta0))),aes(x=x,y=y))
p <- p +  stat_function(fun=function(x) 1 + .3*dnorm(x,mean=beta0j[1],sd=sigma_epsilon)) +  stat_function(fun=function(x) 1 + .3*dnorm(x,mean=beta0j[2],sd=sigma_epsilon)) +  stat_function(fun=function(x) 1 + .3*dnorm(x,mean=beta0j[3],sd=sigma_epsilon))
p <- p + geom_point(data=data.frame(x=beta0j,y=rep(1,length(beta0j))),aes(x=x,y=y)) + geom_point(data=data.frame(x=y,y=rep(0,length(y))),aes(x=x,y=y)) + xlim(-4,4) + ylab("") + xlab("") + annotate("text",x=0,y=1.9,label=expression(beta[0])) + theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
for(i in 1:length(beta0j)) {
  p <- p + annotate("text",x=beta0j[i],y=0.9,label=paste0("beta[list(0,",i,")]"),parse=TRUE)
}
p <- p + geom_segment(data = data.frame(x = beta0j + .5*(beta0 - beta0j), y = rep(1.9,length(beta0j)), xend = beta0j, yend = rep(1.1,length(beta0j))),aes(x=x,y=y,xend=xend,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)
p <- p + geom_segment(data = data.frame(x = rep(beta0j,each=nobs) - .5*(rep(beta0j,each=nobs) - y), y = rep(0.8,length(beta0j)*nobs), xend = y, yend = rep(0.1,length(beta0j)*nobs)),aes(x=x,y=y,xend=xend,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)
p <- p + geom_segment(data = data.frame(x = beta0, xend = beta0j[1], y = 1.05, yend=1.05), aes(x=x,xend=xend,y=y,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)  + annotate("text",x=beta0j[1] + .5*(beta0 - beta0j[1]),y=1.1,label=paste0("gamma[list(0,",1,")]"),parse=TRUE)
p <- p + geom_segment(data = data.frame(x = beta0j[2], xend = y[6], y = -0.05, yend=-0.05), aes(x=x,xend=xend,y=y,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)  + annotate("text",x=y[6] + .5*(beta0j[2] - y[6]),y=-0.1,label=paste0("epsilon[list(2,",2,")]"),parse=TRUE)
p

```


## Parameter estimation

If we could observe the parameters $\beta_{0,j}$ directly, model estimation would be easy. We could just use the methods of Chapter \@ref(ch:simple-GLM). But parameters are not observed, and need to be inferred. This is where the "magic" of linear mixed-effects models happens. In estimating a mixed-effects model, the particular values of the random effects $\gamma_{0,j}$ are not directly the focus. Rather, the objective is to estimate the variance of the random effects, $\sigma^2_{\gamma_0}$. Once we have estimates of the fixed effects and random-effect and residual variances $\sigma^2_\gamma$ and $\sigma^2_\epsilon$, we can obtain predictions of the random effects $\hat{\gamma}$, which are called __best linear unbiased predictions__ (BLUPs).

As we have mentioned previously, maximum likelihood provides biased estimates of variances. As you may recall, the sample variance $S^2 = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}$ is the maximum likelihood estimate of the variance, whilst the unbiased estimator is $\hat{\sigma^2} = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}$.  In the context of linear mixed-effects models, there is a similar issue that maximum likelihood estimation provides biased estimates of the variances of random effects. Unbiased estimates are obtained with a procedure called __restricted maximum likelihood__ (REML). It is beyond the scope of this book to go into the details of this procedure, but it roughly corresponds by first removing any fixed effects from the data, and then estimating the variance components.

Parameter estimation in mixed-effects models is more complicated than for linear models with only fixed effects. This is especially the case for more complex models with random intercepts and slopes and correlations between these. Care must therefore be taken in defining a model which is complex enough to allow you to capture the dependence between observations, but not so complex to render the estimation of parameters so unreliable, or even impossible, to make the inferences effectively meaningless. We will discuss the specification of the appropriate random effects structure in more detail later on. 

## Parameter inference

After estimating the parameters of the linear mixed-effects model, the same procedures of model comparisons can be applied to obtain null hypothesis significance tests for the fixed effects. In the same way as for the General Linear Model, this can consist of computing $F$ statistics from comparisons of the Sum of Squared Errors of a general MODEL G and a restricted MODEL R which sets some of the fixed effects parameters to a priori values, often $\beta = 0$. 

A complication with linear mixed-effects models concerns the appropriate degrees of freedom. In models with only fixed effects, we could use $\text{df}_1 = \text{npar}(G) - \text{npar}(R)$ and $\text{df}_2 = n - \text{npar}(G)$. Unfortunately, this is not so for mixed-effects models. The problem is the random effects $\gamma$. These are not parameters in the sense that the fixed effects $\beta$ and the variances $\sigma$ are parameters, they nevertheless are factors which influence the model errors $\epsilon$. You might think of them as "partial" parameters, which you can assign a weight between 0 and 1, in terms of how they influence the model errors $\epsilon$. Assigning each random effect $\gamma_{0,j}$ a weight of 1 would add, in our example above, a total of $n_g$ parameters to the model (and hence reduce $\text{df}_2$ by $n_g$). But this generally assigns too much importance to the random effects, and the appropriate correction is somewhere between 0 and $n_g$. Precisely what value the correction to the degrees of freedom should have is unfortunately not known for most models. Different approximations have been proposed. Two main ones are the Satterthwaite [@satterthwaite_synthesis_1941] and the Kenward-Roger [@kenward_small_1997] approximation. As the latter is known to provide the best control of Type I errors with the limited sample sizes that are common in psychology studies, this one is generally recommended.

You may also be interested in testing evidence for the existence of random effects. If there are no random effects (i.e. all $\gamma_{0,j} = 0$), that is equivalent to setting $\sigma_{\gamma_0} = 0$. The $F$ statistic is not suitable for comparing a model where we set the standard deviation of a random effect $\gamma$ to $\sigma_{\gamma} = 0$ to a model where we allow it to take any value $\sigma_{\gamma} \geq 0$ (and hence need to estimate it). For such model comparisons, we can revert back to the general idea of comparing models via the likelihood ratio (e.g. Section \@ref(sec:02-likelihood-ratio)):

$$\text{likelihood-ratio} = \frac{p(Y_1,\ldots,Y_n|\text{MODEL R})}{p(Y_1,\ldots,Y_n|\text{MODEL G})}$$
In particular, if the number of total observations $n$ is sufficiently large, we can rely on an important and general statistical theorem by @wilks1938large that shows that as $n \rightarrow \infty$ (i.e., as the number of samples approaches infinity), then under the null-hypothesis that MODEL R is true:
\begin{equation}
-2 \log \left(\text{likelihood-ratio}\right) \sim \mathbf{chi-squared}(\text{npar}(G) - \text{npar}(R))
\end{equation}
In words, under the null-hypothesis that the restricted model is true, the sampling distribution of minus 2 times the natural logarithm of the likelihood ratio is distributed as a Chi-squared ($\chi^2$) distribution. A Chi-squared distribution has one parameter, the degrees of freedom, which here equals the difference in the number of parameters of the two models. In this test, if there are any unknown parameters in the models, the need to be estimated by maximum likelihood (not by restricted maximum likelihood). The parameters to count are the fixed effects $\beta$ and the variances of the random effects $\sigma^2_\gamma$, as well as any covariances between them (we will discuss this later), and the residual error variance $\sigma^2_\epsilon$. In this test, you don't have to worry about adjusting the degrees of freedom for the influence of the random effects $\gamma$. An important caveat is however that Wilks' theorem assumes that the restriction of the parameters in MODEL R are in the "interior" of the parameter space. That means that if parameters have upper or lower bounds, they can not be set at exactly these bounds in MODEL R. A variance can not be negative, so a variance parameter has a lower bound of $\sigma^2 \geq 0$. Unfortunately, in the model comparison to test for random effects, we need to set the variance at exactly this lower bound in MODEL R. As a result, the chi-square distribution is not an accurate approximation to the true sampling distribution. Usually, the tests are too conservative, providing too large $p$-values (suggestions are that when testing a single variance of a random effect, they are approximately twice as large as they should be, see e.g. @pinheiro2006mixed).^[When parameters are set on the bounds of the parameter space, the sampling distribution of the statistic is a _mixture_ of different Chi-squared distributions, which effectively is a weighted sum of chi-squared distributions. Although such mixture distributions are well-defined in principle, it is not straightforward to determine the component weights accurately in practice.] The general conservatism of the likelihood ratio tests mean that if you obtain a significant result, you can be reassured that you have sufficient evidence for an effect. A conservative test procedure however means that the true rate of Type 1 errors will be lower than set by the significance level $\alpha$. When it is crucial to maintain the rate of Type 1 errors sought for by the significance level, a better approach is to use a __parametric bootstrap__ procedure where you simulate a large number of data sets according to the estimated MODEL R, and for each compute the value of the $-2 \log\left(\text{likelihood-ratio}\right)$ value, which provides an "empirical distribution" of the sampling distribution of this statistic. You can then determine how "unusual" the actual value computed for the true data is in the context of this simulated distribution.   

## Application of the random-intercepts model

Having spent a bit of time discussing the estimation and testing of effects in linear mixed-effects models in abstract terms, let's see how we can use a model like this in practice. For this first example, we will consider the data from the anchoring study again, focussing on those (non US or UK based) labs that were able to obtain estimates of the height of Mount Everest in meters, after having povided the participants with a low anchor. The judgements of participants aqcuired by these different labs, as well as the mean judgements and standard errors of the mean (remember, these standard errors are the standard deviation of the sampling distribution of the mean) are shown in Figure \@ref(fig:anchoring-low-boxplot-by-referrer). 

```{r anchoring-low-boxplot-by-referrer, fig.cap="Judged height of Mount Everest after a low anchor for all international labs where participants provided their answer in meters.. Boxes represent the mean in each group plus or minus the standard error of the mena."}
library(dplyr)
library(sdamr)
data("anchoring")
dat <- subset(anchoring, anchor == "low" & us_or_international == "International" & referrer != "lse")
dat %>%
  ggplot(aes(x=referrer,y=everest_meters,colour = referrer)) + stat_summary(fun.data="mean_se",geom="crossbar") + geom_jitter(alpha=.4) + theme(legend.position = "none") + xlab("Lab") + ylab("Height of Mount Everest in meters") 
```

As this figure shows, the mean judgements seem to vary quite a bit between the different labs. As a result, predicting judgements in each group by the mean over all groups will likely introduce dependence between the errors, with errors in groups with a relatively high mean tending to be positive, and errors in groups with a relatively low mean will tend to be negative. We can eliminate such dependence by introducing random intercepts for each group, because the errors will then be deviations of the observations from the group means:
$$\epsilon_{i,j} = Y_{i,j} - \beta_{0,j}$$
We are still interested in the overall mean judgement, reflected by the fixed intercept $\beta_0$, which can be interpreted as the average of all group-wise intercepts $\beta_{0,j}$.

```{r estimate-random-effects-model, message = FALSE, results='hide'}
library(afex)
dat$true_meters <- 8848
modg <- mixed(everest_meters ~ 1 + (1|referrer), data=dat, test_intercept = TRUE)
modr <- lm(everest_meters ~ 1, data=dat)
modr2 <- dat %>% mutate(dev_everest_meters = everest_meters - 8848) %>% 
mixed(dev_everest_meters ~ 1 + (1|referrer), data=., test_intercept = TRUE)
modr3 <- dat %>% mutate(dev_everest_meters = everest_meters - 8848) %>% 
lm(dev_everest_meters ~ 1, data=.) %>% car::Anova(type=3)

```

The estimated model can be written as
$$\begin{align}
Y_{i,j} &= \beta_0 + \gamma_{0,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model),digits=3, nsmall=2)` + \gamma_{0,j} + \epsilon_{i,j} \\
\gamma_{0,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model)$referrer,"stddev"), digits=3, nsmall=2)`) \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model),"sc"), digits=3, nsmall=2)`)
\end{align}$$

We can compare this to a estimated model without random intercepts:
$$\begin{align}
Y_{i,j} &= \beta_0 + \epsilon_{i,j} \\
&= `r format(coefficients(modr)[1],digits=3,nsmall=2)` + \epsilon_{i,j} \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(sqrt(sum(residuals(modr)^2)/modr$df.residual), digits=3,nsmall=2)`)
\end{align}$$

In the latter model, which is similar to the simple model we focused on in Chapter \@ref(ch:simple-GLM), the estimate of the intercept is the sample mean over all the cases. You can see that this differs from the estimated fixed intercept of the random intercepts model. This fixed effect represents the average of the random intercepts $\beta_{0,j}$. Contrary to what you might have thought, the predicted^[In the context of mixed-effects models, it is common to state that random effects are _predicted_, rather than _estimated_.] random intercepts are not equal to the group-wise sample averages. Table \@ref(tab:anchoring-means-random-intercepts-table) shows the sample averages, and the predicted random intercepts.

```{r anchoring-means-random-intercepts-table}
tab <- data.frame(mean = aggregate(everest_meters ~ referrer, data=dat, mean)$everest_meters, n = as.numeric(table(dat$referrer)), intercept = (fixef(modg$full_model) + ranef(modg$full_model)$referrer)[,1], gamma = ranef(modg$full_model)$referrer)
knitr::kable(tab, col.names = c("$\\overline{Y}_j$","$n_j$","$\\hat{\\beta}_{0,j}$", "$\\hat{\\gamma}_{0,j}$"), caption = "Group-wise averages and sample sizes, and predicted random intercepts ($\\beta$) and random effects ($\\gamma$)", digits=2, escape=FALSE)
```

As you can see, the random intercepts $\hat{\beta}_{0,j}$ are closer to the fixed intercept $\hat{\beta}_0 = `r format(fixef(modg$full_model),digits=3, nsmall=2)`$ than the sample means $\overline{Y}_j$. In statistical terms, this is sometimes called __shrinkage__. Due to the assumption that the random effects are Normal-distributed, the random intercepts are pulled towards the average of that distribution, which is fixed intercept $\beta_0$. This is because in a Normal distribution, large deviations from the mean are unlikely. Hence, if the average in a group is far from the overall average over the groups, that is perhaps due to a random fluke in this particular dataset. Hence, adjusting the mean of the group to be closer to those of the other groups makes sense: although each group can differ, they will be some similarities between the groups. Note that the relative adjustment depends not only on the difference between the sample mean from the overall mean, but also on the sample size of each group ($n_j$). The smaller the sample size, the more the estimate will be adjusted towards the overall mean, because there is less information in smaller samples and hence the true group mean can be expected to be further away from the sample mean. 

Because the groups are not completely dissimilar, when estimating the mean of one group, we can partly rely on the data from other groups. This is also called __partial pooling__. It can be contrasted with __complete pooling__, where all the data is treated as if from the same group (i.e. as in the model of Equation \@ref(eq:simple-glm-ch09)). Complete pooling is inappropriate here, because when there are different groupings in the data, this will violate the _iid_ assumption which the complete pooling model relies on. The opposite of complete pooling is __no pooling__. This means that you would estimate the model for each group separately. If the sample sizes of the groups are sufficiently large to allow reliable parameter estimation, this approach is not unreasonable and would not violate the _iid_ assumption. However, compared to partial pooling, there are several disadvantages, most notably that there is no easy way to combine the results of the separate analyses when you are interested in inferences over the whole set of groups. 

Testing whether the fixed intercept $\beta_0$ equals an a priori value, such as $\beta_0 = 8848$, can be essentially done in the same way is in Chapter \@ref(ch:simple-GLM), by estimating a model in which we fix the intercept to that value, and then conducting a model comparison. For completeness, the results of this comparison, using the Kenward-Rogers approximation for the degrees of freedom is $F(`r modr2$anova_table[1,1]`, `r modr2$anova_table[1,2]`) = `r modr2$anova_table[1,3]`$, $p `r pvalue(modr2$anova_table[1,4], limit=TRUE)`$. As such, we reject the null hypothesis that the average judgement is equal to the true height of Mount Everest. Performing the same analysis but with models which don't include random effects would have given $F(`r modr3[1,2]`, `r modr3[2,2]`) = `r modr3[1,3]`$, $p `r pvalue(modr3[1,4], limit=TRUE)`$. The latter test is not a good reflection of the data, because the _iid_ assumption it makes is violated here. This leads to a biased and _overconfident_ test result (i.e., a too high $F$ value). You can also see the Kenward-Rogers approximation of the degrees of freedom at work in test for the mixed-effects model. The value for $\text{df}_1$ is 1 because the model comparison involves one less parameter in MODEL R. Compared to the (wrong) full pooling model, the value of $\text{df}_2$ is substantially lower (`r modr2$anova_table[1,3]` compared to `r modr3[2,2]`). Therefore, test with the mixed model has effectively less power, but as the complete pooling model is not valid, the mixed-model analysis is preferred to the latter. 

## Models with random intercepts and slopes

The model we dealt with so far is the simplest example of a linear mixed-effects model. Let's now consider slightly more complex model, where we will now assess the effect of the anchor that was given to participants. The data from the international labs providing judgements in meters are shown in Figure \@ref(fig:anchoring-both-boxplot-by-referrer). As for the low anchor condition, average judgements in the high anchor condition seem to different between the labs, albeit to a lesser extent.

```{r anchoring-both-boxplot-by-referrer, fig.cap="Judged height of Mount Everest after a low or high anchor for all international labs where participants provided their answer in meters. Boxes represent the mean in each group plus or minus the standard error of the mena.", fig.width=8, fig.height=4}
library(dplyr)
library(sdamr)
data("anchoring")
dat <- subset(anchoring, us_or_international == "International" & referrer != "lse") %>% mutate(anchor=factor(anchor,levels=c("low","high")))
dat %>% 
  ggplot(aes(x=referrer,y=everest_meters,colour = referrer)) + stat_summary(fun.data="mean_se",geom="crossbar") + geom_jitter(alpha=.4) + theme(legend.position = "none") + xlab("Lab") + ylab("Height of Mount Everest in meters") + facet_wrap(~anchor)
```

Accounting for the effect of anchor is done by including a contrast-coding predictor in the model, e.g. a predictor $X_1$ which has the value $\tfrac{1}{2}$ for cases in the high anchor condition, and a value $-\tfrac{1}{2}$ for cases in the low anchor condition. Just adding this predictor to the random intercepts model of Equations \@ref(eq:random-intercept-model-level-1) and \@ref(eq:random-intercept-model-level-2) would mean that we would change Equation \@ref(eq:random-intercept-model-level-1) to
$$Y_{i,j} = \beta_{0,j} + \beta_{1} \times X_{1,i,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})$$
while we could keep Equation \@ref(eq:random-intercept-model-level-2) the same. In this model, as for ANOVA models, the intercepts $\beta_{0,j}$ reflect a grand mean (average of the averages of the two anchor conditions), while the fixed slope $\beta_1$ reflects the difference between the mean of the high and low anchor condition. As there is only a fixed slope $\beta_1$, this model assumes that the effect of the anchors is exactly the same for each lab. That implies that for each lab, the distance between the mean judgement in the high and low anchor is assumed to be identical. The random intercept would allow the midpoint between the two means to vary over the groups, such that if one group has relatively high judgements in the low anchor condition, it would also have relatively high judgements in the high anchor condition. Looking at Figure \@ref(fig:anchoring-both-boxplot-by-referrer), this may be a too strong assumption. For example, referrer $\texttt{swpson}$ has a relatively high mean in the low anchor condition, but a relatively low mean in the high anchor condition (and both are suspiciously close to the true height of Mount Everest). Because we have multiple cases in each lab and condition combination, we could allow the effect of anchor to potentially differ between the groups. This can be achieved by adding a random effect for condition. If we allow a different slope $\beta_{1,j}$ for each grouping $j$, we can write the model as
\begin{equation}
Y_{i,j} = \beta_{0,j} + \beta_{1,j} \times X_{1,i,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
(\#eq:random-intercept-and-slope-model-level-1)
\end{equation}
The level-2 models for the intercept and slope are:
\begin{align}
\beta_{0,j} &= \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0}) \\
\beta_{1,j} &= \beta_1 + \gamma_{1,j} \quad \quad \quad \gamma_{1,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_1})
(\#eq:random-intercept-and-slope-model-level-2)
\end{align}
As for the random intercepts, the random slope $\beta_{1,j}$ consists of a fixed part $\beta_1$, which is the average slope over all groups, and a random part $\gamma_{1,j}$, which is the deviation of the group-wise slope ($\beta_{1,j}$) from the average ($\beta_1$). 

```{r mixed-effects-random-slopes, cache=TRUE, message=FALSE, results='hide'}
library(sdamr)
library(afex)
contrasts(dat$anchor) <- c(-1/2,1/2)
dat$referrer <- as.factor(dat$referrer)
modg <- mixed(everest_meters ~ anchor + (anchor||referrer), data=dat, check_contrasts=FALSE, expand_re=TRUE, test_intercept = TRUE)
```

The estimated model can be stated as
$$\begin{align}
Y_{i,j} &= \beta_0 + \gamma_{0,j} + (\beta_1 + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model)[1],digits=3, nsmall=2)` + \gamma_{0,j} + (`r format(fixef(modg$full_model)[2],digits=3, nsmall=2)` + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
\gamma_{0,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model)$referrer,"stddev"), digits=3, nsmall=2)`) \\
\gamma_{1,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model)$referrer.1,"stddev"), digits=3, nsmall=2)`) \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model),"sc"), digits=3, nsmall=2)`)
\end{align}$$
In terms of inference, we are mainly interested in the fixed effect of anchor. A test of the hypothesis that the slope of anchor is $\beta_1 = 0$, again using the Kenward-Rogers approximation for the degrees of freedom, gives us $F(`r modg$anova_table[2,1]`, `r modg$anova_table[2,1]`) = `r modg$anova_table[2,3]`$, $p `r pvalue(modg$anova_table[2,4], limit=TRUE)`$. Hence, we reject the null hypothesis that the true effect of the anchor equals 0. There clearly is a difference between the low and high anchor condition, such that the judgements in the former are lower on average than in the latter conditions.

### Correlation between random effects

In the model above, we assumed the random effects $\gamma_{0,j}$ and $\gamma_{1,j}$ are independent and drawn from separate Normal distributions (each with a different standard deviation, $\sigma_{\gamma_0}$ and $\sigma_{\gamma_1}$. There are situations where we would however like to allow the random effects to be correlated. In the example above, it might be the case that the effect of the anchor differs between groups who on average give relatively high judgements, compared to groups who tend to give lower judgements. Such a dependence points to a moderation of the effect of anchor by the average (grand mean) judgement. In models without random effects, such a moderation would not make sense, because there is a single intercept, and a single slope for anchor. However, we now have different intercepts and slopes for the groups, such that we could, in principle, estimate a relation (a dependence) between the intercepts and slopes.

In general, linear random-effects models assume that the random effects are samples from a __multivariate Normal distribution__. A multivariate Normal distribution is a distribution over vectors with values, and is parameterized by a mean vector $\boldsymbol{\mu}$ and a covariance matrix $\boldsymbol{\Sigma}$. For the two random effects in our model, we can specify the joint distribution as follows:
\begin{equation}
\left( \begin{matrix} \gamma_{0,j} \\ \gamma_{1,j} \end{matrix} \right)
\sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} \sigma^2_{\gamma_0}&\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1} \\ \rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}&\sigma^2_{\gamma_1} \end{matrix} \right] \right)
\end{equation}
Here, the $\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}$ component in the off-diagonal elements of the covariance matrix are the covariance between $\gamma_{0,j}$ and $\gamma_{1,j}$. A covariance can be written as a product of the correlation $\rho_{\gamma_0,\gamma_1}$ between $\gamma_{0,j}$ and $\gamma_{1,j}$, and the standard deviations $\sigma_{\gamma_0}$ and $\sigma_{\gamma_1}$.

Unfortunately, the added complexity of estimating the correlation between the random effects is sometimes too much, resulting in estimation failures. That was the case for the data analysed above. By including all the labs in the full study, the model is estimable. In this case, we focus on the estimation in feet, rather than meters. In other respects, the model is the same as suggested above. 

```{r mixed-effects-random-slopes-correlated, cache=TRUE, message=FALSE, results='hide'}
library(sdamr)
library(afex)
anchoring$anchor <- factor(anchoring$anchor, levels=c("low","high"))
contrasts(anchoring$anchor) <- c(-1/2,1/2)
anchoring$referrer <- factor(anchoring$referrer)
modg <- mixed(everest_feet ~ anchor + (1 + anchor|referrer), data=anchoring, check_contrasts = FALSE)
```

The estimated model can be written as
$$\begin{align}
Y_{i,j} &= \beta_0 + \gamma_{0,j} + (\beta_1 + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model)[1],digits=3, nsmall=2)` + \gamma_{0,j} + (`r format(fixef(modg$full_model)[2],digits=3, nsmall=2)` + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
\left( \begin{matrix} \gamma_{0,j} \\ \gamma_{1,j} \end{matrix} \right)
&\sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} \sigma^2_{\gamma_0}&\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1} \\ \rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}&\sigma^2_{\gamma_1} \end{matrix} \right] \right) \\
& \sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix}  (`r attr(VarCorr(modg$full_model)$referrer,"stddev")[1]`)^2 & `r attr(VarCorr(modg$full_model)$referrer,"correlation")[2,1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[2]` \\ `r attr(VarCorr(modg$full_model)$referrer,"correlation")[2,1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[2]` & (`r attr(VarCorr(modg$full_model)$referrer,"stddev")[2]`)^2 \end{matrix} \right] \right) \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model),"sc"), digits=3, nsmall=2)`)
\end{align}$$

## Crossed random effects: dating partners in the speed dating experiment

As a final example of a mixed-effects analysis, we will reconsider the data from the speed-dating experiment of @fisman2006gender. 

```{r, cache= TRUE}
data("speeddate")
dat <- speeddate
dat$iid <- as.factor(dat$iid)
dat$pid <- as.factor(dat$pid)
summary(afex::mixed(other_like ~ other_attr + other_sinc + other_intel + other_fun + other_amb + (1|iid) + (1|pid), data=dat))
```


## Choosing the random effects structure

Perhaps more important than the choice of method for evaluating statistical significance is the correct specification of the random-effects structure. Omitting a random effect when there is in fact variability in this effect across the levels of a random effects grouping factor can dramatically increase Type I errors as shown in a number of independent simulation studies  \parencite{judd_treating_2012,barr_random_2013-1,schielzeth_conclusions_2009}. This means that in most cases one should initially start with the __maximal random effects structure__ justified by the design as recommended by @barr_random_2013-1. The maximal model is the model that includes random effects parameters for all sources of stochastic variability (i.e., random effects grouping factors). Specifically, it contains random intercepts as well as random slopes for all fixed effects that vary within the levels of a given random effects grouping factor, plus the correlations among the random effects. 

For the limited sample sizes that are common in psychology and related disciplines a common problem is that the maximal model is not fully identified [@bates_parsimonious_2015], especially for mixed models with complicated random effects structures. Even though the optimization algorithm converges to the optimum (i.e., the maximum-likelihood estimate for a given data set) the variance-covariance matrix of the random effects parameters at the optimum is degenerate or singular. At least for models estimated with \texttt{lme4} this is often signified by convergence warnings. Other signs of singular fits are variance estimates of or near zero and correlation estimates of $\pm 1$. The occurrence of such situations is due to the fact the parameters associated to random effects (e.g., $\sigma^2_{S_\delta}$) are more difficult to estimate than fixed effects (e.g., $\beta_{\delta}$). 

In the case of a singular fit, it is in principle recommended to reduce the random effects structure given that degenerate or overparameterized models can reduce the statistical power of any tests conducted with them \parencite{matuschek_balancing_2017}. As a first step, it seems advisable to remove the correlations among random slopes as these contribute the largest number of random effects parameters if the number of variance parameters for a given random effects grouping factor exceeds three. Additionally, the correlation parameters appear to be even more difficult to estimate than the variance parameters, at least in a frequentist framework \parencite{bates_parsimonious_2015}. If a model still shows problems after removing correlations, other random-effects parameters could be removed, starting with the highest-order random effects parameter with the lowest estimated variance. Empirical-based approaches to address this question and obtain the \emph{optimal} random effects are provided by \textcite{matuschek_balancing_2017} and \textcite{bates_parsimonious_2015}. However, following these recommendation usually requires the researcher to choose a specific model selection procedure and criterion. The consequence of this is that researchers that decide to report results based on a reduced model should make this explicit to the reader and be prepared to defend their choices. In any case, we recommend that one should always start with the maximal model and reduce random effects instead of starting with a minimal model and gradually include random effects. 

One problem that may arise from an iterative procedure for determining the random effects structure is that sometimes it might not be possible to reduce the random-effects structure such that all problematic random effects parameters can be removed (e.g., in cases when there is random variability in higher-order effects, but not in lower-order effects). From the pragmatic standpoint that false positives are in principle more dangerous for scientific progress than false negatives, we suggest that in those cases one can accept a few problematic or degenerate parameters (e.g., variances of zero). This strategy strikes us as more reasonable (i.e., more conservative) than simply removing justifiable random effects and inflating Type I error rates to an unknown degree. It is clear that a model with such problematic or degenerate parameters is not the most adequate from a purely statistical standpoint, but it can nevertheless be a reasonable solution if the focus is ultimately on the fixed effects. In any case, one should compare the fixed-effects estimates and the hypothesis tests regarding the fixed effects across all the estimated models. It is often the case that the testing of fixed effects in highly overparameterized models with degenerate estimates diverge from analogous tests applied to reduced models. In those cases, one should report the results for the reduced model.