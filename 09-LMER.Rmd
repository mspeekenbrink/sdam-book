# Linear mixed-effects models

In this chapter, we will discuss how to deal with the non-independence of errors that can result from repeated-measures of the same individual, or otherwise multiple observations which come from different groupings in the data. Traditionally, such data has been generally analysed with repeated-measures ANOVA models. Linear mixed-effects models offer an alternative to repeated-measures ANOVA with certain benefits, particularly in dealing with missing values. Once you get your head around the idea of random effects, linear mixed-effects models are a natural extension of the General Linear Model, which can then make them easier to understand than repeated-measures ANOVA models, which either rely on complicated model comparison schemes and strong assumptions, or rely on a multivariate definition transforming repeated-measures into a set of orthogonal variables. Linear mixed-effects models offer a more flexible univariate modelling technique. This flexibility does mean, as we will see, that sometimes difficult choices need to be made regarding the random-effects structure of the model. As always, there are no set rules in defining statistical models, and this should be led by substantive concerns, in addition to issues of reliability and maintaining appropriate error rates. With practice and experience, you should become more confident in making such decisions. 

In writing this chapter, I have adapted some sections of @Singmann2019-mixed. This is a very clear introduction to linear mixed-effects models, that you may wish to consult in addition to this Chapter (you can download it [here](http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf)).

## Non-independence in linear models

The General Linear Model we have considered thus far can be stated as follows:

$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
The assumptions of this model concern the residuals or errors $\epsilon_i$. They are assumed to be _independent and identically distributed_ (iid), following a Normal distribution with a mean of 0 and a standard deviation $\sigma_\epsilon$.

The assumption that the errors are independent means that knowing the value of the error for one case $i$ in the data does not give you any information to determine the value of the error for another case $j$. More formally, statistical independence is defined in terms of conditional probabilities. Remember when we discussed the rules of probability (Section \@ref(sec:02-rules-of-probability))? In particular, we used independence to state rule 7, which is a specific case of the multiplication rule (rule 6). We can define the distribution of the error for case $i$ as a conditional probability $p(\epsilon_i | \epsilon_i, X_{1,i}, \ldots, X_{m,i})$, where we conditionalise on all the predictor values, as well as another error term $\epsilon_j$. The errors are independent when 
$$p(\epsilon_i | \epsilon_j, X_{1,i}, \ldots, X_{m,i}) = p(\epsilon_i |  X_{1,i}, \ldots, X_{m,i})$$
i.e. the distribution of $\epsilon_i$, conditional upon knowing the value of the predictors and the error $\epsilon_j$ of another case $j$, is the same as the distribution conditional upon just knowing the value of the predictors. This means that error $\epsilon_j$ provides no information about the error $\epsilon_i$.

When might such independence _not_ hold? Let's for the moment go back to the simplest version of the GLM, the model with just an intercept that was the focus of Chapter \@ref(ch:simple-GLM):
\begin{equation}
Y_i = \beta_0 + \epsilon_i
(\#eq:simple-glm-ch09)
\end{equation}
Suppose $Y$ are again judgements of the height of Mount Everest, but that one of the labs in the ManyLabs study that collected the data, was based in Nepal. We might expect participants in Nepal to have a more accurate idea of the height of Mount Everest than participants in Poland (the country the participants were from in the subset of the data we considered in Chapter \@ref(ch:simple-GLM)). As a result, participants in Nepal would likely be less influenced by the low anchor than those with less knowledge of Mount Everest. If $\beta_0$ represents the mean judgement over all participants, then participants in Nepal would generally provide judgements which are higher than $\beta_0$ (i.e positive errors), whilst participants in Poland would generally provide judgements that are lower than $\beta_0$ (i.e. negative errors). If our model does not account for such grouping in the data, then the errors for cases from Nepal would be correlated, because all would tend to be positive. Similarly, the errors for cases from Poland would also be correlated, as all would tend to be negative. What this example points to is that the simple model does not account for the fact that participants from different countries might have a different average judgement. As such, the model is misspecified. We could of course alleviate this problem by including an effect for country, using a contrast-coding predictor $X_1$ to reflect the difference between participants from Nepal ($X_1 = \tfrac{1}{2}$) and Poland ($X_1 = -\tfrac{1}{2}$) in an expanded model
$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i$$
This model would allow for a different mean judgement for cases from Nepal ($\beta_0 + \tfrac{1}{2} \times \beta_1$) and cases from Poland ($\beta_0 - \tfrac{1}{2} \times \beta_1$). Doing so for all labs in the data quickly becomes cumbersome, however. In this study, there were `r library(sdamr); data("anchoring"); length(unique(anchoring$referrer))` labs involved, meaning we would need `r length(unique(anchoring$referrer)) - 1` contrast-coding predictors to reflect possible differences between them. Moreover, differences between the labs are not of primary interest here. The question we posed in Chapter \@ref(ch:simple-GLM) was whether people in general would, on average, give an accurate judgement of the hight of Mount Everest, not whether the country they reside in influences this.  Thus, country, if it has an effect, can be seen as a _nuisance effect_, something we have to consider for a valid model, but we would otherwise rather ignore. __Linear mixed-effects models__ allow you to solve this conundrum gracefully, providing a way for effects to differ between groupings in the data, without dramatically increasing the number of to-be-estimated parameters. They do so by treating such effects as random, rather than fixed, effects. The model then focuses on estimating the distribution of these random effects, rather than estimating each separately.

You can expect violations of the _iid_ assumption if data are collected from units of observations that are clustered in groups. A common example of this in psychology is when you repeatedly observe behaviour from the same person. Other examples of this are data from experiments collected in group settings, students within classrooms, or patients within hospitals. In such situations one would expect that observations within each cluster (i.e., a specific group, classroom, or hospital) are more similar to each other than observations across clusters. Unfortunately, compared to violations of other assumptions, such as the normality assumption or the assumption of variance homogeneity, standard statistical procedures are usually not robust to violations of the independence assumption [@kenny_consequences_1986;@judd_treating_2012]. They often lead to considerably increased Type I errors (i.e., false positives) and more generally can produce overconfident results (e.g., too narrow standard errors).

## Random intercept models

We will now define a model which allows for each grouping (e.g., lab) in the data to have a different mean. To help define the model, let $Y_{i,j}$ denote the judgement of the height of Mount Everest for participant $i = 1,\ldots,n_j$ in grouping (lab) $j = 1, \ldots, n_g$. The double subscript is solely to distinguish between participants and groupings. We still consider a single dependent variable $Y$. We will allow each group $j$ to have a different mean judgement. The first step to do so is to rewrite the simple model of Equation \@ref(eq:simple-glm-ch09) as
\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
(\#eq:random-intercept-model-level-1)
\end{equation}
For each observation $Y_{i,j}$, we thus assume the intercept $\beta_{0,j}$ which is different for each group $j$. In other respects, the model is exactly the same as the one of Equation \@ref(eq:simple-glm-ch09), and observations can deviate from the intercept (which represents the mean in group $j$) through the error term $\epsilon_{i,j}$, which is assumed Normal-distributed with a mean of 0 and constant (homogeneous) standard deviation $\sigma_\epsilon$. While the model effectively states that the structural part $\beta_{0,j}$ is different for each grouping level $j$, the groupings are linked because they share the same distribution for the error term. The second step is to define a model for the grouping-dependent intercepts:
\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
(\#eq:random-intercept-model-level-2)
\end{equation}
Let's pause for a moment and consider the Equation above in detail. You can think of $\beta_{0,j}$ as the dependent variable here. On the right hand side, there are two terms, an intercept $\beta_0$, and a residual term $\gamma_{0,j}$ which is assumed Normal-distributed with a mean of 0 and a constant (homogeneous) standard deviation $\sigma_{\gamma_0}$. Note that this model is structurally identical to that of Equation \@ref(eq:simple-glm-ch09). If you'd replace $\beta_{0,j}$ by $Y_i$, $\gamma_{0,j}$ by $\epsilon_i$, and $\sigma_{\gamma_0}$ by $\sigma_\epsilon$, you'd reproduce Equation \@ref(eq:simple-glm-ch09) exactly. We thus have a simple model for the parameter $\beta_{0,j}$ as an outcome. In such a __parameters as outcomes__ formulation of linear mixed-effects models, it is customary to call Equation \@ref(eq:random-intercept-model-level-1) a __level 1__ model. It is a model on the level of the observed values $Y_{i,j}$. Equation \@ref(eq:random-intercept-model-level-2) is then a __level 2__ model, a higher-order model which is on the level of the parameters of the model on the level below it (level 2). As we will see later on, further higher-order models can be specified, leading to so-called multilevel models. For present purposes, a level 2 model suffices.

The random-intercepts model defined by equations \@ref(eq:random-intercept-model-level-1) and \@ref(eq:random-intercept-model-level-2) has random elements ($\gamma_{0,j}$ and $\epsilon_{i,j}$), and non-random elements ($\beta_0$). Parameter $\beta_0$ is an (unknown) constant, and in the context of mixed-effects models is called a __fixed effect__. The $\gamma_{0,j}$ terms are called __random effects__. The reflect deviations between the means in the groupings from the overall mean $\beta_0$:
$$\gamma_{j,0} = \beta_{0,j} - \beta_0$$
This is similar to how the residuals $\epsilon_{i,j}$ reflect deviations of observations from the means $\beta_{0,i}$. In the study, differences between the labs (grouping levels) are not of direct interest, and this is generally the case for random effects. As indicated earlier, they are usually nuisance factors, which we include in a model to make the model valid and precise, but we would otherwise like to ignore them. What is of interest in e.g. determining whether people's judgements on average are equal to the true height of Mount Everest is the fixed effect $\beta_0$. In our model with random intercepts $\beta_{0,j}$, this parameter equals the mean of these intercepts, because we can rewrite Equation \@ref(eq:random-intercept-model-level-2) in an equivalent form as
$$\beta_{0,j} \sim \mathbf{Normal}(\beta_0,\sigma_{\gamma_0})$$
A useful view of the model is then as a hierarchical model, as we have already done in our level-1 and level-2 formulation. This is depicted in Figure \@ref(fig:random-intercepts-plot-hierarchy). 

```{r random-intercepts-plot-hierarchy, fig.cap="Graphical depiction of the random-intercepts model. At the top you see the distribution of $\\beta_{0,j}$, which is a Normal distribution with mean $\\beta_0$ and standard deviation $\\sigma_{\\gamma_0}$. Each intercept $\\beta_{0,j}$ in the middle layer is a random sample from this distribution. The $\\beta_{0,j}$ terms are the means of the group-wise distributions from which the data points (lowest layer) are randomly sampled. Each data point $Y_{i,j}$ is drawn from the group-wise distributions depicted on the middle layer, which are all Normal distributions with mean $\\beta_{0,j}$ and standard deviation $\\sigma_{\\gamma_0}$. The random effects $\\gamma_{0,j}$ (only one shown) are the deviations between the group-wise mean $\\beta_{0,j}$ and the overall mean $\\beta_0$. The errors $\\epsilon_{i,j}$ (only one shown) are the deviations between the observations $Y_{i,j}$ and the group-wise means $\\beta_{0,j}$."}
set.seed(872346)
beta0 <- 0
sigma_gamma <- 1
sigma_epsilon <- .4
beta0j <- c(-1.5,1,2)
nobs <- 5
y <- NULL
for(i in 1:length(beta0j)) {
  y <- c(y, rnorm(nobs,beta0j[i],sigma_epsilon))
}
p <- ggplot() + stat_function(fun=function(x) 2 + dnorm(x,mean=beta0,sd=sigma_gamma)) + geom_point(data=data.frame(x=beta0,y=rep(2,length(beta0))),aes(x=x,y=y))
p <- p +  stat_function(fun=function(x) 1 + .3*dnorm(x,mean=beta0j[1],sd=sigma_epsilon)) +  stat_function(fun=function(x) 1 + .3*dnorm(x,mean=beta0j[2],sd=sigma_epsilon)) +  stat_function(fun=function(x) 1 + .3*dnorm(x,mean=beta0j[3],sd=sigma_epsilon))
p <- p + geom_point(data=data.frame(x=beta0j,y=rep(1,length(beta0j))),aes(x=x,y=y)) + geom_point(data=data.frame(x=y,y=rep(0,length(y))),aes(x=x,y=y)) + xlim(-4,4) + ylab("") + xlab("") + annotate("text",x=0,y=1.9,label=expression(beta[0])) + theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
for(i in 1:length(beta0j)) {
  p <- p + annotate("text",x=beta0j[i],y=0.9,label=paste0("beta[list(0,",i,")]"),parse=TRUE)
}
p <- p + geom_segment(data = data.frame(x = beta0j + .5*(beta0 - beta0j), y = rep(1.9,length(beta0j)), xend = beta0j, yend = rep(1.1,length(beta0j))),aes(x=x,y=y,xend=xend,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)
p <- p + geom_segment(data = data.frame(x = rep(beta0j,each=nobs) - .5*(rep(beta0j,each=nobs) - y), y = rep(0.8,length(beta0j)*nobs), xend = y, yend = rep(0.1,length(beta0j)*nobs)),aes(x=x,y=y,xend=xend,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)
p <- p + geom_segment(data = data.frame(x = beta0, xend = beta0j[1], y = 1.05, yend=1.05), aes(x=x,xend=xend,y=y,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)  + annotate("text",x=beta0j[1] + .5*(beta0 - beta0j[1]),y=1.1,label=paste0("gamma[list(0,",1,")]"),parse=TRUE)
p <- p + geom_segment(data = data.frame(x = beta0j[2], xend = y[6], y = -0.05, yend=-0.05), aes(x=x,xend=xend,y=y,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)  + annotate("text",x=y[6] + .5*(beta0j[2] - y[6]),y=-0.1,label=paste0("epsilon[list(2,",2,")]"),parse=TRUE)
p

```


## Parameter estimation

If we could observe the parameters $\beta_{0,j}$ directly, model estimation would be easy. We could just use the methods of Chapter \@ref(ch:simple-GLM). But parameters are not observed, and need to be inferred. This is where the "magic" of linear mixed-effects models happens. In estimating a mixed-effects model, the particular values of the random effects $\gamma_{0,j}$ are not directly the focus. Rather, the objective is to estimate the variance of the random effects, $\sigma^2_{\gamma_0}$. Once we have estimates of the fixed effects, as well as the random-effect variance $\sigma^2_\gamma$ and residual error variance $\sigma^2_\epsilon$, we can obtain predictions of the random effects $\hat{\gamma}$, which are called __best linear unbiased predictions__ (BLUPs). These can be considered a side-product of the estimation, rather than an integral part.

As we have discussed in earlier chapters, maximum likelihood provides biased estimates of variances. As you may recall, the sample variance $S^2 = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}$ is the maximum likelihood estimate of the variance, whilst the unbiased estimator is $\hat{\sigma^2} = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}$.  In the context of linear mixed-effects models, there is a similar issue that maximum likelihood estimation provides biased estimates of the variances of random effects. Unbiased estimates are obtained with a procedure called __restricted maximum likelihood__ (REML). It is beyond the scope of this book to go into the details of this procedure, but it roughly corresponds by first removing any fixed effects from the data, and then estimating the variance components. For most purposes, relying on restricted maximum likelihood estimation is generally recommended. A main exception is when we need to obtain the maximised likelihood to perform a likelihood ratio test (to e.g. test the significance of random effect variances). For these limited cases, you can re-estimate the model with maximum likelihood (ML), solely to obtain the maximised (log) likelihood.

Parameter estimation in mixed-effects models is more complicated than for linear models with only fixed effects. This is especially the case for more complex models with random intercepts and slopes and correlations between these. Care must therefore be taken in defining a model which is complex enough to allow you to capture the dependence between observations, but not so complex to render the estimation of parameters so unreliable, or even impossible, to make the inferences effectively meaningless. We will discuss the specification of the appropriate random effects structure in more detail later on. 

## Parameter inference

After estimating the parameters of the linear mixed-effects model, the same procedures of model comparisons can be applied to obtain null hypothesis significance tests for the fixed effects. In the same way as for the General Linear Model, this can consist of computing $F$ statistics from comparisons of the Sum of Squared Errors of a general MODEL G and a restricted MODEL R which sets some of the fixed effects parameters to a priori values, often $\beta = 0$. 

A complication with linear mixed-effects models concerns the appropriate degrees of freedom. In models with only fixed effects, we could use $\text{df}_1 = \text{npar}(G) - \text{npar}(R)$ and $\text{df}_2 = n - \text{npar}(G)$. Unfortunately, this is not so for mixed-effects models. The problem is the inclusion of the random effects $\gamma$. These are not parameters in the sense that the fixed effects $\beta$ and the variances $\sigma$ are parameters, they nevertheless are factors which influence the model errors $\epsilon$. You might think of them as "partial" parameters, which you can assign a weight between 0 and 1, in terms of how they influence the model errors $\epsilon$. Assigning each random effect $\gamma_{0,j}$ a weight of 1 would add, in our example above, a total of $n_g$ parameters to the model (and hence reduce $\text{df}_2$ by $n_g$). But this generally assigns too much importance to the random effects, and the appropriate correction is somewhere between 0 and $n_g$. Precisely what value the correction to the degrees of freedom should have is unfortunately not known for most models. Different approximations have been proposed. Two main ones are the Satterthwaite [@satterthwaite_synthesis_1941] and the Kenward-Roger [@kenward_small_1997] approximation. As the latter is known to provide the best control of Type I errors with the limited sample sizes that are common in psychology studies, this one is generally recommended.

You may also be interested in testing evidence for the existence of random effects. If there are no random effects (i.e. all $\gamma_{0,j} = 0$), that is equivalent to setting $\sigma_{\gamma_0} = 0$. The $F$ statistic is not suitable for comparing a model where we set the standard deviation of a random effect $\gamma$ to $\sigma_{\gamma} = 0$ to a model where we allow it to take any value $\sigma_{\gamma} \geq 0$ (and hence need to estimate it). For such model comparisons, we can revert back to the general idea of comparing models via the likelihood ratio (e.g. Section \@ref(sec:02-likelihood-ratio)):

$$\text{likelihood-ratio} = \frac{p(Y_1,\ldots,Y_n|\text{MODEL R})}{p(Y_1,\ldots,Y_n|\text{MODEL G})}$$
In particular, if the number of total observations $n$ is sufficiently large, we can rely on an important and general statistical theorem by @wilks1938large that shows that as $n \rightarrow \infty$ (i.e., as the number of samples approaches infinity), then under the null-hypothesis that MODEL R is true:
\begin{equation}
-2 \log \left(\text{likelihood-ratio}\right) \sim \mathbf{chi-squared}(\text{npar}(G) - \text{npar}(R))
\end{equation}
where the log likelihood ratio is usually computed as
$$\begin{aligned}
-2 \log \left(\text{likelihood-ratio}\right) &= -2 \log p(Y_1,\ldots,Y_n|\text{MODEL R}) - (-2 \log p(Y_1,\ldots,Y_n|\text{MODEL G}))
\end{aligned}$$
In words, under the null-hypothesis that the restricted model is true, the sampling distribution of minus twice the natural logarithm of the likelihood ratio is distributed as a Chi-squared ($\chi^2$) distribution. A Chi-squared distribution has one parameter, the degrees of freedom, which here equals the difference in the number of parameters of the two models. In this test, if there are any unknown parameters in the models, they need to be estimated by maximum likelihood (not by restricted maximum likelihood). This is because the ratio needs to be the ratio between the maximum value of each likelihood. The parameters to count are the fixed effects $\beta$ and the variances of the random effects $\sigma^2_\gamma$, as well as any covariances between them (we will discuss this later), and the residual error variance $\sigma^2_\epsilon$. In this test, you don't have to worry about adjusting the degrees of freedom for the influence of the random effects $\gamma$. An important caveat is however that Wilks' theorem assumes that the restriction of the parameters in MODEL R are in the "interior" of the parameter space. That means that if parameters have upper or lower bounds, they can not be set at exactly these bounds in MODEL R. A variance can not be negative, so a variance parameter has a lower bound of $\sigma^2 \geq 0$. Unfortunately, in the model comparison to test for random effects, we need to set the variance at exactly this lower bound in MODEL R. As a result, the chi-squared distribution is not an accurate approximation to the true sampling distribution. Usually, the tests are too conservative, providing too large $p$-values (suggestions are that when testing a single variance of a random effect, they are approximately twice as large as they should be, see e.g. @pinheiro2006mixed).^[When parameters are set on the bounds of the parameter space, the sampling distribution of the statistic is a _mixture_ of different Chi-squared distributions, which effectively is a weighted sum of chi-squared distributions. Although such mixture distributions are well-defined in principle, it is not straightforward to determine the component weights accurately in practice.] The general conservatism of the likelihood ratio tests means that if you obtain a significant result, you can be reassured that you have sufficient evidence for an effect. A conservative test procedure however means that the true rate of Type 1 errors will be lower than set by the significance level $\alpha$. When it is crucial to maintain the rate of Type 1 errors sought for by the significance level, a better approach is to use a __parametric bootstrap__ procedure where you simulate a large number of data sets according to the estimated MODEL R, and for each compute the value of the $-2 \log\left(\text{likelihood-ratio}\right)$ value, which provides an "empirical distribution" of the sampling distribution of this statistic. You can then determine how "unusual" the actual value computed for the true data is in the context of this simulated distribution.   

## Application of the random-intercepts model

Having spent a bit of time discussing the estimation and testing of effects in linear mixed-effects models in abstract terms, let's see how we can use a model like this in practice. For this first example, we will consider the data from the anchoring study again, focussing on those (non US or UK based) labs that were able to obtain estimates of the height of Mount Everest in meters, after having povided the participants with a low anchor. The judgements of participants acquired by these different labs, as well as the mean judgements and standard errors of the mean (remember, these standard errors are the standard deviation of the sampling distribution of the mean) are shown in Figure \@ref(fig:anchoring-low-boxplot-by-referrer). 

```{r anchoring-low-boxplot-by-referrer, fig.cap="Judged height of Mount Everest after a low anchor for all international labs where participants provided their answer in meters. Boxes represent the mean in each group plus or minus the standard error of the mean."}
library(dplyr)
library(sdamr)
data("anchoring")
dat <- subset(anchoring, anchor == "low" & us_or_international == "International" & referrer != "lse") %>%
  mutate(referrer = factor(referrer))
contrasts(dat$referrer) <- contr.sum(length(levels(dat$referrer)))
dat %>%
  ggplot(aes(x=referrer,y=everest_meters,colour = referrer)) + stat_summary(fun.data="mean_se",geom="crossbar") + geom_jitter(alpha=.4) + theme(legend.position = "none") + xlab("Lab") + ylab("Height of Mount Everest in meters") 
```

As this figure shows, the mean judgements seem to vary quite a bit between the different labs. As a result, predicting judgements in each group by the mean over all groups will likely introduce dependence between the errors, with errors in groups with a relatively high mean tending to be positive, and errors in groups with a relatively low mean will tend to be negative. We can eliminate such dependence by introducing random intercepts for each group, because the errors will then be deviations of the observations from the group means:
$$\epsilon_{i,j} = Y_{i,j} - \beta_{0,j}$$
We are still interested in the overall mean judgement, reflected by the fixed intercept $\beta_0$, which can be interpreted as the average of all group-wise intercepts $\beta_{0,j}$.

```{r estimate-random-effects-model, message = FALSE, results='hide'}
library(afex)
dat$true_meters <- 8848
modg <- mixed(everest_meters ~ 1 + (1|referrer), data=dat, test_intercept = TRUE)
modr <- lm(everest_meters ~ 1, data=dat)
modr2 <- dat %>% mutate(dev_everest_meters = everest_meters - 8848) %>% 
mixed(dev_everest_meters ~ 1 + (1|referrer), data=., test_intercept = TRUE)
modr3 <- dat %>% mutate(dev_everest_meters = everest_meters - 8848) %>% 
lm(dev_everest_meters ~ 1, data=.) %>% car::Anova(type=3)

```

The estimated model (by REML) can be written as
$$\begin{aligned}
Y_{i,j} &= \beta_0 + \gamma_{0,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model),digits=3, nsmall=2)` + \gamma_{0,j} + \epsilon_{i,j} \\
\gamma_{0,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model)$referrer,"stddev"), digits=3, nsmall=2)`) \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model),"sc"), digits=3, nsmall=2)`)
\end{aligned}$$

We can compare this to an estimated model without random intercepts:
$$\begin{aligned}
Y_{i,j} &= \beta_0 + \epsilon_{i,j} \\
&= `r format(coefficients(modr)[1],digits=3,nsmall=2)` + \epsilon_{i,j} \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(sqrt(sum(residuals(modr)^2)/modr$df.residual), digits=3,nsmall=2)`)
\end{aligned}$$

In the latter model, which is similar to the simple model we focused on in Chapter \@ref(ch:simple-GLM), the estimate of the intercept is the sample mean over all the cases. You can see that this differs from the estimated fixed intercept of the random intercepts model. This fixed effect represents the average of the random intercepts $\beta_{0,j}$. Contrary to what you might have thought, the predicted^[In the context of mixed-effects models, it is common to state that random effects are _predicted_, rather than _estimated_.] random intercepts are not equal to the group-wise sample averages. Table \@ref(tab:anchoring-means-random-intercepts-table) shows the sample averages, and the predicted random intercepts.

```{r anchoring-means-random-intercepts-table}
tab <- data.frame(mean = aggregate(everest_meters ~ referrer, data=dat, mean)$everest_meters, n = as.numeric(table(dat$referrer)), fixed_intercept = fixef(modg$full_model), intercept = (fixef(modg$full_model) + ranef(modg$full_model)$referrer)[,1], gamma = ranef(modg$full_model)$referrer, sample_minus_fixed_intercept = aggregate(everest_meters ~ referrer, data=dat, mean)$everest_meters - fixef(modg$full_model))
knitr::kable(tab, col.names = c("$\\overline{Y}_j$","$n_j$","$\\hat{\\beta}_{0}$", "$\\hat{\\beta}_{0,j}$", "$\\hat{\\gamma}_{0,j}$", "$(\\overline{Y}_j - \\hat{\\beta}_{0})$"), caption = "Group-wise averages and sample sizes, estimated fixed intercept ($\\hat{\\beta}_0$) and predicted random intercepts ($\\hat{\\beta}_{0,j}$), random effects ($\\hat{\\gamma}_{0,j}$). The latter are the deviations between the random intercepts and teh fixed intercept. For comparison, we also show the deviations between the sample means and the fixed intercept $(\\overline{Y}_j - \\hat{\\beta}_0)$.", digits=2, escape=FALSE, booktabs=TRUE, linesep="")
```

As you can see, the random intercepts $\hat{\beta}_{0,j}$ are closer to the fixed intercept $\hat{\beta}_0 = `r format(fixef(modg$full_model),digits=3, nsmall=2)`$ than the sample means $\overline{Y}_j$. In statistical terms, this is sometimes called __shrinkage__. Due to the assumption that the random effects are Normal-distributed, the random intercepts are pulled towards the average of that distribution, which is the fixed intercept $\beta_0$. This is because in a Normal distribution, large deviations from the mean are unlikely. Hence, if the average in a grouping is far from the overall average (the average over all the groupings), that is perhaps due to a random fluke in this particular dataset. Adjusting the estimated mean of the grouping to be closer to those of the other groupings then makes sense: although each grouping can differ, there will be some similarities between the groupings. Note that the relative adjustment depends not only on the difference between the sample mean from the overall mean, but also on the sample size ($n_j$) of each grouping level. The smaller the sample size, the more the estimate will be adjusted towards the overall mean, because there is less information in smaller samples and hence the true mean can be expected to be further away from the sample mean.

Because the groupings are not completely dissimilar, when estimating the mean for one grouping, we can partly rely on the data from other groupings. This is also called __partial pooling__. It can be contrasted with __complete pooling__, where all the data is treated as if from one single group (i.e. as in the model of Equation \@ref(eq:simple-glm-ch09)). Complete pooling is inappropriate here, because when there are different groupings in the data, this will violate the _iid_ assumption which the complete pooling model relies on. The opposite of complete pooling is __no pooling__. This means that you would estimate the model for each grouping separately. If the sample sizes of the groupings are sufficiently large to allow reliable parameter estimation, this approach is not unreasonable and would not violate the _iid_ assumption. However, compared to partial pooling, there are several disadvantages, most notably that there is no easy way to combine the results of the separate analyses when you are interested in inferences over the whole set of groupings. 

Testing whether the fixed intercept $\beta_0$ equals an a priori value, such as $\beta_0 = 8848$, can be essentially done in the same way is in Chapter \@ref(ch:simple-GLM), by estimating a model in which we fix the intercept to that value, and then conducting a model comparison. For completeness, the results of this comparison, using the Kenward-Roger approximation for the degrees of freedom is $F(`r modr2$anova_table[1,1]`, `r modr2$anova_table[1,2]`) = `r modr2$anova_table[1,3]`$, $p `r pvalue(modr2$anova_table[1,4], limit=TRUE)`$. As such, we reject the null hypothesis that the average judgement is equal to the true height of Mount Everest. Performing the same analysis but with models which don't include random effects would have given $F(`r modr3[1,2]`, `r modr3[2,2]`) = `r modr3[1,3]`$, $p `r pvalue(modr3[1,4], limit=TRUE)`$. The latter test is not a good reflection of the data, because the _iid_ assumption on which it rests is violated here. This leads to a biased and _overconfident_ test result (i.e., a too high $F$ value). You can also see the Kenward-Roger approximation of the degrees of freedom at work in test for the mixed-effects model. The value for $\text{df}_1$ is 1 because the model comparison involves one less parameter in MODEL R. Compared to the (wrong) full pooling model, the value of $\text{df}_2$ is substantially lower (`r modr2$anova_table[1,2]` compared to `r modr3[2,2]`). Therefore, the test with the mixed model has effectively less power, but as the complete pooling model is not valid, the mixed-model analysis is preferred to the latter. 

## Models with random intercepts and slopes

The model we dealt with so far is the simplest example of a linear mixed-effects model. Let's now consider a slightly more complex model, where we will now assess the effect of the anchor that was given to participants. The data from the international labs providing judgements in meters are shown in Figure \@ref(fig:anchoring-both-boxplot-by-referrer). As for the low anchor condition, average judgements in the high anchor condition seem to different between the labs, albeit to a lesser extent.

```{r anchoring-both-boxplot-by-referrer, fig.cap="Judged height of Mount Everest after a low or high anchor for all international labs where participants provided their answer in meters. Boxes represent the mean in each group plus or minus the standard error of the mean.", fig.width=8, fig.height=4}
library(dplyr)
library(sdamr)
data("anchoring")
dat <- subset(anchoring, us_or_international == "International" & referrer != "lse") %>% mutate(anchor=factor(anchor,levels=c("low","high")), referrer = factor(referrer))
dat %>% 
  ggplot(aes(x=referrer,y=everest_meters,colour = referrer)) + stat_summary(fun.data="mean_se",geom="crossbar") + geom_jitter(alpha=.4) + theme(legend.position = "none") + xlab("Lab") + ylab("Height of Mount Everest in meters") + facet_wrap(~anchor)
```

Accounting for the effect of anchor is done by including a contrast-coding predictor in the model, e.g. a predictor $X_1$ which has the value $\tfrac{1}{2}$ for cases in the high anchor condition, and a value $-\tfrac{1}{2}$ for cases in the low anchor condition. Just adding this predictor to the random intercepts model of Equations \@ref(eq:random-intercept-model-level-1) and \@ref(eq:random-intercept-model-level-2) would mean that we would change Equation \@ref(eq:random-intercept-model-level-1) to
$$Y_{i,j} = \beta_{0,j} + \beta_{1} \times X_{1,i,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})$$
while we could keep Equation \@ref(eq:random-intercept-model-level-2) the same. In this model, as for ANOVA models, the intercepts $\beta_{0,j}$ reflect a grand mean (average of the averages of the two anchor conditions), while the fixed slope $\beta_1$ reflects the difference between the mean of the high and low anchor condition. As there is only a fixed slope $\beta_1$, this model assumes that the effect of the anchors is exactly the same for each lab. That implies that for each lab, the distance between the mean judgement in the high and low anchor is assumed to be identical. The random intercept would allow the midpoint between the two means to vary over the groupings, such that if one grouping has relatively high judgements in the low anchor condition, it would also have relatively high judgements in the high anchor condition. Looking at Figure \@ref(fig:anchoring-both-boxplot-by-referrer), this may be a too strong assumption. For example, referrer $\texttt{swpson}$ has a relatively high mean in the low anchor condition, but a relatively low mean in the high anchor condition (and both are suspiciously close to the true height of Mount Everest). Because we have multiple cases in each lab and condition combination, we could allow the effect of anchor to potentially differ between the groupings. This can be achieved by adding a random effect for condition. If we allow a different slope $\beta_{1,j}$ for each grouping $j$, we can write the model as
\begin{equation}
Y_{i,j} = \beta_{0,j} + \beta_{1,j} \times X_{1,i,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
(\#eq:random-intercept-and-slope-model-level-1)
\end{equation}
The level-2 models for the intercept and slope are:
\begin{align}
\beta_{0,j} &= \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0}) \\
\beta_{1,j} &= \beta_1 + \gamma_{1,j} \quad \quad \quad \gamma_{1,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_1})
(\#eq:random-intercept-and-slope-model-level-2)
\end{align}
As for the random intercepts, the random slope $\beta_{1,j}$ consists of a fixed part $\beta_1$, which is the average slope over all groups, and a random part $\gamma_{1,j}$, which is the deviation of the grouping-wise slope ($\beta_{1,j}$) from the average ($\beta_1$). 

```{r mixed-effects-random-slopes, cache=TRUE, message=FALSE, results='hide'}
library(sdamr)
library(afex)
contrasts(dat$anchor) <- c(-1/2,1/2)
dat$referrer <- as.factor(dat$referrer)
contrasts(dat$referrer) <- contr.sum(length(levels(dat$referrer)))
modg <- mixed(everest_meters ~ anchor + (anchor||referrer), data=dat, check_contrasts=FALSE, expand_re=TRUE, test_intercept = TRUE)
```

The estimated model (obtained with REML) can be stated as
$$\begin{aligned}
Y_{i,j} &= \beta_0 + \gamma_{0,j} + (\beta_1 + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model)[1],digits=3, nsmall=2)` + \gamma_{0,j} + (`r format(fixef(modg$full_model)[2],digits=3, nsmall=2)` + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
\gamma_{0,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model)$referrer,"stddev"), digits=3, nsmall=2)`) \\
\gamma_{1,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model)$referrer.1,"stddev"), digits=3, nsmall=2)`) \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model),"sc"), digits=3, nsmall=2)`)
\end{aligned}$$
In terms of inference, we are mainly interested in the fixed effect of anchor. A test of the hypothesis that the slope of anchor is $\beta_1 = 0$, again using the Kenward-Roger approximation for the degrees of freedom, gives us $F(`r modg$anova_table[2,1]`, `r modg$anova_table[2,2]`) = `r modg$anova_table[2,3]`$, $p `r pvalue(modg$anova_table[2,4], limit=TRUE)`$. Hence, we reject the null hypothesis that the true effect of the anchor equals 0. There clearly is a difference between the low and high anchor condition, such that the judgements in the former are lower on average than in the latter conditions.

```{r est-ml-models}
dat$anchor_n <- -1/2
dat$anchor_n[dat$anchor == "high"] <- 1/2
modg_ml <- lme4::lmer(everest_meters ~ anchor_n + (1 + anchor_n||referrer), data=dat, REML=FALSE)
modr_ml <- lme4::lmer(everest_meters ~ anchor_n + (1|referrer), data=dat, REML=FALSE)
```

At this point, let's also consider testing whether there is evidence that the effect of the anchor varies over the groupings. In order to obtain this test, we need to use a likelihood ratio test, or a parametric bootstrap. Here, we'll focus on the former. To test the null-hypothesis $H_0: \sigma^2_{\gamma_1} = 0$ with a likelihood-ratio test, we first re-estimate the full MODEL G (which includes random intercepts and slopes) by maximum likelihood. This provides us a value of 
$$-2 \log p(Y_{i,j}|\text{MODEL G}) = `r format(anova(modg_ml,modr_ml)$deviance[2],scientific=FALSE,nsmall=2)`$$ In MODEL R, we omit the random slopes $\gamma_{1,j}$, but keep everything else the same. Estimating this model gives
$$-2 \log p(Y_{i,j}|\text{MODEL R}) = `r format(anova(modg_ml,modr_ml)$deviance[1],scientific=FALSE,nsmall=2)`$$
The likelihood-ratio test statistic is simply the difference between these two values:
$$-2 \log (\text{likelihood-ratio}) = `r format(anova(modg_ml,modr_ml)$deviance[1],scientific=FALSE,nsmall=2)` - `r format(anova(modg_ml,modr_ml)$deviance[2],scientific=FALSE,nsmall=2)` = `r anova(modg_ml,modr_ml)$Chisq[2]`$$
As MODEL G contains just one more estimated parameter, the degrees of freedom for the test equals $\text{npar}(G) - \text{npar}(R) = `r anova(modg_ml,modr_ml)$npar[2]` - `r anova(modg_ml,modr_ml)$npar[1]` = 1$. The critical value for a Chi-squared distribution with one degree of freedom and significance level $\alpha = .05$ is $\chi^2(1; .05) = `r qchisq(.95,1)`$. Clearly, the value computed for the data is well above the critical value. Hence, we reject the null hypothesis that $\sigma^2_{\gamma_1} = 0$, and hence we have sufficient evidence that the effect of anchor varies over the groupings.

### Correlation between random effects

In the model above, we assumed the random effects $\gamma_{0,j}$ and $\gamma_{1,j}$ are independent and drawn from separate Normal distributions (each with a different standard deviation, $\sigma_{\gamma_0}$ and $\sigma_{\gamma_1}$. There are situations where we would however like to allow the random effects to be correlated. In the example above, it might be the case that the effect of the anchor differs between groups who on average give relatively high judgements, compared to groups who tend to give lower judgements. Such a dependence points to a moderation of the effect of anchor by the average (grand mean) judgement. In models without random effects, such a moderation would not make sense, because there is a single intercept, and a single slope for anchor. However, we now have different intercepts and slopes for the groups, such that we could, in principle, estimate a relation (a dependence) between the intercepts and slopes.

In general, linear random-effects models assume that the random effects are samples from a __multivariate Normal distribution__. A multivariate Normal distribution is a distribution over vectors with values, and is parametrized by a mean vector $\boldsymbol{{\mu}}$ and a covariance matrix $\boldsymbol{{\Sigma}}$. For the two random effects in our model, we can specify the joint distribution as follows:
\begin{equation}
\left( \begin{matrix} \gamma_{0,j} \\ \gamma_{1,j} \end{matrix} \right)
\sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} \sigma^2_{\gamma_0}&\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1} \\ \rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}&\sigma^2_{\gamma_1} \end{matrix} \right] \right)
\end{equation}
Here, the $\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}$ component in the off-diagonal elements of the covariance matrix are the covariance between $\gamma_{0,j}$ and $\gamma_{1,j}$. A covariance can be written as a product of the correlation $\rho_{\gamma_0,\gamma_1}$ between $\gamma_{0,j}$ and $\gamma_{1,j}$, and the standard deviations $\sigma_{\gamma_0}$ and $\sigma_{\gamma_1}$.

Unfortunately, the added complexity of estimating the correlation between the random effects is sometimes too much, resulting in estimation failures. That was the case for the data analysed above. By including all the labs in the full study, the model is estimable. In this case, we focus on the estimation in feet, rather than meters. In other respects, the model is the same as suggested above. 

```{r mixed-effects-random-slopes-correlated, cache=TRUE, message=FALSE, results='hide'}
library(sdamr)
library(afex)
data("anchoring")
anchoring$anchor <- factor(anchoring$anchor, levels=c("low","high"))
contrasts(anchoring$anchor) <- c(-1/2,1/2)
anchoring$referrer <- factor(anchoring$referrer)
contrasts(dat$referrer) <- contr.sum(length(levels(dat$referrer)))
modg <- mixed(everest_feet ~ anchor + (1 + anchor|referrer), data=anchoring, check_contrasts = FALSE)
```

The estimated model can be written as
$$\begin{aligned}
Y_{i,j} &= \beta_0 + \gamma_{0,j} + (\beta_1 + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model)[1],digits=3, nsmall=2)` + \gamma_{0,j} + (`r format(fixef(modg$full_model)[2],digits=3, nsmall=2)` + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
\left( \begin{matrix} \gamma_{0,j} \\ \gamma_{1,j} \end{matrix} \right)
&\sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} \sigma^2_{\gamma_0}&\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1} \\ \rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}&\sigma^2_{\gamma_1} \end{matrix} \right] \right) \\
& \sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix}  (`r attr(VarCorr(modg$full_model)$referrer,"stddev")[1]`)^2 & `r attr(VarCorr(modg$full_model)$referrer,"correlation")[2,1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[2]` \\ `r attr(VarCorr(modg$full_model)$referrer,"correlation")[2,1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[2]` & (`r attr(VarCorr(modg$full_model)$referrer,"stddev")[2]`)^2 \end{matrix} \right] \right) \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model),"sc"), digits=3, nsmall=2)`)
\end{aligned}$$
This shows a negative correlation between the random intercepts and the random slopes. This can be interpreted as indicating that the higher the average judgements in a grouping (the intercept), the smaller the difference is between the low and high anchor conditions within that grouping (the slope). 

## Crossed random effects: dating partners in the speed dating experiment

As a final example of a mixed-effects analysis, we will reconsider the data from the speed-dating experiment of @fisman2006gender. 

Please come back later for this.

<!-- 
```{r, cache= TRUE, results='hide'}
data("speeddate")
dat <- speeddate
dat$iid <- as.factor(dat$iid)
dat$pid <- as.factor(dat$pid)
summary(afex::mixed(other_like ~ other_attr + other_sinc + other_intel + other_fun + other_amb + (1|iid) + (1|pid), data=dat))
```
-->

## Choosing the random effects structure

The correct specification of the random-effects structure in the model is very important. Omitting a random effect when there is in fact variability in this effect across the levels of a grouping factor can dramatically increase Type I errors well beyond the desired significance level [@judd_treating_2012; @barr_random_2013-1]. As such, it has been recommended that one should initially start with the __maximal random effects structure__ [@barr_random_2013-1; @Singmann2019-mixed]. The maximal model is the model that includes random effects for all grouping factors in a study. Specifically, for each grouping factor, the maximal model contains random intercepts as well as random slopes for all fixed effects, plus all correlations among the random effects. 

By using a maximal structure, it is unlikely that you would miss a potentially important source of variability in the data. Inclusion of random effects which are not really there might, given sufficient data, not be too much of an issue, as the model would estimate the variance of such random effects to be low. However, if the sample size is limited, a common problem is that the maximal model is not fully identified [@bates_parsimonious_2015], especially for mixed models with complicated random effects structures. An unidentified model means that not all parameters can be estimated properly. This is often evident by the estimated variance-covariance matrix of the random effects being degenerate or singular. In such cases, you may obtain estimated variances of or near 0, or even negative estimates, and correlations between random effects of $\pm 1$. The occurrence of such situations is due to the fact the parameters associated to random effects (e.g., $\sigma^2_{\gamma_1}$) are more difficult to estimate than fixed effects (e.g., $\beta_1$). 

When the maximal structure can not be used because of these estimation issues, it is generally recommended to reduce the complexity of the random effects structure, because degenerate or overparameterized models can reduce the statistical power of any tests conducted with them [@matuschek_balancing_2017]. A first step is generally to remove correlations among random slopes, as these tend to contribute the largest number of random effects parameters that need to be estimated. They are usually also more difficult to estimate than the variance parameters. If a model still shows problems after removing correlations, other random-effects parameters could be removed, starting with the highest-order random effects parameter with the lowest estimated variance. Iterative approaches to obtain the __optimal random effects structure__, rather than the maximal one, are provided by @matuschek_balancing_2017 and @bates_parsimonious_2015. Applying these iterative procedures can be complicated. They are also data-driven, and hence the results may be influenced by random noise in the data. Sometimes, it might not be possible to reduce the random-effects structure such that all problematic random effects parameters are removed (e.g., in cases when there is random variability in higher-order effects, but not in lower-order effects). In those cases, one might accept a few problematic or degenerate parameters (e.g., variances of zero). This would provide more conservative tests for the fixed effects than simply removing justifiable random effects and inflating Type I error rates to an unknown degree. It is also advisable to check the robustness of results by comparing the fixed-effects estimates and associated hypothesis tests across all estimated models with a different random effects structure. It is often the case that the testing of fixed effects in highly overparameterized models diverge from the corresponding tests in reduced models. In these cases, the tests for the simpler models may be preferred.