<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Path models (SEM 1) | Statistics: Data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Path models (SEM 1) | Statistics: Data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Path models (SEM 1) | Statistics: Data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2023-11-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-generalized-linear-models.html"/>
<link rel="next" href="ch-SEM-latent-variable-models.html"/>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="book_assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="book_assets/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.10/grViz.js"></script>
<script src="book_assets/plotly-binding-4.10.3/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-2.11.1/plotly-latest.min.js"></script>
<link href="book_assets/combineWidgetStyle-0.1/combineWidgets.css" rel="stylesheet" />
<script src="book_assets/combineWidgets-binding-0.11.1/combineWidgets.js"></script>
<script src="book_assets/rglWebGL-binding-1.2.1/rglWebGL.js"></script>
<link href="book_assets/rglwidgetClass-1.2.1/rgl.css" rel="stylesheet" />
<script src="book_assets/rglwidgetClass-1.2.1/rglClass.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/utils.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/buffer.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/subscenes.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/shaders.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/shadersrc.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/textures.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/projection.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/mouse.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/init.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/pieces.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/draw.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/controls.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/selection.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/rglTimer.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/pretty.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/axes.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/animation.src.js"></script>
<script src="book_assets/CanvasMatrix4-1.2.1/CanvasMatrix.src.js"></script>
<script src="book_assets/rglPlayer-binding-1.2.1/rglPlayer.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ch-intro.html"><a href="ch-intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="ch-intro.html"><a href="ch-intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-modeling.html"><a href="ch-modeling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-probability-definition"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-binomial-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="ch-modeling.html"><a href="ch-modeling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-modeling.html"><a href="ch-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-modeling.html"><a href="ch-modeling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-likelihood-ratio"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-modeling.html"><a href="ch-modeling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-modeling.html"><a href="ch-modeling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-modeling.html"><a href="ch-modeling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-modeling.html"><a href="ch-modeling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="ch-modeling.html"><a href="ch-modeling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="ch-modeling.html"><a href="ch-modeling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-a-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has a specific value</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#ch3-confidence-interval"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#effect-size"><i class="fa fa-check"></i><b>3.6</b> Effect size</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.7</b> Assumptions</a></li>
<li class="chapter" data-level="3.8" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.8</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.8.1</b> The Central Limit Theorem in action</a></li>
<li class="chapter" data-level="3.8.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#bootstrapping-a-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Bootstrapping a statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#in-practice"><i class="fa fa-check"></i><b>3.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>4.4.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#summary-2"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-transformations"><i class="fa fa-check"></i><b>5.7.1</b> Transforming variables</a></li>
<li class="chapter" data-level="5.7.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.7.2</b> Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.8</b> Multicollinearity: Redundancy between predictors</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.8.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-outliers"><i class="fa fa-check"></i><b>5.9</b> Outliers</a></li>
<li class="chapter" data-level="5.10" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#in-practice-1"><i class="fa fa-check"></i><b>5.10</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#sec:05-dont-forget-about-the-fun"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#sec-glm-mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#sec:06-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-size-in-anova"><i class="fa fa-check"></i><b>7.6</b> Effect-size in ANOVA</a></li>
<li class="chapter" data-level="7.7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#assumptions"><i class="fa fa-check"></i><b>7.7</b> Assumptions</a></li>
<li class="chapter" data-level="7.8" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.8</b> Multiple testing and post-hoc tests</a></li>
<li class="chapter" data-level="7.9" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#in-practice-2"><i class="fa fa-check"></i><b>7.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#sec:06-main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#the-factorial-anova-model"><i class="fa fa-check"></i><b>8.3</b> The factorial ANOVA model</a></li>
<li class="chapter" data-level="8.4" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.4</b> A threeway factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#interpreting-interactions"><i class="fa fa-check"></i><b>8.4.1</b> Interpreting interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.5</b> Orthogonal contrast codes and unequal sample sizes</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.5.1</b> Comparison schemes and SS types</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#in-practice-3"><i class="fa fa-check"></i><b>8.6</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#subjective-feelings-of-power-and-priming"><i class="fa fa-check"></i><b>9.1</b> Subjective feelings of power and priming</a></li>
<li class="chapter" data-level="9.2" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#acounting-for-pre-existing-differences"><i class="fa fa-check"></i><b>9.2</b> Acounting for pre-existing differences</a></li>
<li class="chapter" data-level="9.3" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#slopes-of-contrast-coded-predictors-in-ancova-models"><i class="fa fa-check"></i><b>9.3</b> Slopes of contrast-coded predictors in ANCOVA models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#homogeneity-of-slopes"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of slopes</a></li>
<li class="chapter" data-level="9.5" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#sec:ANCOVA-power"><i class="fa fa-check"></i><b>9.5</b> Power considerations in ANCOVA</a></li>
<li class="chapter" data-level="9.6" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#models-with-multiple-covariates"><i class="fa fa-check"></i><b>9.6</b> Models with multiple covariates</a></li>
<li class="chapter" data-level="9.7" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#mediation-with-categorical-independent-variables"><i class="fa fa-check"></i><b>9.7</b> Mediation with categorical independent variables</a></li>
<li class="chapter" data-level="9.8" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#ancova-vs-difference-scores"><i class="fa fa-check"></i><b>9.8</b> ANCOVA vs difference scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#non-independence-in-linear-models"><i class="fa fa-check"></i><b>10.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="10.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#the-cheerleader-effect"><i class="fa fa-check"></i><b>10.2</b> The cheerleader effect</a></li>
<li class="chapter" data-level="10.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#as-a-oneway-anova"><i class="fa fa-check"></i><b>10.3</b> As a oneway ANOVA</a></li>
<li class="chapter" data-level="10.4" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#oneway-repeated-measures-anova"><i class="fa fa-check"></i><b>10.4</b> Oneway repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#within-subjects-composite-scores"><i class="fa fa-check"></i><b>10.4.1</b> Within-subjects composite scores</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-composite-for-between-subjects-effects"><i class="fa fa-check"></i><b>10.4.2</b> A composite for between-subjects effects</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#collecting-all-results-and-omnibus-tests"><i class="fa fa-check"></i><b>10.4.3</b> Collecting all results and omnibus tests</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#partitioning-the-variance"><i class="fa fa-check"></i><b>10.5</b> Partitioning the variance</a></li>
<li class="chapter" data-level="10.6" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-mixed-anova-with-between--and-within-subjects-effects"><i class="fa fa-check"></i><b>10.6</b> A mixed ANOVA with between- and within-subjects effects</a></li>
<li class="chapter" data-level="10.7" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#assumptions-1"><i class="fa fa-check"></i><b>10.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#omnibus-tests-and-sphericity"><i class="fa fa-check"></i><b>10.7.1</b> Omnibus tests and sphericity</a></li>
<li class="chapter" data-level="10.7.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#correcting-for-non-sphericity"><i class="fa fa-check"></i><b>10.7.2</b> Correcting for non-sphericity</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#in-practice-4"><i class="fa fa-check"></i><b>10.8</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#non-independence-in-linear-models-1"><i class="fa fa-check"></i><b>11.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="11.2" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#random-intercept-models"><i class="fa fa-check"></i><b>11.2</b> Random intercept models</a></li>
<li class="chapter" data-level="11.3" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-estimation-1"><i class="fa fa-check"></i><b>11.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="11.4" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-inference"><i class="fa fa-check"></i><b>11.4</b> Parameter inference</a></li>
<li class="chapter" data-level="11.5" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#application-of-the-random-intercepts-model"><i class="fa fa-check"></i><b>11.5</b> Application of the random-intercepts model</a></li>
<li class="chapter" data-level="11.6" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#models-with-random-intercepts-and-slopes"><i class="fa fa-check"></i><b>11.6</b> Models with random intercepts and slopes</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#correlation-between-random-effects"><i class="fa fa-check"></i><b>11.6.1</b> Correlation between random effects</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#crossed-random-effects-dating-partners-in-the-speed-dating-experiment"><i class="fa fa-check"></i><b>11.7</b> Crossed random effects: dating partners in the speed dating experiment</a></li>
<li class="chapter" data-level="11.8" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#choosing-the-random-effects-structure"><i class="fa fa-check"></i><b>11.8</b> Choosing the random effects structure</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html"><i class="fa fa-check"></i><b>12</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#link-functions"><i class="fa fa-check"></i><b>12.1</b> Link functions</a></li>
<li class="chapter" data-level="12.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#estimation-1"><i class="fa fa-check"></i><b>12.2</b> Estimation</a></li>
<li class="chapter" data-level="12.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#inference-in-generalized-linear-models"><i class="fa fa-check"></i><b>12.3</b> Inference in generalized linear models</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#wald-test"><i class="fa fa-check"></i><b>12.3.1</b> Wald test</a></li>
<li class="chapter" data-level="12.3.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>12.3.2</b> Likelihood-ratio test</a></li>
<li class="chapter" data-level="12.3.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#confidence-intervals-1"><i class="fa fa-check"></i><b>12.3.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-generalized-linear-models-overall-model-fit"><i class="fa fa-check"></i><b>12.4</b> Assessing model fit</a></li>
<li class="chapter" data-level="12.5" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#logistic-regression"><i class="fa fa-check"></i><b>12.5</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#parameter-interpretation"><i class="fa fa-check"></i><b>12.5.1</b> Parameter interpretation</a></li>
<li class="chapter" data-level="12.5.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-logistic-regression-metacognition"><i class="fa fa-check"></i><b>12.5.2</b> Example: Metacognition in visual perception</a></li>
<li class="chapter" data-level="12.5.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#using-a-different-link-function-probit-regression"><i class="fa fa-check"></i><b>12.5.3</b> Using a different link function: Probit regression</a></li>
<li class="chapter" data-level="12.5.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#welcome-back-paul"><i class="fa fa-check"></i><b>12.5.4</b> Welcome back Paul!</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>12.6</b> Poisson regression</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-poisson-regression-gestures"><i class="fa fa-check"></i><b>12.6.1</b> Example: Gestures in different social contexts</a></li>
<li class="chapter" data-level="12.6.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#overdispersion"><i class="fa fa-check"></i><b>12.6.2</b> Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#log-linear-models"><i class="fa fa-check"></i><b>12.7</b> Log-linear models</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#example-newspapers-and-voting"><i class="fa fa-check"></i><b>12.7.1</b> Example: Newspapers and voting</a></li>
<li class="chapter" data-level="12.7.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#a-three-way-table-example-rock-paper-scissors"><i class="fa fa-check"></i><b>12.7.2</b> A three-way table example: Rock-Paper-Scissors</a></li>
<li class="chapter" data-level="12.7.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sparse-data-and-empty-cells"><i class="fa fa-check"></i><b>12.7.3</b> Sparse data and empty cells</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.8</b> Multinomial logistic regression</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#baseline-category-logit"><i class="fa fa-check"></i><b>12.8.1</b> Baseline category logit</a></li>
<li class="chapter" data-level="12.8.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#example-rock-paper-scissors"><i class="fa fa-check"></i><b>12.8.2</b> Example: Rock-Paper-Scissors</a></li>
<li class="chapter" data-level="12.8.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#reconstructing-probabilities-of-responses"><i class="fa fa-check"></i><b>12.8.3</b> Reconstructing probabilities of responses</a></li>
<li class="chapter" data-level="12.8.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#alternative-logit-models-for-ordinal-categories"><i class="fa fa-check"></i><b>12.8.4</b> Alternative logit models for ordinal categories</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-glmer"><i class="fa fa-check"></i><b>12.9</b> Generalized linear mixed-effects models</a></li>
<li class="chapter" data-level="12.10" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#in-practice-5"><i class="fa fa-check"></i><b>12.10</b> In practice</a></li>
<li class="chapter" data-level="12.11" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#further-reading"><i class="fa fa-check"></i><b>12.11</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html"><i class="fa fa-check"></i><b>13</b> Path models (SEM 1)</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#graphical-models"><i class="fa fa-check"></i><b>13.1</b> Graphical models</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#exogenous-and-endogenous-variables"><i class="fa fa-check"></i><b>13.1.1</b> Exogenous and endogenous variables</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#regression-models"><i class="fa fa-check"></i><b>13.2</b> Regression models</a></li>
<li class="chapter" data-level="13.3" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#sec-sem-path-model-mediation"><i class="fa fa-check"></i><b>13.3</b> Mediation</a></li>
<li class="chapter" data-level="13.4" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#assumptions-and-estimation"><i class="fa fa-check"></i><b>13.4</b> Assumptions and estimation</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>13.4.1</b> The multivariate Normal distribution</a></li>
<li class="chapter" data-level="13.4.2" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#assumptions-exogenous-vs-endogenous-variables"><i class="fa fa-check"></i><b>13.4.2</b> Assumptions: Exogenous vs endogenous variables</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#model-fit"><i class="fa fa-check"></i><b>13.5</b> Model fit</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#test-of-overall-model-fit"><i class="fa fa-check"></i><b>13.5.1</b> Test of overall model fit</a></li>
<li class="chapter" data-level="13.5.2" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#approximate-fit-indices"><i class="fa fa-check"></i><b>13.5.2</b> Approximate fit indices</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#modification-indices"><i class="fa fa-check"></i><b>13.6</b> Modification indices</a></li>
<li class="chapter" data-level="13.7" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#model-comparison-1"><i class="fa fa-check"></i><b>13.7</b> Model comparison</a></li>
<li class="chapter" data-level="13.8" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#evaluation-and-selection-of-the-mediation-path-models"><i class="fa fa-check"></i><b>13.8</b> Evaluation and selection of the mediation path models</a></li>
<li class="chapter" data-level="13.9" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#a-more-complex-path-model"><i class="fa fa-check"></i><b>13.9</b> A more complex path model</a></li>
<li class="chapter" data-level="13.10" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#principles-in-constructing-path-models"><i class="fa fa-check"></i><b>13.10</b> Principles in constructing path models</a>
<ul>
<li class="chapter" data-level="13.10.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#identifiability"><i class="fa fa-check"></i><b>13.10.1</b> Identifiability</a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#model-equivalence"><i class="fa fa-check"></i><b>13.11</b> Model equivalence</a></li>
<li class="chapter" data-level="13.12" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#correlation-vs-causation"><i class="fa fa-check"></i><b>13.12</b> Correlation vs causation</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html"><i class="fa fa-check"></i><b>14</b> Latent variable models (SEM 2)</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#measurement-of-latent-variables"><i class="fa fa-check"></i><b>14.1</b> Measurement of latent variables</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#scaling-and-identification"><i class="fa fa-check"></i><b>14.1.1</b> Scaling and identification</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#confirmatory-factor-analysis"><i class="fa fa-check"></i><b>14.2</b> Confirmatory factor analysis</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#a-single-factor-model"><i class="fa fa-check"></i><b>14.2.1</b> A single factor model</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#full-hierarchical-factor-model-for-the-bfi-2"><i class="fa fa-check"></i><b>14.3</b> Full hierarchical factor model for the BFI-2</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#indentification"><i class="fa fa-check"></i><b>14.3.1</b> Indentification</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#multiple-groups-and-measurement-invariance"><i class="fa fa-check"></i><b>14.4</b> Multiple groups and measurement invariance</a></li>
<li class="chapter" data-level="14.5" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#exploratory-factor-and-principal-components-analysis"><i class="fa fa-check"></i><b>14.5</b> Exploratory factor and principal components analysis</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#determining-the-number-of-factors"><i class="fa fa-check"></i><b>14.5.1</b> Determining the number of factors</a></li>
<li class="chapter" data-level="14.5.2" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#factor-rotation"><i class="fa fa-check"></i><b>14.5.2</b> Factor rotation</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a></li>
<li class="chapter" data-level="14.7" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#in-practice-6"><i class="fa fa-check"></i><b>14.7</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html"><i class="fa fa-check"></i><b>15</b> Introduction to Bayesian estimation</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#fundamentals-of-bayesian-inference"><i class="fa fa-check"></i><b>15.1</b> Fundamentals of Bayesian inference</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#probability-in-times-of-covid"><i class="fa fa-check"></i><b>15.1.1</b> Probability in times of Covid</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#bayes-rule"><i class="fa fa-check"></i><b>15.1.2</b> Bayes’ rule</a></li>
<li class="chapter" data-level="15.1.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#we-missed-you-paul"><i class="fa fa-check"></i><b>15.1.3</b> We missed you Paul!</a></li>
<li class="chapter" data-level="15.1.4" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#the-marginal-likelihood-and-prior-predictive-distribution"><i class="fa fa-check"></i><b>15.1.4</b> The marginal likelihood and prior predictive distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#markov-chain-monte-carlo-mcmc"><i class="fa fa-check"></i><b>15.2</b> Markov chain Monte Carlo (MCMC)</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#convergence"><i class="fa fa-check"></i><b>15.2.1</b> Convergence</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#autocorrelation"><i class="fa fa-check"></i><b>15.2.2</b> Autocorrelation</a></li>
<li class="chapter" data-level="15.2.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#effective-sample-size"><i class="fa fa-check"></i><b>15.2.3</b> Effective sample size</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#concerning-prior-distributions"><i class="fa fa-check"></i><b>15.3</b> Concerning prior distributions</a></li>
<li class="chapter" data-level="15.4" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#multiple-regression-example"><i class="fa fa-check"></i><b>15.4</b> Multiple regression example</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html"><i class="fa fa-check"></i><b>16</b> Introduction to Bayesian hypothesis testing</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#hypothesis-testing-relative-evidence-and-the-bayes-factor"><i class="fa fa-check"></i><b>16.1</b> Hypothesis testing, relative evidence, and the Bayes factor</a></li>
<li class="chapter" data-level="16.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#parameter-estimates-and-credible-intervals"><i class="fa fa-check"></i><b>16.2</b> Parameter estimates and credible intervals</a></li>
<li class="chapter" data-level="16.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#a-bayesian-t-test"><i class="fa fa-check"></i><b>16.3</b> A Bayesian t-test</a></li>
<li class="chapter" data-level="16.4" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#bayes-factors-for-general-linear-models"><i class="fa fa-check"></i><b>16.4</b> Bayes factors for General Linear Models</a></li>
<li class="chapter" data-level="16.5" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#some-objections-to-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>16.5</b> Some objections to null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-is-not-a-proper-measure-of-evidential-support"><i class="fa fa-check"></i><b>16.5.1</b> The <span class="math inline">\(p\)</span>-value is not a proper measure of evidential support</a></li>
<li class="chapter" data-level="16.5.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-depends-on-researcher-intentions"><i class="fa fa-check"></i><b>16.5.2</b> The <span class="math inline">\(p\)</span>-value depends on researcher intentions</a></li>
<li class="chapter" data-level="16.5.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#results-of-a-nhst-are-often-misinterpreted"><i class="fa fa-check"></i><b>16.5.3</b> Results of a NHST are often misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#to-bayes-or-not-to-bayes-a-pragmatic-view"><i class="fa fa-check"></i><b>16.6</b> To Bayes or not to Bayes? A pragmatic view</a></li>
<li class="chapter" data-level="16.7" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#in-practice-7"><i class="fa fa-check"></i><b>16.7</b> In practice</a></li>
<li class="chapter" data-level="16.8" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#summary-3"><i class="fa fa-check"></i><b>16.8</b> “Summary”</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html"><i class="fa fa-check"></i><b>17</b> Being a responsible data analyst</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#consider-analysis-before-data-collection"><i class="fa fa-check"></i><b>17.1</b> Consider analysis before data collection</a></li>
<li class="chapter" data-level="17.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#explore-the-data"><i class="fa fa-check"></i><b>17.2</b> Explore the data</a></li>
<li class="chapter" data-level="17.3" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#evaluate-the-assumptions-underlying-your-analyses"><i class="fa fa-check"></i><b>17.3</b> Evaluate the assumptions underlying your analyses</a></li>
<li class="chapter" data-level="17.4" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#distinguish-between-confirmatory-and-exploratory-analyses"><i class="fa fa-check"></i><b>17.4</b> Distinguish between confirmatory and exploratory analyses</a></li>
<li class="chapter" data-level="17.5" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#aim-for-openness-and-reproducibility"><i class="fa fa-check"></i><b>17.5</b> Aim for openness and reproducibility</a></li>
<li class="chapter" data-level="17.6" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#communicate-clearly-and-concisely"><i class="fa fa-check"></i><b>17.6</b> Communicate clearly and concisely</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-multiple-regression-analysis"><i class="fa fa-check"></i><b>17.6.1</b> Example of reporting a multiple regression analysis</a></li>
<li class="chapter" data-level="17.6.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-factorial-anova"><i class="fa fa-check"></i><b>17.6.2</b> Example of reporting a factorial ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: Data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-SEM-path-models" class="section level1 hasAnchor" number="13">
<h1><span class="header-section-number">Chapter 13</span> Path models (SEM 1)<a href="ch-SEM-path-models.html#ch-SEM-path-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- https://www.joophox.net/publist/semfamre.pdf -->
<!-- https://stats.oarc.ucla.edu/r/seminars/rsem/ -->
<p>Up to now, we have mostly considered models with a single dependent variable, allowing the value of this variable to depend on multiple predictors or independent variables. Although flexible, this can still be restrictive. There are situations where we would like to simultaneously model the inter-relations within a set of variables. We saw an example of this when we considered <em>mediation</em> (see Chapter <a href="ch-moderation-mediation.html#ch-moderation-mediation">6</a>), where we were interested in how an independent variable affects a mediator variable, and how this mediator variable subsequently affects a dependent variable. Such a <em>causal chain</em> can not be directly modelled in a single regression model. Applying the causal steps approach, we needed to estimate three separate models, with inference being based on a comparison of the parameters between these models.</p>
<p>In this chapter, and the next, we will consider how to account for the relations between multiple dependent variables. We will focus on a class of models called <strong>Structural Equation Models</strong> (SEM). As we will see, this class incorporates a wide variety of models, including many of those we have discussed before. Structural equation models also allow us to incorporate <strong>latent variables</strong>, which are variables that are not directly observed, but whose values can be inferred (or measured) from their relations with observed variables. Such latent variable models are the focus of Chapter <a href="ch-SEM-latent-variable-models.html#ch-SEM-latent-variable-models">14</a>. Here, we will focus on models which contain only observed variables. These models are commonly referred to as <strong>path models</strong>.</p>
<p>Path models were introduced by <span class="citation">Wright (<a href="#ref-wright1920relative">1920</a>)</span>, who used regression equations to define direct and indirect effects of observed variables onto others in population genetics, and applied causal interpretations to these effects. Figure <a href="ch-SEM-path-models.html#fig:wright-path-model-image">13.1</a> shows one of the first illustrations of a path model by Sewell Wright, to depict genetic relations between generations of guinea pigs.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:wright-path-model-image"></span>
<img src="https://openmx.ssri.psu.edu/sites/default/files/Wright20aFigure5_1.jpg" alt="Graphical representation of a path model by Sewell Wright. Source: [OpenMx ](https://openmx.ssri.psu.edu/sites/default/files/Wright20aFigure5_1.jpg)"  />
<p class="caption">
Figure 13.1: Graphical representation of a path model by Sewell Wright. Source: <a href="https://openmx.ssri.psu.edu/sites/default/files/Wright20aFigure5_1.jpg">OpenMx</a>
</p>
</div>
<!--
However, we will let go of the clear distinction between predictor variables and dependent variables. We will simply denote each observed variable as $Y_j$. 

Structural Equation Models are, at their heart, *linear* models. The conditional mean $\mu_{Y_j|Y_1, \ldots, Y_{j-1}, Y_{j+1}, \ldots, Y_m}$ of each variable $Y_j$ is assumed to be expressible as a linear function of the other variables $Y_1,\ldots, Y_{j-1}, Y_{j+1}, \ldots, Y_m$ in the model. It is possible to include link functions, as in Chapter \@ref(ch-generalized-linear-models), to allow for a wide variety in the conditional distributions of the variables $Y_j$. But we will stick to the simpler case, where these conditional distributions are Normal.
-->
<div id="graphical-models" class="section level2 hasAnchor" number="13.1">
<h2><span class="header-section-number">13.1</span> Graphical models<a href="ch-SEM-path-models.html#graphical-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Structural Equation Models are often expressed in a graphical form, where <strong>nodes</strong> represent variables, and <strong>edges</strong> (arrows) represent relations between the nodes (variables). Graphical depictions of SEM models have some conventions to allow a straightforward interpretation. These are:</p>
<ul>
<li>Squares or rectangles represent <em>observed</em> variables</li>
<li>Circles or ovals represent <em>latent</em> variables</li>
<li>Triangles represent (known) <em>constants</em></li>
<li>One-directional (single-headed) arrows represent a <em>causal relation</em> between two variables. The arrow starts from the cause (independent variable), and ends on its effect (dependent variable).</li>
<li>Bi-directional (double-headed) arrows represent <em>non-causal relations</em> (e.g. covariance or correlation). A bi-directional arrow from one variable onto itself represents the (residual) variance of a variable (which technically is equal to the covariance of a variable with itself)</li>
<li>Broken arrows represent <em>fixed</em> parameters. These parameters are assumed to be equal to the true parameter values, rather than estimates of the true values.</li>
</ul>
<p>We will see several examples of graphical depictions of SEM models in this chapter, and the next.</p>
<!--

Graph theory is a branch of mathematics, which provides an abstract way to analyse relations between objects. A graph in this context consists of a set of **nodes** (the objects), and a set of **edges** between them (the relations). A well-known example is a social network. Here, the nodes represent people, and the edges represent friendship relations, or lines of communication. The edges can be bidirectional, when both people connected call each other friends or communicate with each other, or uni-directional, when only one person calls the other a friend or communicates with them, but not vice versa. Such networks can be represented as a matrix, with rows and columns representing people, and the entries in the matrix representing e.g. whether a row-person states they are a friend of a column-person.

### Probabilistic graphical models

In the context of statistical models, **nodes** are (random) variables, and **edges** indicate  conditional dependencies between these variables.

**Probabilistic graphical models** refer to an abstract way to represent dependencies and independences in a set of random variables. ...  

Probabilistic graphical models take many guises. For one thing, edges can be directed, or undirected. A directed edge means that the relation goes one way. For instance, the value of one variable can cause the value of another. SO a directed edge represents a **causal relation**. An undirected relation means that two variables covary, but a causal relation is not assumed. This can be also represented by a dual-directed edge (indicating that both variables influence each other).

A nice aspect of graphical models is that they visualise often complex multiple relations in a large set of variables. This can aid our understanding of a model. Graphs are, in a sense, a visual language to express models. The model itself doesn't need the graph, but our understanding of them often does.

In **directed graphs** with directed edges (arrows), nodes (variables) can have parents (edges coming in from other nodes) and children (edges going out to other nodes). Relations between parents and children can be viewed as lines of communication. Parents can send messages to their children, but in a directed graph, not the other way round.

### Conventions in graphical representations of structural equation models

Graphical representations of structural equation models are often slightly more detailed than general probabilistic graphical models. In particular, different symbols are used to distinguish between observed variables, latent (unobserved) variables, constants (e.g. intercepts).

* Squares or rectangles: observed variables
* Circles or ovals: errors, factors, latent variables
* Triangles: constants
* One-directional (single-headed) arrows: linear relationship between two variables. Starts from an independent variable and ends on a dependent variable.
* Bi-directional (double-headed) arrows: variance of a variable or covariance between two variables

-->
<div id="exogenous-and-endogenous-variables" class="section level3 hasAnchor" number="13.1.1">
<h3><span class="header-section-number">13.1.1</span> Exogenous and endogenous variables<a href="ch-SEM-path-models.html#exogenous-and-endogenous-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An important distinction in SEM models is that between exogenous and endogenous variables. <strong>Exogenous variables</strong> are variables that are not (partially) caused by the other variables in the model. The word “exogenous” means “from without (the outside)”. In terms of a graphical model, these variables have no “parents” (i.e. incoming one-directional arrows from other variables). Exogenous variables have causes that are not incorporated in the model; their causes are unknown as far as the model is concerned. In the language of the general linear model, exogenous variables are the independent variables. This term is somewhat confusing though, as exogenous variables are often assumed to be correlated to each other. <strong>Endogenous variables</strong> are variables which are (partly) caused by other variables in the model. The word “endogenous” means “from within (the inside)”. In terms of a graphical model, endogenous variables have parents (e.g. incoming one-directional arrows from other variables in the model). In terms of the general linear model, endogenous variables are dependent variables. In SEM models, endogenous variables can be related to each other, in the sense that one endogenous variable can (partially) cause another endogenous variable. Linking endogenous variables in this way is a main objective in path models.</p>
<p>We will follow the convention to denote exogenous variables as <span class="math inline">\(X_j\)</span>, <span class="math inline">\(j=1, \ldots, m\)</span>, and endogenous variables as <span class="math inline">\(Y_j\)</span>, <span class="math inline">\(j=1, \ldots, k\)</span>. Combining the exogenous and endogenous variables, a SEM model accounts for a total of <span class="math inline">\(P = m + k\)</span> variables.</p>
</div>
</div>
<div id="regression-models" class="section level2 hasAnchor" number="13.2">
<h2><span class="header-section-number">13.2</span> Regression models<a href="ch-SEM-path-models.html#regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SEM-simple-regression-structural-plot"></span>
<img src="13-SEM_files/figure-html/SEM-simple-regression-structural-plot-1.svg" alt="Graphical representation of a simple regression model" width="80%" />
<p class="caption">
Figure 13.2: Graphical representation of a simple regression model
</p>
</div>
<p>The simplest example of a path model is a simple (bivariate) regression model. Figure <a href="ch-SEM-path-models.html#fig:SEM-simple-regression-structural-plot">13.2</a> depicts a simple regression model in graphical form. The model has two observed variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, indicated by rectangles. The model has two constant terms, indicated by triangles. Variable <span class="math inline">\(X\)</span> is an exogenous variable. It has an incoming one-directional arrow only from a constant, not from a variable. The path from the constant term to <span class="math inline">\(X\)</span> is an intercept term to represent the mean of this variable. The bi-directional arrow from <span class="math inline">\(X\)</span> onto itself represents the (residual) variance of this variable around its mean. The implied model for <span class="math inline">\(X\)</span> can be written as
<span class="math display">\[X = \beta_{0,x} \times 1 + \epsilon_{x} \quad \quad \epsilon_{x} \sim \mathbf{Normal}(0, \sigma_{\epsilon_{x}}) \]</span>
Variable <span class="math inline">\(Y\)</span> is an endogenous variable. It has two incoming uni-directional arrows. One from another constant term, indicated by a triangle, and one from variable <span class="math inline">\(X\)</span>. The bi-directional arrow from <span class="math inline">\(Y\)</span> to itself allows for residual variation in <span class="math inline">\(Y\)</span> that is not explained by the causal relation with <span class="math inline">\(X\)</span>. The implied model for <span class="math inline">\(Y\)</span> can be stated as
<span class="math display">\[Y = \beta_{0,y} \times 1 + \beta_{x} \times X + \epsilon_{y} \quad \quad \epsilon_{y} \sim \mathbf{Normal}(0, \sigma_{\epsilon_{y}}) \]</span>
The exogenous variable <span class="math inline">\(X\)</span> is assumed fixed, in the sense that the values it takes are not random. Although a fixed variable can still be assigned an intercept (<span class="math inline">\(\beta_{0,x}\)</span>) and residual variation term (<span class="math inline">\(\sigma_{\epsilon_{x}}\)</span>), these parameters do not need to be estimated. For a fixed variable, we assume that all the possible values are in the data. Hence, its parameters can be computed directly from the data. They do not need to be estimated. This is indicated by the broken lines in the arrows, which represent non-estimated (i.e. fixed) parameters.</p>
<p>A multiple regression model has multiple exogenous variables <span class="math inline">\(X_j\)</span>, and a single endogenous variable <span class="math inline">\(Y\)</span>. Figure <a href="ch-SEM-path-models.html#fig:SEM-multiple-regression-speeddate-plot">13.3</a> shows a graphical depiction of a multiple regression model, predicting whether participants in a speed-dating experiment (see Section <a href="ch-moderation-mediation.html#sec-glm-mediation">6.2</a>) like their partner (<span class="math inline">\(\texttt{like}\)</span>), as a function of how attractive (<span class="math inline">\(\texttt{attr}\)</span>), sincere (<span class="math inline">\(\texttt{sinc}\)</span>), intelligent (<span class="math inline">\(\texttt{intel}\)</span>), fun (<span class="math inline">\(\texttt{fun}\)</span>), and ambitious (<span class="math inline">\(\texttt{amb}\)</span>) they rate them. The predictors <span class="math inline">\(\texttt{attr}\)</span>, <span class="math inline">\(\texttt{sinc}\)</span>, <span class="math inline">\(\texttt{intel}\)</span>, <span class="math inline">\(\texttt{fun}\)</span>, and <span class="math inline">\(\texttt{amb}\)</span>, are all exogenous variables. They are allowed to covary, which is indicated by the bi-directional arrows linking them. They also have variances, which are indicated by the self-pointing bi-directional arrows. And their means are indicated through the constant terms (triangles). All the arrows are labelled by the values of the corresponding parameters. Note that as before, the parameters for the means and variances of the exogenous variables are treated as fixed. This is indicated by the broken lines of the arrows. Variable <span class="math inline">\(\texttt{like}\)</span> is an endogenous variable. It has incoming uni-directional arrows from all the predictors. In addition, the endogenous variable has an intercept (shown as the arrow from the triangle), and residual variance (shown as the bi-directional arrow from <span class="math inline">\(\texttt{like}\)</span> onto itself).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SEM-multiple-regression-speeddate-plot"></span>
<img src="13-SEM_files/figure-html/SEM-multiple-regression-speeddate-plot-1.svg" alt="Graphical representation of a multiple regression model" width="100%" />
<p class="caption">
Figure 13.3: Graphical representation of a multiple regression model
</p>
</div>
<p>Graphical depictions of SEM models, such as Figure <a href="ch-SEM-path-models.html#fig:SEM-multiple-regression-speeddate-plot">13.3</a>, contain a wealth of information about the assumed relations between the variables and the estimated parameters. However, sometimes we need more precise information, as well as parameter tests, which can be provided in a table such as Table <a href="ch-SEM-path-models.html#tab:sem-table-multiple-regression-speeddate">13.1</a>. This table lists all the parameters of the path model in order (regression slopes, intercepts, residual variances, residual covariances). For estimated parameters, the table also shows the standard error of the estimate, and the results of Wald tests of the null-hypothesis that the parameter equals 0. For fixed parameters, only the value of the parameter is shown. Fixed parameters can be computed, rather than estimated, and hence they do not have a standard error. The final part of the table contains so-called <em>fit indices</em>. We will discuss these later on.</p>
<table>
<caption>
<span id="tab:sem-table-multiple-regression-speeddate">Table 13.1: </span> Results of a SEM analysis of a multiple regression model.
</caption>
<table style="padding-right:20px;padding-left:20px;">
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'>Model</td>
</tr>
<tr>
<td>
</td>
<td colspan = '1'; align = 'center'>Estimate</td>
<td colspan = '1'; align = 'center'>Std. Err.</td>
<td colspan = '1'; align = 'center'>z</td>
<td colspan = '1'; align = 'center'>p</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Regression Slopes</span></td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">like</span></td>
</tr>
<tr>
<td>
attr
</td>
<td>
0.34
</td>
<td>
0.02
</td>
<td>
17.01
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
sinc
</td>
<td>
0.13
</td>
<td>
0.03
</td>
<td>
4.83
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
intel
</td>
<td>
0.15
</td>
<td>
0.03
</td>
<td>
4.62
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
fun
</td>
<td>
0.35
</td>
<td>
0.02
</td>
<td>
15.44
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
amb
</td>
<td>
0.06
</td>
<td>
0.02
</td>
<td>
2.62
</td>
<td>
.009
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Intercepts</span></td>
</tr>
<tr>
<td>
like
</td>
<td>
-0.75
</td>
<td>
0.17
</td>
<td>
-4.43
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
attr
</td>
<td>
6.35<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
sinc
</td>
<td>
7.31<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
intel
</td>
<td>
7.53<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
fun
</td>
<td>
6.56<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
amb
</td>
<td>
6.95<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Residual Variances</span></td>
</tr>
<tr>
<td>
like
</td>
<td>
1.27
</td>
<td>
0.05
</td>
<td>
26.35
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
attr
</td>
<td>
3.71<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
sinc
</td>
<td>
2.46<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
intel
</td>
<td>
2.21<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
fun
</td>
<td>
3.32<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
amb
</td>
<td>
2.95<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Residual Covariances</span></td>
</tr>
<tr>
<td>
attr w/sinc
</td>
<td>
1.12<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
attr w/intel
</td>
<td>
1.04<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
attr w/fun
</td>
<td>
2.11<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
attr w/amb
</td>
<td>
1.22<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
sinc w/intel
</td>
<td>
1.63<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
sinc w/fun
</td>
<td>
1.29<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
sinc w/amb
</td>
<td>
1.39<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
intel w/fun
</td>
<td>
1.23<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
intel w/amb
</td>
<td>
1.59<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
fun w/amb
</td>
<td>
1.59<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Fit Indices</span></td>
</tr>
<tr>
<td>
χ<sup>2</sup>
</td>
<td>
0.00(0)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
CFI
</td>
<td>
1.00
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
SRMR
</td>
<td>
0.00
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA
</td>
<td>
0.00
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan = '5'; align = 'left'><sup>+</sup>Fixed parameter</td>
</tr>
</table>
<p><br></p>
</table>
<p>Table <a href="ch-SEM-path-models.html#tab:sem-table-multiple-regression-speeddate">13.1</a> indicates that all the exogenous variables have a significant and positive effect on the endogenous variable. The intercept of the endogenous variable is also significant, which indicates that the mean of the endogenous variable can not be predicted accurately from the means of the exogenous variables. The residual variance indicates that not all the variation in the endogenous variable is accounted for by the causal relation with the exogenous variables. The residual covariances account for multi-collinearity between the predictors. Unlike a standard multiple regression model, this path model accounts for the covariation between all the (independent and dependent) variables in the model.</p>
</div>
<div id="sec-sem-path-model-mediation" class="section level2 hasAnchor" number="13.3">
<h2><span class="header-section-number">13.3</span> Mediation<a href="ch-SEM-path-models.html#sec-sem-path-model-mediation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Mediation involves a causal chain from an exogenous variable <span class="math inline">\((X)\)</span> to a mediating endogenous variable <span class="math inline">\((Y_1)\)</span>, which in turn is causally related to a final endogenous variable <span class="math inline">\((Y_2)\)</span>. For example, <span class="citation">Zaval et al. (<a href="#ref-zaval_how_2015">2015</a>)</span> considered whether the motive to leave a positive legacy in the world would result in an intention to behave in a pro-environmental manner, which in turn would result in actual pro-environmental behaviours. In Section <a href="ch-moderation-mediation.html#sec-glm-mediation">6.2</a> we discussed using three different regression models to assess whether there is evidence that the relation between legacy motive (<span class="math inline">\(\texttt{legacy}\)</span>) and behaviour (<span class="math inline">\(\texttt{donation}\)</span>; donating to an environmental cause) is mediated by intention (<span class="math inline">\(\texttt{intention}\)</span>). Here, we will use a path model to analyse this causal chain in a single model.</p>
<p>The <em>full mediation</em> path model has one exogenous variable <span class="math inline">\((\texttt{legacy})\)</span>. This exogenous variable has a causal link to the first endogenous variable <span class="math inline">\((\texttt{intention})\)</span>, which is the mediator variable. This endogenous mediator variable is causally linked to the second endogenous variable <span class="math inline">\((\texttt{donation})\)</span>. The estimated full mediation model is depicted in Figure <a href="ch-SEM-path-models.html#fig:path-full-mediation-legacy-motive">13.4</a>. This model assumes an indirect effect of <span class="math inline">\(\texttt{legacy}\)</span> on <span class="math inline">\(\texttt{donation}\)</span>, which is equal to the product of the regression terms in the path from <span class="math inline">\(\texttt{legacy}\)</span>, via <span class="math inline">\(\texttt{intention}\)</span>, to <span class="math inline">\(\texttt{donation}\)</span>. This indirect effect is equal to <span class="math inline">\(0.27 \times 1.07 = 0.28\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:path-full-mediation-legacy-motive"></span>
<img src="13-SEM_files/figure-html/path-full-mediation-legacy-motive-1.svg" alt="A full mediation model" width="80%" />
<p class="caption">
Figure 13.4: A full mediation model
</p>
</div>
<p>A path model allows us to estimate an assumed causal chain, or other patterns of relations between exogenous and endogenous variables, with a single model. However, we do not know whether the estimated model provides the best – or even a good – description of the data. Other path models are possible for this data. For example, we could also allow motivation to have a direct effect on behaviour, in addition to its indirect effect via intention. This model is depicted in Figure <a href="ch-SEM-path-models.html#fig:path-partial-mediation-legacy-motive">13.5</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:path-partial-mediation-legacy-motive"></span>
<img src="13-SEM_files/figure-html/path-partial-mediation-legacy-motive-1.svg" alt="A partial mediation model" width="80%" />
<p class="caption">
Figure 13.5: A partial mediation model
</p>
</div>
<p>Alternatively, we might assume that motivation causes both intention and behaviour, but that intention and behaviour are conditionally independent. This is also called a <strong>common cause model</strong>. This model is depicted in Figure <a href="ch-SEM-path-models.html#fig:path-common-cause-legacy-motive">13.6</a>. Note the broken bi-directional arrow between <span class="math inline">\(\texttt{donation}\)</span> and <span class="math inline">\(\texttt{intention}\)</span> with a value of 0. This indicates that the residual covariance between these variables is fixed to 0.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:path-common-cause-legacy-motive"></span>
<img src="13-SEM_files/figure-html/path-common-cause-legacy-motive-1.svg" alt="A common cause model" width="80%" />
<p class="caption">
Figure 13.6: A common cause model
</p>
</div>
<p>We now have three alternative models for the relations between legacy motive, intention, and donation. An obvious question is: which one provides the best account of the data?</p>
</div>
<div id="assumptions-and-estimation" class="section level2 hasAnchor" number="13.4">
<h2><span class="header-section-number">13.4</span> Assumptions and estimation<a href="ch-SEM-path-models.html#assumptions-and-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we consider how well a SEM accounts for the data, we first need to consider the assumptions underlying the model. Traditional SEM models assume the variables in the model follow a multivariate Normal distribution.</p>
<div id="the-multivariate-normal-distribution" class="section level3 hasAnchor" number="13.4.1">
<h3><span class="header-section-number">13.4.1</span> The multivariate Normal distribution<a href="ch-SEM-path-models.html#the-multivariate-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have introduced the multivariate Normal distribution in Chapter <a href="ch-linear-mixed-effects-models.html#ch-linear-mixed-effects-models">11</a>. A <strong>multivariate Normal distribution</strong> is a distribution over <em>vectors</em> of values. An example for a vector of two variables, e.g. <span class="math inline">\([X_1,Y_1]\)</span>, is shown in Figure <a href="ch-SEM-path-models.html#fig:bivariate-normal-distribution-interactive-plot">13.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bivariate-normal-distribution-interactive-plot"></span>
<style>#rgl36486 {margin:auto;}</style>
<div id="rgl36486" style="width:672px;height:480px;" class="rglWebGL html-widget " aria-labelledby="rgl36486-aria"></div>
<script type="application/json" data-for="rgl36486">{"x":{"material":{"color":"#000000","alpha":1,"lit":true,"ambient":"#000000","specular":"#FFFFFF","emission":"#000000","shininess":50,"smooth":true,"front":"filled","back":"filled","size":3,"lwd":1,"fog":true,"point_antialias":false,"line_antialias":false,"texture":null,"textype":"rgb","texmode":"modulate","texmipmap":false,"texminfilter":"linear","texmagfilter":"linear","texenvmap":false,"depth_mask":true,"depth_test":"less","isTransparent":false,"polygon_offset":[0,0],"margin":"","floating":false,"tag":"","blend":["src_alpha","one_minus_src_alpha"]},"rootSubscene":7,"objects":{"15":{"id":15,"type":"spheres","material":{},"vertices":"0","colors":"1","radii":[[0.1000000014901161]],"centers":"2","ignoreExtent":false,"fastTransparency":true,"flags":32771},"16":{"id":16,"type":"surface","material":{"front":"lines"},"vertices":"3","colors":"5","dim":[[40,40]],"centers":"6","normals":"4","ignoreExtent":false,"flipped":true,"flags":36867},"11":{"id":11,"type":"light","vertices":[[0,0,1]],"colors":[[1,1,1,1],[1,1,1,1],[1,1,1,1]],"viewpoint":true,"finite":false},"14":{"id":14,"type":"light","vertices":[[0,0.258819043636322,0.9659258127212524]],"colors":[[1,1,1,1],[1,1,1,1],[1,1,1,1]],"viewpoint":true,"finite":false},"10":{"id":10,"type":"background","material":{},"colors":"7","centers":"8","sphere":false,"fogtype":"none","fogscale":1,"flags":32768},"12":{"id":12,"type":"background","material":{"lit":false,"back":"lines"},"colors":"9","centers":"10","sphere":false,"fogtype":"none","fogscale":1,"flags":32768},"13":{"id":13,"type":"background","material":{"lit":false,"back":"lines"},"colors":"11","centers":"12","sphere":false,"fogtype":"none","fogscale":1,"flags":32768},"7":{"id":7,"type":"subscene","par3d":{"antialias":8,"FOV":30,"ignoreExtent":false,"listeners":7,"mouseMode":{"none":"none","left":"trackball","right":"zoom","middle":"fov","wheel":"pull"},"observer":[0,0,14.37485408782959],"modelMatrix":[[1,0,0,-0.04369843006134033],[0,0.3420201539993286,0.9396926164627075,-1.82487428188324],[0,-0.9396926164627075,0.3420201539993286,-15.02346706390381],[0,0,0,1]],"projMatrix":[[3.732050895690918,0,0,0],[0,3.732050895690918,0,0],[0,0,-3.863703489303589,-51.81968688964844],[0,0,-1,0]],"skipRedraw":false,"userMatrix":[[1,0,0,0],[0,0.3420201433256682,0.9396926207859085,0],[0,-0.9396926207859085,0.3420201433256682,0],[0,0,0,1]],"userProjection":[[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],"scale":[1,1,1],"viewport":{"x":0,"y":0,"width":1,"height":1},"zoom":1,"bbox":[-2.194757461547852,2.282154321670532,-2.149457693099976,2.178752183914185,-0.1000000014901161,3.973318815231323],"windowRect":[60,159,316,415],"family":"sans","font":1,"cex":1,"useFreeType":true,"fontname":"/home/maarten/R/x86_64-pc-linux-gnu-library/4.3/rgl/fonts/FreeSans.ttf","maxClipPlanes":8,"glVersion":4.6,"activeSubscene":0},"embeddings":{"viewport":"replace","projection":"replace","model":"replace","mouse":"replace"},"objects":[13,15,16,11,14],"subscenes":[],"flags":37123}},"crosstalk":{"key":[],"group":[],"id":[],"options":[]},"width":672,"height":480,"buffer":{"accessors":[{"bufferView":0,"componentType":5121,"count":1,"type":"VEC3"},{"bufferView":1,"componentType":5126,"count":1,"type":"VEC4"},{"bufferView":2,"componentType":5121,"count":1,"type":"VEC3"},{"bufferView":3,"componentType":5126,"count":1600,"type":"VEC3"},{"bufferView":4,"componentType":5126,"count":1600,"type":"VEC3"},{"bufferView":5,"componentType":5126,"count":1,"type":"VEC4"},{"bufferView":6,"componentType":5126,"count":1521,"type":"VEC3"},{"bufferView":7,"componentType":5126,"count":1,"type":"VEC4"},{"bufferView":8,"componentType":5121,"count":1,"type":"VEC3"},{"bufferView":9,"componentType":5121,"count":1,"type":"VEC4"},{"bufferView":10,"componentType":5121,"count":1,"type":"VEC3"},{"bufferView":11,"componentType":5126,"count":1,"type":"VEC4"},{"bufferView":12,"componentType":5121,"count":1,"type":"VEC3"}],"bufferViews":[{"buffer":0,"byteLength":3,"byteOffset":0},{"buffer":0,"byteLength":16,"byteOffset":4},{"buffer":0,"byteLength":3,"byteOffset":20},{"buffer":0,"byteLength":19200,"byteOffset":24},{"buffer":0,"byteLength":19200,"byteOffset":19224},{"buffer":0,"byteLength":16,"byteOffset":38424},{"buffer":0,"byteLength":18252,"byteOffset":38440},{"buffer":0,"byteLength":16,"byteOffset":56692},{"buffer":0,"byteLength":3,"byteOffset":56708},{"buffer":0,"byteLength":4,"byteOffset":56711},{"buffer":0,"byteLength":3,"byteOffset":56715},{"buffer":0,"byteLength":16,"byteOffset":56720},{"buffer":0,"byteLength":3,"byteOffset":56736}],"buffers":[{"byteLength":56739,"bytes":"AAAAAM3MTD/NzEw/AACAPwAAgD8AAAAA6HYMwLeQCcCEtR48JR4FwLeQCcB710o8xIr7v7eQ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\nmZiYPgAAgD8AAAABAQEBAAAAAACJiAg/7+7uPu/u7j4AAIA/AAAA"}]},"context":{"shiny":false,"rmarkdown":null},"vertexShader":"#line 2 1\n// File 1 is the vertex shader\n#ifdef GL_ES\n#ifdef GL_FRAGMENT_PRECISION_HIGH\nprecision highp float;\n#else\nprecision mediump float;\n#endif\n#endif\n\nattribute vec3 aPos;\nattribute vec4 aCol;\nuniform mat4 mvMatrix;\nuniform mat4 prMatrix;\nvarying vec4 vCol;\nvarying vec4 vPosition;\n\n#ifdef NEEDS_VNORMAL\nattribute vec3 aNorm;\nuniform mat4 normMatrix;\nvarying vec4 vNormal;\n#endif\n\n#if defined(HAS_TEXTURE) || defined (IS_TEXT)\nattribute vec2 aTexcoord;\nvarying vec2 vTexcoord;\n#endif\n\n#ifdef FIXED_SIZE\nuniform vec3 textScale;\n#endif\n\n#ifdef FIXED_QUADS\nattribute vec3 aOfs;\n#endif\n\n#ifdef IS_TWOSIDED\n#ifdef HAS_NORMALS\nvarying float normz;\nuniform mat4 invPrMatrix;\n#else\nattribute vec3 aPos1;\nattribute vec3 aPos2;\nvarying float normz;\n#endif\n#endif // IS_TWOSIDED\n\n#ifdef FAT_LINES\nattribute vec3 aNext;\nattribute vec2 aPoint;\nvarying vec2 vPoint;\nvarying float vLength;\nuniform float uAspect;\nuniform float uLwd;\n#endif\n\n#ifdef USE_ENVMAP\nvarying vec3 vReflection;\n#endif\n\nvoid main(void) {\n  \n#ifndef IS_BRUSH\n#if defined(NCLIPPLANES) || !defined(FIXED_QUADS) || defined(HAS_FOG) || defined(USE_ENVMAP)\n  vPosition = mvMatrix * vec4(aPos, 1.);\n#endif\n  \n#ifndef FIXED_QUADS\n  gl_Position = prMatrix * vPosition;\n#endif\n#endif // !IS_BRUSH\n  \n#ifdef IS_POINTS\n  gl_PointSize = POINTSIZE;\n#endif\n  \n  vCol = aCol;\n  \n// USE_ENVMAP implies NEEDS_VNORMAL\n\n#ifdef NEEDS_VNORMAL\n  vNormal = normMatrix * vec4(-aNorm, dot(aNorm, aPos));\n#endif\n\n#ifdef USE_ENVMAP\n  vReflection = normalize(reflect(vPosition.xyz/vPosition.w, \n                        normalize(vNormal.xyz/vNormal.w)));\n#endif\n  \n#ifdef IS_TWOSIDED\n#ifdef HAS_NORMALS\n  /* normz should be calculated *after* projection */\n  normz = (invPrMatrix*vNormal).z;\n#else\n  vec4 pos1 = prMatrix*(mvMatrix*vec4(aPos1, 1.));\n  pos1 = pos1/pos1.w - gl_Position/gl_Position.w;\n  vec4 pos2 = prMatrix*(mvMatrix*vec4(aPos2, 1.));\n  pos2 = pos2/pos2.w - gl_Position/gl_Position.w;\n  normz = pos1.x*pos2.y - pos1.y*pos2.x;\n#endif\n#endif // IS_TWOSIDED\n  \n#ifdef NEEDS_VNORMAL\n  vNormal = vec4(normalize(vNormal.xyz/vNormal.w), 1);\n#endif\n  \n#if defined(HAS_TEXTURE) || defined(IS_TEXT)\n  vTexcoord = aTexcoord;\n#endif\n  \n#if defined(FIXED_SIZE) && !defined(ROTATING)\n  vec4 pos = prMatrix * mvMatrix * vec4(aPos, 1.);\n  pos = pos/pos.w;\n  gl_Position = pos + vec4(aOfs*textScale, 0.);\n#endif\n  \n#if defined(IS_SPRITES) && !defined(FIXED_SIZE)\n  vec4 pos = mvMatrix * vec4(aPos, 1.);\n  pos = pos/pos.w + vec4(aOfs,  0.);\n  gl_Position = prMatrix*pos;\n#endif\n  \n#ifdef FAT_LINES\n  /* This code was inspired by Matt Deslauriers' code in \n   https://mattdesl.svbtle.com/drawing-lines-is-hard */\n  vec2 aspectVec = vec2(uAspect, 1.0);\n  mat4 projViewModel = prMatrix * mvMatrix;\n  vec4 currentProjected = projViewModel * vec4(aPos, 1.0);\n  currentProjected = currentProjected/currentProjected.w;\n  vec4 nextProjected = projViewModel * vec4(aNext, 1.0);\n  vec2 currentScreen = currentProjected.xy * aspectVec;\n  vec2 nextScreen = (nextProjected.xy / nextProjected.w) * aspectVec;\n  float len = uLwd;\n  vec2 dir = vec2(1.0, 0.0);\n  vPoint = aPoint;\n  vLength = length(nextScreen - currentScreen)/2.0;\n  vLength = vLength/(vLength + len);\n  if (vLength > 0.0) {\n    dir = normalize(nextScreen - currentScreen);\n  }\n  vec2 normal = vec2(-dir.y, dir.x);\n  dir.x /= uAspect;\n  normal.x /= uAspect;\n  vec4 offset = vec4(len*(normal*aPoint.x*aPoint.y - dir), 0.0, 0.0);\n  gl_Position = currentProjected + offset;\n#endif\n  \n#ifdef IS_BRUSH\n  gl_Position = vec4(aPos, 1.);\n#endif\n}","fragmentShader":"#line 2 2\n// File 2 is the fragment shader\n#ifdef GL_ES\n#ifdef GL_FRAGMENT_PRECISION_HIGH\nprecision highp float;\n#else\nprecision mediump float;\n#endif\n#endif\nvarying vec4 vCol; // carries alpha\nvarying vec4 vPosition;\n#if defined(HAS_TEXTURE) || defined (IS_TEXT)\nvarying vec2 vTexcoord;\nuniform sampler2D uSampler;\n#endif\n\n#ifdef HAS_FOG\nuniform int uFogMode;\nuniform vec3 uFogColor;\nuniform vec4 uFogParms;\n#endif\n\n#if defined(IS_LIT) && !defined(FIXED_QUADS)\nvarying vec4 vNormal;\n#endif\n\n#if NCLIPPLANES > 0\nuniform vec4 vClipplane[NCLIPPLANES];\n#endif\n\n#if NLIGHTS > 0\nuniform mat4 mvMatrix;\n#endif\n\n#ifdef IS_LIT\nuniform vec3 emission;\nuniform float shininess;\n#if NLIGHTS > 0\nuniform vec3 ambient[NLIGHTS];\nuniform vec3 specular[NLIGHTS]; // light*material\nuniform vec3 diffuse[NLIGHTS];\nuniform vec3 lightDir[NLIGHTS];\nuniform bool viewpoint[NLIGHTS];\nuniform bool finite[NLIGHTS];\n#endif\n#endif // IS_LIT\n\n#ifdef IS_TWOSIDED\nuniform bool front;\nvarying float normz;\n#endif\n\n#ifdef FAT_LINES\nvarying vec2 vPoint;\nvarying float vLength;\n#endif\n\n#ifdef USE_ENVMAP\nvarying vec3 vReflection;\n#endif\n\nvoid main(void) {\n  vec4 fragColor;\n#ifdef FAT_LINES\n  vec2 point = vPoint;\n  bool neg = point.y < 0.0;\n  point.y = neg ? (point.y + vLength)/(1.0 - vLength) :\n                 -(point.y - vLength)/(1.0 - vLength);\n#if defined(IS_TRANSPARENT) && defined(IS_LINESTRIP)\n  if (neg && length(point) <= 1.0) discard;\n#endif\n  point.y = min(point.y, 0.0);\n  if (length(point) > 1.0) discard;\n#endif // FAT_LINES\n  \n#ifdef ROUND_POINTS\n  vec2 coord = gl_PointCoord - vec2(0.5);\n  if (length(coord) > 0.5) discard;\n#endif\n  \n#if NCLIPPLANES > 0\n  for (int i = 0; i < NCLIPPLANES; i++)\n    if (dot(vPosition, vClipplane[i]) < 0.0) discard;\n#endif\n    \n#ifdef FIXED_QUADS\n    vec3 n = vec3(0., 0., 1.);\n#elif defined(IS_LIT)\n    vec3 n = normalize(vNormal.xyz);\n#endif\n    \n#ifdef IS_TWOSIDED\n    if ((normz <= 0.) != front) discard;\n#endif\n\n#ifdef IS_LIT\n    vec3 eye = normalize(-vPosition.xyz/vPosition.w);\n    vec3 lightdir;\n    vec4 colDiff;\n    vec3 halfVec;\n    vec4 lighteffect = vec4(emission, 0.);\n    vec3 col;\n    float nDotL;\n#ifdef FIXED_QUADS\n    n = -faceforward(n, n, eye);\n#endif\n    \n#if NLIGHTS > 0\n    for (int i=0;i<NLIGHTS;i++) {\n      colDiff = vec4(vCol.rgb * diffuse[i], vCol.a);\n      lightdir = lightDir[i];\n      if (!viewpoint[i])\n        lightdir = (mvMatrix * vec4(lightdir, 1.)).xyz;\n      if (!finite[i]) {\n        halfVec = normalize(lightdir + eye);\n      } else {\n        lightdir = normalize(lightdir - vPosition.xyz/vPosition.w);\n        halfVec = normalize(lightdir + eye);\n      }\n      col = ambient[i];\n      nDotL = dot(n, lightdir);\n      col = col + max(nDotL, 0.) * colDiff.rgb;\n      col = col + pow(max(dot(halfVec, n), 0.), shininess) * specular[i];\n      lighteffect = lighteffect + vec4(col, colDiff.a);\n    }\n#endif\n    \n#else // not IS_LIT\n    vec4 colDiff = vCol;\n    vec4 lighteffect = colDiff;\n#endif\n    \n#ifdef IS_TEXT\n    vec4 textureColor = lighteffect*texture2D(uSampler, vTexcoord);\n#endif\n    \n#ifdef HAS_TEXTURE\n\n// These calculations use the definitions from \n// https://docs.gl/gl3/glTexEnv\n\n#ifdef USE_ENVMAP\n    float m = 2.0 * sqrt(dot(vReflection, vReflection) + 2.0*vReflection.z + 1.0);\n    vec4 textureColor = texture2D(uSampler, vReflection.xy / m + vec2(0.5, 0.5));\n#else\n    vec4 textureColor = texture2D(uSampler, vTexcoord);\n#endif\n\n#ifdef TEXTURE_rgb\n\n#if defined(TEXMODE_replace) || defined(TEXMODE_decal)\n    textureColor = vec4(textureColor.rgb, lighteffect.a);\n#endif \n\n#ifdef TEXMODE_modulate\n    textureColor = lighteffect*vec4(textureColor.rgb, 1.);\n#endif\n\n#ifdef TEXMODE_blend\n    textureColor = vec4((1. - textureColor.rgb) * lighteffect.rgb, lighteffect.a);\n#endif\n\n#ifdef TEXMODE_add\n    textureColor = vec4(lighteffect.rgb + textureColor.rgb, lighteffect.a);\n#endif\n\n#endif //TEXTURE_rgb\n        \n#ifdef TEXTURE_rgba\n\n#ifdef TEXMODE_replace\n// already done\n#endif \n\n#ifdef TEXMODE_modulate\n    textureColor = lighteffect*textureColor;\n#endif\n\n#ifdef TEXMODE_decal\n    textureColor = vec4((1. - textureColor.a)*lighteffect.rgb) +\n                     textureColor.a*textureColor.rgb, \n                     lighteffect.a);\n#endif\n\n#ifdef TEXMODE_blend\n    textureColor = vec4((1. - textureColor.rgb) * lighteffect.rgb,\n                    lighteffect.a*textureColor.a);\n#endif\n\n#ifdef TEXMODE_add\n    textureColor = vec4(lighteffect.rgb + textureColor.rgb,\n                    lighteffect.a*textureColor.a);\n#endif\n    \n#endif //TEXTURE_rgba\n    \n#ifdef TEXTURE_alpha\n#if defined(TEXMODE_replace) || defined(TEXMODE_decal)\n    textureColor = vec4(lighteffect.rgb, textureColor.a);\n#endif \n\n#if defined(TEXMODE_modulate) || defined(TEXMODE_blend) || defined(TEXMODE_add)\n    textureColor = vec4(lighteffect.rgb, lighteffect.a*textureColor.a);\n#endif\n \n#endif\n    \n// The TEXTURE_luminance values are not from that reference    \n#ifdef TEXTURE_luminance\n    float luminance = dot(vec3(1.,1.,1.),textureColor.rgb)/3.;\n\n#if defined(TEXMODE_replace) || defined(TEXMODE_decal)\n    textureColor = vec4(luminance, luminance, luminance, lighteffect.a);\n#endif \n\n#ifdef TEXMODE_modulate\n    textureColor = vec4(luminance*lighteffect.rgb, lighteffect.a);\n#endif\n\n#ifdef TEXMODE_blend\n    textureColor = vec4((1. - luminance)*lighteffect.rgb,\n                        lighteffect.a);\n#endif\n\n#ifdef TEXMODE_add\n    textureColor = vec4(luminance + lighteffect.rgb, lighteffect.a);\n#endif\n\n#endif // TEXTURE_luminance\n \n    \n#ifdef TEXTURE_luminance_alpha\n    float luminance = dot(vec3(1.,1.,1.),textureColor.rgb)/3.;\n\n#if defined(TEXMODE_replace) || defined(TEXMODE_decal)\n    textureColor = vec4(luminance, luminance, luminance, textureColor.a);\n#endif \n\n#ifdef TEXMODE_modulate\n    textureColor = vec4(luminance*lighteffect.rgb, \n                        textureColor.a*lighteffect.a);\n#endif\n\n#ifdef TEXMODE_blend\n    textureColor = vec4((1. - luminance)*lighteffect.rgb,\n                        textureColor.a*lighteffect.a);\n#endif\n\n#ifdef TEXMODE_add\n    textureColor = vec4(luminance + lighteffect.rgb, \n                        textureColor.a*lighteffect.a);\n\n#endif\n\n#endif // TEXTURE_luminance_alpha\n    \n    fragColor = textureColor;\n\n#elif defined(IS_TEXT)\n    if (textureColor.a < 0.1)\n      discard;\n    else\n      fragColor = textureColor;\n#else\n    fragColor = lighteffect;\n#endif // HAS_TEXTURE\n    \n#ifdef HAS_FOG\n    // uFogParms elements: x = near, y = far, z = fogscale, w = (1-sin(FOV/2))/(1+sin(FOV/2))\n    // In Exp and Exp2: use density = density/far\n    // fogF will be the proportion of fog\n    // Initialize it to the linear value\n    float fogF;\n    if (uFogMode > 0) {\n      fogF = (uFogParms.y - vPosition.z/vPosition.w)/(uFogParms.y - uFogParms.x);\n      if (uFogMode > 1)\n        fogF = mix(uFogParms.w, 1.0, fogF);\n      fogF = fogF*uFogParms.z;\n      if (uFogMode == 2)\n        fogF = 1.0 - exp(-fogF);\n      // Docs are wrong: use (density*c)^2, not density*c^2\n      // https://gitlab.freedesktop.org/mesa/mesa/-/blob/master/src/mesa/swrast/s_fog.c#L58\n      else if (uFogMode == 3)\n        fogF = 1.0 - exp(-fogF*fogF);\n      fogF = clamp(fogF, 0.0, 1.0);\n      gl_FragColor = vec4(mix(fragColor.rgb, uFogColor, fogF), fragColor.a);\n    } else gl_FragColor = fragColor;\n#else\n    gl_FragColor = fragColor;\n#endif // HAS_FOG\n    \n}","players":[],"webGLoptions":{"preserveDrawingBuffer":true}},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 13.7: A multivariate Normal distribution for two variables. You can rotate the plot with a mouse by clicking and moving the mouse.
</p>
</div>
<p>A multivariate Normal distribution is parametrized by a mean vector <span class="math inline">\(\boldsymbol{{\mu}}\)</span> and a variance-covariance matrix <span class="math inline">\(\boldsymbol{{\Sigma}}\)</span>.</p>
<p>The <strong>mean vector</strong> contains the means for all variables in the model</p>
<p><span class="math display">\[\boldsymbol{{\mu}} = \left[ \begin{matrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_P \end{matrix} \right]\]</span>
In a path model, the means are determined by the intercepts and regressions from parents of variables.</p>
<p>The <strong>variance-covariance matrix</strong> contains the variances <span class="math inline">\(\sigma_j^2\)</span> of each variable in the model, as well as the covariances <span class="math inline">\(\sigma_{i,j}\)</span> between all pairs of variables in the model:
<span class="math display">\[\boldsymbol{{\Sigma}} = \left[ \begin{matrix} \sigma^2_1 &amp; \sigma_{1,2} &amp; \ldots &amp; \sigma_{1,P} \\ \sigma_{2,1} &amp; \sigma_{2}^2 &amp; \ldots &amp; \sigma_{2,P} \\ \vdots &amp; \vdots &amp; &amp; \vdots \\ \sigma_{P,1} &amp; \sigma_{P,2} &amp; \ldots &amp; \sigma_{P}^2 \end{matrix} \right]\]</span>
The theoretical variances and covariances, as implied by a SEM model, are determined by regression relations and (residual) variances. For example, in a simple regression model
<span class="math display">\[Y = \beta_0 + \beta_x \times X + \epsilon \quad \quad \epsilon \sim \mathbf{Normal}(0, \sigma_\epsilon)\]</span>
the implied theoretical variance of <span class="math inline">\(Y\)</span>, denoted as <span class="math inline">\(\sigma^2_y\)</span>, is a function of the regression coefficient <span class="math inline">\(\beta_x\)</span>, the variance of <span class="math inline">\(X\)</span> <span class="math inline">\((\sigma_x^2)\)</span>, and the residual variance <span class="math inline">\((\sigma_\epsilon^2)\)</span>:
<span class="math display">\[\sigma_y^2 = \beta^2_x \times \sigma_x^2 + \sigma^2_\epsilon\]</span>
The covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a function of the regression coefficient and the variance of <span class="math inline">\(X\)</span>:
<span class="math display">\[\sigma_{x,y} = \beta_x \times \sigma_x^2\]</span>
As <span class="math inline">\(X\)</span> is an exogenous variable, its variance is not implied by the model. If <span class="math inline">\(X\)</span> is considered fixed, then its true variance <span class="math inline">\(\sigma_x^2\)</span> can be computed from the data directly.</p>
<p>A multivariate Normal distribution is completely determined by the mean vector and variance-covariance matrix. The basic idea underlying estimation of SEM model parameters is to minimize the discrepancy between the sample means and (co-)variances, and the model-implied theoretical means and (co-)variances. There are a number of ways in which to do this. The most commonly employed way is by maximum likelihood. If the data follows a multivariate distribution, it can be shown that the likelihood is a function of the sample means <span class="math inline">\(\overline{\mathbf{C}} = (\overline{X}_1, \ldots, \overline{X}_m, \overline{Y}_1, \ldots, \overline{Y}_k)\)</span> (i.e. <span class="math inline">\(\overline{\mathbf{C}}\)</span> is a vector with the sample averages for all <span class="math inline">\(m\)</span> exogenous variables and <span class="math inline">\(k\)</span> endogenous variables), the sample variance-covariance matrix <span class="math inline">\(\mathbf{S}\)</span> (a <span class="math inline">\(P\)</span> by <span class="math inline">\(P\)</span> matrix, where <span class="math inline">\(P=m+k\)</span>, with all the covariances of the endogenous and exogenous variables and their variances on the diagonal), and the model-implied mean vector <span class="math inline">\(\boldsymbol{\mu}(\theta)\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}(\theta)\)</span>, where <span class="math inline">\(\theta\)</span> denotes all the parameters in the model.<a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a> This implies that you don’t need the full dataset to estimate a SEM model by maximum likelihood: you just need the sample means and (co-)variances.</p>
<p>The assumption of multivariate Normality is necessary to derive maximum likelihood estimators, and corresponding standard errors of the estimates and sampling distributions for test statistics. Estimation and tests are however reasonably robust against (mild) deviation from multivariate Normality, e.g. as long as the distribution is not overly skewed <span class="citation">(<a href="#ref-bollen2011structural">Bollen &amp; Noble, 2011</a>)</span>. Alternatives to maximum likelihood are Generalized Least Squares (GLS), which is also based on the assumption of multivariate Normality, and Weighted Least Squares (WLS; also called asymptotically distribution free or ADF), which does not require the assumption of multivariate Normality.
<!-- see https://web.pdx.edu/~newsomj/semclass/ho_estimate.pdf for estimation methods --></p>
<div id="sec-SEM-path-models-variance-covariance-algebra" class="section level4 hasAnchor" number="13.4.1.1">
<h4><span class="header-section-number">13.4.1.1</span> Variance-Covariance algebra<a href="ch-SEM-path-models.html#sec-SEM-path-models-variance-covariance-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The elements of the model-implied variance-covariance matrix can be determined using the following rules for variances and covariances (where <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(Z\)</span> are variables, and <span class="math inline">\(c\)</span> a constant):</p>
<p><span class="math display">\[
\begin{aligned}
(1) &amp;&amp; \text{Var}(X + Y) &amp;= \text{Var}(X) + \text{Var}(Y) + 2 \times \text{Cov}(X,Y) \\
(2) &amp;&amp; \text{Var}(X - Y) &amp;= \text{Var}(X) + \text{Var}(Y) - 2 \times \text{Cov}(X,Y) \\
(3) &amp;&amp; \text{Var}(c + Y) &amp;= \text{Var}(Y) \\
(4) &amp;&amp;\text{Var}(c \times Y) &amp;= c^2 \times \text{Var}(Y) \\
(5) &amp;&amp;\text{Cov}(c, Y) &amp;= 0 \\
(6) &amp;&amp;\text{Cov}(c\times X, Y) &amp;= c \times \text{Cov}(X,Y) \\
(7) &amp;&amp;\text{Cov}(X+Y, Z) &amp;= \text{Cov}(X,Z) + \text{Cov}(Y,Z)
\end{aligned}
\]</span>
For example, if we take the simple regression model:
<span class="math display">\[Y = \beta_0 + \beta_x \times X + \epsilon_y \quad \quad \epsilon_y \sim \mathbf{Normal}(0, \sigma_{\epsilon_y})\]</span>
we can use rule 4 to work out the variance of <span class="math inline">\(\beta_x \times X\)</span> (which is <span class="math inline">\(\beta_x^2 \times \text{Var}(X)\)</span>), and then apply rule 1 to work out the variance of the sum <span class="math inline">\(\beta_x \times X + \epsilon_y\)</span> (which is <span class="math inline">\(\beta_x^2 \times \text{Var}(X) + \sigma^2_{\epsilon_y}\)</span>), and then use rule 3 to work out the variance of <span class="math inline">\(\beta_0 + \beta_x \times X + \epsilon_y\)</span> (which remains <span class="math inline">\(\beta_x^2 \times \text{Var}(X) + \sigma^2_{\epsilon_y}\)</span>). The model-implied variance is thus <span class="math inline">\(\sigma^2_y = \beta_x^2 \times \sigma^2_x + \sigma^2_{\epsilon_y}\)</span>.</p>
<p>In principle, these rules can be used recursively to work out the total implied variance-covariance matrix for a given model. This can become rather tedious, however. Matrix algebra provides an easier way to determine the predicted mean vector and variance-covariance matrix. However, an introduction to matrix algebra is beyond the scope of this book. The key thing to understand is that SEM models imply a variance-covariance matrix and mean vector, which can be compared to the sample variance-covariance matrix and mean vector.</p>
<!-- TODO: could consider Wright's path tracing rules. See e.g. http://sachaepskamp.com/files/SEM2_2020/SEM2Week1_2020_path.pdf -->
</div>
</div>
<div id="assumptions-exogenous-vs-endogenous-variables" class="section level3 hasAnchor" number="13.4.2">
<h3><span class="header-section-number">13.4.2</span> Assumptions: Exogenous vs endogenous variables<a href="ch-SEM-path-models.html#assumptions-exogenous-vs-endogenous-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The assumption that the set of <em>all</em> variables in the model follows a multivariate Normal distribution is different from the assumptions of multiple regression and the General Linear Model. In the GLM, the assumptions solely concern the <em>residuals</em> of the dependent variable. No assumptions have to be made about the distributions of the predictors. So how can a SEM model be equivalent to a GLM?</p>
<p>The easiest way to deal with non-normal exogenous variables is to treat them as fixed. As fixed variables are not random and as such have no probability distribution, they don’t need to be included in the calculation of the likelihood (although they are needed to calculate the model-implied parameters of the endogenous variables). Assuming exogenous variables are fixed allows you to also include <em>categorical</em> exogenous variables, using e.g. contrast-codes. Fixed exogenous variables are still included in the sample and model-implied mean vectors and variance-covariance matrices. However, the model-implied values are identical to the corresponding sample values, as these parameters can be computed from the data. As a result, for fixed exogenous variables, the fit is perfect.</p>
<p>Exogenous variables can be treated as random as well, in which case their sample means and (co-)variances should be treated as estimates of the corresponding true (or “population”) parameters. In that case, assumptions about their distribution are important. In general however, the assumption of multivariate Normality is mainly of importance for the <em>endogenous</em> variables. If it cannot be assumed that they are approximately (conditionally) multivariate Normal-distributed, special techniques should be employed to account for this. This is particularly relevant for dichotomous or polytomous endogenous variables. Specialised software (e.g. <a href="https://www.statmodel.com/">Mplus</a>) can estimate SEM models with such categorical endogenous variables. Currently, the R package <a href="https://www.lavaan.ugent.be/">lavaan</a> can deal with <em>ordinal</em> categorical variables. Here, we will focus on continuous endogenous variables which can be assumed to follow a multivariate Normal distribution.
<!--
So what do we do if some of our variables are not Normal-distributed? Well, it depends. If these variables are *exogenous* variables, there is no need to worry. The target of ML estimation for SEMs is the sample mean vector and variance-covariance matrix. By including the prerequisite parameters, we can fit the mean and variance of these exogenous variables perfectly. As such, deviations from Normality for exogenous variables don't affect the fit of the model; it also does not affect the parameter estimates. 

For *endogenous* variables, however, we do need a similar assumption as in the GLM, namely that the conditional distribution follows a Normal distribution, with a constant variance. In practice, this implies that the residuals of the endogenous variables should follow a multivariate Normal distribution. --></p>
</div>
</div>
<div id="model-fit" class="section level2 hasAnchor" number="13.5">
<h2><span class="header-section-number">13.5</span> Model fit<a href="ch-SEM-path-models.html#model-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A first consideration for any SEM model is whether it describes the observed data well. As in any statistical model, describing the data well means that the implied distribution of the data is more-or-less equal to the empirical distribution of the data.</p>
<div id="test-of-overall-model-fit" class="section level3 hasAnchor" number="13.5.1">
<h3><span class="header-section-number">13.5.1</span> Test of overall model fit<a href="ch-SEM-path-models.html#test-of-overall-model-fit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A test of overall model fit is similar to that used for generalized linear models (see Section <a href="ch-generalized-linear-models.html#sec-generalized-linear-models-overall-model-fit">12.4</a>), which is based on likelihood-ratio test comparing the model under consideration (MODEL M) to a saturated MODEL S:
<span class="math display">\[\begin{aligned}
\hat{\chi}^2_M &amp;= -2 \log \frac{p(\text{DATA}|\text{MODEL M})}{p(\text{DATA}|\text{MODEL S})}
\end{aligned}\]</span>
Note that we use the symbol <span class="math inline">\(\hat{\chi}^2\)</span> here, rather than the residual deviance <span class="math inline">\(D_R\)</span> used in the context of generalized linear models, as the former is more common in the SEM literature. As usual, this statistic approximately follows a Chi-squared distribution with degrees of freedom equal to <span class="math inline">\(\text{df} = \text{npar}(S) - \text{npar}(M)\)</span>. The saturated MODEL S sets <span class="math inline">\(\boldsymbol{\Sigma}(\theta) = \mathbf{S}\)</span> and <span class="math inline">\(\boldsymbol{\mu}(\theta) = \overline{C}\)</span>. In words, for the saturated model, the model-implied means and (co-)variances are set to their sample values. For a model with a total of <span class="math inline">\(P = m+k\)</span> observed variables, the saturated MODEL S therefore uses a total of
<span class="math display" id="eq:npar-saturated-sem-model">\[\begin{equation}
\text{npar}(S) = \frac{P \times (P - 1)}{2} + 2 \times P
\tag{13.1}
\end{equation}\]</span>
parameters. The covariance matrix contains <span class="math inline">\(\frac{P \times (P - 1)}{2} + P\)</span> unique terms (<span class="math inline">\(P\)</span> variances, and <span class="math inline">\(\frac{P \times (P-1)}{2}\)</span> unique covariances<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a>). The mean vector contains <span class="math inline">\(P\)</span> unique parameters (a mean for each variable). Adding these up provides the specification above.</p>
<p>If the model under consideration uses less parameters, then the test result can provide a meaningful assessment of the overall model fit.
There are some issues however. Particularly, the number of observations <span class="math inline">\(n\)</span> should be sufficient (e.g. <span class="math inline">\(n &gt; 200\)</span>), and the data should follow a multivariate Normal distribution. But if the number of observations <span class="math inline">\(n\)</span> is very large, the test becomes very powerful. The test will then often be significant, even when the model provides a good (but not perfect) account of the data. Due to these issues (and others), a wide variety of approximate fit indices have been proposed.</p>
</div>
<div id="approximate-fit-indices" class="section level3 hasAnchor" number="13.5.2">
<h3><span class="header-section-number">13.5.2</span> Approximate fit indices<a href="ch-SEM-path-models.html#approximate-fit-indices" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fit indices can be grouped into those that concern comparative fit to a baseline model and measures based on errors of approximation <span class="citation">(<a href="#ref-kaplan2001sem">Kaplan, 2001</a>)</span>. We will discuss commonly used measures in each group in turn.</p>
<div id="comparative-fit-to-a-baseline" class="section level4 hasAnchor" number="13.5.2.1">
<h4><span class="header-section-number">13.5.2.1</span> Comparative fit to a baseline<a href="ch-SEM-path-models.html#comparative-fit-to-a-baseline" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The idea behind measures that compare the fit of a MODEL M to that of a baseline MODEL B is that a simple baseline model may already fit the data well. We should therefore be interested in how much the added complexity of MODEL M contributes. The baseline MODEL B is usually one which specifies complete independence between the observed variables (i.e. all the covariances are equal to 0).</p>
<p>Comparing a MODEL M to a baseline MODEL B via a likelihood-ratio test is a first option. This leads to the baseline Chi-squared test:
<span class="math display">\[\begin{aligned}
\text{baseline } \hat{\chi}^2_M &amp;= -2 \log \frac{p(\text{DATA}|\text{MODEL B})}{p(\text{DATA}|\text{MODEL M})}
\end{aligned}\]</span>
with <span class="math inline">\(\text{df} = \text{npar}(B) - \text{npar}(M)\)</span>. This statistic can be used to test the null-hypothesis that the baseline MODEL B describes the data equally well as the more complex MODEL M. One would generally like to reject this null-hypothesis (so a significant test result is good).</p>
<p>The <strong>normed fit index</strong> (NFI) is computed from the overall model fit tests of MODEL M and MODEL B as a ratio similar to that of the <span class="math inline">\(R^2\)</span> measure for the GLM:
<span class="math display" id="eq:sem-NFI-definition">\[\begin{equation}
\text{NFI} = \frac{\hat{\chi}^2_B - \hat{\chi}^2_M}{\hat{\chi}^2_B}
\tag{13.2}
\end{equation}\]</span>
Note that as the baseline MODEL B is simpler than MODEL M, it can never perform better, so <span class="math inline">\(\hat{\chi}^2_B \geq \hat{\chi}^2_M\)</span>. The NFI therefore ranges between 0 (same fit as baseline model) and 1 (perfect fit compared to the baseline model). Values of the NFI larger than .95 are considered to indicate a good fit of the model.</p>
<p>The <strong>comparative fit index</strong> (CFI) is another common fit index comparing a model to a baseline. Like the NFI, it uses the overall model fit statistics <span class="math inline">\(\hat{\chi}^2_M\)</span> and <span class="math inline">\(\hat{\chi}^2_B\)</span>, but considers how these deviate from their expected values (under the null-hypothesis that each model is equal to MODEL S). For a Chi-squared distribution, the expected value (the mean) is equal to the degrees of freedom. Hence, the expected values are <span class="math inline">\(\text{df}_M = \text{npar}(S) - \text{npar}(M)\)</span> and <span class="math inline">\(\text{df}_B = \text{npar}(S) - \text{npar}(B)\)</span> respectively. The CFI is computed as
<span class="math display" id="eq:sem-CFI-definition">\[\begin{equation}
\text{CFI} = \frac{\max(\hat{\chi}^2_B - \text{df}_B, 0) - \max(\hat{\chi}^2_M - \text{df}_M, 0)}{\max(\hat{\chi}^2_B - \text{df}_B, 0)}
\tag{13.3}
\end{equation}\]</span>
The values of the CFI range between 0 (bad) and 1 (good), and a value larger than .95 can be considered satisfactory <span class="citation">(<a href="#ref-hu1999cutoff">Hu &amp; Bentler, 1999</a>)</span>.</p>
<p>The <strong>Tucker-Lewis index</strong> (TLI) has a similar aim to the CFI. The TLI (also called the non-normed fit index, NNFI) is computed as:
<span class="math display" id="eq:sem-TLI-definition">\[\begin{equation}
\text{TLI} = \frac{\hat{\chi}^2_B/\text{df}_B - \hat{\chi}^2_M/\text{df}_M}{\hat{\chi}^2_B/\text{df}_B - 1}
\tag{13.4}
\end{equation}\]</span>
The TLI is often interpreted as a measure between 0 (bad) and 1 (good), although it technically can have values below 0 or above 1. In any case, a value larger than .95 can be considered satisfactory <span class="citation">(<a href="#ref-hu1999cutoff">Hu &amp; Bentler, 1999</a>)</span>.</p>
</div>
<div id="errors-of-approximation" class="section level4 hasAnchor" number="13.5.2.2">
<h4><span class="header-section-number">13.5.2.2</span> Errors of approximation<a href="ch-SEM-path-models.html#errors-of-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>Standardized Root Mean Residual</strong> (SRMR) is an absolute measure of fit. It is a standardized measure of the discrepancy between the sample (co-)variances <span class="math inline">\(s_{ij}\)</span> and the model-implied (co-)variances <span class="math inline">\(\sigma_{ij}(\theta)\)</span>. This measure can be computed as <span class="citation">(cf. <a href="#ref-hu1999cutoff">Hu &amp; Bentler, 1999</a>)</span>:
<span class="math display" id="eq:sem-SRMR-definition">\[\begin{equation}
\text{SRMR} = \sqrt{\frac{2 \sum_{i=2}^T \sum_{j=1}^i \left(\frac{s_{ij} - \sigma_{ij}(\theta)}{s_{ii} \times s_{jj}}\right)^2}{P \times (P+1)}}
\tag{13.5}
\end{equation}\]</span>
If the model matches the sample (co-)variances perfectly, then the value would be 0. Smaller values indicate better fit, and a value of <span class="math inline">\(\text{SRMR} \leq .08\)</span> is considered satisfactory <span class="citation">(<a href="#ref-hu1999cutoff">Hu &amp; Bentler, 1999</a>)</span>.</p>
<p>Like the SRMR, the <strong>Root Mean Square Error of Approximation</strong> (RMSEA) is a an absolute fit index, with 0 indicating perfect fit. It is based on <span class="math inline">\(\hat{\chi}^2_M\)</span>, the likelihood-ratio statistic comparing a MODEL M to the saturated MODEL S. But like the TLI, it considers the deviation of this statistic from its expected value (under the null-hypothesis that MODEL M is equal to MODEL S), which is <span class="math inline">\(\text{df}_M = \text{npar}(S) - \text{npar}(M)\)</span>. The test statistic would never be significant for a MODEL M with <span class="math inline">\(\hat{\chi}^2_M \leq \text{df}_M\)</span>. For such a model, the RMSEA measure is set to 0. For other models, the measure is larger than 0, with its magnitude reflecting “misfit”, corrected for the degrees of freedom. The RMSEA is defined as:
<span class="math display" id="eq:sem-RMSEA-definition">\[\begin{equation}
\text{RMSEA} = \sqrt{\frac{\max(0, \hat{\chi}^2_M - \text{df}_M)}{\text{df}_M (n-1)}}
\tag{13.6}
\end{equation}\]</span>
where <span class="math inline">\(\text{max}(0,\hat{\chi}^2_M - \text{df}_M)\)</span> returns either <span class="math inline">\(0\)</span> or <span class="math inline">\(\hat{\chi}^2_M - \text{df}_M\)</span> (whichever is larger), and <span class="math inline">\(n\)</span> denotes the number of observations in the data. As confidence intervals can be computed for this measure, it is common to report these. Values of <span class="math inline">\(\text{RMSEA} \leq .06\)</span> are considered satisfactory <span class="citation">(<a href="#ref-hu1999cutoff">Hu &amp; Bentler, 1999</a>)</span>.</p>
<p><span class="citation">Kline (<a href="#ref-kline2015principles">2015</a>)</span> proposes to, when possible, report the following “minimal set” of fit measures:</p>
<ol style="list-style-type: decimal">
<li>Model chi-square <span class="math inline">\(\hat{\chi}^2_M\)</span> with its degrees of freedom and <span class="math inline">\(p\)</span> value.</li>
<li>Root Mean Square Error of Approximation (RMSEA) and its 90% confidence interval.</li>
<li>Bentler Comparative Fit Index (CFI).</li>
<li>Standardized Root Mean Square Residual (SRMR).</li>
</ol>
<!-- ### Modification indices

Modification indices -->
</div>
</div>
</div>
<div id="modification-indices" class="section level2 hasAnchor" number="13.6">
<h2><span class="header-section-number">13.6</span> Modification indices<a href="ch-SEM-path-models.html#modification-indices" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When a SEM does not fit the data adequately, it <em>may</em> be reasonable to change it by allowing some parameters which were fixed to 0 to be estimated. When doing so, the main guide should be theory: Does the model still make sense after adding the new parameters? Is the model still identifiable? Modification indices can provide another guide. The <strong>modification index</strong> for a fixed-to-zero parameter is an estimate of the improvement in the model chi-square <span class="math inline">\(\hat{\chi}^2_M\)</span> that would result from allowing the parameter to be freely estimated. As such, a fixed-to-zero parameter with a large modification index might be a good candidate for improving model fit.</p>
<p>Allowing fixed-to-zero parameters to be freely estimated increases the complexity of the model, and should therefore be done with caution. Blindly following modification indices to respecify a model is not advised. A main concern is that a model specification search via modification indices is data-driven. Data is noisy. A parameter that appears a cause of misspecification in one sample might not pose a problem in another. In other words, by changing a model to fit one sample, we may arrive at a model that fits less well to new data. This is also referred to as <em>capitalization on chance</em>, and <span class="citation">MacCallum, Roznowski, &amp; Necowitz (<a href="#ref-maccallum1992model">1992</a>)</span> show its detrimental effects rather clearly. Used sparingly, modification indices can provide a useful guide for model improvement, but whether freeing a parameter makes theoretical sense is more important.</p>
<p>Differences in the observed correlations between variables, and the “predicted” correlations in the estimated model, provide another guide for model improvement. These differences in correlations are also called <strong>residual correlations</strong>. For example, when the observed correlation between two variables is much larger than predicted, it may make sense to add a direct path between these variables.</p>
</div>
<div id="model-comparison-1" class="section level2 hasAnchor" number="13.7">
<h2><span class="header-section-number">13.7</span> Model comparison<a href="ch-SEM-path-models.html#model-comparison-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Nested models can, as usual, be compared via a likelihood-ratio test.<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a> However, in SEM, we often want to compare non-nested models. In that case, we can revert to model selection criteria such as the Akaike Information Criterion <span class="citation">(<a href="#ref-Akaike1973">Akaike, 1992[1973]</a>)</span> or Bayesian Information Criterion <span class="citation">(<a href="#ref-Schwarz1978">Schwarz, 1978</a>)</span>. Both criteria are derived in order to select the best model from a set. And both measures can be viewed as correcting the “minus 2 log-likelihood” measure for model complexity. The <strong>Akaike Information Criterion</strong><a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a> (AIC) is defined as
<span class="math display" id="eq:AIC-definition">\[\begin{equation}
\text{AIC} = -2 \log p(\text{DATA}|\text{MODEL M}) + 2 \times \text{npar}(M)
\tag{13.7}
\end{equation}\]</span>
where <span class="math inline">\(\text{npar}(M)\)</span> denotes the number of freely estimated parameters in MODEL M. The AIC can be viewed as a measure of how well the model will generalize to new data sets. The <strong>Bayesian Information Criterion</strong> (BIC) is defined as
<span class="math display" id="eq:BIC-definition">\[\begin{equation}
\text{BIC} = -2 \log p(\text{DATA}|\text{MODEL M}) + \text{npar}(M) \times \log (n)
\tag{13.8}
\end{equation}\]</span>
where <span class="math inline">\(n\)</span> is the number of (multivariate) observations. The BIC can be viewed as an approximation to the Bayesian “marginal likelihood”. We will consider what this means in a later chapter.</p>
<p>For both measures, lower values are better. The measures are not test statistics in the sense that their sampling distribution can be used to compute <span class="math inline">\(p\)</span>-values. And the absolute value of each measure is not obvious. Their application is straightforward however: Pick the model with the lowest AIC or BIC. Both criteria add an additional penalty to the “minus two log likelihood” measure of misfit. The penalty of the AIC (<span class="math inline">\(2 \times \text{npar}(M)\)</span>) is only based on the number of freely estimated parameters in MODEL M. For the BIC, the penalty (<span class="math inline">\(\text{npar}(M) \times \log (n)\)</span>) is also based on the number of observations. The larger the dataset, the larger the relative penalty for additional parameters.</p>
</div>
<div id="evaluation-and-selection-of-the-mediation-path-models" class="section level2 hasAnchor" number="13.8">
<h2><span class="header-section-number">13.8</span> Evaluation and selection of the mediation path models<a href="ch-SEM-path-models.html#evaluation-and-selection-of-the-mediation-path-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>With these technical details behind us, we can finally assess the fit of the three models proposed in Section <a href="ch-SEM-path-models.html#sec-sem-path-model-mediation">13.3</a>. These were the full mediation, partial mediation, and the common cause model. The overall fit tests and fit measures of these models are provided in Table <a href="ch-SEM-path-models.html#tab:sem-medation-fit-measures">13.2</a>.</p>
<table>
<caption>
<span id="tab:sem-medation-fit-measures">Table 13.2: </span>Fit tests and measures for three candidate path models for relating legacy motive, intention, and donations.
</caption>
<thead>
<tr>
<th style="text-align:right;">
measure
</th>
<th style="text-align:right;">
value
</th>
<th style="text-align:right;">
df
</th>
<th style="text-align:right;">
<span class="math inline">\(p\)</span>
</th>
<th style="text-align:right;">
90% lower
</th>
<th style="text-align:right;">
90% upper
</th>
</tr>
</thead>
<tbody>
<tr grouplength="7">
<td colspan="6" style="border-bottom: 1px solid;">
<strong>Full Mediation</strong>
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
<span class="math inline">\(\hat{\chi}^2\)</span>
</td>
<td style="text-align:right;">
5.989
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
.014
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
baseline <span class="math inline">\(\hat{\chi}^2\)</span>
</td>
<td style="text-align:right;">
51.556
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
&lt; .001
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
CFI
</td>
<td style="text-align:right;">
0.897
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
SRMR
</td>
<td style="text-align:right;">
0.048
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
RMSEA
</td>
<td style="text-align:right;">
0.145
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.052
</td>
<td style="text-align:right;">
0.266
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
AIC
</td>
<td style="text-align:right;">
1779.588
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
BIC
</td>
<td style="text-align:right;">
1800.397
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr grouplength="7">
<td colspan="6" style="border-bottom: 1px solid;">
<strong>Partial Mediation</strong>
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
<span class="math inline">\(\hat{\chi}^2\)</span>
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
baseline <span class="math inline">\(\hat{\chi}^2\)</span>
</td>
<td style="text-align:right;">
51.556
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
&lt; .001
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
CFI
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
SRMR
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
RMSEA
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
AIC
</td>
<td style="text-align:right;">
1775.599
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
BIC
</td>
<td style="text-align:right;">
1799.876
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr grouplength="7">
<td colspan="6" style="border-bottom: 1px solid;">
<strong>Common Cause</strong>
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
<span class="math inline">\(\hat{\chi}^2\)</span>
</td>
<td style="text-align:right;">
18.048
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
&lt; .001
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
baseline <span class="math inline">\(\hat{\chi}^2\)</span>
</td>
<td style="text-align:right;">
51.556
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
&lt; .001
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
CFI
</td>
<td style="text-align:right;">
0.649
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
SRMR
</td>
<td style="text-align:right;">
0.084
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
RMSEA
</td>
<td style="text-align:right;">
0.268
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.169
</td>
<td style="text-align:right;">
0.383
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
AIC
</td>
<td style="text-align:right;">
1791.648
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:right;padding-left: 2em;" indentlevel="1">
BIC
</td>
<td style="text-align:right;">
1812.456
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
<p>The Full Mediation model is rejected by the overall model fit test. The comparative fit to baseline test is significant, indicating that the model does fit better than the baseline model. Whilst the SRMR is below the cutoff value of .08, the CFI is below the cutoff value .95, and the RMSEA above the cutoff value .06, indicating relatively poor fit.</p>
<p>The Partial Mediation model is a saturated model. As such, it fits the data perfectly according to most measures. The only statistics of relevance are the AIC and BIC, which can be compared against those of the other models.</p>
<p>The Common Cause model performs worse than the Full Mediation model. The Full Mediation and Common Cause model are both nested under the (saturated) Partial Mediation model (each sets one of the regression parameters to 0). As the Partial Mediation model is a saturated model, the <span class="math inline">\(\hat{\chi}^2\)</span> overall model fit tests for the Full Mediation and Common Cause model are equal to a model comparison with the Partial Mediation model. Hence, both can be said to fit significantly worse than the Partial Mediation model.
As the Full Mediation model and Common Cause model are not nested, they cannot be compared with a likelihood ratio test. For comparison of non-nested models, the AIC and BIC can be used. Both the AIC and the BIC are lowest for the Partial Mediation model. However, the difference in the BIC measure between the Full and Partial mediation model is very small. Taking a Bayesian viewpoint, the evidence for the Partial over the Full Mediation model is not strong.</p>
</div>
<div id="a-more-complex-path-model" class="section level2 hasAnchor" number="13.9">
<h2><span class="header-section-number">13.9</span> A more complex path model<a href="ch-SEM-path-models.html#a-more-complex-path-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are a number of additional variables in the dataset from the study on legacy beliefs in pro-environmental behaviour <span class="citation">(<a href="#ref-zaval_how_2015">Zaval et al., 2015</a>)</span>. These are participants’ age and income, as well as their belief in climate change. Relying solely on intuition, I hypothesized that education would be causally related to belief and income, and age to legacy motive (older people being more concerned with their legacy) and income (due to e.g. superannuation). I further hypothesized that belief would be causally related to intention. I also thought income might be related to intention, reasoning that richer people having less intention to behave pro-environmentally as that might hurt their investments and bank balance.<a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a> I also thought income might be causally related to donation (richer people being able to donate more). This rather haphazard “theory” led to the path model depicted in Figure <a href="ch-SEM-path-models.html#fig:complex-path-legacy-motive-1">13.8</a> (for clarity, the constant terms are hidden in this plot). Age and education are exogenous variables in this model, and allowed to covariate. The remaining variables are endogenous, and apart from residual variances, are assumed to be fully explained by the other variables in the model.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:complex-path-legacy-motive-1"></span>
<img src="13-SEM_files/figure-html/complex-path-legacy-motive-1-1.svg" alt="A more complex path model." width="100%" />
<p class="caption">
Figure 13.8: A more complex path model.
</p>
</div>
<p>Perhaps surprisingly, apart from the effect of age on legacy and income, all the paths in the model were significant. Table <a href="ch-SEM-path-models.html#tab:sem-path-mod-complex-results">13.3</a> shows the results in more detail. Unfortunately, the model fit indices indicate that this model does not provide a satisfactory account of all the relations in the data.</p>
<table>
<caption>
<span id="tab:sem-path-mod-complex-results">Table 13.3: </span>Results of an initial full path model for the legacy motive data. Only results for regression parameters and selected model fit indices are included.
</caption>
<table style="padding-right:20px;padding-left:20px;">
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'>Model</td>
</tr>
<tr>
<td>
</td>
<td colspan = '1'; align = 'center'>Estimate</td>
<td colspan = '1'; align = 'center'>Std. Err.</td>
<td colspan = '1'; align = 'center'>z</td>
<td colspan = '1'; align = 'center'>p</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Regression Slopes</span></td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">belief</span></td>
</tr>
<tr>
<td>
education
</td>
<td>
0.13
</td>
<td>
0.05
</td>
<td>
2.72
</td>
<td>
.006
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">legacy</span></td>
</tr>
<tr>
<td>
age
</td>
<td>
-0.01
</td>
<td>
0.01
</td>
<td>
-1.12
</td>
<td>
.263
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">income</span></td>
</tr>
<tr>
<td>
age
</td>
<td>
0.00
</td>
<td>
0.01
</td>
<td>
0.25
</td>
<td>
.800
</td>
</tr>
<tr>
<td>
education
</td>
<td>
0.38
</td>
<td>
0.10
</td>
<td>
3.76
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">intention</span></td>
</tr>
<tr>
<td>
belief
</td>
<td>
0.74
</td>
<td>
0.06
</td>
<td>
11.62
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
legacy
</td>
<td>
0.12
</td>
<td>
0.05
</td>
<td>
2.52
</td>
<td>
.012
</td>
</tr>
<tr>
<td>
income
</td>
<td>
-0.08
</td>
<td>
0.03
</td>
<td>
-2.71
</td>
<td>
.007
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">donation</span></td>
</tr>
<tr>
<td>
intention
</td>
<td>
1.10
</td>
<td>
0.21
</td>
<td>
5.15
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
income
</td>
<td>
0.26
</td>
<td>
0.12
</td>
<td>
2.11
</td>
<td>
.035
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Fit Indices</span></td>
</tr>
<tr>
<td>
χ<sup>2</sup>
</td>
<td>
41.29(11)
</td>
<td>
</td>
<td>
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
CFI
</td>
<td>
0.84
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
SRMR
</td>
<td>
0.08
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA
</td>
<td>
0.11
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
AIC
</td>
<td>
3540.61
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
BIC
</td>
<td>
3605.43
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan = '5'; align = 'left'><sup>+</sup>Fixed parameter</td>
</tr>
</table>
<p><br></p>
</table>
<p>Table <a href="ch-SEM-path-models.html#tab:complex-path-legacy-motive-mi">13.4</a> shows the modification indices for fixed-to-zero parameters in this model, ordered by magnitude and ignoring parameters for which the modification index is smaller than 2.</p>
<table>
<caption>
<span id="tab:complex-path-legacy-motive-mi">Table 13.4: </span>Modification indices and expected change in parameter values for fixed-to-zero parameters.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Parameter
</th>
<th style="text-align:right;">
Modification index
</th>
<th style="text-align:right;">
Expected change
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{belief} \rightarrow \texttt{legacy}\)</span>
</td>
<td style="text-align:right;">
15.99
</td>
<td style="text-align:right;">
0.347
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{intention} \rightarrow \texttt{legacy}\)</span>
</td>
<td style="text-align:right;">
14.05
</td>
<td style="text-align:right;">
0.431
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{legacy} \rightarrow \texttt{belief}\)</span>
</td>
<td style="text-align:right;">
13.99
</td>
<td style="text-align:right;">
0.189
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{legacy} \leftrightarrow \texttt{belief}\)</span>
</td>
<td style="text-align:right;">
13.54
</td>
<td style="text-align:right;">
0.169
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{donation} \rightarrow \texttt{legacy}\)</span>
</td>
<td style="text-align:right;">
10.70
</td>
<td style="text-align:right;">
0.072
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{intention} \leftrightarrow \texttt{legacy}\)</span>
</td>
<td style="text-align:right;">
7.67
</td>
<td style="text-align:right;">
1.644
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{age} \rightarrow \texttt{intention}\)</span>
</td>
<td style="text-align:right;">
7.67
</td>
<td style="text-align:right;">
0.011
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{donation} \rightarrow \texttt{intention}\)</span>
</td>
<td style="text-align:right;">
7.59
</td>
<td style="text-align:right;">
-0.073
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{donation} \leftrightarrow \texttt{intention}\)</span>
</td>
<td style="text-align:right;">
7.59
</td>
<td style="text-align:right;">
-0.584
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{legacy} \rightarrow \texttt{donation}\)</span>
</td>
<td style="text-align:right;">
5.25
</td>
<td style="text-align:right;">
0.459
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{donation} \leftrightarrow \texttt{legacy}\)</span>
</td>
<td style="text-align:right;">
5.22
</td>
<td style="text-align:right;">
0.415
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{donation} \rightarrow \texttt{belief}\)</span>
</td>
<td style="text-align:right;">
4.92
</td>
<td style="text-align:right;">
0.047
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{belief} \rightarrow \texttt{donation}\)</span>
</td>
<td style="text-align:right;">
4.66
</td>
<td style="text-align:right;">
0.704
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{education} \rightarrow \texttt{legacy}\)</span>
</td>
<td style="text-align:right;">
4.47
</td>
<td style="text-align:right;">
0.135
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{legacy} \rightarrow \texttt{education}\)</span>
</td>
<td style="text-align:right;">
4.42
</td>
<td style="text-align:right;">
0.146
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{donation} \leftrightarrow \texttt{belief}\)</span>
</td>
<td style="text-align:right;">
3.56
</td>
<td style="text-align:right;">
0.322
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{income} \rightarrow \texttt{legacy}\)</span>
</td>
<td style="text-align:right;">
3.50
</td>
<td style="text-align:right;">
0.076
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{intention} \rightarrow \texttt{age}\)</span>
</td>
<td style="text-align:right;">
2.63
</td>
<td style="text-align:right;">
1.469
</td>
</tr>
</tbody>
</table>
<p>The parameter with the largest modification index is the direct path from belief to legacy motive. Whilst one might be tempted to therefore add a direct path from belief to legacy motive, I do not find it immediately plausible that belief in climate change would cause motivation to leave a positive legacy in the world. I also did not think it plausible that intention to act in a pro-environmental manner would cause a legacy motive (this effect having the second largest modification index). Instead, given that age appears unrelated to income and legacy motive, I reasoned that age was perhaps better placed as an exogenous influence on intention. I also reasoned that legacy motive might cause belief in climate change, as someone who is concerned with leaving a legacy in the world might be interested in knowing whether the world will continue to exist to sustain this legacy. Figure <a href="ch-SEM-path-models.html#fig:complex-path-legacy-motive-2">13.9</a> depicts the estimated relations in this revised path model. This model assumes that in addition to age and education, legacy motive is also an exogenous variable. The remaining variables are endogenous.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:complex-path-legacy-motive-2"></span>
<img src="13-SEM_files/figure-html/complex-path-legacy-motive-2-1.svg" alt="A more complex path model." width="100%" />
<p class="caption">
Figure 13.9: A more complex path model.
</p>
</div>
<p>Clearly, the model in Figure <a href="ch-SEM-path-models.html#fig:complex-path-legacy-motive-2">13.9</a> is based on post-hoc inferences, and so we are in the territory of <em>exploratory</em> data analysis. But my original model was not based on strong theory anyway, and the key idea of mediation of legacy motive and belief by intention is kept in the new model. That said, you should keep in mind that changing and refining a path model based on the results of a prior analysis is an exploratory exercise. A model arrived with a procedure like this should be taken with a grain of salt and ideally be tested properly with a new data set.</p>
<p>Table <a href="ch-SEM-path-models.html#tab:sem-path-mod-complex-results2">13.5</a> provides more detailed results. These results show that all regression coefficients of the paths included in the model are significant. Moreover, the overall model test is not significant, indicating good fit. This conclusion is also supported by satisfactory values for the SRMR and RSMEA, and CFI.</p>
<table>
<caption>
<span id="tab:sem-path-mod-complex-results2">Table 13.5: </span> Results of a second full path model for the legacy motive data. Only results for regression parameters and selected model fit indices are included.
</caption>
<table style="padding-right:20px;padding-left:20px;">
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'>Model</td>
</tr>
<tr>
<td>
</td>
<td colspan = '1'; align = 'center'>Estimate</td>
<td colspan = '1'; align = 'center'>Std. Err.</td>
<td colspan = '1'; align = 'center'>z</td>
<td colspan = '1'; align = 'center'>p</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Regression Slopes</span></td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">belief</span></td>
</tr>
<tr>
<td>
education
</td>
<td>
0.11
</td>
<td>
0.05
</td>
<td>
2.23
</td>
<td>
.026
</td>
</tr>
<tr>
<td>
legacy
</td>
<td>
0.19
</td>
<td>
0.05
</td>
<td>
3.90
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">income</span></td>
</tr>
<tr>
<td>
education
</td>
<td>
0.38
</td>
<td>
0.10
</td>
<td>
3.75
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">intention</span></td>
</tr>
<tr>
<td>
belief
</td>
<td>
0.75
</td>
<td>
0.06
</td>
<td>
11.52
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
legacy
</td>
<td>
0.13
</td>
<td>
0.05
</td>
<td>
2.64
</td>
<td>
.008
</td>
</tr>
<tr>
<td>
income
</td>
<td>
-0.08
</td>
<td>
0.03
</td>
<td>
-2.83
</td>
<td>
.005
</td>
</tr>
<tr>
<td>
age
</td>
<td>
0.01
</td>
<td>
0.00
</td>
<td>
2.83
</td>
<td>
.005
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">donation</span></td>
</tr>
<tr>
<td>
intention
</td>
<td>
1.10
</td>
<td>
0.21
</td>
<td>
5.28
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
income
</td>
<td>
0.26
</td>
<td>
0.12
</td>
<td>
2.11
</td>
<td>
.035
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Fit Indices</span></td>
</tr>
<tr>
<td>
χ<sup>2</sup>
</td>
<td>
14.26(9)
</td>
<td>
</td>
<td>
</td>
<td>
.113
</td>
</tr>
<tr>
<td>
CFI
</td>
<td>
0.97
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
SRMR
</td>
<td>
0.04
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA
</td>
<td>
0.05
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
AIC
</td>
<td>
2900.63
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
BIC
</td>
<td>
2958.62
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan = '5'; align = 'left'><sup>+</sup>Fixed parameter</td>
</tr>
</table>
<p><br></p>
</table>
</div>
<div id="principles-in-constructing-path-models" class="section level2 hasAnchor" number="13.10">
<h2><span class="header-section-number">13.10</span> Principles in constructing path models<a href="ch-SEM-path-models.html#principles-in-constructing-path-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As you may have noticed, path modelling provides a lot of freedom to the analyst. Which variables do you take to be exogenous, and which ones as endogenous? And which variables have causal links to other variables? And we haven’t even considered adding residual covariances between endogenous variables. In the example above with seven variables, the number of possible models is huge. But even for a smaller set of three variables, as in the mediation example, the number of possible models is large. How do you choose one model when the possibilities are seemingly endless?</p>
<p>The main guiding principle in defining and comparing SEM models should be: <strong>theory</strong>! Particular causal links are often implausible. For example, I would consider a causal relation from income to age implausible, at least for the present data.<a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a> And causal links from donation to any of the other variables are implausible too (if only because the donation question was unannounced and posed at end of the experiment). If you have <em>no</em> theory to identify the likely causes and effects to guide your model building, it is probably best to steer clear from SEM. I certainly would not advise completely data-driven SEM modelling. A first reason is that an exhaustive search over all possible models is generally infeasible. A second reason is that, unless you have a <em>very</em> large dataset, conducting an exhaustive search through all possible structural models and selecting the one that fits that data best is ill-advised, as you are likely to <em>overfit</em> particular quirks in a dataset. Furthermore, there are generally multiple equivalent models, which propose different causal relations, but end up providing exactly the same fit to the data when estimated.</p>
<div id="identifiability" class="section level3 hasAnchor" number="13.10.1">
<h3><span class="header-section-number">13.10.1</span> Identifiability<a href="ch-SEM-path-models.html#identifiability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Beyond theory, a main consideration is model identifiability, which refers to whether the parameters of a model can be estimated in the first place. Identifiability is a general concept in statistical modelling. In its essence, identifiability means that, for a given model, there should not be two different parameter settings, <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta&#39;\)</span>, which always give the same likelihood value for any data. So a model is <em>not</em> identifiable if:
<span class="math display">\[p(\text{DATA}|\theta) = p(\text{DATA}|\theta&#39;) \quad \quad \text{for any } \theta \neq \theta&#39;\]</span></p>
<p>In the context of SEM, an unidentified model often implies the model has too many parameters. A first and simple rule is that a model is not identifiable if it has more parameters than unique values in the sample variance-covariance matrix and mean vector (assuming that the means are part of the model specification). In other words, the model can not have more parameters than the saturated model (Equation <a href="ch-SEM-path-models.html#eq:npar-saturated-sem-model">(13.1)</a>).</p>
<p>Not all models with <span class="math inline">\(\text{npar}(M) \leq \text{npar}(S)\)</span> are identifiable, however. There are additional conditions. To get an intuition about non-identifiability, consider the following equation:
<span class="math display">\[x + y = 2\]</span>
We can solve this equation for <span class="math inline">\(x\)</span> (i.e. <span class="math inline">\(x = 2 -y\)</span>) or <span class="math inline">\(y\)</span> (i.e. <span class="math inline">\(y = 2-x\)</span>), but we can not determine the exact value of both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> simultaneously. There are an infinite number of values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> that respect the equality above, such as
<span class="math display">\[(x=1, y=1), (x=2, y=0), (x = \tfrac{1}{300}, y = \tfrac{599}{300}), \ldots\]</span>
As another example, consider the following regression model:
<span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_i + \beta_2 \times X_i + \epsilon_i\]</span>
where we include the same predictor (<span class="math inline">\(X\)</span>) twice, but with separate slopes (<span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>). In a simple regression equation, where <span class="math inline">\(X\)</span> is included only once
<span class="math display">\[Y_i = \beta_0 + \beta_x \times X_i + \epsilon_i\]</span>
we would be able to estimate the slope of <span class="math inline">\(X\)</span> as <span class="math inline">\(\hat{\beta}_x = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}\)</span>. But we can not estimate two slopes for the same predictor. To see why, we can rewrite the earlier regression equation as
<span class="math display">\[Y_i = \beta_0 + (\beta_1 + \beta_2) \times X_i + \epsilon_i\]</span>
As such, we could say that
<span class="math display">\[\hat{\beta}_x = \hat{\beta}_1 + \hat{\beta}_2\]</span>
but then we have exactly the same problem as before, when dealing with <span class="math inline">\(x + y = 2\)</span>: there are an infinite number of possible values of <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_2\)</span> such that their sum equals <span class="math inline">\(\hat{\beta}_x\)</span>. This is the reason why perfectly correlated predictors can not be entered into a regression model. In a path model, similar issues may arise, although they may be harder to identify initially.</p>
<p>When considering the identifiability of SEM models, it is useful to distinguish between recursive and non-recursive models <span class="citation">(<a href="#ref-kline2015principles">Kline, 2015</a>)</span>. In a <strong>recursive model</strong>, there are no “causal loops”, and endogenous variables have no residual covariances. Technically, this implies that the endogenous variables are conditionally independent, given the exogenous variables. Less technically, this implies that for a graphical representation such as in Figure <a href="ch-SEM-path-models.html#fig:complex-path-legacy-motive-2">13.9</a>, if we pick any starting variable and then follow any outwards arrow (excluding the dotted fixed arrows and any arrows pointing from a variable onto itself), we should not be able to find a path back to the starting variable. For example, you could start with <span class="math inline">\(\texttt{eduction}\)</span>, then follow the outwards arrow to <span class="math inline">\(\texttt{belief}\)</span>, and then follow the outgoing arrow to <span class="math inline">\(\texttt{intention}\)</span>, and then to <span class="math inline">\(\texttt{donation}\)</span>. But from there, there is no path back to <span class="math inline">\(\texttt{education}\)</span>. Such recursive models are <em>always</em> identifiable <span class="citation">(<a href="#ref-kline2015principles">Kline, 2015</a>)</span>.</p>
<p><strong>Non-recursive models</strong> may have causal feedback loops, or endogenous variables with residual covariances. For such models, it is more difficult to establish whether they are identifiable or not. <span class="citation">Kline (<a href="#ref-kline2015principles">2015</a>)</span> provides a relatively readable exposition on this topic (Chapters 7 and 8). Problems of identification generally lead to errors or warnings from the software used to estimate a SEM model, or implausible parameter estimates. If your theory allows, it is safer to stick to recursive models, as these are much easier to estimate and interpret.</p>
</div>
</div>
<div id="model-equivalence" class="section level2 hasAnchor" number="13.11">
<h2><span class="header-section-number">13.11</span> Model equivalence<a href="ch-SEM-path-models.html#model-equivalence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Another tricky issue with path models, and SEM models in general, is that different structural models can – when estimated from the data – imply exactly the same variance-covariance matrix and mean vector. This is related to the issue of identifiability, as these models have the same likelihood. But rather than identifiability of the parameters, this is an issue of <strong>model identifiability</strong>.</p>
<p>As a simple example, let’s consider a regression model where we predict votes for Donald Trump as a function of the number of active hate groups in a state. In SEM terms, we would place a causal direction from the number of hate groups to the percentage of votes for Trump. <!-- As an exogenous variable, we would not be able to predict the variance in the the number of hate groups over states. We would just take this as given. However, as an endogenous variable, we would account for some of the variation in votes for Trump via its causal relation with the number of hate groups.--> Let <span class="math inline">\(V\)</span> denote the percentage of votes for Trump, and <span class="math inline">\(H\)</span> the number of hate groups, then our model states
<span class="math display">\[V_i = \beta_{0,V} + \beta_{H} \times H_i + \epsilon_{V} \quad \quad \epsilon_V \sim \mathbf{Normal}(0, \sigma_{\epsilon_V})\]</span>
But what if we would consider the causal direction in this model to be reversed, so that votes for Trump cause the number of hate groups in a state. The model then becomes<br />
<span class="math display">\[H_i = \beta_{0,H} + \beta_{V} \times V_i + \epsilon_{H} \quad \quad \epsilon_H \sim \mathbf{Normal}(0, \sigma_{\epsilon_H})\]</span>
These two models are equivalent, in the sense that if we would estimate them from the same dataset, we would obtain <em>exactly</em> the same fit to the data. The model-implied variance-covariance matrix of the first model is
<span class="math display">\[\boldsymbol{\Sigma}(\theta_1) = \left[ \begin{matrix}
\sigma^2_H &amp; \beta_H \times \sigma^2_H \\
\beta_H \times \sigma^2_H &amp; \beta_{H}^2 \times \sigma_{H}^2 + \sigma^2_{\epsilon_V}
\end{matrix} \right]\]</span>
with parameters <span class="math inline">\(\theta_1 = (\sigma_H, \beta_H, \sigma_{\epsilon_V})\)</span>
The model-implied variance-covariance matrix of the second model is
<span class="math display">\[\boldsymbol{\Sigma}(\theta_1) = \left[ \begin{matrix}
\beta_{V}^2 \times \sigma_{V}^2 + \sigma^2_{\epsilon_H} &amp; \beta_V \times \sigma^2_V \\
\beta_V \times \sigma^2_V &amp; \sigma^2_V
\end{matrix} \right]\]</span>
with parameters <span class="math inline">\(\theta_2 = (\sigma_V, \beta_V, \sigma_{\epsilon_H})\)</span>.
The parameter sets <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> can be translated from one into the other. Equating the elements in the model-implied variance-covariance matrices, we have
<span class="math display">\[\begin{aligned}
\sigma^2_H &amp;= \beta_{V}^2 \times \sigma_{V}^2 + \sigma^2_{\epsilon_H} \\
\beta_H \times \sigma^2_H &amp;= \beta_V \times \sigma^2_V \\
\beta_{H}^2 \times \sigma_{H}^2 + \sigma^2_{\epsilon_V} &amp;= \sigma^2_V
\end{aligned}\]</span>
Solving this system of equations for the parameters <span class="math inline">\(\theta_1\)</span>, we get
<span class="math display">\[\begin{aligned}
\sigma_H &amp;= \sqrt{\beta_{V}^2 \times \sigma_{V}^2 + \sigma^2_{\epsilon_H}} \\
\beta_H  &amp;= \frac{\beta_V \times \sigma^2_V}{\beta_{V}^2 \times \sigma_{V}^2 + \sigma^2_{\epsilon_H}} \\
\sigma_{\epsilon_V} &amp;= \sqrt{\sigma^2_V - \frac{(\beta_V \times \sigma^2_V)^2}{\beta_{V}^2 \times \sigma_{V}^2 + \sigma^2_{\epsilon_H}}}
\end{aligned}\]</span>
Alternatively, we can solve for the parameters <span class="math inline">\(\theta_2\)</span> and get
<span class="math display">\[\begin{aligned}
\sigma_V &amp;= \sqrt{\beta_{H}^2 \times \sigma_{H}^2 + \sigma^2_{\epsilon_V}} \\
\beta_V  &amp;= \frac{\beta_H \times \sigma^2_H}{\beta_{H}^2 \times \sigma_{H}^2 + \sigma^2_{\epsilon_V}} \\
\sigma_{\epsilon_H} &amp;= \sqrt{\sigma^2_H - \frac{(\beta_H \times \sigma^2_H)^2}{\beta_{H}^2 \times \sigma_{H}^2 + \sigma^2_{\epsilon_V}}}
\end{aligned}\]</span>
The key thing is that the parameters <span class="math inline">\(\theta_1\)</span> can be defined as a function of parameters <span class="math inline">\(\theta_2\)</span>, and vice versa. If parameters <span class="math inline">\(\theta_1\)</span> maximise the likelihood of the data, there are translated parameters <span class="math inline">\(\theta_2\)</span> which would do the same. Hence, we can get exactly the same fit for both models.</p>
<p>This was a very simple example of equivalent models, but the general idea extends to more complex SEM models. It is beyond the scope of this Chapter to go into further details, but <span class="citation">Raykov &amp; Penev (<a href="#ref-raykov1999structural">1999</a>)</span> discuss rules for determining the set of equivalent SEM models for a given target model. The main thing to keep in mind for now is that, for a given SEM model, there may be equivalent models that fit the data equally well, but have a different causal interpretation. As a result, it is generally not possible to uniquely identify a single SEM model from its fit to data.</p>
</div>
<div id="correlation-vs-causation" class="section level2 hasAnchor" number="13.12">
<h2><span class="header-section-number">13.12</span> Correlation vs causation<a href="ch-SEM-path-models.html#correlation-vs-causation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Whilst SEM models are constructed as causal models, they just reflect the means of and (co-)variation between variables. A variance-covariation matrix can easily be converted into a correlation matrix.<a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a> So SEM models effectively target the correlation between variables. And as the well-worn adage goes: <strong>Correlation does not imply causation</strong>!</p>
<p>When conducting SEM analyses, it is important to keep this in mind. Although SEM models provide a useful and effective way to formulate a causal theory in terms of a statistical model, the evidence for this model is based on correlation. Whilst model comparison may be used to rule out particular alternative models, for a given model, there are usually several equivalent models which will fit any data equally well, but which have a different causal interpretation. As such, a SEM analysis does not provide proof for the causal relations it entails. The gold standard for proving a causal relation is to intervene on the cause with e.g. an experiment. Proving causation from purely observational data is generally not possible. Nevertheless, this does not prevent you from applying a causal interpretation to SEM models, as long as you keep in mind that you have not excluded other causal patterns <span class="citation">(see also <a href="#ref-pearl2012causal">Pearl, 2012</a>; and <a href="#ref-rohrer2018thinking">Rohrer, 2018</a>)</span>.</p>
<!-- https://journals.sagepub.com/doi/full/10.1177/2515245917745629 -->
<!--

### Multivariate Normal distribution

### Covariation and correlation matrices

-->
<!-- ### A little bit of matrix algebra -->
<!-- https://bookdown.org/compfinezbook/introFinRbook/Matrix-Algebra-Review.html -->
<!-- https://bkenkel.com/pdaps/matrix.html -->
<!-- ## Errors-in-variable models -->

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Akaike1973" class="csl-entry">
Akaike, H. (1992[1973]). Information theory and an extension of the maximum likelihood principle. In S. Kotz &amp; K. L. Johnson (Eds.), <em>Breakthroughs in statistics. <span>Vol 1</span></em> (pp. 610–624). London: Springer-Verlag.
</div>
<div id="ref-bollen2011structural" class="csl-entry">
Bollen, K. A., &amp; Noble, M. D. (2011). Structural equation models and the quantification of behavior. <em>Proceedings of the National Academy of Sciences</em>, <em>108</em>, 15639–15646.
</div>
<div id="ref-hu1999cutoff" class="csl-entry">
Hu, L., &amp; Bentler, P. M. (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. <em>Structural Equation Modeling: A Multidisciplinary Journal</em>, <em>6</em>, 1–55.
</div>
<div id="ref-kaplan2001sem" class="csl-entry">
Kaplan, D. (2001). <a href="https://doi.org/10.1016/B0-08-043076-7/00776-2">Structural equation modeling</a>. In N. J. Smelser &amp; P. B. Baltes (Eds.), <em>International encyclopedia of the social &amp; behavioral sciences</em> (pp. 15215–15222). Oxford: Pergamon.
</div>
<div id="ref-kline2015principles" class="csl-entry">
Kline, R. B. (2015). <em>Principles and practice of structural equation modeling</em> (4th edition). Guilford Press.
</div>
<div id="ref-maccallum1992model" class="csl-entry">
MacCallum, R. C., Roznowski, M., &amp; Necowitz, L. B. (1992). Model modifications in covariance structure analysis: The problem of capitalization on chance. <em>Psychological Bulletin</em>, <em>111</em>, 490.
</div>
<div id="ref-pearl2012causal" class="csl-entry">
Pearl, J. (2012). The causal foundations of structural equation modeling. In R. H. Hoyle (Ed.), <em>Handbook of structural equation modelling</em> (pp. 68–91). New York: California Univ Los Angeles Dept of Computer Science; Guilford Press.
</div>
<div id="ref-raykov1999structural" class="csl-entry">
Raykov, T., &amp; Penev, S. (1999). On structural equation model equivalence. <em>Multivariate Behavioral Research</em>, <em>34</em>, 199–244.
</div>
<div id="ref-rohrer2018thinking" class="csl-entry">
Rohrer, J. M. (2018). <a href="https://doi.org/10.1177/2515245917745629">Thinking clearly about correlations and causation: Graphical causal models for observational data</a>. <em>Advances in Methods and Practices in Psychological Science</em>, <em>1</em>, 27–42.
</div>
<div id="ref-Schwarz1978" class="csl-entry">
Schwarz, G. (1978). Estimating the dimension of a model. <em>Annals of Statistics</em>, <em>6</em>, 461–464.
</div>
<div id="ref-wright1920relative" class="csl-entry">
Wright, S. (1920). The relative importance of heredity and environment in determining the piebald pattern of guinea-pigs. <em>Proceedings of the National Academy of Sciences</em>, <em>6</em>, 320–332.
</div>
<div id="ref-zaval_how_2015" class="csl-entry">
Zaval, L., Markowitz, E. M., &amp; Weber, E. U. (2015). <a href="https://doi.org/10.1177/0956797614561266">How will <span>I</span> be remembered? Conserving the environment for the sake of one’s legacy</a>. <em>Psychological Science</em>, <em>26</em>, 231–236.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="41">
<li id="fn41"><p>Maximum likelihood estimation of SEM models consists of finding the parameter values <span class="math inline">\(\theta\)</span> that minimize the following function <span class="citation">(<a href="#ref-bollen2011structural">Bollen &amp; Noble, 2011</a>)</span>: <span class="math display">\[F_\text{ML} = \log |\boldsymbol{\Sigma}(\theta)| - \log | \mathbf{S} |  + \text{trace}\left( \boldsymbol{\Sigma}^{-1}(\theta) \mathbf{S}\right) - \text{dim}(\overline{\mathbf{C}}) + \left(\overline{\mathbf{C}} - \boldsymbol{\mu}(\theta)\right)^\top \boldsymbol{\Sigma}^{-1}(\theta) \left(\overline{\mathbf{C}} - \boldsymbol{\mu}(\theta)\right)\]</span> where <span class="math inline">\(|\cdot|\)</span> denotes the determinant of a matrix, <span class="math inline">\(\text{trace}(\cdot)\)</span> the trace of a matrix, <span class="math inline">\(\text{dim}(\cdot)\)</span> refers to the dimension (number of elements) in a vector, <span class="math inline">\(\cdot^{-1}\)</span> denotes the matrix inverse, and <span class="math inline">\(\cdot^\top\)</span> for the matrix transpose. Unless you know some matrix algebra, these terms of the formula are probably not meaningful. The key is that the observations <span class="math inline">\(X_{1,i},\ldots,X_{m,i},Y_{1,i}\ldots,Y_{k,i}\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>, are <strong>not</strong> needed in the computation of this, just the means <span class="math inline">\(\overline{\mathbf{C}}\)</span> and covariances <span class="math inline">\(\mathbf{S}\)</span>. The log likelihood of the full data is simply <span class="math inline">\(n\times F_{\text{ML}}\)</span>, where <span class="math inline">\(n\)</span> is the number of observations.<a href="ch-SEM-path-models.html#fnref41" class="footnote-back">↩︎</a></p></li>
<li id="fn42"><p>A covariance <span class="math inline">\(\sigma_{1,2}\)</span> above the diagonal is equal to the covariance <span class="math inline">\(\sigma_{2,1}\)</span> below the diagonal.<a href="ch-SEM-path-models.html#fnref42" class="footnote-back">↩︎</a></p></li>
<li id="fn43"><p>Technically, this assumes that the simpler MODEL R does not fix some free parameters of the more complex MODEL G at the bounds of their admissible values. For instance, if MODEL R can <strong>not</strong> fix a variance term of MODEL G to 0, as variances cannot be smaller than 0, this is exactly on the bound of the permissible values.<a href="ch-SEM-path-models.html#fnref43" class="footnote-back">↩︎</a></p></li>
<li id="fn44"><p><span class="citation">Akaike (<a href="#ref-Akaike1973">1992[1973]</a>)</span> actually called it “An Information Criterion”, but as the first letter of his last name matches that of “An”, and Akaike Information Criterion seems more specific, the latter is what it is now known as.<a href="ch-SEM-path-models.html#fnref44" class="footnote-back">↩︎</a></p></li>
<li id="fn45"><p>This reasoning is not entirely independent from the current political developments in the United Kingdom at the time of writing this (17 November 2022).<a href="ch-SEM-path-models.html#fnref45" class="footnote-back">↩︎</a></p></li>
<li id="fn46"><p>Income is likely to have some effect on how long people live (due to e.g. better healthcare) and as such will likely be predictive of age in the sense that, on average, the oldest people are likely to be those ones with a higher income.<a href="ch-SEM-path-models.html#fnref46" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p>A correlation coefficient is just the covariation divided by the product of the standard deviation of the variables: <span class="math display">\[\rho_{x,y} = \frac{\sigma_{x,y}}{\sigma_x \times \sigma_y}\]</span>. And if all variables in the variance-covariance matrix are <span class="math inline">\(Z\)</span>-transformed, the variance-covariance matrix is identical to the correlation matrix.<a href="ch-SEM-path-models.html#fnref47" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-generalized-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-SEM-latent-variable-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"source": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
