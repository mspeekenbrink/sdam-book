<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 14 Latent variable models (SEM 2) | Statistics: Data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 14 Latent variable models (SEM 2) | Statistics: Data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 14 Latent variable models (SEM 2) | Statistics: Data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2023-11-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-SEM-path-models.html"/>
<link rel="next" href="ch-Bayes-estimation.html"/>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="book_assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="book_assets/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.10/grViz.js"></script>
<script src="book_assets/plotly-binding-4.10.3/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-2.11.1/plotly-latest.min.js"></script>
<link href="book_assets/combineWidgetStyle-0.1/combineWidgets.css" rel="stylesheet" />
<script src="book_assets/combineWidgets-binding-0.11.1/combineWidgets.js"></script>
<script src="book_assets/rglWebGL-binding-1.2.1/rglWebGL.js"></script>
<link href="book_assets/rglwidgetClass-1.2.1/rgl.css" rel="stylesheet" />
<script src="book_assets/rglwidgetClass-1.2.1/rglClass.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/utils.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/buffer.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/subscenes.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/shaders.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/shadersrc.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/textures.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/projection.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/mouse.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/init.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/pieces.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/draw.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/controls.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/selection.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/rglTimer.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/pretty.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/axes.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/animation.src.js"></script>
<script src="book_assets/CanvasMatrix4-1.2.1/CanvasMatrix.src.js"></script>
<script src="book_assets/rglPlayer-binding-1.2.1/rglPlayer.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ch-intro.html"><a href="ch-intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="ch-intro.html"><a href="ch-intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-modeling.html"><a href="ch-modeling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-probability-definition"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-binomial-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="ch-modeling.html"><a href="ch-modeling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-modeling.html"><a href="ch-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-modeling.html"><a href="ch-modeling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-likelihood-ratio"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-modeling.html"><a href="ch-modeling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-modeling.html"><a href="ch-modeling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-modeling.html"><a href="ch-modeling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-modeling.html"><a href="ch-modeling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="ch-modeling.html"><a href="ch-modeling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="ch-modeling.html"><a href="ch-modeling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-a-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has a specific value</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#ch3-confidence-interval"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#effect-size"><i class="fa fa-check"></i><b>3.6</b> Effect size</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.7</b> Assumptions</a></li>
<li class="chapter" data-level="3.8" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.8</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.8.1</b> The Central Limit Theorem in action</a></li>
<li class="chapter" data-level="3.8.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#bootstrapping-a-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Bootstrapping a statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#in-practice"><i class="fa fa-check"></i><b>3.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>4.4.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#summary-2"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-transformations"><i class="fa fa-check"></i><b>5.7.1</b> Transforming variables</a></li>
<li class="chapter" data-level="5.7.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.7.2</b> Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.8</b> Multicollinearity: Redundancy between predictors</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.8.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-outliers"><i class="fa fa-check"></i><b>5.9</b> Outliers</a></li>
<li class="chapter" data-level="5.10" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#in-practice-1"><i class="fa fa-check"></i><b>5.10</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#sec:05-dont-forget-about-the-fun"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#sec-glm-mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#sec:06-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-size-in-anova"><i class="fa fa-check"></i><b>7.6</b> Effect-size in ANOVA</a></li>
<li class="chapter" data-level="7.7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#assumptions"><i class="fa fa-check"></i><b>7.7</b> Assumptions</a></li>
<li class="chapter" data-level="7.8" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.8</b> Multiple testing and post-hoc tests</a></li>
<li class="chapter" data-level="7.9" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#in-practice-2"><i class="fa fa-check"></i><b>7.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#sec:06-main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#the-factorial-anova-model"><i class="fa fa-check"></i><b>8.3</b> The factorial ANOVA model</a></li>
<li class="chapter" data-level="8.4" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.4</b> A threeway factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#interpreting-interactions"><i class="fa fa-check"></i><b>8.4.1</b> Interpreting interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.5</b> Orthogonal contrast codes and unequal sample sizes</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.5.1</b> Comparison schemes and SS types</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#in-practice-3"><i class="fa fa-check"></i><b>8.6</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#subjective-feelings-of-power-and-priming"><i class="fa fa-check"></i><b>9.1</b> Subjective feelings of power and priming</a></li>
<li class="chapter" data-level="9.2" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#acounting-for-pre-existing-differences"><i class="fa fa-check"></i><b>9.2</b> Acounting for pre-existing differences</a></li>
<li class="chapter" data-level="9.3" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#slopes-of-contrast-coded-predictors-in-ancova-models"><i class="fa fa-check"></i><b>9.3</b> Slopes of contrast-coded predictors in ANCOVA models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#homogeneity-of-slopes"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of slopes</a></li>
<li class="chapter" data-level="9.5" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#sec:ANCOVA-power"><i class="fa fa-check"></i><b>9.5</b> Power considerations in ANCOVA</a></li>
<li class="chapter" data-level="9.6" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#models-with-multiple-covariates"><i class="fa fa-check"></i><b>9.6</b> Models with multiple covariates</a></li>
<li class="chapter" data-level="9.7" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#mediation-with-categorical-independent-variables"><i class="fa fa-check"></i><b>9.7</b> Mediation with categorical independent variables</a></li>
<li class="chapter" data-level="9.8" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#ancova-vs-difference-scores"><i class="fa fa-check"></i><b>9.8</b> ANCOVA vs difference scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#non-independence-in-linear-models"><i class="fa fa-check"></i><b>10.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="10.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#the-cheerleader-effect"><i class="fa fa-check"></i><b>10.2</b> The cheerleader effect</a></li>
<li class="chapter" data-level="10.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#as-a-oneway-anova"><i class="fa fa-check"></i><b>10.3</b> As a oneway ANOVA</a></li>
<li class="chapter" data-level="10.4" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#oneway-repeated-measures-anova"><i class="fa fa-check"></i><b>10.4</b> Oneway repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#within-subjects-composite-scores"><i class="fa fa-check"></i><b>10.4.1</b> Within-subjects composite scores</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-composite-for-between-subjects-effects"><i class="fa fa-check"></i><b>10.4.2</b> A composite for between-subjects effects</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#collecting-all-results-and-omnibus-tests"><i class="fa fa-check"></i><b>10.4.3</b> Collecting all results and omnibus tests</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#partitioning-the-variance"><i class="fa fa-check"></i><b>10.5</b> Partitioning the variance</a></li>
<li class="chapter" data-level="10.6" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-mixed-anova-with-between--and-within-subjects-effects"><i class="fa fa-check"></i><b>10.6</b> A mixed ANOVA with between- and within-subjects effects</a></li>
<li class="chapter" data-level="10.7" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#assumptions-1"><i class="fa fa-check"></i><b>10.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#omnibus-tests-and-sphericity"><i class="fa fa-check"></i><b>10.7.1</b> Omnibus tests and sphericity</a></li>
<li class="chapter" data-level="10.7.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#correcting-for-non-sphericity"><i class="fa fa-check"></i><b>10.7.2</b> Correcting for non-sphericity</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#in-practice-4"><i class="fa fa-check"></i><b>10.8</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#non-independence-in-linear-models-1"><i class="fa fa-check"></i><b>11.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="11.2" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#random-intercept-models"><i class="fa fa-check"></i><b>11.2</b> Random intercept models</a></li>
<li class="chapter" data-level="11.3" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-estimation-1"><i class="fa fa-check"></i><b>11.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="11.4" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-inference"><i class="fa fa-check"></i><b>11.4</b> Parameter inference</a></li>
<li class="chapter" data-level="11.5" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#application-of-the-random-intercepts-model"><i class="fa fa-check"></i><b>11.5</b> Application of the random-intercepts model</a></li>
<li class="chapter" data-level="11.6" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#models-with-random-intercepts-and-slopes"><i class="fa fa-check"></i><b>11.6</b> Models with random intercepts and slopes</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#correlation-between-random-effects"><i class="fa fa-check"></i><b>11.6.1</b> Correlation between random effects</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#crossed-random-effects-dating-partners-in-the-speed-dating-experiment"><i class="fa fa-check"></i><b>11.7</b> Crossed random effects: dating partners in the speed dating experiment</a></li>
<li class="chapter" data-level="11.8" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#choosing-the-random-effects-structure"><i class="fa fa-check"></i><b>11.8</b> Choosing the random effects structure</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html"><i class="fa fa-check"></i><b>12</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#link-functions"><i class="fa fa-check"></i><b>12.1</b> Link functions</a></li>
<li class="chapter" data-level="12.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#estimation-1"><i class="fa fa-check"></i><b>12.2</b> Estimation</a></li>
<li class="chapter" data-level="12.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#inference-in-generalized-linear-models"><i class="fa fa-check"></i><b>12.3</b> Inference in generalized linear models</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#wald-test"><i class="fa fa-check"></i><b>12.3.1</b> Wald test</a></li>
<li class="chapter" data-level="12.3.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>12.3.2</b> Likelihood-ratio test</a></li>
<li class="chapter" data-level="12.3.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#confidence-intervals-1"><i class="fa fa-check"></i><b>12.3.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-generalized-linear-models-overall-model-fit"><i class="fa fa-check"></i><b>12.4</b> Assessing model fit</a></li>
<li class="chapter" data-level="12.5" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#logistic-regression"><i class="fa fa-check"></i><b>12.5</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#parameter-interpretation"><i class="fa fa-check"></i><b>12.5.1</b> Parameter interpretation</a></li>
<li class="chapter" data-level="12.5.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-logistic-regression-metacognition"><i class="fa fa-check"></i><b>12.5.2</b> Example: Metacognition in visual perception</a></li>
<li class="chapter" data-level="12.5.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#using-a-different-link-function-probit-regression"><i class="fa fa-check"></i><b>12.5.3</b> Using a different link function: Probit regression</a></li>
<li class="chapter" data-level="12.5.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#welcome-back-paul"><i class="fa fa-check"></i><b>12.5.4</b> Welcome back Paul!</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>12.6</b> Poisson regression</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-poisson-regression-gestures"><i class="fa fa-check"></i><b>12.6.1</b> Example: Gestures in different social contexts</a></li>
<li class="chapter" data-level="12.6.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#overdispersion"><i class="fa fa-check"></i><b>12.6.2</b> Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#log-linear-models"><i class="fa fa-check"></i><b>12.7</b> Log-linear models</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#example-newspapers-and-voting"><i class="fa fa-check"></i><b>12.7.1</b> Example: Newspapers and voting</a></li>
<li class="chapter" data-level="12.7.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#a-three-way-table-example-rock-paper-scissors"><i class="fa fa-check"></i><b>12.7.2</b> A three-way table example: Rock-Paper-Scissors</a></li>
<li class="chapter" data-level="12.7.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sparse-data-and-empty-cells"><i class="fa fa-check"></i><b>12.7.3</b> Sparse data and empty cells</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.8</b> Multinomial logistic regression</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#baseline-category-logit"><i class="fa fa-check"></i><b>12.8.1</b> Baseline category logit</a></li>
<li class="chapter" data-level="12.8.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#example-rock-paper-scissors"><i class="fa fa-check"></i><b>12.8.2</b> Example: Rock-Paper-Scissors</a></li>
<li class="chapter" data-level="12.8.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#reconstructing-probabilities-of-responses"><i class="fa fa-check"></i><b>12.8.3</b> Reconstructing probabilities of responses</a></li>
<li class="chapter" data-level="12.8.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#alternative-logit-models-for-ordinal-categories"><i class="fa fa-check"></i><b>12.8.4</b> Alternative logit models for ordinal categories</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-glmer"><i class="fa fa-check"></i><b>12.9</b> Generalized linear mixed-effects models</a></li>
<li class="chapter" data-level="12.10" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#in-practice-5"><i class="fa fa-check"></i><b>12.10</b> In practice</a></li>
<li class="chapter" data-level="12.11" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#further-reading"><i class="fa fa-check"></i><b>12.11</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html"><i class="fa fa-check"></i><b>13</b> Path models (SEM 1)</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#graphical-models"><i class="fa fa-check"></i><b>13.1</b> Graphical models</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#exogenous-and-endogenous-variables"><i class="fa fa-check"></i><b>13.1.1</b> Exogenous and endogenous variables</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#regression-models"><i class="fa fa-check"></i><b>13.2</b> Regression models</a></li>
<li class="chapter" data-level="13.3" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#sec-sem-path-model-mediation"><i class="fa fa-check"></i><b>13.3</b> Mediation</a></li>
<li class="chapter" data-level="13.4" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#assumptions-and-estimation"><i class="fa fa-check"></i><b>13.4</b> Assumptions and estimation</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>13.4.1</b> The multivariate Normal distribution</a></li>
<li class="chapter" data-level="13.4.2" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#assumptions-exogenous-vs-endogenous-variables"><i class="fa fa-check"></i><b>13.4.2</b> Assumptions: Exogenous vs endogenous variables</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#model-fit"><i class="fa fa-check"></i><b>13.5</b> Model fit</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#test-of-overall-model-fit"><i class="fa fa-check"></i><b>13.5.1</b> Test of overall model fit</a></li>
<li class="chapter" data-level="13.5.2" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#approximate-fit-indices"><i class="fa fa-check"></i><b>13.5.2</b> Approximate fit indices</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#modification-indices"><i class="fa fa-check"></i><b>13.6</b> Modification indices</a></li>
<li class="chapter" data-level="13.7" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#model-comparison-1"><i class="fa fa-check"></i><b>13.7</b> Model comparison</a></li>
<li class="chapter" data-level="13.8" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#evaluation-and-selection-of-the-mediation-path-models"><i class="fa fa-check"></i><b>13.8</b> Evaluation and selection of the mediation path models</a></li>
<li class="chapter" data-level="13.9" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#a-more-complex-path-model"><i class="fa fa-check"></i><b>13.9</b> A more complex path model</a></li>
<li class="chapter" data-level="13.10" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#principles-in-constructing-path-models"><i class="fa fa-check"></i><b>13.10</b> Principles in constructing path models</a>
<ul>
<li class="chapter" data-level="13.10.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#identifiability"><i class="fa fa-check"></i><b>13.10.1</b> Identifiability</a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#model-equivalence"><i class="fa fa-check"></i><b>13.11</b> Model equivalence</a></li>
<li class="chapter" data-level="13.12" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#correlation-vs-causation"><i class="fa fa-check"></i><b>13.12</b> Correlation vs causation</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html"><i class="fa fa-check"></i><b>14</b> Latent variable models (SEM 2)</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#measurement-of-latent-variables"><i class="fa fa-check"></i><b>14.1</b> Measurement of latent variables</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#scaling-and-identification"><i class="fa fa-check"></i><b>14.1.1</b> Scaling and identification</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#confirmatory-factor-analysis"><i class="fa fa-check"></i><b>14.2</b> Confirmatory factor analysis</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#a-one-factor-model"><i class="fa fa-check"></i><b>14.2.1</b> A one-factor model</a></li>
<li class="chapter" data-level="14.2.2" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#heywood-cases"><i class="fa fa-check"></i><b>14.2.2</b> Heywood cases</a></li>
<li class="chapter" data-level="14.2.3" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#full-hierarchical-factor-model-for-the-bfi-2"><i class="fa fa-check"></i><b>14.2.3</b> Full hierarchical factor model for the BFI-2</a></li>
<li class="chapter" data-level="14.2.4" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#predicting-factor-scores"><i class="fa fa-check"></i><b>14.2.4</b> Predicting factor scores</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#exploratory-factor-and-principal-components-analysis"><i class="fa fa-check"></i><b>14.3</b> Exploratory factor and principal components analysis</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#determining-the-number-of-factors"><i class="fa fa-check"></i><b>14.3.1</b> Determining the number of factors</a></li>
<li class="chapter" data-level="14.3.2" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#factor-rotation"><i class="fa fa-check"></i><b>14.3.2</b> Factor rotation</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#in-practice-6"><i class="fa fa-check"></i><b>14.4</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html"><i class="fa fa-check"></i><b>15</b> Introduction to Bayesian estimation</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#fundamentals-of-bayesian-inference"><i class="fa fa-check"></i><b>15.1</b> Fundamentals of Bayesian inference</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#probability-in-times-of-covid"><i class="fa fa-check"></i><b>15.1.1</b> Probability in times of Covid</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#bayes-rule"><i class="fa fa-check"></i><b>15.1.2</b> Bayes’ rule</a></li>
<li class="chapter" data-level="15.1.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#we-missed-you-paul"><i class="fa fa-check"></i><b>15.1.3</b> We missed you Paul!</a></li>
<li class="chapter" data-level="15.1.4" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#the-marginal-likelihood-and-prior-predictive-distribution"><i class="fa fa-check"></i><b>15.1.4</b> The marginal likelihood and prior predictive distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#markov-chain-monte-carlo-mcmc"><i class="fa fa-check"></i><b>15.2</b> Markov chain Monte Carlo (MCMC)</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#convergence"><i class="fa fa-check"></i><b>15.2.1</b> Convergence</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#autocorrelation"><i class="fa fa-check"></i><b>15.2.2</b> Autocorrelation</a></li>
<li class="chapter" data-level="15.2.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#effective-sample-size"><i class="fa fa-check"></i><b>15.2.3</b> Effective sample size</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#concerning-prior-distributions"><i class="fa fa-check"></i><b>15.3</b> Concerning prior distributions</a></li>
<li class="chapter" data-level="15.4" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#multiple-regression-example"><i class="fa fa-check"></i><b>15.4</b> Multiple regression example</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html"><i class="fa fa-check"></i><b>16</b> Introduction to Bayesian hypothesis testing</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#hypothesis-testing-relative-evidence-and-the-bayes-factor"><i class="fa fa-check"></i><b>16.1</b> Hypothesis testing, relative evidence, and the Bayes factor</a></li>
<li class="chapter" data-level="16.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#parameter-estimates-and-credible-intervals"><i class="fa fa-check"></i><b>16.2</b> Parameter estimates and credible intervals</a></li>
<li class="chapter" data-level="16.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#a-bayesian-t-test"><i class="fa fa-check"></i><b>16.3</b> A Bayesian t-test</a></li>
<li class="chapter" data-level="16.4" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#bayes-factors-for-general-linear-models"><i class="fa fa-check"></i><b>16.4</b> Bayes factors for General Linear Models</a></li>
<li class="chapter" data-level="16.5" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#some-objections-to-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>16.5</b> Some objections to null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-is-not-a-proper-measure-of-evidential-support"><i class="fa fa-check"></i><b>16.5.1</b> The <span class="math inline">\(p\)</span>-value is not a proper measure of evidential support</a></li>
<li class="chapter" data-level="16.5.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-depends-on-researcher-intentions"><i class="fa fa-check"></i><b>16.5.2</b> The <span class="math inline">\(p\)</span>-value depends on researcher intentions</a></li>
<li class="chapter" data-level="16.5.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#results-of-a-nhst-are-often-misinterpreted"><i class="fa fa-check"></i><b>16.5.3</b> Results of a NHST are often misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#to-bayes-or-not-to-bayes-a-pragmatic-view"><i class="fa fa-check"></i><b>16.6</b> To Bayes or not to Bayes? A pragmatic view</a></li>
<li class="chapter" data-level="16.7" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#in-practice-7"><i class="fa fa-check"></i><b>16.7</b> In practice</a></li>
<li class="chapter" data-level="16.8" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#summary-3"><i class="fa fa-check"></i><b>16.8</b> “Summary”</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html"><i class="fa fa-check"></i><b>17</b> Being a responsible data analyst</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#consider-analysis-before-data-collection"><i class="fa fa-check"></i><b>17.1</b> Consider analysis before data collection</a></li>
<li class="chapter" data-level="17.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#explore-the-data"><i class="fa fa-check"></i><b>17.2</b> Explore the data</a></li>
<li class="chapter" data-level="17.3" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#evaluate-the-assumptions-underlying-your-analyses"><i class="fa fa-check"></i><b>17.3</b> Evaluate the assumptions underlying your analyses</a></li>
<li class="chapter" data-level="17.4" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#distinguish-between-confirmatory-and-exploratory-analyses"><i class="fa fa-check"></i><b>17.4</b> Distinguish between confirmatory and exploratory analyses</a></li>
<li class="chapter" data-level="17.5" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#aim-for-openness-and-reproducibility"><i class="fa fa-check"></i><b>17.5</b> Aim for openness and reproducibility</a></li>
<li class="chapter" data-level="17.6" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#communicate-clearly-and-concisely"><i class="fa fa-check"></i><b>17.6</b> Communicate clearly and concisely</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-multiple-regression-analysis"><i class="fa fa-check"></i><b>17.6.1</b> Example of reporting a multiple regression analysis</a></li>
<li class="chapter" data-level="17.6.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-factorial-anova"><i class="fa fa-check"></i><b>17.6.2</b> Example of reporting a factorial ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: Data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-SEM-latent-variable-models" class="section level1 hasAnchor" number="14">
<h1><span class="header-section-number">Chapter 14</span> Latent variable models (SEM 2)<a href="ch-SEM-latent-variable-models.html#ch-SEM-latent-variable-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The previous chapter concerned a multivariate model to describe relations between <em>observed</em> variables. In this Chapter, we will extend this idea by adding <strong>latent variables</strong>. Latent variables are variables which can not be directly observed. Instead, they can be measured or inferred via their relation with observed variables. A classic example in psychology is intelligence. Another example are personality traits such as extraversion. People’s tendency to be outwards facing is not directly observable. However, there are indicators (e.g., someone actively seeking out busy social situations, liking to be the centre of attention, etc.) which together may allow one to determine a person’s level of extraversion.</p>
<p>The measurement of psychological constructs is the focus of <strong>psychometrics</strong>, a field with a long history and its own societies and journals.</p>
<div id="measurement-of-latent-variables" class="section level2 hasAnchor" number="14.1">
<h2><span class="header-section-number">14.1</span> Measurement of latent variables<a href="ch-SEM-latent-variable-models.html#measurement-of-latent-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s start with a simple example: measuring temperature. Temperature is a physical quantity that reflects the kinetic energy of atoms in a substance or the air. This kinetic energy is not directly observable, but can be measured with a thermometer. A classic mercury thermometer has a bulb filled with mercury attached to a narrow tube of glass. An increase in heat expands the volume of the mercury so that the level of mercury in the narrow tube rises. In other words, the temperature is the <em>cause</em> of the state of the measurement device. Markers along the narrow tube can be placed such that the level of mercury in the tube corresponds to a standard scale, such as the Fahrenheit or Celcius scale. In the latter, the scale is calibrated such that <span class="math inline">\(0^\circ \textrm{C}\)</span> represents the freezing point of water, and <span class="math inline">\(100^\circ \textrm{C}\)</span> represents the boiling point of water. In the Fahrenheit scale, the freezing point of water corresponds to <span class="math inline">\(32^\circ \textrm{F}\)</span>, and the boiling point to <span class="math inline">\(212^\circ \textrm{F}\)</span>.</p>
<p>Thermometers are relatively accurate measurement devices, but not devoid of <strong>measurement error</strong>. That means that the measurement device can provide different readings for exactly the same true temperature. Such measurement error may be due to factors affecting the measurement device (e.g. the volume of mercury not being deterministically related to the temperature) as well as factors affecting the observer (e.g. the person reading the value of the thermometer not being able to see very small changes on the thermometer).</p>
<p>As a statistical model, we might propose something like the following linear model:
<span class="math display">\[\texttt{measurement}_i = \beta_0 + \beta_1 \times \textrm{temperature}_i + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0, \sigma_\epsilon)\]</span>
This model is depicted graphically in Figure <a href="ch-SEM-latent-variable-models.html#fig:temperature-measurement-model">14.1</a>. If temperature were a directly observed variable, this is just a simple regression model and we would be able to estimate all parameters (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\sigma_\epsilon\)</span>) using e.g. maximum likelihood. However, temperature is a latent variable and we can only observe the measurement.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:temperature-measurement-model"></span>
<img src="14-SEM-2_files/figure-html/temperature-measurement-model-1.svg" alt="Measurement model for temperature." width="80%" />
<p class="caption">
Figure 14.1: Measurement model for temperature.
</p>
</div>
<div id="scaling-and-identification" class="section level3 hasAnchor" number="14.1.1">
<h3><span class="header-section-number">14.1.1</span> Scaling and identification<a href="ch-SEM-latent-variable-models.html#scaling-and-identification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we start working towards general SEM models with latent variables, let’s write down our model for temperature measurements in more abstract terms as:
<span class="math display">\[\begin{aligned}
Y_{i} &amp;= \alpha + \lambda \times \eta_{i} + \epsilon_{i} \\
\eta_i &amp;\sim \mathbf{Normal}(\mu_\eta, \sigma_\eta)\\
\epsilon_i &amp;\sim \mathbf{Normal}(0, \sigma_\epsilon)
\end{aligned}\]</span>
Note that in addition to relabelling the intercept as <span class="math inline">\(\alpha\)</span>, the slope as <span class="math inline">\(\lambda\)</span>, and the latent variable as <span class="math inline">\(\eta\)</span>, we have also added an assumption about the values of the latent variable, namely that these are Normal-distributed. We need to make such an assumption about the distribution of the latent variable because without it, we cannot infer properties of the latent variable from measurements <span class="math inline">\(Y\)</span>.</p>
<p>The model above implies that <span class="math inline">\(Y\)</span> is Normal-distributed with mean and variance
<span class="math display">\[\begin{aligned}
\mu_Y &amp;= \alpha + \lambda \times \mu_\eta \\
\sigma^2_Y &amp;= \lambda^2 \times \sigma^2_\eta + \sigma^2_\epsilon
\end{aligned}
\]</span>
From observations of <span class="math inline">\(Y\)</span>, we can estimate the mean and variance of <span class="math inline">\(Y\)</span> in the usual manner as <span class="math inline">\(\hat{\mu}_Y = \overline{Y}\)</span> and <span class="math inline">\(\hat{\sigma}^2_Y = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}\)</span>. We then have two “observed” values, whilst our latent variable model has 5 parameters. We cannot estimate 5 parameters from what are effectively two properties of the data. For example, consider
<span class="math display">\[\hat{\mu}_Y = \alpha + \lambda \times \mu_\eta\]</span></p>
<p>We could set <span class="math inline">\(\alpha = \hat{\mu}_Y\)</span> and <span class="math inline">\(\lambda = 0\)</span>. This would provide a perfect fit, but <span class="math inline">\(\mu_\eta\)</span> could then be anything. We could also set <span class="math inline">\(\alpha = 0\)</span>, <span class="math inline">\(\lambda = 1\)</span>, and <span class="math inline">\(\mu_\eta = \hat{\mu}_Y\)</span>. This would again provide a perfect fit, but so would setting <span class="math inline">\(\alpha = 0\)</span>, <span class="math inline">\(\lambda = 2\)</span>, and <span class="math inline">\(\mu_\eta = \tfrac{1}{2} \hat{\mu}_Y\)</span>.</p>
<p>There are two issues here. The first is the number of parameters relative to the number of observed properties of the data. The second is that latent variables don’t have an inherent numerical scale. Temperature can be measured in degrees Celcius or Fahrenheit. Both are valid ways to provide a number to temperature. But temperature itself does not has an inherent number beyond such measurements. The numeric scale of a latent variable is therefore arbitrary. What matters is the relation between a latent variable and its measurements: an increase in the latent variable results in an increase in the measurements. Beyond that, we can choose the scale of the latent variable as we like. This is similar to regression, where centering or changing the scale of the predictor affects the intercept and slopes, but does not change the model fit itself.</p>
<p>As the scale of the latent variable is arbitrary, we can choose how the numerical values of the latent variable corresponds to numerical values of the measurements. There are two common choices to scale latent variables. In both, we set <span class="math inline">\(\mu_\eta = 0\)</span>. The first is to then set <span class="math inline">\(\lambda = 1\)</span>. This implies that a one-unit increase in the latent variable is equal to a one-unit increase in the measurement. The second way is to set <span class="math inline">\(\sigma^2_\eta = 1\)</span>. This implies that the latent variable follows a standard Normal distribution. A one-unit increase in the latent variable would then result in a one standard deviation increase in the measurement (i.e. an increase of <span class="math inline">\(\sigma_y\)</span>). As the correlation between the latent variable and the measurement is the same for both these choices, they are both valid ways to scale the latent variable. The first is more common nowadays, and has the benefit that the scale of the latent variable may be more easily interpretable, as it relates directly to the scale of the measurement.</p>
<p>Let’s focus on the first choice of scaling. Setting <span class="math inline">\(\mu_\eta=0\)</span> and <span class="math inline">\(\lambda = 1\)</span> reduces the number of free parameters to 3. This is still more than the two observed properties of the data. Hence, there is still no way to uniquely determine the remaining parameters.</p>
<p>Now, suppose we have readings from <em>two</em> thermometers, <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>. Suppose also that we don’t know the scale of each (they could measure degrees Celcius, Fahrenheit, or something else, and each could have a different scale), and that each thermometer can have a different measurement error. All we know is that readings on both are caused by the true temperature <span class="math inline">\(\eta\)</span>. Setting <span class="math inline">\(\mu_\eta = 0\)</span>, we can express the model for simultaneous readings of both thermometers as:
<span class="math display">\[\begin{aligned}
Y_{1,i} &amp;= \alpha_1 + \lambda_1 \times \eta_{i} + \epsilon_{1,i} \\
Y_{2,i} &amp;= \alpha_2 + \lambda_2 \times \eta_{i} + \epsilon_{2,i} \\
\eta_i &amp;\sim \mathbf{Normal}(0, \sigma_\eta)\\
\epsilon_{1,i} &amp;\sim \mathbf{Normal}(0, \sigma_{\epsilon_1}) \\
\epsilon_{2,i} &amp;\sim \mathbf{Normal}(0, \sigma_{\epsilon_2})
\end{aligned}\]</span>
Using variance-covariance algebra (Section <a href="ch-SEM-path-models.html#sec-SEM-path-models-variance-covariance-algebra">13.4.1.1</a>), this implies that
<span class="math display">\[\begin{equation}
\begin{aligned}
\left( \begin{matrix} Y_{1,i} \\ Y_{2,i} \end{matrix} \right)
&amp;\sim \mathbf{Normal}\left(\boldsymbol{\mu}_Y, \boldsymbol{\Sigma}_Y \right) \\
&amp;\sim \mathbf{Normal}\left( \left[ \begin{matrix} \alpha_1 \\ \alpha_2 \end{matrix} \right] , \left[ \begin{matrix} \lambda_1^2 \times \sigma_{\eta}^2 + \sigma_{\epsilon_1}^2 &amp; \lambda_1 \times \lambda_2 \times \sigma^2_\eta  \\  \lambda_1 \times \lambda_2 \times \sigma^2_\eta &amp; \lambda_2^2 \times \sigma_{\eta}^2 + \sigma_{\epsilon_2}^2 \end{matrix} \right] \right)
\end{aligned}
\end{equation}\]</span>
We have two parameters (<span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span>) for two means (<span class="math inline">\(\mu_{Y_1}\)</span> and <span class="math inline">\(\mu_{Y_2}\)</span>), which is fine. However, the implied covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}_Y\)</span> contains five parameters: <span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>, <span class="math inline">\(\sigma_\eta\)</span>, <span class="math inline">\(\sigma_{\epsilon_1}\)</span>, <span class="math inline">\(\sigma_{\epsilon_2}\)</span>. But the observed covariance matrix has three unique values: <span class="math inline">\(\hat{\sigma}^2_{Y_1}\)</span>, <span class="math inline">\(\hat{\sigma}^2_{Y_2}\)</span>, and <span class="math inline">\(\hat{\sigma}_{Y_1,Y_2}\)</span>. To scale the latent variable, we can set one of the <span class="math inline">\(\lambda\)</span>’s to 1, e.g. <span class="math inline">\(\lambda_1 = 1\)</span>. But we cannot set both <span class="math inline">\(\lambda\)</span>’s to 1, as the thermometers may have a different scale. We could alternatively set <span class="math inline">\(\sigma_\epsilon = 1\)</span>. Both choices would reduce the number of parameters to four. We therefore still have too many parameters relative to the properties of the data.</p>
<p>When we have readings from <em>three</em> thermometers, we finally get to an estimable model. The model for simultaneous readings of thermometers <span class="math inline">\(Y_{j}\)</span> can be expressed as:
<span class="math display">\[\begin{aligned}
Y_{j,i} &amp;= \alpha_1 + \lambda_j \times \eta_{i} + \epsilon_{j,i} &amp;&amp; j = 1, \ldots, 3\\
\eta_i &amp;\sim \mathbf{Normal}(0, \sigma_\eta)\\
\epsilon_{j,i} &amp;\sim \mathbf{Normal}(0, \sigma_{\epsilon_1}) &amp;&amp; j = 1, \ldots, 3
\end{aligned}\]</span>
which implies
<span class="math display">\[\begin{equation}
\begin{aligned}
\left( \begin{matrix} Y_{1,i} \\ Y_{2,i} \\ Y_{3,i} \end{matrix} \right)
&amp;\sim \mathbf{Normal}\left( \left[ \begin{matrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{matrix} \right] , \left[ \begin{matrix} \lambda_1^2 \times \sigma_{\eta}^2 + \sigma_{\epsilon_1}^2 &amp; \lambda_1 \times \lambda_2 \times \sigma^2_\eta &amp; \lambda_1 \times \lambda_3 \times \sigma^2_\eta \\  \lambda_1 \times \lambda_2 \times \sigma^2_\eta &amp; \lambda_2^2 \times \sigma_{\eta}^2 + \sigma_{\epsilon_2}^2 &amp; \lambda_2 \times \lambda_3 \times \sigma^2_\eta \\ \lambda_1 \times \lambda_3 \times \sigma^2_\eta &amp; \lambda_2 \times \lambda_3 \times \sigma^2_\eta &amp; \lambda_3^2 \times \sigma_{\eta}^2 + \sigma_{\epsilon_3}^2 \end{matrix} \right] \right)
\end{aligned}
\end{equation}\]</span>
The implied covariance matrix has seven parameters, whilst the observed covariance matrix has six unique values. Setting e.g. <span class="math inline">\(\lambda_1 = 1\)</span> to scale the latent variable, we are left with six parameters for six observed (co-)variances. Hence, although saturated, the model is estimable.</p>
<p>To measure a latent variable, we thus need at least three measured variables. Such measured variables are also called indicators.</p>
</div>
</div>
<div id="confirmatory-factor-analysis" class="section level2 hasAnchor" number="14.2">
<h2><span class="header-section-number">14.2</span> Confirmatory factor analysis<a href="ch-SEM-latent-variable-models.html#confirmatory-factor-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Factor analysis is a technique for the measurement of latent variables when the indicators or measured variables are assumed to follow a (multivariate) Normal distribution. A distinction can be made between <em>confirmatory</em> and <em>exploratory</em> factor analysis. The latter, as the name suggests, is part of exploratory data analysis. Exploratory factor analysis aims to describe relations between observed variables via a smaller set of latent variables or factors, and was first proposed by <span class="citation">Spearman (<a href="#ref-spearman1904general">1904</a>)</span>. Exploratory factor analysis is essentially based on saturated models. Confirmatory factor analysis, on the other hand, poses more restrictions. A main goal of confirmatory factor analysis is to assess the viability of an assumed measurement model.</p>
<p>In the following, we will introduce confirmatory factor analysis with an example of personality measurement. “Big Five” personality traits such as extraversion are generally assessed with self-report questionnaires. One commonly used questionnaire is the Big Five Inventory [BFI; <span class="citation">John, Donahue, &amp; Kentle (<a href="#ref-john1991big">1991</a>)</span>]. In the BFI-2 <span class="citation">(<a href="#ref-soto2017next">Soto &amp; John, 2017</a>)</span>, participants are asked to rate their agreement with 60 statements on a 5-point scale ranging “disagree strongly” to “agree strongly”. There are 12 statements relating to each of the following main personality dimensions: Extraversion, Agreeableness, Conscientiousness, Negative Emotionality (traditionally called “neuroticism”), and Open-Mindedness (tranditionally called “openness to experience”). Example statements are related to extraversion are <em>“I am someone who is outgoing, sociable”</em> and <em>“I am am someone who is sometimes shy, introverted”</em>. Note that agreement with the first statement should be <em>positively</em> related to extraversion, whilst agreement with the second statement should be <em>negatively</em> related to extraversion. Mixing such positively and negatively stated items is common practice in the design of psychological measurement questionnaires. Here, we will consider data collected with a Czech translation of the BFI-2 <span class="citation">(<a href="#ref-hvrebivckova2020big">Hřebı́čková et al., 2020</a>)</span>, with a total of <span class="math inline">\(n=1733\)</span> respondents.</p>
<div id="a-one-factor-model" class="section level3 hasAnchor" number="14.2.1">
<h3><span class="header-section-number">14.2.1</span> A one-factor model<a href="ch-SEM-latent-variable-models.html#a-one-factor-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The BFI-2 contains <span class="math inline">\(k=12\)</span> items which relate to extraversion. As these items pertain to the same psychological dimension, we would expect answers to them to be correlated. A confirmatory one-factor model aims to account for these correlations by assuming that all answers are “caused” by a single underlying latent variable (factor). The one-factor model can be expressed as
<span class="math display">\[\begin{aligned}
Y_{j,i} &amp;= \alpha_j + \lambda_j \times \eta_{i} + \epsilon_{j,i} &amp;&amp; j=1,\ldots,k\\
\eta_i &amp;\sim \mathbf{Normal}(0, \sigma_\eta)\\
\epsilon_{j,i} &amp;\sim \mathbf{Normal}(0, \sigma_{\epsilon_j}) &amp;&amp; j=1,\ldots,k
\end{aligned}\]</span>
In the context of factor analysis, the latent variable <span class="math inline">\(\eta\)</span> is referred to as a latent factor, and the parameters <span class="math inline">\(\lambda_j\)</span> which relate the factor to the indicators (measured variables) <span class="math inline">\(Y_j\)</span> as <strong>factor loadings</strong>. Each indicator is allowed to have measurement errors, such that none provides a perfect measurement of the latent factor. But a key assumption is that the measurement errors of different indicators are independent and therefore uncorrelated. Any correlation between scores on the indicators is then fully accounted for by the underlying latent factor. This independence of measurement errors allow indicators to be more informative about the latent factor. Going back to our temperature example, if one thermometer provides a too high measurement, that would not imply that other thermometers would also provide too high measurements, allowing the measurement errors to cancel each other out. Correlated measurement errors, on the other hand, make this more difficult. Positively correlated measurement errors would imply that if one thermometer provides a too high measurement, so would others. And if one thermometer provides a too low measurement, so would others. When measurement errors are highly correlated, there is then relatively little to gain from adding more measurement instruments.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:BFI2-extr-1-plot"></span>
<img src="14-SEM-2_files/figure-html/BFI2-extr-1-plot-1.svg" alt="Graphical representation of a single factor model for Extraversion. Items i1, i16, i31, and i46 relate to the Sociability facet of Extraversion, with items i16 and i31 stated in the negative direction. Items i6, i21, i36, and i51 relate to the Assertiveness facet of Extraversion, with items i36 and i51 stated in the negative direction. Items 11, 26, 41, 56 relate to the Energy Level facet of Extraversion, with items i11 and i26 stated in the negative direction." width="100%" />
<p class="caption">
Figure 14.2: Graphical representation of a single factor model for Extraversion. Items i1, i16, i31, and i46 relate to the Sociability facet of Extraversion, with items i16 and i31 stated in the negative direction. Items i6, i21, i36, and i51 relate to the Assertiveness facet of Extraversion, with items i36 and i51 stated in the negative direction. Items 11, 26, 41, 56 relate to the Energy Level facet of Extraversion, with items i11 and i26 stated in the negative direction.
</p>
</div>
<p>Figure <a href="ch-SEM-latent-variable-models.html#fig:BFI2-extr-1-plot">14.2</a> shows the estimated parameters of a single factor model for the 12 items related to extraversion in the BFI-2. The factor loading of the first item was fixed to 1 in order to scale the latent factor. Note that all estimated factor loadings are positive, whilst 6 of the 12 items were stated in a negative direction (i.e. higher scores should indicate <em>less</em> extraversion). If all items relate to extraversion in the intended manner, we should expect a mix of positive and negative factor loadings. Table <a href="ch-SEM-latent-variable-models.html#tab:BFI2-extr-1-results">14.1</a> shows the parameter estimates and test results, as well as the model fit indices. As can be seen there, the single factor model is rejected by the overall model <span class="math inline">\(\chi^2\)</span> test, and the CFI, SRMR and RMSEA indicate relatively poor model fit.</p>
<table>
<caption>
<span id="tab:BFI2-extr-1-results">Table 14.1: </span> Results of a single factor model for extraversion.
</caption>
<table style="padding-right:20px;padding-left:20px;">
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'>Model</td>
</tr>
<tr>
<td>
</td>
<td colspan = '1'; align = 'center'>Estimate</td>
<td colspan = '1'; align = 'center'>Std. Err.</td>
<td colspan = '1'; align = 'center'>z</td>
<td colspan = '1'; align = 'center'>p</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Factor Loadings</span></td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">E</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
1.43
</td>
<td>
0.05
</td>
<td>
27.13
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
1.25
</td>
<td>
0.05
</td>
<td>
25.06
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
1.14
</td>
<td>
0.04
</td>
<td>
25.30
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i6
</td>
<td>
0.97
</td>
<td>
0.04
</td>
<td>
23.55
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
1.15
</td>
<td>
0.05
</td>
<td>
24.69
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
0.55
</td>
<td>
0.04
</td>
<td>
14.14
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
1.05
</td>
<td>
0.04
</td>
<td>
23.57
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i11
</td>
<td>
0.47
</td>
<td>
0.04
</td>
<td>
11.36
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
0.51
</td>
<td>
0.03
</td>
<td>
14.72
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
0.79
</td>
<td>
0.04
</td>
<td>
20.15
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
0.67
</td>
<td>
0.04
</td>
<td>
18.45
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Residual Variances</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
0.62
</td>
<td>
0.02
</td>
<td>
26.29
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
0.72
</td>
<td>
0.03
</td>
<td>
23.96
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
0.84
</td>
<td>
0.03
</td>
<td>
25.80
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
0.66
</td>
<td>
0.03
</td>
<td>
25.63
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i6
</td>
<td>
0.65
</td>
<td>
0.02
</td>
<td>
26.64
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
0.76
</td>
<td>
0.03
</td>
<td>
26.03
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
0.93
</td>
<td>
0.03
</td>
<td>
28.81
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
0.77
</td>
<td>
0.03
</td>
<td>
26.63
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i11
</td>
<td>
1.12
</td>
<td>
0.04
</td>
<td>
29.06
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
0.72
</td>
<td>
0.03
</td>
<td>
28.75
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
0.75
</td>
<td>
0.03
</td>
<td>
27.81
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
0.69
</td>
<td>
0.02
</td>
<td>
28.18
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Latent Variances</span></td>
</tr>
<tr>
<td>
E
</td>
<td>
0.49
</td>
<td>
0.03
</td>
<td>
14.86
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Fit Indices</span></td>
</tr>
<tr>
<td>
χ<sup>2</sup>
</td>
<td>
1594.65(54)
</td>
<td>
</td>
<td>
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
CFI
</td>
<td>
0.79
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
SRMR
</td>
<td>
0.08
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA
</td>
<td>
0.13
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA (lower bound)
</td>
<td>
0.12
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA (upper bound)
</td>
<td>
0.13
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
AIC
</td>
<td>
57031.05
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
BIC
</td>
<td>
57162.03
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan = '5'; align = 'left'><sup>+</sup>Fixed parameter</td>
</tr>
</table>
<p><br></p>
</table>
<p>The poor model fit of the single factor model indicates that the correlations between answers on the 12 items are not completely accounted for by a single factor. <span class="citation">Soto &amp; John (<a href="#ref-soto2017next">2017</a>)</span> suggest that participants’ responses to the BFI-2 might be affected by their “acquiescent response style”. This refers to the tendency of an individual to
consistently agree or consistently disagree with questionnaire items, regardless of their content. Such general (dis)agreement would distort the factor loadings. They suggest to account for Acquiescence by including a second factor in the model, with fixed factor loadings on all items. By fixing the factor loadings to be identical for all items, this factor can account for a general tendency to provide low or high scores on all items, as such a tendency would be expected to affect answers on all items in the same way.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:BFI2-extr-1acq-plot"></span>
<img src="14-SEM-2_files/figure-html/BFI2-extr-1acq-plot-1.svg" alt="Graphical representation of a single factor model for Extraversion with an additional Acquiescence factor." width="100%" />
<p class="caption">
Figure 14.3: Graphical representation of a single factor model for Extraversion with an additional Acquiescence factor.
</p>
</div>
<p>By fixing all the factor loadings, this additional factor only requires one additional parameter for the variance of the latent Acquiescence factor. The resulting model is depicted in Figure <a href="ch-SEM-latent-variable-models.html#fig:BFI2-extr-1acq-plot">14.3</a> and the resulting estimates and fit measures are provided in Table <a href="ch-SEM-latent-variable-models.html#tab:BFI2-extr-1acq-results">14.2</a>. Whilst there are now a few negative factor loadings and the fit is somewhat improved, the model is still not adequate.</p>
<table>
<caption>
<span id="tab:BFI2-extr-1acq-results">Table 14.2: </span> Results of a single factor model for Extraversion with an additional Acquiescence factor.
</caption>
<table style="padding-right:20px;padding-left:20px;">
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'>Model</td>
</tr>
<tr>
<td>
</td>
<td colspan = '1'; align = 'center'>Estimate</td>
<td colspan = '1'; align = 'center'>Std. Err.</td>
<td colspan = '1'; align = 'center'>z</td>
<td colspan = '1'; align = 'center'>p</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Factor Loadings</span></td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">E</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
1.89
</td>
<td>
0.10
</td>
<td>
18.37
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
1.59
</td>
<td>
0.09
</td>
<td>
17.97
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
1.17
</td>
<td>
0.07
</td>
<td>
16.97
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i6
</td>
<td>
0.70
</td>
<td>
0.06
</td>
<td>
12.43
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
0.93
</td>
<td>
0.07
</td>
<td>
14.23
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
0.20
</td>
<td>
0.06
</td>
<td>
3.36
</td>
<td>
.001
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
0.73
</td>
<td>
0.06
</td>
<td>
12.11
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i11
</td>
<td>
-0.16
</td>
<td>
0.07
</td>
<td>
-2.40
</td>
<td>
.016
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
-0.07
</td>
<td>
0.06
</td>
<td>
-1.25
</td>
<td>
.210
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
0.44
</td>
<td>
0.05
</td>
<td>
8.19
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
0.20
</td>
<td>
0.05
</td>
<td>
3.90
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Acq</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i6
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i11
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Residual Variances</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
0.60
</td>
<td>
0.02
</td>
<td>
26.34
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
0.49
</td>
<td>
0.03
</td>
<td>
15.48
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
0.72
</td>
<td>
0.03
</td>
<td>
22.73
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
0.64
</td>
<td>
0.03
</td>
<td>
25.64
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i6
</td>
<td>
0.67
</td>
<td>
0.02
</td>
<td>
27.33
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
0.82
</td>
<td>
0.03
</td>
<td>
27.38
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
0.90
</td>
<td>
0.03
</td>
<td>
27.16
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
0.80
</td>
<td>
0.03
</td>
<td>
27.66
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i11
</td>
<td>
0.94
</td>
<td>
0.04
</td>
<td>
25.56
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
0.58
</td>
<td>
0.02
</td>
<td>
23.92
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
0.71
</td>
<td>
0.03
</td>
<td>
27.27
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
0.60
</td>
<td>
0.02
</td>
<td>
25.98
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Latent Variances</span></td>
</tr>
<tr>
<td>
E
</td>
<td>
0.26
</td>
<td>
0.03
</td>
<td>
8.79
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Acq
</td>
<td>
0.28
</td>
<td>
0.01
</td>
<td>
19.44
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Latent Covariances</span></td>
</tr>
<tr>
<td>
E w/Acq
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Fit Indices</span></td>
</tr>
<tr>
<td>
χ<sup>2</sup>
</td>
<td>
1214.16(53)
</td>
<td>
</td>
<td>
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
CFI
</td>
<td>
0.84
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
SRMR
</td>
<td>
0.07
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA
</td>
<td>
0.11
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA (lower bound)
</td>
<td>
0.11
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA (upper bound)
</td>
<td>
0.12
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
AIC
</td>
<td>
56652.56
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
BIC
</td>
<td>
56789.00
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan = '5'; align = 'left'><sup>+</sup>Fixed parameter</td>
</tr>
</table>
<p><br></p>
</table>
<p>The BFI-2 was designed to cover different aspects of each personality trait. For example, extraversion was assumed to consist of the “facets” sociability, assertiveness, and energy level. The BFI-2 contains four items for each of these facets. Insofar as these facets reflect different aspects of extraversion, we might expect then to be related to general extraversion, but to also be somewhat independent. To allow for this possibility, we could consider a model where general extraversion “causes” values on each latent facet, which in turn cause scores on the different items. Including the Aqcuiescence factor, the resulting model is depicted in Figure <a href="ch-SEM-latent-variable-models.html#fig:BFI2-extr-31acq-plot">14.4</a>. Note that the three facets have additional residual variance, but any correlation between the facets is entirely accounted for by the higher-order factor. As there are just three facets for the higher-order extraversion factor, that factor is “just-identified” (similar to needing at least three indicators to measure one factor). The part of the model consisting of one higher-order factor and three lower-order factors is saturated, meaning that the model can perfectly account for any the variances and covariances of the facets. Because each facet loads onto four indicators, the part of the model linking facets to indicators is not saturated. As there are 12 observed variables, there are 12 means, 12 variances, and <span class="math inline">\(\frac{12 \times 11}{2} = 66\)</span> covariances. A fully saturated model would thus have <span class="math inline">\(\text{npar}(S) = 90\)</span> parameters. The present model is more constrained, using <span class="math inline">\(\text{npar}(M) = 49\)</span> parameters.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:BFI2-extr-31acq-plot"></span>
<img src="14-SEM-2_files/figure-html/BFI2-extr-31acq-plot-1.svg" alt="Graphical representation of a hierarchical factor model for Extraversion with an additional Acquiescence factor. Es represents extraversion-sociability, Ea  extraversion-assertiveness, and Ee extraversion-energy-level" width="100%" />
<p class="caption">
Figure 14.4: Graphical representation of a hierarchical factor model for Extraversion with an additional Acquiescence factor. Es represents extraversion-sociability, Ea extraversion-assertiveness, and Ee extraversion-energy-level
</p>
</div>
<p>Table <a href="ch-SEM-latent-variable-models.html#tab:BFI2-extr-31acq-results">14.3</a> provides more detailed results. Although the model is rejected by the overall fit chi-square test, the fit indices indicate a mostly reasonable fit. Inspecting the estimated variance of the latent variables shows something peculiar though: the variance of the <span class="math inline">\(\texttt{Es}\)</span> factor is estimated to be negative. A variance can never be smaller than 0, so this estimate is by definition not a good representation of the true variance in the DGP. Negative variance estimates are common in structural equation modelling <span class="citation">(<a href="#ref-kolenikov2012testing">Kolenikov &amp; Bollen, 2012</a>)</span>.</p>
<table>
<caption>
<span id="tab:BFI2-extr-31acq-results">Table 14.3: </span> Results of a hierarchical factor model for Extraversion with an additional Acquiescence factor.
</caption>
<table style="padding-right:20px;padding-left:20px;">
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'>Model</td>
</tr>
<tr>
<td>
</td>
<td colspan = '1'; align = 'center'>Estimate</td>
<td colspan = '1'; align = 'center'>Std. Err.</td>
<td colspan = '1'; align = 'center'>z</td>
<td colspan = '1'; align = 'center'>p</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Factor Loadings</span></td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">E</span></td>
</tr>
<tr>
<td>
Es
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
Ea
</td>
<td>
0.42
</td>
<td>
0.12
</td>
<td>
3.63
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Ee
</td>
<td>
0.22
</td>
<td>
0.07
</td>
<td>
3.08
</td>
<td>
.002
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Es</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
1.95
</td>
<td>
0.10
</td>
<td>
19.53
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
1.53
</td>
<td>
0.08
</td>
<td>
19.29
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
1.19
</td>
<td>
0.06
</td>
<td>
18.45
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Ea</span></td>
</tr>
<tr>
<td>
i6
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
1.68
</td>
<td>
0.09
</td>
<td>
18.66
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
0.49
</td>
<td>
0.06
</td>
<td>
8.90
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
1.30
</td>
<td>
0.07
</td>
<td>
19.13
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Ee</span></td>
</tr>
<tr>
<td>
i11
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
0.23
</td>
<td>
0.09
</td>
<td>
2.52
</td>
<td>
.012
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
1.47
</td>
<td>
0.18
</td>
<td>
8.37
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Acq</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i6
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i11
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Residual Variances</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
0.59
</td>
<td>
0.02
</td>
<td>
26.08
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
0.41
</td>
<td>
0.03
</td>
<td>
12.01
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
0.73
</td>
<td>
0.03
</td>
<td>
23.19
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
0.63
</td>
<td>
0.02
</td>
<td>
25.30
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i6
</td>
<td>
0.56
</td>
<td>
0.02
</td>
<td>
25.04
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
0.35
</td>
<td>
0.03
</td>
<td>
10.99
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
0.85
</td>
<td>
0.03
</td>
<td>
27.30
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
0.55
</td>
<td>
0.03
</td>
<td>
21.43
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i11
</td>
<td>
0.92
</td>
<td>
0.04
</td>
<td>
25.88
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
0.60
</td>
<td>
0.02
</td>
<td>
24.62
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
0.65
</td>
<td>
0.03
</td>
<td>
23.85
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
0.39
</td>
<td>
0.04
</td>
<td>
10.35
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Latent Variances</span></td>
</tr>
<tr>
<td>
E
</td>
<td>
0.38
</td>
<td>
0.09
</td>
<td>
4.02
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Es
</td>
<td>
-0.09
</td>
<td>
0.09
</td>
<td>
-0.96
</td>
<td>
.335
</td>
</tr>
<tr>
<td>
Ea
</td>
<td>
0.23
</td>
<td>
0.02
</td>
<td>
9.30
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Ee
</td>
<td>
0.13
</td>
<td>
0.02
</td>
<td>
6.25
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Acq
</td>
<td>
0.22
</td>
<td>
0.02
</td>
<td>
13.41
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Latent Covariances</span></td>
</tr>
<tr>
<td>
E w/Acq
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Fit Indices</span></td>
</tr>
<tr>
<td>
χ<sup>2</sup>
</td>
<td>
401.63(51)
</td>
<td>
</td>
<td>
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
CFI
</td>
<td>
0.95
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
SRMR
</td>
<td>
0.05
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA
</td>
<td>
0.06
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA (lower bound)
</td>
<td>
0.06
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA (upper bound)
</td>
<td>
0.07
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
AIC
</td>
<td>
55844.03
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
BIC
</td>
<td>
55991.38
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan = '5'; align = 'left'><sup>+</sup>Fixed parameter</td>
</tr>
</table>
<p><br></p>
</table>
</div>
<div id="heywood-cases" class="section level3 hasAnchor" number="14.2.2">
<h3><span class="header-section-number">14.2.2</span> Heywood cases<a href="ch-SEM-latent-variable-models.html#heywood-cases" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Negative variance estimates (and estimated correlations with an absolute value larger than 1) are also called “<strong>Heywood cases</strong>” (after the statistician H. B. Heywood). Such illogical estimates can be problematic, as they may indicate model misspecification. Other reasons for negative variance estimates include sampling fluctuations and true variances close to 0, outliers, missing data, nonconvergence of the numerical estimation procedure, and empirical underidentification (e.g. the parameters cannot be estimated adequately due to extremely low or high correlations).</p>
<p>For negative variances, <span class="citation">Kolenikov &amp; Bollen (<a href="#ref-kolenikov2012testing">2012</a>)</span> suggest to perform a hypothesis test whether the “true variance” is smaller than 0. If the best possible value of a variance with respect to the DGP is negative, that would indicate a structural misspecification of the model (i.e. the structural model does not correspond to the structure of the DGP). Confidence intervals can also be used for the same purpose. In the example above, the Wald test for the variance of Es is not significant. As such, there is no strong evidence for a misspecification, and we could conclude that the negative error variance might be due to estimation error.</p>
</div>
<div id="full-hierarchical-factor-model-for-the-bfi-2" class="section level3 hasAnchor" number="14.2.3">
<h3><span class="header-section-number">14.2.3</span> Full hierarchical factor model for the BFI-2<a href="ch-SEM-latent-variable-models.html#full-hierarchical-factor-model-for-the-bfi-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The BFI-2 was designed with a hierarchical structure in mind for each of the “big five” factors. Each factor was assumed to be further broken down into three facets: Extraversion (E) into sociability (Es), assertiveness (Ea) and energy level (Ee), Agreeableness into compassion (Ac), respectfulness (Ar), and trust (At), Conscientiousness into organization (Co), productiveness (Cp), and responsibility (Cr), Negative Emotionality into anxiety (Na), depression (Nd), and emotional volatility (Ne), and Open-Mindedness into intellectual curiosity (Oi), aesthetic sensitivity (Oa), and creative imagination (Oc. Earlier, we estimated a hierarchical factor model for extraversion. Here, we estimate a hierarchical factor model for all “big five” factors, also including the additional Acquiescence factor.</p>
<p>Figure <a href="ch-SEM-latent-variable-models.html#fig:BFI2-full-31acq-plot">14.5</a> shows the estimated model. Note that the higher-order latent factors were assumed to be independent (i.e. their covariances were fixed to 0). Independence of the factors implies that each represents a separable aspect of personality, in the sense that knowing someone’s relative extraversion would not help you infer their relative agreeableness. Each higher-order factor loads on three lower-order factors (the “facets”). These facets have additional residual variance, but any correlation between the facets is entirely accounted for by the higher-order factors. As in the earlier hierarchical model for extraversion, these higher-order factors are “just-identified”.</p>
<p>The BFI-2 consists of <span class="math inline">\(P=60\)</span> items. Hence, there are 60 means, 60 variances, and <span class="math inline">\(\frac{60 \times 59}{2} = 1770\)</span> covariances. A saturated model for this data would thus have a total of <span class="math inline">\(\text{npar}(S) = \frac{P \times (P-1)}{2} + 2 \times P = 1890\)</span> parameters. The estimated model has a total of <span class="math inline">\(\text{npar}(M) = 196\)</span> parameters. Whilst complex, the model is rather restricted as compared to a saturated model. Detailed results are provided in Table <a href="ch-SEM-latent-variable-models.html#tab:BFI2-cfa-hier-results">14.6</a> at the end of this chapter.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:BFI2-full-31acq-plot"></span>
<img src="14-SEM-2_files/figure-html/BFI2-full-31acq-plot-1.svg" alt="Graphical representation of a hierarchical factor model for all personality domains and an additional Acquiescence factor." width="100%" />
<p class="caption">
Figure 14.5: Graphical representation of a hierarchical factor model for all personality domains and an additional Acquiescence factor.
</p>
</div>
<p>Table <a href="ch-SEM-latent-variable-models.html#tab:BFI2-cfa-hier-comparison">14.4</a> shows selected fit measures of this model, as well as of an alternative Dependent model which allows the “big five” factors to be correlated whilst all are independent from the Acquiescence factor. Fit measures for the Independent model are less then convincing, with a significant overall model fit test, a too low value for the CFI, and a too high one for the SRMR. The Dependent model fits better, and is acceptable according to the RMSEA and SRMR, but the CFI indicates relatively poor fit, and this model is also rejected by the overall model fit. As the Independent model is nested in the Dependent model, the models can be compared with a likelihood ratio test. The result is significant, indicating that the Dependent model provides a better fit and hence the factors are unlikely to be independent.</p>
<table>
<caption><span id="tab:BFI2-cfa-hier-comparison">Table 14.4: </span>Fit measures for hierarchical factor models for the BFI-2 with an additional Acquiescence factor. The Independent model assumes all factors are independent. The Dependent model allows dependence between the “big five” factors, but assumes these are independent from the Acquiescence factor</caption>
<colgroup>
<col width="17%" />
<col width="13%" />
<col width="7%" />
<col width="5%" />
<col width="7%" />
<col width="8%" />
<col width="7%" />
<col width="23%" />
<col width="4%" />
<col width="5%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\chi^2\)</span></th>
<th align="right">df</th>
<th align="right"><span class="math inline">\(p\)</span></th>
<th align="right">CFI</th>
<th align="right">RMSEA</th>
<th align="right">SRMR</th>
<th align="right"><span class="math inline">\(\Delta \chi^2\)</span></th>
<th align="left">df</th>
<th align="left"><span class="math inline">\(p\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Dependent</td>
<td align="right">10324</td>
<td align="right">1684</td>
<td align="right">0</td>
<td align="right">0.79</td>
<td align="right">0.05</td>
<td align="right">0.08</td>
<td align="right">NA</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
<tr class="even">
<td align="left">Independent</td>
<td align="right">11043</td>
<td align="right">1694</td>
<td align="right">0</td>
<td align="right">0.78</td>
<td align="right">0.06</td>
<td align="right">0.12</td>
<td align="right">719</td>
<td align="left">10</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p>Whilst the Dependent model fits better, there are still signs of misfit. Table <a href="ch-SEM-latent-variable-models.html#tab:BFI2-cfa-hier2-modification">14.5</a> shows the fixed-to-zero parameters with the ten highest modification indices. These indicate several routes for improvement in model fit. The highest modification index is for the residual covariance between the Negative Emotionality factor and Acquiescence factor. The expected parameter change indicates that this parameter would be estimated to be negative (i.e. there would be a negative correlation between these factors). This could indicate that those with higher Negative Emotionality are less likely to agree to statements in general. Whilst that could be plausible in one way or another, it is not something that was <em>a priori</em> expected. More importantly, the idea of the latent Acquiescence factor was to account for a tendency to agree or disagree with statements irrespective of their content, and by controlling for such a tendency to allow for better measurement of the remaining latent factors. Introducing a correlation between the Acquiescence factor and the factors of substantial interest would counteract this goal. When considering to change a model to improve empirical fit, such theoretical concerns are more important than high modification indices. The second-highest modification index is for the residual covariance between items 54 and 39, which appear to have a positive correlation which is not fully accounted for by the model. Item 54 and 39 both loads directly onto the <span class="math inline">\(\texttt{Nd}\)</span> facet.</p>
<table>
<caption><span id="tab:BFI2-cfa-hier2-modification">Table 14.5: </span>Modification indices and expected change in parameter values for fixed-to-zero parameters in the hierarchical factor model for the BFI-2.</caption>
<colgroup>
<col width="55%" />
<col width="24%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Parameter</th>
<th align="right">Modification index</th>
<th align="right">Expected change</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\texttt{Acq} \leftrightarrow \texttt{N}\)</span></td>
<td align="right">280</td>
<td align="right">-0.188</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\texttt{i54} \leftrightarrow \texttt{i39}\)</span></td>
<td align="right">179</td>
<td align="right">0.336</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\texttt{A} \leftrightarrow \texttt{Ea}\)</span></td>
<td align="right">172</td>
<td align="right">-0.080</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\texttt{Nd} \leftrightarrow \texttt{Ee}\)</span></td>
<td align="right">169</td>
<td align="right">-0.064</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\texttt{i53} \leftrightarrow \texttt{i38}\)</span></td>
<td align="right">165</td>
<td align="right">0.251</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\texttt{N} \leftrightarrow \texttt{Ee}\)</span></td>
<td align="right">148</td>
<td align="right">-0.105</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\texttt{i41} \rightarrow \texttt{Nd}\)</span></td>
<td align="right">141</td>
<td align="right">-0.471</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\texttt{i37} \leftrightarrow \texttt{i47}\)</span></td>
<td align="right">140</td>
<td align="right">0.317</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\texttt{i45} \rightarrow \texttt{Oi}\)</span></td>
<td align="right">139</td>
<td align="right">1.361</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\texttt{i52} \leftrightarrow \texttt{i7}\)</span></td>
<td align="right">135</td>
<td align="right">0.129</td>
</tr>
</tbody>
</table>
</div>
<div id="predicting-factor-scores" class="section level3 hasAnchor" number="14.2.4">
<h3><span class="header-section-number">14.2.4</span> Predicting factor scores<a href="ch-SEM-latent-variable-models.html#predicting-factor-scores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once a SEM with latent variables is estimated, we may be interested in inferring the values of the latent variables underlying each observation. For example, given their answers to the items in the BFI-2, is participant A relatively extravert? And how much more than participant B? In a confirmatory factor analysis model, the latent factors are assumed to cause the values of the observed variables. Factor loadings then represent the effect of the latent variable upon the observed variables. When predicting latent factor values, we need to go in the opposite direction, from observed to latent variables. Such reverse inference is not necessarily straightforward, and there are many possible methods which are all consistent with the underlying model, yet give different results. This is called “<strong>factor indeterminancy</strong>” <span class="citation">(<a href="#ref-maraun1996metaphor">Maraun, 1996</a>)</span>.</p>
<p>Factor score predictions are weighted sums of the values of the observed variables (indicators). The weights given to each variable are not identical to the factor loadings, but related to these. The factor prediction weights take other aspects into consideration as well, such as residual variances and covariances between factors. There are two widely-used methods to predict the values of the latent variables: the regression predictor and the Bartlett predictor <span class="citation">(<a href="#ref-devlieger2023using">Devlieger &amp; Rosseel, 2023</a>)</span>. It is beyond the scope of this book to go into the details of these methods. Predicting factor scores is a controversial issue. <span class="citation">Bartholomew, Deary, &amp; Lawn (<a href="#ref-bartholomew2009origin">2009</a>)</span> argue that the regression method has a more relevant justification than the Bartlett method. <!-- Predictions according to the regression method aim to maximise the correlation between predicted factor scores and the true values on the factor.
see https://stats.oarc.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/ --></p>
<!--
## Multiple groups and measurement invariance
-->
</div>
</div>
<div id="exploratory-factor-and-principal-components-analysis" class="section level2 hasAnchor" number="14.3">
<h2><span class="header-section-number">14.3</span> Exploratory factor and principal components analysis<a href="ch-SEM-latent-variable-models.html#exploratory-factor-and-principal-components-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although not really a part of Structural Equation Models, it is instructive to also briefly discuss <strong>data reduction</strong> techniques such as principal components analysis (PCA) and exploratory factor analysis (EFA). These methods aim to describe a <span class="math inline">\(P \times P\)</span> covariance (or correlation) matrix by via a relatively small set of latent variables. Essentially, both methods rely on saturated models which can replicate the sample covariance matrix perfectly. Neither method concerns the means of the observed variables <span class="math inline">\(Y_j\)</span>, and assume that these are centered beforehand such that all means are equal to 0.</p>
<p>The differences between principal component analysis and exploratory factor analysis are somewhat subtle. <strong>Principal components analysis</strong> (PCA) can be seen as applying a <strong>formative model</strong>, where observed variables “cause” the latent variables (called a principal components in this context). <strong>Principal components</strong> (which we will denote as <span class="math inline">\(\text{PC}_j\)</span>) are simply linear functions of the variables:
<span class="math display">\[\begin{aligned}
\text{PC}_{1,i} &amp;= w_{1,1} \times Y_{1,i} + w_{1,2} \times Y_{2,i} + \ldots + w_{1,P} \times  Y_{P,i} \\
\text{PC}_2 &amp;= w_{2,1} \times Y_{1,i} + w_{2,2} \times Y_{2,i} + \ldots + w_{2,m} \times Y_{P,i} \\
&amp; \vdots \\
\text{PC}_P &amp;= w_{P,1} \times Y_{1,i} + w_{P,2} \times Y_{2,i} + \ldots + w_{P,P} \times Y_{P,i}
\end{aligned}\]</span>
Note that the principal components <span class="math inline">\(\text{PC}_j\)</span> have no error terms. They don’t need to be estimated, but rather are <em>computed</em> from the variables <span class="math inline">\(Y_j\)</span>. Also note that we use a total of <span class="math inline">\(P\)</span> principal components to model a total of <span class="math inline">\(P\)</span> variables. In a PCA, the weights are chosen such that the principal components are uncorrelated. In addition, they are chosen such that the variances of the principal components are ordered, such that the first principal component <span class="math inline">\(\text{PC}_1\)</span> has the largest variance, then the second component <span class="math inline">\(\text{PC}_2\)</span>, etc. Whilst there are an infinite number of other saturated models which also match the observed covariances in the data perfectly, these restrictions provide a unique solution to computing the weights.</p>
<p>By contrast, <strong>exploratory factor analysis</strong> (EFA) concerns a <strong>reflective model</strong>, where the latent variables “cause” the observed variables. An EFA model for <span class="math inline">\(P\)</span> variables can at most contain <span class="math inline">\(P-1\)</span> factors. The EFA model with <span class="math inline">\(P-1\)</span> factors can be written as:
<span class="math display">\[\begin{aligned}
Y_{1,i} &amp;= \lambda_{1,1} \times \eta_{1,i} + \lambda_{1,2} \times \eta_{2,i} + \ldots + \lambda_{1,P-1} \times \eta_{P-1,i} + \epsilon_{1,i} &amp;&amp; \epsilon_{1,i} \sim \mathbf{Normal}(0, \sigma_{\epsilon_1}) \\
Y_{2,i} &amp;= \lambda_{2,1} \times \eta_{1,i} + \lambda_{2,2} \times \eta_{2,i} + \ldots + \lambda_{2,P-1} \times \eta_{P-1,i} + \epsilon_{2,i} &amp;&amp; \epsilon_{2,i} \sim \mathbf{Normal}(0, \sigma_{\epsilon_2}) \\
\vdots \\
Y_{P,i} &amp;= \lambda_{P,1} \times \eta_{1,i} + \lambda_{P,2} \times \eta_{2,i} + \ldots + \lambda_{P,P-1} \times  \eta_{P-1,i} + \epsilon_{P,i} &amp;&amp; \epsilon_{P,i} \sim \mathbf{Normal}(0, \sigma_{\epsilon_P}) \\
\eta_j &amp;\sim \mathbf{Normal}(0,1)
\end{aligned}\]</span>
Whilst scaling the latent factors to be standard Normal variables, this model has still more parameters than unique values in the variance-covariance matrix. Compared to the PCA model, the EFA model contains additional residual variances <span class="math inline">\(\sigma^2_{\epsilon_j}\)</span>. To allow some form of identification, EFA therefore first removes the (estimated) residual variances from the variance-covariance matrix. After this, the reduced variance-covariance matrix only contains variation in the observed variables which is shared with the latent factors, and there is no need to estimate the residual variances. After this step, a saturated model is possible, although factor loadings can not be uniquely estimated without further constraints.</p>
<p>Both PCA and EFA use saturated models. The key difference is that PCA targets the full variance-covariance matrix of the observed variables, whilst EFA targets a reduced variance-covariance matrix with the unique residual variance of each variable removed. PCA imposes further restrictions (independent components and maximising the variance of successive components) in order to arrive at a unique solution. These restrictions can also be applied in EFA, but are often relaxed to provide a potentially more meaningful solution.</p>
<!-- with the number of factors $q < m$. In a EFA, the observed variables $Y_j$ are considered conditionally independent, given the factors $F_k$. This is evident from the (independently distributed) error terms $\epsilon_j$.  -->
<!-- 
Although not really a part of Structural Equation Models, it is instructive to also briefly discuss **data reduction** techniques such as principal components analysis and exploratory factor analysis. These methods aim to describe a $P \times P$ covariance (or correlation) matrix by means of relatively small set of variables. Essentially, both methods rely on saturated models which can replicate the sample covariance matrix perfectly.

The differences between principal component analysis and exploratory factor analysis are somewhat subtle. **Principal components analysis** (PCA) can be seen as applying a **formative model**, where observed variables "cause" the latent variables (called a principal components in this context). **Principal components** (which we will denote as $\text{PC}_j$) are simply linear functions of the variables:
$$\begin{aligned}
\text{PC}_1 &= w_{1,1} \times X_{1} + w_{1,2} \times X_{2} + w_{1,m} \times \ldots X_{m} \\
\text{PC}_2 &= w_{2,1} \times X_{1} + w_{2,2} \times X_{2} + w_{2,m} \times \ldots X_{m} \\
& \vdots \\
\text{PC}_m &= w_{m,1} \times X_{1} + w_{m,2} \times X_{2} + w_{m,m} \times \ldots X_{m}
\end{aligned}$$
Note that the principal components $\text{PC}_j$ have no error terms. They don't need to be estimated, but rather are *computed* from the variables $X_j$. Also note that we use a total of $m$ principal components to model a total of $m$ variables. 

By contrast, **exploratory factor analysis** (PCA) concerns a **reflective model**, where the latent variables "cause" the observed variables. A EFA model can be written as:
$$\begin{aligned}
Y_1 &= \lambda_{1,1} \times F_{1} + \lambda_{1,2} \times F_{2} + \ldots + \lambda_{1,q} \times  F_{q} + \epsilon_{1} && \epsilon_{1} \sim \mathbf{Normal}(0, \sigma_{\epsilon_1}) \\
Y_2 &= \lambda_{2,1} \times F_{1} + \lambda_{2,2} \times F_{2} + \ldots + \lambda_{2,q} \times F_{q} + \epsilon_{2} && \epsilon_{2} \sim \mathbf{Normal}(0, \sigma_{\epsilon_1}) \\
& \vdots \\
Y_m &= \lambda_{m,1} \times F_{1} + \lambda_{m,2} \times F_{2} + \ldots + \lambda_{m,q} \times F_{q} + \epsilon_{m} && \epsilon_{m} \sim \mathbf{Normal}(0, \sigma_{\epsilon_m}) 
\end{aligned}
(\#eq:sem-2-efa-definition)$$
with the number of factors $q < m$. In a EFA, the observed variables $Y_j$ are considered conditionally independent, given the factors $F_k$. This is evident from the (independently distributed) error terms $\epsilon_j$. 

-->
<div id="determining-the-number-of-factors" class="section level3 hasAnchor" number="14.3.1">
<h3><span class="header-section-number">14.3.1</span> Determining the number of factors<a href="ch-SEM-latent-variable-models.html#determining-the-number-of-factors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A main aim in both PCA and EFA is to determine the number of factors that could sufficiently explain the inter-relations between the variables in the data. For both PCA and EFA, a model with the same number of latent factors as observed variables will always fit the data perfectly (as each component or factor is allowed to be connected to all variables, unlike in CFA, where particular loadings tend to be fixed to zero).</p>
<p>A key consideration is how much variability in the dataset each component or factor accounts for (or “explains”). Underlying both PCA and EFA is a mathematical technique called “<strong>eigendecomposition</strong>”, which represents symmetric matrices such as covariance or correlation matrices in a unified manner by decomposing it into “eigenvalues” and “eigenvectors”. The specifics of this go beyond the scope of this book. Roughly, the elements in eigenvectors are related to component loadings: the values in the eigenvector multiplied by the square root of the eigenvalue gives the component loadings, which can be interpreted as the correlation of each item with the principal component. The eigenvalues represent the variance accounted for by each component on a standardized scale where each variable has a variance of 1. You can think of this is applying the analysis to the variables after <span class="math inline">\(Z\)</span>-transforming each variable so they have a mean of 0 and a variance (and standard deviation) of 1. The sum of eigenvalues is equal to the number of variable, so each variable adds a value of 1 to the total amount of variability to be accounted for. An eigenvalue of 1 then tells you that a component accounts for more variance than a single variable. In other words, it accounts for shared variability in at least two variables. As we are looking for latent variables that account for relations between measured variables, it is reasonable to search for components that can account for more than the variance of a single variable. So an eigenvalue of at least 1 is a reasonable lower bound.</p>
<p>One method for determining the number of components of factors is then to determine how many have eigenvalues larger than 1. Ideally, one would consider the sampling error in this evaluation. This is not straightforward, as the sampling distribution of eigenvalues is complex. The current “gold standard” is to conduct a <strong>parallel analysis</strong>, which is effectively a bootstrap method method to compare estimated eigenvalues to those of randomly simulated data. Figure <a href="ch-SEM-latent-variable-models.html#fig:BFI-parallel-analysis">14.6</a> shows the results of a parallel analysis for both PCA and EFA. Note that as EFA targets the shared, and not the unique variance of the variables in the dataset, the eigenvalues are expected to be less than 1. Hence, the comparison line is lower for EFA compared to PCA. For both, the main idea is to consider components or factors which account for more variation than those based on randomly generated data. Using this criterion, PCA would select 9 components, and EFA 12 factors.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:BFI-parallel-analysis"></span>
<img src="14-SEM-2_files/figure-html/BFI-parallel-analysis-1.svg" alt="Results of a parallel PCA and EFA to determine the number of componentd/factors for the BFI-2 data." width="672" />
<p class="caption">
Figure 14.6: Results of a parallel PCA and EFA to determine the number of componentd/factors for the BFI-2 data.
</p>
</div>
<pre><code>## Parallel analysis suggests that the number of factors =  12  and the number of components =  9</code></pre>
</div>
<div id="factor-rotation" class="section level3 hasAnchor" number="14.3.2">
<h3><span class="header-section-number">14.3.2</span> Factor rotation<a href="ch-SEM-latent-variable-models.html#factor-rotation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The initial solution for a EFA is obtained by eigendecopmposition. This constrains the solution to have independent factors which are ordered in the amount the variance they account for. Whilst these restrictions provide a unique solution, there are an infinite number of equivalent models which fit the date equally well. In EFA, it is common practice to search for equivalent solutions which might be more easily interpretable. The main aim is to search for a <strong>simple structure</strong>, such that each item loads highly only on a single factor.</p>
<p>There are two main types of factor rotation: in <strong>orthogonal rotation</strong>, the factors are constrained to be independent of each other (i.e. there is no residual covariation between the factors). In <strong>oblique rotation</strong>, factors are allowed to have residual covariation.</p>
<!-- As EFA targets the shared variance -- and not the unique variance - of the variables in the dataset, the eigenvalues are expected to be less than 1. 

After $Z$-transforming each variable, the covariance matrix is identical to the correlation matrix. Using such standardized variables (and hence correlation matrices) makes it easier to devise general methods for determining the number of factors underlying a dataset, as the scale of the measured variables is no longer relevant. -->
<!-- The **eigenvalues** of an eigendecomposition reflect the variability that each factor can account for. An eigenvalue of 1 -> 

### Factor rotation

<!-- There are two main types of factor rotation: in **orthogonal rotation**, the factors are constrained to be independent of each other (i.e. there is no residual covariation between the factors). In **oblique rotation**, factors are allowed to have residual covariation. 
-->
<!-- ## Ordinal variables -->
<!-- ## Conclusion -->
<!-- use PoliticalDemocracy data from lavaan. Code from https://www.lavaan.ugent.be/tutorial/sem.html -->
<!-- ## Multi-group SEM -->
</div>
</div>
<div id="in-practice-6" class="section level2 hasAnchor" number="14.4">
<h2><span class="header-section-number">14.4</span> In practice<a href="ch-SEM-latent-variable-models.html#in-practice-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Structural Equation modelling is a <strong>large sample technique</strong>. To give meaningful results, the means and (co-)variances in the data should be close to the true values in the DGP. So, the more observations, the better. But how many observations is enough? There is no definite answer to that question, but you could consider the <span class="math inline">\(n/\text{npar}(M)\)</span> heuristic <span class="citation">(<a href="#ref-kline2015principles">Kline, 2015</a>)</span>, which refers to the ratio of observations to model parameters. The recommended ratio is to have at least 20 observations for each parameter in the model. For example, if the model has <span class="math inline">\(\text{npar}(M) = 30\)</span> parameters, you would want to have at least <span class="math inline">\(n=600\)</span> observations. This is not a hard limit, as the required sample size depends on various aspects of the data. If the data is truly continuous and multivariate-Normal distributed, with linear relations between the variables, less observations may be needed. But when the data does not respect the assumptions of SEM, substantially more data may be needed to allow inferences to be meaningful. The <span class="math inline">\(n/\text{npar}(M) = 20\)</span> heuristic aims to provide a reasonable guideline for most cases.</li>
<li>SEM is commonly applied to data which is not truly continuous. The BFI-2 data considered here are responses to statements on a Likert scale wth 5 options. Such data can be consider ordinal, but is not metric or continuous. Whilst the assumption of multivariate-Normal distributed variables is therefore violated by definition, SEM may still be a reasonable technique to model the inter-relations between the variables. When there are at least 5 response options scored in a numerical way (e.g. as <span class="math inline">\(1, 2, \ldots, 5\)</span>), covariances assuming continuity may not differ too much from those computed when respecting the ordinal nature of the data. The more principled way of dealing with ordinal data is to use <strong>polychoric correlations</strong>, which assume the ordinal responses are generated by an underlying continuous variable (based on the assumption that each ordinal response corresponds to a region or “bin” of the continuous variable) and aim to capture the correlation between these underlying continuous variables from the ordinal responses. Most SEM software allows you to estimate models based purely on sample correlation matrices. This could provide a reasonable alternative to the assumption of multivariate-Normal distributed variables. That said, the results will often not differ too much when the ordinal responses have at least 5 values and the data is sufficiently large.</li>
<li>The flexibility of SEM, especially when introducing latent variables, opens up a world of theorizing. Always remember though that a SEM is just a way to identify the means and (co-)variances between variables. Although SEMs may contain causal links between variables, <strong>correlation is not causation</strong>. Keep in mind that there will be equivalent other models, with other causal paths, which imply <em>exactly the same means and covariances</em>.</li>
</ol>
<table>
<caption>
<span id="tab:BFI2-cfa-hier-results">Table 14.6: </span> Results of a hierarchical factor model for the BFI-2 with an additional Acquiescence factor.
</caption>
<table style="padding-right:20px;padding-left:20px;">
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'>Model</td>
</tr>
<tr>
<td>
</td>
<td colspan = '1'; align = 'center'>Estimate</td>
<td colspan = '1'; align = 'center'>Std. Err.</td>
<td colspan = '1'; align = 'center'>z</td>
<td colspan = '1'; align = 'center'>p</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Factor Loadings</span></td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">E</span></td>
</tr>
<tr>
<td>
Es
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
Ea
</td>
<td>
0.78
</td>
<td>
0.06
</td>
<td>
13.56
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Ee
</td>
<td>
0.41
</td>
<td>
0.04
</td>
<td>
9.42
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Es</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
1.66
</td>
<td>
0.06
</td>
<td>
27.52
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
1.41
</td>
<td>
0.06
</td>
<td>
25.38
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
1.17
</td>
<td>
0.05
</td>
<td>
24.41
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Ea</span></td>
</tr>
<tr>
<td>
i6
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
1.43
</td>
<td>
0.05
</td>
<td>
26.94
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
0.61
</td>
<td>
0.04
</td>
<td>
14.94
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
1.22
</td>
<td>
0.05
</td>
<td>
25.69
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Ee</span></td>
</tr>
<tr>
<td>
i11
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
0.79
</td>
<td>
0.09
</td>
<td>
8.67
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
1.74
</td>
<td>
0.16
</td>
<td>
10.90
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
1.48
</td>
<td>
0.14
</td>
<td>
10.85
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">A</span></td>
</tr>
<tr>
<td>
Ac
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
Ar
</td>
<td>
0.94
</td>
<td>
0.09
</td>
<td>
10.51
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
At
</td>
<td>
1.46
</td>
<td>
0.13
</td>
<td>
11.65
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Ac</span></td>
</tr>
<tr>
<td>
i2
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i17
</td>
<td>
1.34
</td>
<td>
0.07
</td>
<td>
18.13
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i32
</td>
<td>
0.78
</td>
<td>
0.05
</td>
<td>
15.31
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i47
</td>
<td>
1.48
</td>
<td>
0.09
</td>
<td>
16.00
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Ar</span></td>
</tr>
<tr>
<td>
i7
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i22
</td>
<td>
1.66
</td>
<td>
0.12
</td>
<td>
14.12
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i37
</td>
<td>
2.14
</td>
<td>
0.15
</td>
<td>
14.41
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i52
</td>
<td>
0.92
</td>
<td>
0.07
</td>
<td>
12.67
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">At</span></td>
</tr>
<tr>
<td>
i12
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i27
</td>
<td>
0.59
</td>
<td>
0.06
</td>
<td>
10.27
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i42
</td>
<td>
0.65
</td>
<td>
0.06
</td>
<td>
11.39
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i57
</td>
<td>
0.89
</td>
<td>
0.07
</td>
<td>
13.47
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">C</span></td>
</tr>
<tr>
<td>
Co
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
Cp
</td>
<td>
1.13
</td>
<td>
0.09
</td>
<td>
12.91
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Cr
</td>
<td>
1.05
</td>
<td>
0.08
</td>
<td>
13.02
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Co</span></td>
</tr>
<tr>
<td>
i3
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i18
</td>
<td>
1.77
</td>
<td>
0.09
</td>
<td>
19.77
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i33
</td>
<td>
1.80
</td>
<td>
0.09
</td>
<td>
19.79
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i48
</td>
<td>
1.53
</td>
<td>
0.08
</td>
<td>
18.33
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Cp</span></td>
</tr>
<tr>
<td>
i8
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i23
</td>
<td>
1.08
</td>
<td>
0.06
</td>
<td>
16.75
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i38
</td>
<td>
1.07
</td>
<td>
0.06
</td>
<td>
18.89
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i53
</td>
<td>
1.17
</td>
<td>
0.06
</td>
<td>
19.06
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Cr</span></td>
</tr>
<tr>
<td>
i13
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i28
</td>
<td>
0.99
</td>
<td>
0.07
</td>
<td>
14.77
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i43
</td>
<td>
0.89
</td>
<td>
0.06
</td>
<td>
15.67
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i58
</td>
<td>
1.26
</td>
<td>
0.08
</td>
<td>
15.88
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">N</span></td>
</tr>
<tr>
<td>
Na
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
Nd
</td>
<td>
0.77
</td>
<td>
0.04
</td>
<td>
20.71
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Ne
</td>
<td>
0.98
</td>
<td>
0.04
</td>
<td>
23.30
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Na</span></td>
</tr>
<tr>
<td>
i4
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i19
</td>
<td>
0.84
</td>
<td>
0.03
</td>
<td>
24.76
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i34
</td>
<td>
0.79
</td>
<td>
0.03
</td>
<td>
23.67
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i49
</td>
<td>
0.81
</td>
<td>
0.04
</td>
<td>
23.22
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Nd</span></td>
</tr>
<tr>
<td>
i9
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i24
</td>
<td>
0.99
</td>
<td>
0.05
</td>
<td>
20.26
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i39
</td>
<td>
1.46
</td>
<td>
0.06
</td>
<td>
25.55
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i54
</td>
<td>
1.50
</td>
<td>
0.06
</td>
<td>
25.58
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Ne</span></td>
</tr>
<tr>
<td>
i14
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i29
</td>
<td>
1.09
</td>
<td>
0.04
</td>
<td>
30.65
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i44
</td>
<td>
0.79
</td>
<td>
0.03
</td>
<td>
24.28
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i59
</td>
<td>
0.97
</td>
<td>
0.03
</td>
<td>
28.23
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">O</span></td>
</tr>
<tr>
<td>
Oi
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
Oa
</td>
<td>
2.07
</td>
<td>
0.31
</td>
<td>
6.71
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Oc
</td>
<td>
1.30
</td>
<td>
0.19
</td>
<td>
6.66
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Oi</span></td>
</tr>
<tr>
<td>
i10
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i25
</td>
<td>
2.79
</td>
<td>
0.30
</td>
<td>
9.23
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i40
</td>
<td>
1.38
</td>
<td>
0.16
</td>
<td>
8.45
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i55
</td>
<td>
2.88
</td>
<td>
0.31
</td>
<td>
9.32
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Oa</span></td>
</tr>
<tr>
<td>
i5
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i20
</td>
<td>
1.01
</td>
<td>
0.03
</td>
<td>
29.90
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i35
</td>
<td>
0.78
</td>
<td>
0.03
</td>
<td>
28.19
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i50
</td>
<td>
0.70
</td>
<td>
0.03
</td>
<td>
21.64
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Oc</span></td>
</tr>
<tr>
<td>
i15
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i30
</td>
<td>
1.13
</td>
<td>
0.05
</td>
<td>
20.84
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i45
</td>
<td>
0.66
</td>
<td>
0.04
</td>
<td>
14.67
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i60
</td>
<td>
1.11
</td>
<td>
0.05
</td>
<td>
21.84
</td>
<td>
.000
</td>
</tr>
<tr>
<td colspan = '1'; align = 'left'><span style="text-decoration: underline;">Acq</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i6
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i11
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i2
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i17
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i32
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i47
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i7
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i22
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i37
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i52
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i12
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i27
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i42
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i57
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i3
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i18
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i33
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i48
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i8
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i23
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i38
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i53
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i13
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i28
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i43
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i58
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i4
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i19
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i34
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i49
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i9
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i24
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i39
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i54
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i14
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i29
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i44
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i59
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i10
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i25
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i40
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i55
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i5
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i20
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i35
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i50
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i15
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i30
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i45
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
i60
</td>
<td>
1.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Residual Variances</span></td>
</tr>
<tr>
<td>
i1
</td>
<td>
0.57
</td>
<td>
0.02
</td>
<td>
25.72
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i16
</td>
<td>
0.48
</td>
<td>
0.03
</td>
<td>
16.83
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i31
</td>
<td>
0.73
</td>
<td>
0.03
</td>
<td>
23.43
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i46
</td>
<td>
0.62
</td>
<td>
0.02
</td>
<td>
24.72
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i6
</td>
<td>
0.56
</td>
<td>
0.02
</td>
<td>
24.50
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i21
</td>
<td>
0.40
</td>
<td>
0.03
</td>
<td>
15.22
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i36
</td>
<td>
0.88
</td>
<td>
0.03
</td>
<td>
28.10
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i51
</td>
<td>
0.54
</td>
<td>
0.03
</td>
<td>
21.51
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i11
</td>
<td>
0.96
</td>
<td>
0.04
</td>
<td>
27.41
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i26
</td>
<td>
0.63
</td>
<td>
0.02
</td>
<td>
27.28
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i41
</td>
<td>
0.55
</td>
<td>
0.03
</td>
<td>
18.66
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i56
</td>
<td>
0.49
</td>
<td>
0.02
</td>
<td>
20.66
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i2
</td>
<td>
0.37
</td>
<td>
0.02
</td>
<td>
22.54
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i17
</td>
<td>
0.43
</td>
<td>
0.02
</td>
<td>
18.78
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i32
</td>
<td>
0.40
</td>
<td>
0.02
</td>
<td>
25.58
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i47
</td>
<td>
1.17
</td>
<td>
0.05
</td>
<td>
24.59
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i7
</td>
<td>
0.44
</td>
<td>
0.02
</td>
<td>
25.99
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i22
</td>
<td>
0.61
</td>
<td>
0.03
</td>
<td>
22.58
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i37
</td>
<td>
0.72
</td>
<td>
0.04
</td>
<td>
19.51
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i52
</td>
<td>
0.37
</td>
<td>
0.01
</td>
<td>
25.85
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i12
</td>
<td>
0.81
</td>
<td>
0.04
</td>
<td>
21.00
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i27
</td>
<td>
0.94
</td>
<td>
0.03
</td>
<td>
27.05
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i42
</td>
<td>
0.84
</td>
<td>
0.03
</td>
<td>
26.14
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i57
</td>
<td>
0.71
</td>
<td>
0.03
</td>
<td>
21.84
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i3
</td>
<td>
0.96
</td>
<td>
0.03
</td>
<td>
28.11
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i18
</td>
<td>
0.34
</td>
<td>
0.02
</td>
<td>
17.55
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i33
</td>
<td>
0.34
</td>
<td>
0.02
</td>
<td>
17.13
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i48
</td>
<td>
0.73
</td>
<td>
0.03
</td>
<td>
25.56
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i8
</td>
<td>
0.84
</td>
<td>
0.03
</td>
<td>
26.12
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i23
</td>
<td>
0.86
</td>
<td>
0.03
</td>
<td>
25.66
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i38
</td>
<td>
0.41
</td>
<td>
0.02
</td>
<td>
21.49
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i53
</td>
<td>
0.44
</td>
<td>
0.02
</td>
<td>
20.59
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i13
</td>
<td>
0.59
</td>
<td>
0.02
</td>
<td>
25.17
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i28
</td>
<td>
0.69
</td>
<td>
0.03
</td>
<td>
25.86
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i43
</td>
<td>
0.44
</td>
<td>
0.02
</td>
<td>
24.73
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i58
</td>
<td>
0.79
</td>
<td>
0.03
</td>
<td>
24.25
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i4
</td>
<td>
0.64
</td>
<td>
0.03
</td>
<td>
23.13
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i19
</td>
<td>
0.69
</td>
<td>
0.03
</td>
<td>
25.46
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i34
</td>
<td>
0.72
</td>
<td>
0.03
</td>
<td>
26.05
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i49
</td>
<td>
0.81
</td>
<td>
0.03
</td>
<td>
26.28
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i9
</td>
<td>
0.88
</td>
<td>
0.03
</td>
<td>
27.06
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i24
</td>
<td>
0.92
</td>
<td>
0.03
</td>
<td>
27.21
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i39
</td>
<td>
0.48
</td>
<td>
0.02
</td>
<td>
19.83
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i54
</td>
<td>
0.50
</td>
<td>
0.03
</td>
<td>
19.62
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i14
</td>
<td>
0.83
</td>
<td>
0.03
</td>
<td>
24.93
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i29
</td>
<td>
0.46
</td>
<td>
0.02
</td>
<td>
19.43
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i44
</td>
<td>
0.76
</td>
<td>
0.03
</td>
<td>
26.38
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i59
</td>
<td>
0.62
</td>
<td>
0.03
</td>
<td>
23.67
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i10
</td>
<td>
0.74
</td>
<td>
0.03
</td>
<td>
27.96
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i25
</td>
<td>
0.89
</td>
<td>
0.04
</td>
<td>
21.34
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i40
</td>
<td>
0.62
</td>
<td>
0.02
</td>
<td>
26.63
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i55
</td>
<td>
0.52
</td>
<td>
0.03
</td>
<td>
15.31
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i5
</td>
<td>
0.68
</td>
<td>
0.03
</td>
<td>
21.24
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i20
</td>
<td>
0.47
</td>
<td>
0.03
</td>
<td>
17.37
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i35
</td>
<td>
0.46
</td>
<td>
0.02
</td>
<td>
22.18
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i50
</td>
<td>
0.95
</td>
<td>
0.04
</td>
<td>
26.71
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i15
</td>
<td>
0.46
</td>
<td>
0.02
</td>
<td>
21.54
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i30
</td>
<td>
0.62
</td>
<td>
0.03
</td>
<td>
22.05
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i45
</td>
<td>
0.74
</td>
<td>
0.03
</td>
<td>
27.30
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
i60
</td>
<td>
0.36
</td>
<td>
0.02
</td>
<td>
17.02
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Latent Variances</span></td>
</tr>
<tr>
<td>
E
</td>
<td>
0.39
</td>
<td>
0.04
</td>
<td>
10.84
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Es
</td>
<td>
0.06
</td>
<td>
0.02
</td>
<td>
2.62
</td>
<td>
.009
</td>
</tr>
<tr>
<td>
Ea
</td>
<td>
0.24
</td>
<td>
0.02
</td>
<td>
11.06
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Ee
</td>
<td>
0.07
</td>
<td>
0.01
</td>
<td>
5.61
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
A
</td>
<td>
0.12
</td>
<td>
0.01
</td>
<td>
7.85
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Ac
</td>
<td>
0.12
</td>
<td>
0.01
</td>
<td>
8.75
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Ar
</td>
<td>
0.03
</td>
<td>
0.01
</td>
<td>
3.42
</td>
<td>
.001
</td>
</tr>
<tr>
<td>
At
</td>
<td>
0.17
</td>
<td>
0.03
</td>
<td>
6.13
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
C
</td>
<td>
0.19
</td>
<td>
0.02
</td>
<td>
8.51
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Co
</td>
<td>
0.11
</td>
<td>
0.01
</td>
<td>
8.16
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Cp
</td>
<td>
0.12
</td>
<td>
0.02
</td>
<td>
7.77
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Cr
</td>
<td>
0.04
</td>
<td>
0.01
</td>
<td>
3.92
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
N
</td>
<td>
0.69
</td>
<td>
0.05
</td>
<td>
15.19
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Na
</td>
<td>
0.06
</td>
<td>
0.02
</td>
<td>
3.06
</td>
<td>
.002
</td>
</tr>
<tr>
<td>
Nd
</td>
<td>
0.11
</td>
<td>
0.01
</td>
<td>
8.49
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Ne
</td>
<td>
0.20
</td>
<td>
0.02
</td>
<td>
9.72
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
O
</td>
<td>
0.06
</td>
<td>
0.01
</td>
<td>
4.33
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Oi
</td>
<td>
0.01
</td>
<td>
0.01
</td>
<td>
1.05
</td>
<td>
.293
</td>
</tr>
<tr>
<td>
Oa
</td>
<td>
0.68
</td>
<td>
0.05
</td>
<td>
13.79
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Oc
</td>
<td>
0.29
</td>
<td>
0.02
</td>
<td>
11.88
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
Acq
</td>
<td>
0.07
</td>
<td>
0.00
</td>
<td>
16.26
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Latent Covariances</span></td>
</tr>
<tr>
<td>
E w/A
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
E w/C
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
E w/N
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
E w/O
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
E w/Acq
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
A w/C
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
A w/N
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
A w/O
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
A w/Acq
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
C w/N
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
C w/O
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
C w/Acq
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
N w/O
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
N w/Acq
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
O w/Acq
</td>
<td>
0.00<sup>+</sup>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td colspan = '4'; align = 'center'><span style="text-decoration: underline;">Fit Indices</span></td>
</tr>
<tr>
<td>
χ<sup>2</sup>
</td>
<td>
11042.55(1694)
</td>
<td>
</td>
<td>
</td>
<td>
.000
</td>
</tr>
<tr>
<td>
CFI
</td>
<td>
0.77
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
SRMR
</td>
<td>
0.12
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA
</td>
<td>
0.06
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA (lower bound)
</td>
<td>
0.06
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
RMSEA (upper bound)
</td>
<td>
0.06
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
AIC
</td>
<td>
277382.98
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
BIC
</td>
<td>
278125.22
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan = '5'; align = 'left'><sup>+</sup>Fixed parameter</td>
</tr>
</table>
<p><br></p>
</table>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-bartholomew2009origin" class="csl-entry">
Bartholomew, D. J., Deary, I. J., &amp; Lawn, M. (2009). The origin of factor scores: Spearman, thomson and bartlett. <em>British Journal of Mathematical and Statistical Psychology</em>, <em>62</em>, 569–582.
</div>
<div id="ref-devlieger2023using" class="csl-entry">
Devlieger, I., &amp; Rosseel, Y. (2023). Using factor scores in structural equation modeling. In R. H. Hoyle (Ed.), <em>Handbook of structural equation modelling</em> (pp. 316–328). Guilford Press.
</div>
<div id="ref-hvrebivckova2020big" class="csl-entry">
Hřebı́čková, M., Jelı́nek, M., Květon, P., Benkovič, A., Botek, M., Sudzina, F., … John, O. P. (2020). Big five inventory 2 (BFI-2): Hierarchick<span>ỳ</span> model s 15 sub<span>š</span>k<span>á</span>lami. <em>Ceskoslovenska Psychologie</em>, <em>64</em>.
</div>
<div id="ref-john1991big" class="csl-entry">
John, O. P., Donahue, E. M., &amp; Kentle, R. L. (1991). Big five inventory. <em>Journal of Personality and Social Psychology</em>.
</div>
<div id="ref-kline2015principles" class="csl-entry">
Kline, R. B. (2015). <em>Principles and practice of structural equation modeling</em> (4th edition). Guilford Press.
</div>
<div id="ref-kolenikov2012testing" class="csl-entry">
Kolenikov, S., &amp; Bollen, K. A. (2012). Testing negative error variances: Is a <span>H</span>eywood case a symptom of misspecification? <em>Sociological Methods &amp; Research</em>, <em>41</em>, 124–167.
</div>
<div id="ref-maraun1996metaphor" class="csl-entry">
Maraun, M. D. (1996). Metaphor taken as math: Indeterminancy in the factor analysis model. <em>Multivariate Behavioral Research</em>, <em>31</em>, 517–538.
</div>
<div id="ref-soto2017next" class="csl-entry">
Soto, C. J., &amp; John, O. P. (2017). The next big five inventory (BFI-2): Developing and assessing a hierarchical model with 15 facets to enhance bandwidth, fidelity, and predictive power. <em>Journal of Personality and Social Psychology</em>, <em>113</em>, 117.
</div>
<div id="ref-spearman1904general" class="csl-entry">
Spearman, C. (1904). General intelligence,objectively determined and measured. <em>The American Journal of Psychology</em>, <em>15</em>, 201–292.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-SEM-path-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-Bayes-estimation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"source": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
