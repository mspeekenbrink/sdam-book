<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 A model with a mean (one sample t-test) | Statistics: data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 A model with a mean (one sample t-test) | Statistics: data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 A model with a mean (one sample t-test) | Statistics: data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2020-10-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-modelling.html"/>
<link rel="next" href="simple-linear-regression.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="book_assets/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="book_assets/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.6.1/grViz.js"></script>
<script src="book_assets/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-1.52.2/plotly-latest.min.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-modelling.html"><a href="statistical-modelling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#what-is-probability"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-modelling.html"><a href="statistical-modelling.html#flipping-a-biased-coin-an-alternative-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-modelling.html"><a href="statistical-modelling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a><ul>
<li class="chapter" data-level="2.4.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="statistical-modelling.html"><a href="statistical-modelling.html#comparing-models-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a><ul>
<li class="chapter" data-level="2.5.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="statistical-modelling.html"><a href="statistical-modelling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-modelling.html"><a href="statistical-modelling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="statistical-modelling.html"><a href="statistical-modelling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a><ul>
<li class="chapter" data-level="3.1" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a><ul>
<li class="chapter" data-level="3.1.1" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a><ul>
<li class="chapter" data-level="3.2.1" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#testing-whether-mu-has-an-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has an specific value</a><ul>
<li class="chapter" data-level="3.4.1" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#confidence-intervals"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#assumptions"><i class="fa fa-check"></i><b>3.6</b> Assumptions</a></li>
<li class="chapter" data-level="3.7" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.7</b> The Central Limit Theorem</a><ul>
<li class="chapter" data-level="3.7.1" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.7.1</b> The Central Limit Theorem in action</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a><ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#voting-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Voting and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="statistical-modelling.html"><a href="statistical-modelling.html#estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="a-model-with-a-mean-one-sample-t-test.html"><a href="a-model-with-a-mean-one-sample-t-test.html#confidence-intervals"><i class="fa fa-check"></i><b>4.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#assumptions-and-outliers"><i class="fa fa-check"></i><b>4.6</b> Assumptions and outliers</a></li>
<li class="chapter" data-level="4.7" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>4.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a><ul>
<li class="chapter" data-level="5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>5.1</b> The model</a></li>
<li class="chapter" data-level="5.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#estimation"><i class="fa fa-check"></i><b>5.2</b> Estimation</a></li>
<li class="chapter" data-level="5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#inference"><i class="fa fa-check"></i><b>5.3</b> Inference</a></li>
<li class="chapter" data-level="5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#multicollinearity"><i class="fa fa-check"></i><b>5.4</b> Multicollinearity</a></li>
<li class="chapter" data-level="5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.5</b> Polynomial regression</a></li>
<li class="chapter" data-level="5.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>5.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a><ul>
<li class="chapter" data-level="6.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#moderated-regression"><i class="fa fa-check"></i><b>6.1</b> Moderated regression</a></li>
<li class="chapter" data-level="6.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a><ul>
<li class="chapter" data-level="7.1" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#two-sample-t-test"><i class="fa fa-check"></i><b>7.1</b> Two-sample t-test</a></li>
<li class="chapter" data-level="7.2" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#contrast-coding"><i class="fa fa-check"></i><b>7.2</b> Contrast coding</a></li>
<li class="chapter" data-level="7.3" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#oneway-anova"><i class="fa fa-check"></i><b>7.3</b> Oneway ANOVA</a></li>
<li class="chapter" data-level="7.4" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#factorial-anova"><i class="fa fa-check"></i><b>7.4</b> Factorial ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html"><i class="fa fa-check"></i><b>8</b> Mixing categorical and metric predictors (ANCOVA)</a></li>
<li class="chapter" data-level="9" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html"><i class="fa fa-check"></i><b>9</b> Repeated-measures ANOVA</a></li>
<li class="chapter" data-level="10" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>10</b> Linear mixed-effects models</a></li>
<li class="chapter" data-level="11" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html"><i class="fa fa-check"></i><b>11</b> Introduction to Bayesian hypothesis testing</a></li>
<li class="chapter" data-level="12" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html"><i class="fa fa-check"></i><b>12</b> Being a responsible data analyst</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-model-with-a-mean-one-sample-t-test" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> A model with a mean (one sample t-test)</h1>
<p>In this chapter, we will start our journey into the General Linear Model with the most basic example within the GLM: a model concerning a single Normal distribution, where we are interested in statements about the mean of that distribution. We will look at the Normal distribution in detail, and cover estimation of its parameters, before focusing on testing hypotheses about the mean. We end the chapter with a look ahead towards the other chapters, by providing a brief overview of the General Linear Model.</p>
<div id="numeric-judgement-and-anchoring" class="section level2">
<h2><span class="header-section-number">3.1</span> Numeric judgement and anchoring</h2>
<p>In 1906, Sir Francis Galton attended The West of England Fat Stock and Poultry Exhibition in Plymouth. One of the attractions at the fair was a contest to guess the weight of a “dressed” ox on display. The nearest guess to the actual weight of the ox would win a prize, and a total of 800 attendees participated for a small fee. Not only was Francis Galton a eugenicist<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> – and rightly condemned for this in more modern times – he was also a statistician, and bought the used tickets off the stall holder. Back home he analysed the 787 usable tickets and later published his findings <span class="citation">(Galton <a href="#ref-galton1907vox" role="doc-biblioref">1907</a>)</span>. What surprised Galton was that while the individual guesses were often far from the true weight of the ox, the average guess of the group as a whole was highly accurate. Such findings are now often referred to as the “Wisdom of the crowds”.</p>
<p>Although the wisdom of the crowds has been demonstrated in various domains, from decades of research, we also know that human judgement is subject to a long list of biases. One such bias is called “anchoring”, and it pertains to the influence of prior exposure to a more or less arbitrary numerical value on the subsequent numerical judgement. For instance, if I ask you “to estimate the height of”How tall do you think Mount Everest is?", but first state that Mount Everest is taller than 2000 feet, you are likely to give a smaller number than if I first state that Mount Everest is shorter than 45,500 feet.</p>
<p>In this example, we will focus on data obtained from a large scale replication study, covering the above described anchoring manipulation, as well as many other experiments <span class="citation">(Klein et al. <a href="#ref-klein2014investigating" role="doc-biblioref">2014</a>)</span>. This “Many Labs” study contained data collected by a large number of labs from different countries, and here we will use data collected by Konrad Bocian and Natalia Frankowska from the University of Social Sciences
and Humanities Campus in Sopot, Poland. In particular, we will focus on the judged height of Mount Everest (in meters) after a low anchor. From the wisdom of the crowds idea, we might expect the average judgement to be identical (or at least very close) to the true height of Mount Everest, which is 8848 meters above sea level. On the other hand, if the low anchor biased the judgements, we might expect the average judgement to deviate from the true height.</p>
<div id="exploring-the-data" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Exploring the data</h3>
<p>Before diving into statistical modelling, as discussed, it is always good to explore the data. A graphical overview in the form of a histogram and raincloud plot is given in Figure <a href="a-model-with-a-mean-one-sample-t-test.html#fig:anchoring-eda-plots">3.1</a>. The minimum was judged height was 650 meters (so quite close to the anchor), and the maximum was 12,459 meters. The median judgement was 8000 and the mean 6312.193. The sample variance of the judgements was 9,742,645 and the sample standard deviation 3121.321.</p>
<div class="figure" style="text-align: center"><span id="fig:anchoring-eda-plots"></span>
<img src="_main_files/figure-html/anchoring-eda-plots-1.svg" alt="Histogram and boxplot of participants' judgments" width="48%" /><img src="_main_files/figure-html/anchoring-eda-plots-2.svg" alt="Histogram and boxplot of participants' judgments" width="48%" />
<p class="caption">
Figure 3.1: Histogram and boxplot of participants’ judgments
</p>
</div>
<p>Looking at Figure <a href="a-model-with-a-mean-one-sample-t-test.html#fig:anchoring-eda-plots">3.1</a>, you can see a peak in the distribution around the true height of Mount Everest (8848 meters), but the judgements are also quite varying, with quite a large number of judgements substantially below the actual height. This may be due to some people knowing the correct answer, whilst others don’t and have to guess. Those guessing would likely be more influenced by the low anchor. The question is whether, on average, the group’s judgements are equal to the actual height of Mount Everest.</p>
</div>
</div>
<div id="a-statistical-model-of-judgements" class="section level2">
<h2><span class="header-section-number">3.2</span> A statistical model of judgements</h2>
<p>Clearly, not everyone gave a correct answer, and there is substantial variation in people’s judgements. The goal of a statistical model is to account for this variation. In this chapter, we will consider one of the simplest statistical models for metric data such as the judgements, namely the Normal distribution. In this model, we will assume that the variation in judgements is entirely random. We may also assume that, on average, the judgements are equal to the actual height of Mount Everest, but some people will overestimate, and some people underestimate the height in their judgements. Moreover, we might assume that the probability of an underestimation is equal to the probability of an overestimation, and that very large over- or underestimations are less likely than smaller over- or underestimations. The Normal distribution encapsulates such assumptions.</p>
<div id="the-normal-distribution" class="section level3">
<h3><span class="header-section-number">3.2.1</span> The Normal distribution</h3>
<p>The <strong>Normal distribution</strong> is the well-known bell-shaped curve depicted in Figure <a href="a-model-with-a-mean-one-sample-t-test.html#fig:normal-density-plot">3.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:normal-density-plot"></span>
<img src="_main_files/figure-html/normal-density-plot-1.svg" alt="The Normal density function for $\mu = 0$ and $\sigma = 1$" width="80%" />
<p class="caption">
Figure 3.2: The Normal density function for <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma = 1\)</span>
</p>
</div>
<p>The curve is given by the following function:</p>
<p><span class="math display" id="eq:normal-density-function">\[\begin{equation}
p(y) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(y-\mu)^2}{2\sigma^2}}
\tag{3.1}
\end{equation}\]</span></p>
<p>If that looks complicated, don’t worry. First, let me point out that <span class="math inline">\(\pi\)</span> refers to the mathematical constant pi, i.e. <span class="math inline">\(\pi= 3.141593\ldots\)</span>, and <span class="math inline">\(e\)</span> refers to the mathematical constant also known as Euler’s number, i.e. <span class="math inline">\(e = 2.718282\ldots\)</span>. They are known constants and not parameters. The Normal distribution does have two parameters: <span class="math inline">\(\mu\)</span> (“mu”), the mean, and <span class="math inline">\(\sigma\)</span> (“sigma”), also called the standard deviation. Confusingly, the formula above is not actually the formula for the Normal probability distribution. Rather, it is the formula of the Normal <strong>density function</strong>. The Normal distribution applies to continuous variables. Technically, you can’t assign a probability to a particular value of a continuous variable, you can only assign probabilities to ranges of values. This is because a continuous variable has an infinite number of values (even if we restrict ourselves to a range). If we gave each unique value a probability of anything larger than 0, the total probability would not be 1, but infinite. If you add up an infinite amount of values, no matter how tiny they are, at some point you would reach a sum of 1, and then you’d have to keep on adding more. No matter how large the sum becomes, you would have to keep on going. Adding up an infinite number of values is not light work…</p>
<p>While we can’t define probability of any particular value, the probability that an observation is within a particular range is well-defined. So, rather than asking “What is the probability that the height is exactly 8567.46384634748763…?”, we can ask “What is the probability that the height is between 8567 and 8568?”, or “What is the probability that the height is between 8567.575 and 8567.576?”. These probabilities are defined by the “area under the curve” within that range. Using calculus, they can be computed as:</p>
<p><span class="math display">\[P(a \leq Y \leq b) = \int_a^b \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(y-\mu)^2}{2\sigma^2}} d y\]</span></p>
<p>If you are unfamiliar with calculus and have never seen an integration sign (<span class="math inline">\(\int\)</span>), don’t worry, you don’t have to solve equations like this, and there won’t be many more like this. You can just see it as a mathematical way of referring to the area under the curve between point <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. A proper probability density function respects the rule of total probability. The Normal distribution is defined over all real numbers (i.e. all possible numbers between minus infinity and infinity). Thus, the rule of total probability rquires that <span class="math inline">\(P(-\infty \leq Y \leq \infty) = 1\)</span>, and this is true for the Normal distribution. Figure <a href="a-model-with-a-mean-one-sample-t-test.html#fig:normal-distribution-plot">3.3</a> shows the probability of three ranges, each symmetrical around the mean. The probability of a value falling in the range between <span class="math inline">\(\mu - 3 \times \sigma\)</span> and <span class="math inline">\(\mu + 3 \times \sigma\)</span> is very close to 1. Hence, values more than three standard deviations from the mean are extremely unlikely.</p>
<div class="figure" style="text-align: center"><span id="fig:normal-distribution-plot"></span>
<img src="_main_files/figure-html/normal-distribution-plot-1.svg" alt="Normal distribution and the probability of a value falling in one of three overlapping ranges. Note that the ranges are overlapping, so that the range between $\mu - 3 \sigma$ and $\mu + 3 \sigma$ also covers the range between $\mu - 2 \sigma$ and $\mu + 2 \sigma$, but that this isn't shown in the colours." width="672" />
<p class="caption">
Figure 3.3: Normal distribution and the probability of a value falling in one of three overlapping ranges. Note that the ranges are overlapping, so that the range between <span class="math inline">\(\mu - 3 \sigma\)</span> and <span class="math inline">\(\mu + 3 \sigma\)</span> also covers the range between <span class="math inline">\(\mu - 2 \sigma\)</span> and <span class="math inline">\(\mu + 2 \sigma\)</span>, but that this isn’t shown in the colours.
</p>
</div>
<p>Because the Normal distribution is symmetric, with a single peak in the middle at <span class="math inline">\(\mu\)</span>, the parameter <span class="math inline">\(\mu\)</span> not only equals the mean, but also the median (50% of all values are lower than <span class="math inline">\(\mu\)</span>, and 50% are higher), and the mode.</p>
<p>If the judgements are on average correct, this implies that <span class="math inline">\(\mu = 8848\)</span>. Alternatively, the average judgement might be biased, for instance because of the low anchor. If this is so, what would the average judgement be? It is difficult to make a clear prediction about this. As for Paul’s probability of a correct prediction if he were (somewhat) psychic, we can then simply assume that this parameter can have any value.</p>
<p>Before we continue, let’s introduce some new notation. As before, we will use <span class="math inline">\(Y\)</span> to denote the dependent variable (i.e. people’s judgements of the height of Mount Everest), and <span class="math inline">\(Y_i\)</span> to denote the <span class="math inline">\(i\)</span>-th value of the dependent variable (i.e. the judgement by participant <span class="math inline">\(i\)</span>), where <span class="math inline">\(i=1,\ldots,n\)</span>. We will state the assumption that <span class="math inline">\(Y\)</span> follows a Normal distribution as
<span class="math display">\[Y_i \sim \mathbf{Normal}(\mu,\sigma)\]</span>
You can read this as “<span class="math inline">\(Y_i\)</span> is sampled from a Normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>”. So the tilde (<span class="math inline">\(\sim\)</span>) stands for “sampled from”. When we denote distributions, such as the Normal distribution, we will use a bold font for the name of the distribution, and between the parentheses that follow we will indicate the parameters of the distribution.</p>
<p>Using this notation, we can describe our two alternative models for people’s judgements as:</p>
<ul>
<li>MODEL R: <span class="math inline">\(\quad Y_i \sim \mathbf{Normal}(8848, \sigma)\)</span></li>
<li>MODEL G: <span class="math inline">\(\quad Y_i \sim \mathbf{Normal}(\mu, \sigma)\)</span></li>
</ul>
<p>MODEL R is the restricted model, a special case of the more general MODEL G. The restriction is on <span class="math inline">\(\mu\)</span>. In MODEL G, <span class="math inline">\(-\infty \leq \mu \leq \infty\)</span> (remember that <span class="math inline">\(\infty\)</span> stands for <em>infinity</em>). MODEL R picks a specific value <span class="math inline">\(\underline{\mu}\)</span> from this infinite range, namely <span class="math inline">\(\mu = 8848\)</span>. In both models, the standard deviation is unknown. Standard deviations, like variances, can never be negative, so <span class="math inline">\(\sigma \geq 0\)</span>, but this is all that we will specify in advance for that parameter.</p>
</div>
</div>
<div id="parameter-estimation" class="section level2">
<h2><span class="header-section-number">3.3</span> Parameter estimation</h2>
<p>MODEL G has two parameters: the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>. The maximum likelihood estimate of <span class="math inline">\(\mu\)</span> is the sample mean:</p>
<p><span class="math display" id="eq:ml-estimator-mean-simple-model">\[\begin{equation}
\hat{\mu} = \overline{Y} = \frac{\sum_{i=1}^n Y_i}{n}
\tag{3.2}
\end{equation}\]</span></p>
<p>The maximum likelihood estimate of the variance is the sample variance (Equation <a href="intro.html#eq:definition-sample-variance">(1.2)</a>) <span class="math inline">\(\hat{\sigma}^2_\text{ML} = S^2\)</span>. However, this estimator of the variance is biased. It is therefore common to estimate the variance with the unbiased estimator:
<span class="math display" id="eq:unbiased-estimator-variance-simple-model">\[\begin{equation}
\hat{\sigma}^2 = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}
\tag{3.3}
\end{equation}\]</span>
The difference is to divide the sum of squared deviations by <span class="math inline">\(n-1\)</span>, rather than <span class="math inline">\(n\)</span>. Intuitively, you can think of the reason for this as follows: When we use the sample mean <span class="math inline">\(\overline{Y}\)</span> rather than the true mean <span class="math inline">\(\mu\)</span> in computing the variance, we don’t take into account that the sample mean is a noisy estimate of the true mean <span class="math inline">\(\mu\)</span>. Compared to <span class="math inline">\(\overline{Y}\)</span>, the true mean could be somewhat higher or lower, and therefore the sample variance is likely to be an underestimate of the true variance. In other words, it is biased. By dividing by <span class="math inline">\(n-1\)</span> instead of <span class="math inline">\(n\)</span>, the resulting estimate is a little higher, which eliminates this bias. When <span class="math inline">\(n\)</span> is large (i.e. there are a large number of observations), there will be little difference between the unbiased estimate and the sample variance, but for small <span class="math inline">\(n\)</span> the difference will be more marked.</p>
<p>Note that the estimator above is for <span class="math inline">\(\sigma^2\)</span> (sigma squared, i.e. sigma raised to the power of 2). If we want an estimator of the standard deviation, we can simply take the square-root to get the following estimator of <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[\hat{\sigma} = \sqrt{\hat{\sigma}^2}\]</span></p>
<p>If the mean is known to equal a particular value, <span class="math inline">\(\mu = \underline{\mu}\)</span>, as in MODEL R, then we should use that value <span class="math inline">\(\underline{\mu}\)</span> instead of <span class="math inline">\(\overline{Y}\)</span> to estimate the variance and standard deviation. The unbiased estimate of the standard deviation for MODEL R is
<span class="math display">\[\hat{\sigma}_R = \sqrt{\frac{\sum_{i=1}^n (Y_i - \underline{\mu})^2}{n}}\]</span>
Perhaps confusingly, we can now divide by <span class="math inline">\(n\)</span> instead of by <span class="math inline">\(n-1\)</span> to get an unbiased estimate. The reason for this is that when we know the true mean, there is no estimation error like we had for <span class="math inline">\(\overline{Y}\)</span>, and hence no additional source of variability which would bias the estimate.</p>
<p>Usually, the mean <span class="math inline">\(\mu\)</span> is of more interest than the standard deviation <span class="math inline">\(\sigma\)</span>. The standard deviation (or equivalently, the variance) are often so-called <em>nuisance parameters</em>.</p>
<div id="sampling-distribution-of-the-estimated-mean" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Sampling distribution of the estimated mean</h3>
<p>Remember that the estimator of the mean is an algorithm that provides estimates from data. Different data sets will give different estimates, even when these are generated from the same Data Generating Process. Our model of the Data Generating Process is the Normal distribution, which has two parameters: <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. Let’s pick some values for these parameters, so that we can use our model to simulate data. For instance, suppose the judgements are on average equal to the true value, so <span class="math inline">\(\mu = 8848\)</span>. Individual judgements are quite variable, however, so let’s take <span class="math inline">\(\sigma = 2000\)</span>. Figure <a href="a-model-with-a-mean-one-sample-t-test.html#fig:histogram-sampling-mean-simple">3.4</a> shows the distribution of the estimated mean of 10,000 data sets each consisting of <span class="math inline">\(n=109\)</span> observations (just like in the anchoring data). Also shown (as a dotted line) is the Normal distribution of the model we used to generate the data. Clearly, the estimated means are much less variable than the simulated judgements.</p>
<div class="figure" style="text-align: center"><span id="fig:histogram-sampling-mean-simple"></span>
<img src="_main_files/figure-html/histogram-sampling-mean-simple-1.svg" alt="Estimated means for 10000 simulated data sets of $n = 109$, drawn from a Normal distribution with mean 8848 and standard deviation 2000. The dotted line is represents the Normal distribution from which the data sets were generated, and the solid line the theoretical distribution of the estimated means." width="672" />
<p class="caption">
Figure 3.4: Estimated means for 10000 simulated data sets of <span class="math inline">\(n = 109\)</span>, drawn from a Normal distribution with mean 8848 and standard deviation 2000. The dotted line is represents the Normal distribution from which the data sets were generated, and the solid line the theoretical distribution of the estimated means.
</p>
</div>
<p>Simulating data and then looking at the resulting distribution of estimates is straightforward, but also noisy. Luckily, it is quite easy to derive the true distribution of the estimated means (i.e. the sample means). It is also a Normal distribution, with a mean equal to <span class="math inline">\(\mu\)</span> (so the estimator is unbiased), and a standard deviation equal to <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>, i.e. the standard deviation of the dependent variable divided by <span class="math inline">\(\sqrt{n}\)</span>. Dividing by <span class="math inline">\(\sqrt{n}\)</span> implies that the standard deviation of the sample means is smaller than the standard deviation of the dependent variable. And, as <span class="math inline">\(n\)</span> increases, it becomes smaller and smaller, so the estimator is consistent. Using our new notation, we can write the sampling distribution of the estimated means as
<span class="math display" id="eq:sampling-distribution-mean-simple">\[\begin{equation}
\overline{Y} \sim \mathbf{Normal}(\mu, \frac{\sigma}{\sqrt{n}})
\tag{3.4}
\end{equation}\]</span>
The standard deviation of the sampling distribution of estimates is also called the <strong>standard error</strong> of the estimates.</p>
<p>To simulate judgements, we had to pick an arbitrary value of <span class="math inline">\(\sigma\)</span>. But how can we simulate the data when <span class="math inline">\(\sigma\)</span> is unknown. We could of course use the unbiased estimate <span class="math inline">\(\hat{\sigma}\)</span>. For MODEL R and the present data, that would be <span class="math inline">\(\hat{\sigma} = 4021.562\)</span>, which is obviously substantially larger than 2000. If we’d use this value to simulate data sets and look at the distribution of the sample mean, we’d get a similar plot to the one of Figure <a href="a-model-with-a-mean-one-sample-t-test.html#fig:histogram-sampling-mean-simple">3.4</a>, but with a larger standard deviation.</p>
<p>Suppose that MODEL R is true, that the Data Generating Process indeed results in a Normal distribution of people’s judgements with a mean <span class="math inline">\(\mu = 8848\)</span> and a standard deviation <span class="math inline">\(\sigma\)</span>. If we knew the value of <span class="math inline">\(\sigma\)</span>, then we’d know everything there is to know about the distribution of the DGP. And knowing this, we’d know everything there is to know about the distribution of the sample means. But we don’t know <span class="math inline">\(\sigma\)</span>. While it makes sense to use an estimate of <span class="math inline">\(\sigma\)</span>, this estimate will be noisy. We shouldn’t just pretend that our estimate <span class="math inline">\(\hat{\sigma}\)</span> is identical to the true <span class="math inline">\(\sigma\)</span>. The problem is that different values of <span class="math inline">\(\sigma\)</span> lead to different sampling distributions of the mean. Key to working out the sampling distribution of the mean when <span class="math inline">\(\sigma\)</span> is unknown is to also take into account the sampling distribution of the estimates of <span class="math inline">\(\sigma\)</span>. Roughly, the idea is that, for a given data set, we can work out how likely different values of <span class="math inline">\(\sigma\)</span> are, and we can then derive the average of all the Normal distributions that follow from each possible value of <span class="math inline">\(\sigma\)</span>. The resulting distribution is not a Normal distribution. It was derived by William Sealy Gosset (1876–1937) in 1904. Gosset worked as Head Experimental Brewer for Guinness in Dublin, and the company had a rule forbidding their chemists to publish their findings <span class="citation">(Zabell <a href="#ref-zabell2008student" role="doc-biblioref">2008</a>)</span>. Gosset was able to convince his boss that his mathematical work was of no practical use to competing brewers, and was allowed to publish them in <span class="citation">Student (<a href="#ref-student1908probable" role="doc-biblioref">1908</a>)</span>, but under a pseudonym to avoid his colleagues getting similar ideas. “Student” was the pseudonym chosen by the managing director of Guinness, and hence the distribution is now known as Student’s t-distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:comparing-normal-and-t-simple"></span>
<img src="_main_files/figure-html/comparing-normal-and-t-simple-1.svg" alt="Student t-distribution which is the true sampling distribution of the mean under MODEL R with unknown $\sigma$ (solid line) and the Normal distribution that would be the sampling distribution if it is known that $\sigma = \hat{\sigma}$ (broken line). Note that these curves are drawn for the case of $n=10$ observations, to make the differences between the distributions more marked. For $n=109$, the t-distribution is almost identical to the Normal distribution." width="672" />
<p class="caption">
Figure 3.5: Student t-distribution which is the true sampling distribution of the mean under MODEL R with unknown <span class="math inline">\(\sigma\)</span> (solid line) and the Normal distribution that would be the sampling distribution if it is known that <span class="math inline">\(\sigma = \hat{\sigma}\)</span> (broken line). Note that these curves are drawn for the case of <span class="math inline">\(n=10\)</span> observations, to make the differences between the distributions more marked. For <span class="math inline">\(n=109\)</span>, the t-distribution is almost identical to the Normal distribution.
</p>
</div>
<p>You can see a comparison of the t-distribution and the Normal distribution in Figure <a href="a-model-with-a-mean-one-sample-t-test.html#fig:comparing-normal-and-t-simple">3.5</a>. The main thing to notice is that the t-distribution is also bell-shaped and symmetric, but it is wider (has fatter tails) than the Normal distribution. The difference between the t-distribution and Normal distribution depends on the value of <span class="math inline">\(n-1\)</span>. When <span class="math inline">\(n&gt;30\)</span>, the difference is, for most practical purposes, negligible.</p>
</div>
</div>
<div id="testing-whether-mu-has-an-specific-value" class="section level2">
<h2><span class="header-section-number">3.4</span> Testing whether <span class="math inline">\(\mu\)</span> has an specific value</h2>
<p>Now we know what happens to</p>
<div id="the-classical-way" class="section level3">
<h3><span class="header-section-number">3.4.1</span> The classical way</h3>
<p><span class="math display">\[\begin{equation}
t = \frac{\hat{\mu} - \underline{\mu}}{\hat{\sigma}/\sqrt{n}} = \frac{\overline{Y} - \underset{\sim}{\mu}}{S/\sqrt{n}}
\end{equation}\]</span></p>
<p>This is also called a one-sample t-test.</p>
<div id="two-tailed-and-one-tailed-tests" class="section level4">
<h4><span class="header-section-number">3.4.1.1</span> Two-tailed and one-tailed tests</h4>
</div>
</div>
<div id="the-model-comparison-way" class="section level3">
<h3><span class="header-section-number">3.4.2</span> The model comparison way</h3>
</div>
</div>
<div id="confidence-intervals" class="section level2">
<h2><span class="header-section-number">3.5</span> Confidence intervals</h2>
<p>You can view a particularly nice animation of the confidence intervals on the <a href="https://seeing-theory.brown.edu/frequentist-inference/index.html#section2">Seeing Theory</a> website.</p>
</div>
<div id="assumptions" class="section level2">
<h2><span class="header-section-number">3.6</span> Assumptions</h2>
</div>
<div id="the-central-limit-theorem" class="section level2">
<h2><span class="header-section-number">3.7</span> The Central Limit Theorem</h2>
<p>A main reason that the Normal distribution is used so often (and perhaps called “Normal”, rather than “Abnormal”), is due to a mathematical fact known as the <strong>Central Limit Theorem</strong>:</p>

<div class="definition">
<p><span id="def:clt" class="definition"><strong>Definition 3.1  (Central Limit Theorem)  </strong></span>The distribution of the sum of <span class="math inline">\(n\)</span> independent variables approaches the Normal distribution as the number of variables approaches infinity (<span class="math inline">\(n \rightarrow \infty\)</span>).</p>
</div>

<p>This is quite an amazing theorem. Although you might wonder why you should care about sums of an infinite number of random variables, there are two things to note. Firstly, the rate at which the distribution “approaches” the normal distribution can be relatively quick. Secondly, the sample mean can be viewed as the (normalized) sum of <span class="math inline">\(n\)</span> variables:</p>
<p><span class="math display">\[\overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i\]</span></p>
<p>So the central limit theorem implies that as long as the number of samples is large enough, the sample mean will follow a Normal distribution.</p>
<div id="the-central-limit-theorem-in-action" class="section level3">
<h3><span class="header-section-number">3.7.1</span> The Central Limit Theorem in action</h3>
<p>To see the central limit theorem in action, let’s pick an arbitrary probability distribution over 5 values, as shown in Figure <a href="a-model-with-a-mean-one-sample-t-test.html#fig:weird-prob-dist">3.6</a>. This will be the true distribution (i.e. the Data Generating Process) from which we can sample values.</p>
<div class="figure" style="text-align: center"><span id="fig:weird-prob-dist"></span>
<img src="_main_files/figure-html/weird-prob-dist-1.svg" alt="An aribitrary probability distribution over 5 values, clearly not a Normal distribution!" width="50%" />
<p class="caption">
Figure 3.6: An aribitrary probability distribution over 5 values, clearly not a Normal distribution!
</p>
</div>
<p>When we repeatedly sample 5 values and calculate the mean, we can look at the distribution of these sampled means. This is shown in Figure <a href="a-model-with-a-mean-one-sample-t-test.html#fig:clt-animation-1">3.7</a>. Figure <a href="#fig:clt-animation-2"><strong>??</strong></a> shows the same when we repeatedly sample 20 values and calculate the mean.</p>
<div class="figure" style="text-align: center"><span id="fig:clt-animation-1"></span>
<img src="_main_files/figure-html/clt-animation-1-1.gif" alt="The distribution of the mean of 5 samples (from the distribution in Figure XX" width="50%" />
<p class="caption">
Figure 3.7: The distribution of the mean of 5 samples (from the distribution in Figure XX
</p>
</div>
<!--
<div class="figure" style="text-align: center">
<img src="_main_files/figure-html/clt-animation-2-1.gif" alt="The distribution of the mean of 20 samples from the distribution in Figure XX"  />
<p class="caption">(\#fig:clt-animation-2)The distribution of the mean of 20 samples from the distribution in Figure XX</p>
</div>
-->
<p>So according to the Central Limit Theorem, if we focus on the mean of a sufficiently large number of independent observations, we can reasonably assume that the sampling distribution of the mean will follow a Normal distribution, even if the distribution from which the actual values were drawn is far from Normal. We might also imagine that if a variable is the outcome of a process in which lots of independent sources of noise are added, that the resulting variable will follow a Normal distribution. [todo: add example]</p>
<!--
http://www.stats.ox.ac.uk/~dlunn/b8_02/b8pdf_8.pdf
http://www.stat.ucla.edu/~hqxu/stat105/pdf/ch09.pdf
https://www.math.arizona.edu/~jwatkins/v-anova.pdf
-->
<p>There is an equivalent way to perform the hypothesis test that <span class="math inline">\(\mu = \underline{\mu}\)</span>, by comparing two versions of a statistical model, one in which we assume we know the value of <span class="math inline">\(\mu\)</span> to be <span class="math inline">\(\underline{\mu}\)</span> (Model R), and one in which we don’t (Model G). As we did for Paul the Octopus, we then look at the likelihood ratio to see whether Model R is tenable. Note that in both models, the standard deviation is considered unknown and will have to be estimated.</p>
<!-- 
## The General Linear Model

$$Y_i = \beta_0 + \beta_1 X_{1i} + \ldots + \beta_K X_{Ki} + \epsilon_i \quad \quad \epsilon_i \sim N(0,\sigma^2)$$
-->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-galton1907vox">
<p>Galton, Francis. 1907. “Vox Populi.” <em>Nature</em> 75: 450–51.</p>
</div>
<div id="ref-klein2014investigating">
<p>Klein, Richard A, Kate A Ratliff, Michelangelo Vianello, Reginald B Adams Jr, Štěpán Bahnı́k, Michael J Bernstein, Konrad Bocian, et al. 2014. “Investigating Variation in Replicability: A "Many Labs"" Replication Project.” <em>Social Psychology</em>, 142–52.</p>
</div>
<div id="ref-student1908probable">
<p>Student. 1908. “The Probable Error of a Mean.” <em>Biometrika</em> 6: 1–25.</p>
</div>
<div id="ref-zabell2008student">
<p>Zabell, Sandy L. 2008. “On Student’s 1908 Article "the Probable Error of a Mean"".” <em>Journal of the American Statistical Association</em> 103 (481): 1–7.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>He founded the Galton Laboratory of National Eugenics at UCL.<a href="a-model-with-a-mean-one-sample-t-test.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-modelling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"source": null,
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
