<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Statistical modeling | Statistics, data analysis, and modelling</title>
  <meta name="description" content="Book/course notes" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Statistical modeling | Statistics, data analysis, and modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Book/course notes" />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Statistical modeling | Statistics, data analysis, and modelling" />
  
  <meta name="twitter:description" content="Book/course notes" />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2020-09-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="a-model-with-a-mean-introduction-to-the-general-linear-model.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="book_assets/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="book_assets/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="book_assets/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-1.52.2/plotly-latest.min.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#analysis-and-modeling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modeling</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-modeling.html"><a href="statistical-modeling.html"><i class="fa fa-check"></i><b>2</b> Statistical modeling</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#what-is-probability"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-modeling.html"><a href="statistical-modeling.html#flipping-a-biased-coin-an-alternative-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a><ul>
<li class="chapter" data-level="2.4.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.1</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="statistical-modeling.html"><a href="statistical-modeling.html#null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>2.5</b> Null-hypothesis significance testing</a><ul>
<li class="chapter" data-level="2.5.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-modeling.html"><a href="statistical-modeling.html#confidence-intervals"><i class="fa fa-check"></i><b>2.6</b> Confidence intervals</a></li>
<li class="chapter" data-level="2.7" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="statistical-modeling.html"><a href="statistical-modeling.html#exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><i class="fa fa-check"></i><b>3</b> A model with a mean: Introduction to the General Linear Model</a><ul>
<li class="chapter" data-level="3.1" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#numeric-judgment-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgment and anchoring</a><ul>
<li class="chapter" data-level="3.1.1" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.3" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.3</b> The Central Limit Theorem</a><ul>
<li class="chapter" data-level="3.3.1" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.3.1</b> The Central Limit Theorem in action</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#parameter-estimation"><i class="fa fa-check"></i><b>3.4</b> Parameter estimation</a><ul>
<li class="chapter" data-level="3.4.1" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.4.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#testing-whether-mu-has-an-specific-value"><i class="fa fa-check"></i><b>3.5</b> Testing whether <span class="math inline">\(\mu\)</span> has an specific value</a><ul>
<li class="chapter" data-level="3.5.1" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#the-classical-way"><i class="fa fa-check"></i><b>3.5.1</b> The classical way</a></li>
<li class="chapter" data-level="3.5.2" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.5.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="a-model-with-a-mean-introduction-to-the-general-linear-model.html"><a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html#the-general-linear-model"><i class="fa fa-check"></i><b>3.6</b> The General Linear Model</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear regression</a><ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#the-model"><i class="fa fa-check"></i><b>4.1</b> The model</a></li>
<li class="chapter" data-level="4.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#estimation"><i class="fa fa-check"></i><b>4.2</b> Estimation</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.3</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="4.3.1" data-path="linear-regression.html"><a href="linear-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.3.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-regression.html"><a href="linear-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.3.2</b> Model comparison</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#confidence-intervals"><i class="fa fa-check"></i><b>4.4</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.5" data-path="linear-regression.html"><a href="linear-regression.html#assumptions-and-outliers"><i class="fa fa-check"></i><b>4.5</b> Assumptions and outliers</a></li>
<li class="chapter" data-level="4.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html#the-model"><i class="fa fa-check"></i><b>5.1</b> The model</a></li>
<li class="chapter" data-level="5.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#estimation"><i class="fa fa-check"></i><b>5.2</b> Estimation</a></li>
<li class="chapter" data-level="5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#inference"><i class="fa fa-check"></i><b>5.3</b> Inference</a></li>
<li class="chapter" data-level="5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#multicollinearity"><i class="fa fa-check"></i><b>5.4</b> Multicollinearity</a></li>
<li class="chapter" data-level="5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.5</b> Polynomial regression</a></li>
<li class="chapter" data-level="5.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>5.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moderation-and-interactions-glm-3.html"><a href="moderation-and-interactions-glm-3.html"><i class="fa fa-check"></i><b>6</b> Moderation and interactions (GLM 3)</a><ul>
<li class="chapter" data-level="6.1" data-path="moderation-and-interactions-glm-3.html"><a href="moderation-and-interactions-glm-3.html#moderated-regression"><i class="fa fa-check"></i><b>6.1</b> Moderated regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="a-model-of-means-glm-4-anova.html"><a href="a-model-of-means-glm-4-anova.html"><i class="fa fa-check"></i><b>7</b> A model of means (GLM 4: ANOVA)</a><ul>
<li class="chapter" data-level="7.1" data-path="a-model-of-means-glm-4-anova.html"><a href="a-model-of-means-glm-4-anova.html#two-sample-t-test"><i class="fa fa-check"></i><b>7.1</b> Two-sample t-test</a></li>
<li class="chapter" data-level="7.2" data-path="a-model-of-means-glm-4-anova.html"><a href="a-model-of-means-glm-4-anova.html#contrast-coding"><i class="fa fa-check"></i><b>7.2</b> Contrast coding</a></li>
<li class="chapter" data-level="7.3" data-path="a-model-of-means-glm-4-anova.html"><a href="a-model-of-means-glm-4-anova.html#oneway-anova"><i class="fa fa-check"></i><b>7.3</b> Oneway ANOVA</a></li>
<li class="chapter" data-level="7.4" data-path="a-model-of-means-glm-4-anova.html"><a href="a-model-of-means-glm-4-anova.html#factorial-anova"><i class="fa fa-check"></i><b>7.4</b> Factorial ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixing-categorical-and-metric-predictors-glm-5-ancova.html"><a href="mixing-categorical-and-metric-predictors-glm-5-ancova.html"><i class="fa fa-check"></i><b>8</b> Mixing categorical and metric predictors (GLM 5: ANCOVA)</a></li>
<li class="chapter" data-level="9" data-path="repeated-measures-glm-6-repeated-measures-anova.html"><a href="repeated-measures-glm-6-repeated-measures-anova.html"><i class="fa fa-check"></i><b>9</b> Repeated-measures (GLM 6: Repeated-measures ANOVA)</a></li>
<li class="chapter" data-level="10" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>10</b> Linear mixed-effects models</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics, data analysis, and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistical-modeling" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Statistical modeling</h1>
<p>Was Paul truly an oracle? Or was Paul (or his keeper more likely) just really lucky? How do we know? And what really are the chances of predicting the winner of 8 matches correct? These are questions we will focus on now.</p>
<p>In this chapter, we will start defining statistical models and using them to make inferences about the Data Generating Process.</p>
<div id="coin-flipping-defining-a-statistical-model" class="section level2">
<h2><span class="header-section-number">2.1</span> Coin flipping: Defining a statistical model</h2>
<p>What does it mean for Paul to be an oracle? Sometimes it is easier to answer this question by considering the opposite: When would Paul <em>not</em> be an oracle? If Paul had no way to tell the future, he would be “merely guessing”. Purely random guessing between two options can be implemented by flipping a coin. Suppose Paul made his choices by “flipping a mental coin”, and choosing the box on the left when the outcome was “heads”, and the box on the right when the outcome was “tails”. Great, we now have a model of the Data Generating Process!</p>
</div>
<div id="probability" class="section level2">
<h2><span class="header-section-number">2.2</span> Probability</h2>
<p>Let’s simulate Paul’s decisions for the FIFA world cup. Grab a coin and flip it 8 times. What are the outcomes? When I did this, the outcomes were:</p>
<p>tails, tails, tails, heads, tails, heads, heads, tails</p>
<p>Remember, Paul would go left upon heads, and right upon tails. Following these rules, Paul’s decisions are then:</p>
<p>right, right, right, left, right, left, left, right</p>
<p>To know whether these guesses are correct or incorrect, we need to know which team was placed in which box. This information is provided in Table <a href="intro.html#tab:fifa2010">1.2</a>: the first country under <code>Match</code> is the box on the left, and the second the box on the right. Using this, our simulation then provides the following:</p>
<p>incorrect, correct, correct, correct, correct, correct, incorrect, correct</p>
<p>So in our simulation, Paul made 6 correct predictions out of 8. Not quite as good as the real Paul, but still a reasonable performance.</p>
<p>Notice how we went through quite a few steps. We defined a model of non-psychic-and-guessing Paul as making decisions by flipping a coin, we then simulated Paul’s decisions by flipping a coin, then transformed these coin flips into a variable containing the decisions to open the left or right box, and finally transformed this variable into another variable containing the accuracy of the predictions. The first transformation is easy. We could have simply put stickers on the coin relabeling “heads” as “left” and “tails” as “right”. The second transformation is not as obvious as it depends on a second variable (whether the winning team was represented by the left or right box). In this case, we could have saved ourselves a bit of work by immediately relabeling our coin flips as “correct” or “incorrect”. To see why, we need to know a bit more about how to calculate probabilities.</p>
<p>Let’s start at the beginning. For one coin flip, there are two possible outcomes: the coin lands on heads or on tails. The outcome can’t be neither (we are assuming the coin is thin enough to not land on its side), nor two heads, or something else. The coin must land on either heads or tails, so there are two outcomes. We call the set of all possible outcomes the <strong>outcome space</strong>, and we’ll use a nice curly <span class="math inline">\(\mathcal{S}\)</span> to denote it:</p>
<p><span class="math display">\[\mathcal{S} = \{\text{heads},\text{tails}\}\]</span></p>

<div class="definition">
<p><span id="def:outcome-space" class="definition"><strong>Definition 2.1  (outcome space)  </strong></span>
The outcome space is the set of all possible events.</p>
</div>

<p>Our coin flip is a <strong>random variable</strong> that takes one of the values in the outcome space. It is traditional to denote random variables with capital Roman letters, and the values with small Roman letters. When we want to make generic statements that apply to different random variables (such as a coin flip, the roll of a die, the outcome of a prediction), we tend to use this notation, and <span class="math inline">\(P(Y=y)\)</span> then means ‘The probability that random variable <span class="math inline">\(Y\)</span> has value <span class="math inline">\(y\)</span>’. Other times, it may be easier to use more descriptive names. In this book, when we use a name for a variable, we will use a computer font, such as <code>coin_flip</code>, for the random variable. So <span class="math inline">\(P(\texttt{coin_flip} = \text{heads})\)</span> means ‘The probability that random variable <code>coin_flip</code> has the value “heads”’.</p>
<div id="what-is-probability" class="section level3">
<h3><span class="header-section-number">2.2.1</span> What is probability?</h3>
<p>I have just introduced the word “probability” without telling you what it means. You probably have some intuition yourself, for instance that probability is the chance of something happening. You might be surprised to know that even though statisticians can all perform probability calculations comfortably, there is quite some disagreement on what probability means. According to the traditional, <strong>Frequentist view</strong>, probability means <em>long-run relative frequency</em>. For instance, the probability of a coin flip landing on heads is defined as the proportion of times a coin lands heads when I flip a coin for a very, very large number of times. Table <a href="statistical-modeling.html#tab:long-run-frequency-table">2.1</a> shows the outcome and relative frequency calculations for 15 coin flips.</p>
<table>
<caption><span id="tab:long-run-frequency-table">Table 2.1: </span>Relative frequency of heads for 1 to 15 flips. The frequency is the number of heads thus far, and the relative frequency the frequency divided by the number of flips.</caption>
<thead>
<tr class="header">
<th align="right">flip</th>
<th align="left">outcome</th>
<th align="right">frequency heads</th>
<th align="right">relative frequency heads</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">tails</td>
<td align="right">0</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">tails</td>
<td align="right">0</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">heads</td>
<td align="right">1</td>
<td align="right">0.333</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">heads</td>
<td align="right">2</td>
<td align="right">0.500</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">heads</td>
<td align="right">3</td>
<td align="right">0.600</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left">tails</td>
<td align="right">3</td>
<td align="right">0.500</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="left">tails</td>
<td align="right">3</td>
<td align="right">0.429</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="left">tails</td>
<td align="right">3</td>
<td align="right">0.375</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="left">tails</td>
<td align="right">3</td>
<td align="right">0.333</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="left">tails</td>
<td align="right">3</td>
<td align="right">0.300</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="left">tails</td>
<td align="right">3</td>
<td align="right">0.273</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="left">tails</td>
<td align="right">3</td>
<td align="right">0.250</td>
</tr>
<tr class="odd">
<td align="right">13</td>
<td align="left">heads</td>
<td align="right">4</td>
<td align="right">0.308</td>
</tr>
<tr class="even">
<td align="right">14</td>
<td align="left">heads</td>
<td align="right">5</td>
<td align="right">0.357</td>
</tr>
<tr class="odd">
<td align="right">15</td>
<td align="left">heads</td>
<td align="right">6</td>
<td align="right">0.400</td>
</tr>
</tbody>
</table>
<p>Figure <a href="statistical-modeling.html#fig:coin-flip-relative-frequency">2.1</a> shows the relative frequency (i.e. the proportion) of heads after each coin flip, when flipping a coin for 2000 times.</p>
<div class="figure" style="text-align: center"><span id="fig:coin-flip-relative-frequency"></span>
<img src="_main_files/figure-html/coin-flip-relative-frequency-1.png" alt="Relative frequency of heads when flipping a coin for 2000 times" width="672" />
<p class="caption">
Figure 2.1: Relative frequency of heads when flipping a coin for 2000 times
</p>
</div>
<p>As you can see, the relative frequency fluctuates quite a bit, but becomes more stable with more flips. But even after 5000 flips, it is not equal to 0.5. Flipping a coin can be viewed as an experiment, that we can repeat. Figure <a href="statistical-modeling.html#fig:coin-flip-relative-frequency-multiple">2.2</a> shows the results of flipping a coin for 5000 times when repeating the experiment for 5 times. As you can see, the results of our experiments (the relative frequencies) are different each time. The differences are quite marked for a small number of flips, but become more alike after flipping the coin more times. This illustrates something known as <strong>The Law of Large Numbers</strong>, to which we will come back soon. For now, let’s focus on a different matter. If probability is a relative frequency, then which one is it? Each repetition of the experiment, as well as each number of throws, would give us a different answer!</p>
<div class="figure" style="text-align: center"><span id="fig:coin-flip-relative-frequency-multiple"></span>
<img src="_main_files/figure-html/coin-flip-relative-frequency-multiple-1.png" alt="Relative frequency of heads when flipping a coin 5 times for 5000 times" width="672" />
<p class="caption">
Figure 2.2: Relative frequency of heads when flipping a coin 5 times for 5000 times
</p>
</div>
<p>To give a single answer, a “long-run relative frequency” must be the relative frequency when flipping the coin for an infinite number of times. That may seem disappointing. A nice aspect of the Frequentist View is that probabilities are something of the real world, but surely, we can’t flip a coin for an infinite number of times in the real world. That is true, but to understand what probability is, we don’t necessarily have to perform such infinite experiments. When we have to estimate the probability from a given (non-infinite) number of coin flips, we do have to worry about these issues. This, indeed, is what statistics is about: dealing with sample variability to estimate and infer unknown things. But for defining what a probability is, we can use mathematics to show that the long run relative frequency converges to a single number as the number of coin flips gets closer and closer to infinity. Which, in other words, means there is a single long-run relative frequency, and hence the probability is mathematically well-defined.</p>
<p>There is an alternative to the Frequentist View, called the <strong>Bayesian view</strong> or <strong>Subjective view</strong>, according to which probability means a <em>rational degree of belief</em>. This, in a sense, takes probability out of the real world, and into our minds. By doing so, it allows statements of probabilities for single events, such as what is the probability that it will rain tomorrow. In the Frequentist View, it either rains tomorrow or not. We can ask what the probability is that it rains on the 25th of July, by considering the long-run relative frequency of rain on every 25th of July in the past, present, and future. But there is no long-run relative frequency for the 25th of July 2020.</p>
<div id="the-rules-of-probability" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> The rules of probability</h4>
<!-- https://bolt.mph.ufl.edu/6050-6052/unit-3/module-6/ -->
<!-- https://bolt.mph.ufl.edu/6050-6052/unit-3/module-7/ -->
<p>Although the interpretation of probability is debated, the mathematical rules of calculating probabilities are generally agreed upon. Suppose we have an outcome space <span class="math inline">\(\mathcal{S} = \{E_1, E_2, E_3, \ldots\}\)</span>. That is, the outcome space consists of (abstract) events <span class="math inline">\(E_1\)</span>, <span class="math inline">\(E_2\)</span>, <span class="math inline">\(E_3\)</span>, etc. To refer to any of these, we can use the notation <span class="math inline">\(E_i\)</span>, where <span class="math inline">\(i\)</span> can equal 1, 2, 3, etc.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(0 \leq P(E_i) \leq 1\)</span>. The probability of any event <span class="math inline">\(E_i\)</span> is greater or equal to 0 and smaller than or equal to 1. This rule requires little further explanation. Probabilities are chosen to lie on a scale between 0 and 1.</li>
<li><span class="math inline">\(P(E_1 \text{ or } E_2 \text{ or } E_3 \text{ or } \ldots) = P(\mathcal{S}) = 1\)</span>. In words, this means that the probability of at least one event in the outcome space occurring is 1.</li>
<li>If <span class="math inline">\(E_i\)</span> and <span class="math inline">\(E_j\)</span> are <em>mutually exclusive</em> events (if one of them occurs, the other cannot occur), then <span class="math inline">\(P(E_i \text{ or } E_j) = P(E_i) + P(E_j)\)</span>. This is also called the <strong>sum rule</strong>.</li>
<li><span class="math inline">\(P(\neg E_i) = 1 - P(E_i)\)</span>. Here, <span class="math inline">\(\neg\)</span> means “not”, so the probability of “not <span class="math inline">\(E_i\)</span>” is 1 minus the probability of <span class="math inline">\(E_i\)</span> occurring. This is also called the <strong>complement rule</strong>.</li>
<li>For any events <span class="math inline">\(E_i\)</span> and <span class="math inline">\(E_j\)</span>, <span class="math inline">\(P(E_i \text{ or } E_j) = P(E_i) + P(E_j) - P(E_i \text{ and } E_j)\)</span>. This rule holds whether the events are mutually exclusive or not, and can be called the <strong>general sum rule</strong>.</li>
</ol>
<p>Now, all these rules seem a little cumbersome when we are dealing with a simple coin flip. So let’s consider a slightly more complex situation: betting on the outcome a roll of a six-sided die. In this betting game, like in roulette, you are allowed to bet on many things, such as the exact number, but also whether the number is odd or even, whether the number is greater than 3, whether the number is greater smaller than 1, etc. All these things are events in the outcome space:</p>
<p><span class="math display">\[\mathcal{S} = \{1, 2, 3, 4, 5, 6, \text{even}, \text{odd}, &gt;3, &lt; 1, \geq 4, 1 \text{ or } 3, \ldots \}\]</span></p>
<p>The first six events in this outcome space (the numbers 1 to 6) are called <strong>elementary events</strong>: they are mutually exclusive (a single roll cannot result in both a 1 and a 3), and one of them <em>must</em> occur. So we know that <span class="math inline">\(P(1) + P(2) + P(3) + P(4) + P(5) + P(6) = 1\)</span>. But wait, there are more events in the outcome space, and <span class="math inline">\(P(\mathcal{S}) = 1\)</span>, and these six events are only a small part of the outcome space, so does that imply that they all have probability 0? No! That is because not all events in the outcome space are mutually exclusive. For instance, if the outcome is 5, then the outcome is also odd, and it is also <span class="math inline">\(&gt;3\)</span>, <span class="math inline">\(\geq 4\)</span>, etc. As you might have noticed, the other events are actually themselves sets of the elementary events. For instance, <span class="math inline">\(\text{odd} = \{1, 3, 5\}\)</span>, <span class="math inline">\(\text{even} = \{2, 4, 6\}\)</span>, and <span class="math inline">\(&gt; 3 = \{4, 5, 6 \}\)</span>. Figure <a href="statistical-modeling.html#fig:euler-diagram-betting-die">2.3</a> depicts the relations between these events, as well as the elementary events.</p>
<div class="figure" style="text-align: center"><span id="fig:euler-diagram-betting-die"></span>
<img src="_main_files/figure-html/euler-diagram-betting-die-1.png" alt="A diagram depicting the relations between 9 events in the outcome space of the die betting game" width="672" />
<p class="caption">
Figure 2.3: A diagram depicting the relations between 9 events in the outcome space of the die betting game
</p>
</div>
<p>We just really need one more rule and then we’re done. This rule is to compute the probability of a conjunction of events (the probability of both events occurring). To be able to state this rule, we first need to consider the concept of <strong>conditional probability</strong>. In words, such a probability refers to the probability of one event <em>given that another event occurred</em>. For instance, we can consider the probability of the event “greater than 3” given that the event “odd” occurred (i.e., the probability that the outcome was greater than 3 given that the outcome was an odd number). If we know that the outcome was odd (i.e. 1, 3, or 5), we can rule out all occurrences where the outcome was even (i.e. 2, 4, or 6). To now consider the probability that the outcome was greater than 3, we only need the probability of an outcome greater than 3 within the set of odd numbers. If we are sure that the outcome was odd, we can set <span class="math inline">\(P(\text{odd}|\text{odd}) = 1\)</span>. There are three outcomes, each with equal probability, so <span class="math inline">\(P(1|\text{odd}) = P(3|\text{odd}) = P(5|\text{odd}) = \frac{1}{3}\)</span>. In the set of odd outcomes, there is only one number greater than 3, namely 5. That means that the conditional probability <span class="math inline">\(P(&gt;3|\text{odd}) = P(5|\text{odd}) = \frac{1}{3}\)</span>. Conversely, we can also work out the conditional probability <span class="math inline">\(P(\leq 3| = P(5|\text{odd})) = P(1|\text{odd}) + P(3|\text{odd}) = \frac{2}{3}\)</span>. In a sense, a conditional probability is just looking at the relative occurrence of one event <span class="math inline">\(E_i\)</span> (e.g., the event “<span class="math inline">\(&gt;3\)</span>”) within the set of outcomes defined by another event (e.g., the event “odd”).</p>
<p><span class="math display">\[P(E_i|E_j) = \frac{P(E_i \text{ and } E_j) }{P(E_j)}\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li><span class="math inline">\(P(E_i \text{ and } E_j) = P(E_i) \times P(E_j | E_i)\)</span>. <strong>Multiplication rule</strong></li>
<li>For two <strong>independent</strong> events <span class="math inline">\(E_i\)</span> and <span class="math inline">\(E_j\)</span>, <span class="math inline">\(P(E_i \text{ and } E_j) = P(E_i) \times P(E_j)\)</span></li>
</ol>
<p>Kolmogorov, a rather brilliant mathematician, showed that you only need rules 1, 2 and 3; all the other rules follow from the first three. In mathematical terms, rules 1, 2, and 3 are <em>axioms</em>, statements which we assume are true without being able to prove them, while the remaining rules are <em>theorems</em>, statements which follow from the axioms and can be proven to be true.</p>
</div>
<div id="a-model-of-flipping-an-unbiased-coin" class="section level4">
<h4><span class="header-section-number">2.2.1.2</span> A model of flipping an unbiased coin</h4>
<p>Now that we know more about probability, let’s get back to our model for Paul. With a balanced coin (and a not too cunning flipper), the probability of the coin landing heads should equal the probability of the coin landing tails. In mathematical notation, we can state this as</p>
<p><span class="math display">\[P(\text{heads}) = P(\text{tails})\]</span>
Furthermore,</p>
<p><span class="math display">\[P(\text{heads} \text{ or } \text{tails}) = 1\]</span></p>
<p><span class="math display">\[P(\text{correct}) = P(\text{incorrect}) = 0.5\]</span></p>
<p><span class="math display">\[P(\texttt{prediction}_i = \text{correct}) = P(\texttt{prediction}_i = \text{incorrect}) = 0.5\]</span></p>
<p><span class="math display">\[P(Y_i = \text{correct}) = P(Y_i = \text{incorrect}) = 0.5\]</span></p>
<p>which you can translate into words as</p>
<blockquote>
<p>The probability that observation <span class="math inline">\(i\)</span> of variable <span class="math inline">\(Y\)</span> equals ‘correct’ is identical to the probability that observation <span class="math inline">\(i\)</span> of variable <span class="math inline">\(Y\)</span> equals ‘incorrect’, and both equal 0.5.</p>
</blockquote>
</div>
</div>
<div id="distributions" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Distributions</h3>
<p>A probability distribution, as the name suggests, defines how probability is distributed over all possible values of a variable. You can think of “all probability” as a pie, and the probability distribution as slicing up the pie in possibly unequal pieces, and each possible value of a variable is given its own slice.</p>
<p>According to the coin-flipping model, it does not matter which of Paul’s prediction we look at (e.g. whether the first, the second, the last…), to determine . In fact, we could generate the data in the reverse order, or start with the Germany-Austria match and then . In fancy terms, this means that the observations <span class="math inline">\(Y_i\)</span> are <em>independent and identically distributed</em> (IID).</p>
</div>
</div>
<div id="flipping-a-biased-coin-an-alternative-model" class="section level2">
<h2><span class="header-section-number">2.3</span> Flipping a biased coin: An alternative model</h2>
<p>Suppose that Paul did not make his decisions by flipping a mental coin. Perhaps Paul had extensive knowledge of all things football, or perhaps he could see into the future. In that case, we might expect Paul to have a higher rate of correct answers than predicted by our coin flipping model. But perhaps Paul, although an oracle, wasn’t so benevolent to his caretakers, or didn’t want to spoil the match for them. In that case, we might expect the rate of correct answers to be lower than predicted by the coin flipping model. In both case, we’d expect the probability of a correct answer to be different from .5: <span class="math inline">\(P(\text{correct}) \neq .5\)</span>. But whilst not .5, we don’t otherwise know the value. We call such an unknown value which determines a probability distribution a <em>parameter</em>. We will generally use symbols from the Greek alphabet to denote such parameters. Here, we will use <span class="math inline">\(\theta\)</span> (pronounced as “thee-ta”). Our alternative to the coin flipping model can now be stated as:</p>
<p><span class="math display">\[P(\text{correct}) = \theta\]</span></p>
<p>Like a <em>variable</em>, a parameter such as <span class="math inline">\(\theta\)</span> can take different values. It belongs to a model, and its value can not be observed, only inferred. As our parameter reflects a probability, we know that it can never be lower than 0, and never larger than 1, so we know that:</p>
<p><span class="math display">\[0 \leq \theta \leq 1\]</span></p>
<p>(“zero is less than or equal to <span class="math inline">\(\theta\)</span>, which is less than or equal to 1”). Note that the possible values in this range includes <span class="math inline">\(\theta = 0.5\)</span>. Our (unbiased) coin flipping model is actually one of the possible versions (a “special case”) of our new more general model. Some other ways of saying this are that the coin flipping model is contained within, or nested under, the more general new model. As we will see, <em>nested models</em> play an important role in statistical modeling. In particular, we will often compare how well a special-case or nested model describes the data compared to a more general model. In such model comparisons, we will usually refer to the special-case model as <code>MODEL S</code>, and the more general model as <code>MODEL G</code>. It is important to realise that these names (like variables) are containers and we can put any models in these, as long as the one we put into <code>MODEL S</code> is a special case of the one we put into <code>MODEL G</code>.</p>
</div>
<div id="estimation" class="section level2">
<h2><span class="header-section-number">2.4</span> Estimation</h2>
<p>From data produced by The Data Generating Process (TDGP), we can estimate the value of a parameter. Remember, when we collect data in real life, it is obtained from TDGP. Our model, on the other hand, merely provides “a data generating process” (adgp). See what I did there with CAPS and no caps? The TDGP is, as the name should make clear, The Truth. It is how the real world generated the data. Our model, on the other hand, is almost surely an approximation to The Truth. Think about it. Can you summarize Paul’s choices by a mathematical formula with a distribution that takes a single parameter (<span class="math inline">\(\theta\)</span>)? Probably not when you see Paul swimming in his tank, catching a glimmer of light shining from one of the boxes, than being distracted by the sound of a distant door opening, then going back and sensing a vibration in the water. How would all these serendipitous things not affect Paul’s decision at the time? Our model ignores all these factors, and for a reason. Imagine we had to take all sensory input of Paul into account. We would not only have to take into account the actual stimuli present at the time, but also how they affected Paul’s neuronal reactions to those. Even if we only take into account the neurons responding directly to external input, the number of possible neuronal excitation patterns quickly become enormous. When you think about it, you’d also want to take into account the activation state of all the neurons at the time just before (and possibly before that), which makes the number of possibilities even more enormous. And what about the chemical consistency of the water at the time? Learning a model of particular decision by a particular person at a particular time seems simply impossible without assuming that some of these things don’t really matter to the decision at hand.</p>
<p>Estimation, in some sense, means an “educated guess”. It is a guess, because we cannot be completely sure that we have picked the true value. It is educated because we didn’t just pluck the value from thin air. We used information in the data to guide our guess.</p>
<div id="properties-of-good-estimators" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Properties of good estimators</h3>
<p>An <strong>estimator</strong> is an algorithm which takes data as input and produces a parameter estimate as output. Evaluating the quality of the resulting estimates is generally done under the assumption that our model represents the Data Generating Process accurately, i.e. our model is a true representation of the DGP. If our model has unknown parameters, that means that there are values for those parameters such that our model produces data with exactly the same distribution as the DGP. We can call those parameter values the true parameter values. Now, even if we have access to such a true model, estimates of its parameters from a given data set are unlikely to be identical to the true parameters. The main reason is that any actual data set will not cover all of the values that are possible from the Data Generating Process, and/or not in their expected proportions. But the fact is that in practice we have to rely on such limited data sets; we never have access to all the data that could be produced by the Data Generating Process. So we will have to work with estimates that are different from the true parameters. One thing we can do is quantify how much our estimates are likely to deviate from the true parameters. This, in a nutshell, is what statistical estimation theory is all about.</p>
<div id="unbiasedness" class="section level4">
<h4><span class="header-section-number">2.4.1.1</span> Unbiasedness</h4>
</div>
<div id="consistency" class="section level4">
<h4><span class="header-section-number">2.4.1.2</span> Consistency</h4>
</div>
<div id="efficiency" class="section level4">
<h4><span class="header-section-number">2.4.1.3</span> Efficiency</h4>
<p>A good estimator should become more precise as we have more data. The larger the number of observations from the Data Generating Process, the closer the resulting estimates should be to the true parameters.</p>
</div>
</div>
</div>
<div id="null-hypothesis-significance-testing" class="section level2">
<h2><span class="header-section-number">2.5</span> Null-hypothesis significance testing</h2>
<div id="decisions-and-types-of-error" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Decisions and types of error</h3>
</div>
<div id="significance-and-power" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Significance and power</h3>
</div>
</div>
<div id="confidence-intervals" class="section level2">
<h2><span class="header-section-number">2.6</span> Confidence intervals</h2>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">2.7</span> Summary</h2>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">2.8</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li>Maybe Paul has a preference to go to the right hand container.
<ol style="list-style-type: lower-alpha">
<li>Suppose Paul always opens the right container first. So <span class="math inline">\(P(\text{Paul chooses right}) = 1.0\)</span>. Furthermore, suppose that his keeper has ability to predict the winner of a match, in that he is correct on 60% of his predictions, i.e. <span class="math inline">\(P(\text{keeper is correct}) = .6\)</span>. Finally, let’s suppose that the keeper wants to help Paul, so he always puts the flag of the country he predicts to win on the right container. What is the probability that Paul’s “predictions” are correct? And what if Paul’s spatial preference was less marked at <span class="math inline">\(P(\text{Paul chooses right}) = .6\)</span>?</li>
</ol></li>
<li></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-model-with-a-mean-introduction-to-the-general-linear-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mspeekenbrink/sdam-book/edit/master/02-modeling.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/mspeekenbrink/sdam-book/blob/master/02-modeling.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
