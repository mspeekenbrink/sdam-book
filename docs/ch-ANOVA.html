<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 A model of means (ANOVA) | Statistics: Data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 A model of means (ANOVA) | Statistics: Data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 A model of means (ANOVA) | Statistics: Data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2021-09-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="moderation-and-mediation.html"/>
<link rel="next" href="ch-factorial-ANOVA.html"/>
<script src="book_assets/header-attrs-2.7/header-attrs.js"></script>
<script src="book_assets/jquery-3.5.1/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="book_assets/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.6.1/grViz.js"></script>
<script src="book_assets/plotly-binding-4.9.3/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-1.57.1/plotly-latest.min.js"></script>
<link href="book_assets/combineWidgetStyle-0.1/combineWidgets.css" rel="stylesheet" />
<script src="book_assets/combineWidgets-binding-0.10.1/combineWidgets.js"></script>
<script src="book_assets/rglWebGL-binding-0.106.6/rglWebGL.js"></script>
<link href="book_assets/rglwidgetClass-0.106.6/rgl.css" rel="stylesheet" />
<script src="book_assets/rglwidgetClass-0.106.6/rglClass.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/utils.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/subscenes.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/shaders.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/textures.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/projection.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/mouse.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/init.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/pieces.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/draw.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/controls.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/selection.src.js"></script>
<script src="book_assets/rglwidgetClass-0.106.6/rglTimer.src.js"></script>
<script src="book_assets/CanvasMatrix4-0.106.6/CanvasMatrix.src.js"></script>
<script src="book_assets/rglPlayer-binding-0.106.6/rglPlayer.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-modeling.html"><a href="ch-modeling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-probability-definition"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-binomial-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="ch-modeling.html"><a href="ch-modeling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-modeling.html"><a href="ch-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-modeling.html"><a href="ch-modeling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-likelihood-ratio"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-modeling.html"><a href="ch-modeling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-modeling.html"><a href="ch-modeling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-modeling.html"><a href="ch-modeling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-modeling.html"><a href="ch-modeling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="ch-modeling.html"><a href="ch-modeling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="ch-modeling.html"><a href="ch-modeling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-an-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has an specific value</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#confidence-intervals"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#effect-size"><i class="fa fa-check"></i><b>3.6</b> Effect size</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.7</b> Assumptions</a></li>
<li class="chapter" data-level="3.8" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.8</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.8.1</b> The Central Limit Theorem in action</a></li>
<li class="chapter" data-level="3.8.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#bootstrapping-a-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Bootstrapping a statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#in-practice"><i class="fa fa-check"></i><b>3.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#effect-size-and-the-importance-of-predictors"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assumptions</a></li>
<li class="chapter" data-level="5.8" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.8</b> Multicollinearity: Redundancy between predictors</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.8.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-outliers"><i class="fa fa-check"></i><b>5.9</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#sec:05-dont-forget-about-the-fun"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#sec:06-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.6</b> Multiple testing and post-hoc tests</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#sec:06-main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#the-factorial-anova-model"><i class="fa fa-check"></i><b>8.3</b> The factorial ANOVA model</a></li>
<li class="chapter" data-level="8.4" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.4</b> A threeway factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#interpreting-interactions"><i class="fa fa-check"></i><b>8.4.1</b> Interpreting interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.5</b> Orthogonal contrast codes and unequal sample sizes</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.5.1</b> Comparison schemes and SS types</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#subjective-feelings-of-power-and-priming"><i class="fa fa-check"></i><b>9.1</b> Subjective feelings of power and priming</a></li>
<li class="chapter" data-level="9.2" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#acounting-for-pre-existing-differences"><i class="fa fa-check"></i><b>9.2</b> Acounting for pre-existing differences</a></li>
<li class="chapter" data-level="9.3" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#slopes-of-contrast-coding-predictors-in-ancova-models"><i class="fa fa-check"></i><b>9.3</b> Slopes of contrast-coding predictors in ANCOVA models</a></li>
<li class="chapter" data-level="9.4" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#homogeneity-of-slopes"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of slopes</a></li>
<li class="chapter" data-level="9.5" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#sec:ANCOVA-power"><i class="fa fa-check"></i><b>9.5</b> Power considerations in ANCOVA</a></li>
<li class="chapter" data-level="9.6" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#models-with-multiple-covariates"><i class="fa fa-check"></i><b>9.6</b> Models with multiple covariates</a></li>
<li class="chapter" data-level="9.7" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#mediation-with-categorical-independent-variables"><i class="fa fa-check"></i><b>9.7</b> Mediation with categorical independent variables</a></li>
<li class="chapter" data-level="9.8" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#ancova-vs-difference-scores"><i class="fa fa-check"></i><b>9.8</b> ANCOVA vs difference scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a></li>
<li class="chapter" data-level="11" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#non-independence-in-linear-models"><i class="fa fa-check"></i><b>11.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="11.2" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#random-intercept-models"><i class="fa fa-check"></i><b>11.2</b> Random intercept models</a></li>
<li class="chapter" data-level="11.3" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#parameter-estimation-1"><i class="fa fa-check"></i><b>11.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="11.4" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#parameter-inference"><i class="fa fa-check"></i><b>11.4</b> Parameter inference</a></li>
<li class="chapter" data-level="11.5" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#application-of-the-random-intercepts-model"><i class="fa fa-check"></i><b>11.5</b> Application of the random-intercepts model</a></li>
<li class="chapter" data-level="11.6" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#models-with-random-intercepts-and-slopes"><i class="fa fa-check"></i><b>11.6</b> Models with random intercepts and slopes</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#correlation-between-random-effects"><i class="fa fa-check"></i><b>11.6.1</b> Correlation between random effects</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#crossed-random-effects-dating-partners-in-the-speed-dating-experiment"><i class="fa fa-check"></i><b>11.7</b> Crossed random effects: dating partners in the speed dating experiment</a></li>
<li class="chapter" data-level="11.8" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#choosing-the-random-effects-structure"><i class="fa fa-check"></i><b>11.8</b> Choosing the random effects structure</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html"><i class="fa fa-check"></i><b>12</b> Introduction to Bayesian hypothesis testing</a>
<ul>
<li class="chapter" data-level="12.1" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#fundamentals-of-bayesian-inference"><i class="fa fa-check"></i><b>12.1</b> Fundamentals of Bayesian inference</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#probability-in-times-of-covid"><i class="fa fa-check"></i><b>12.1.1</b> Probability in times of Covid</a></li>
<li class="chapter" data-level="12.1.2" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#bayes-rule"><i class="fa fa-check"></i><b>12.1.2</b> Bayes’ rule</a></li>
<li class="chapter" data-level="12.1.3" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#we-missed-you-paul"><i class="fa fa-check"></i><b>12.1.3</b> We missed you Paul!</a></li>
<li class="chapter" data-level="12.1.4" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#the-marginal-likelihood-and-prior-predictive-distribution"><i class="fa fa-check"></i><b>12.1.4</b> The marginal likelihood and prior predictive distribution</a></li>
<li class="chapter" data-level="12.1.5" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#hypothesis-testing-relative-evidence-and-the-bayes-factor"><i class="fa fa-check"></i><b>12.1.5</b> Hypothesis testing, relative evidence, and the Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#a-bayesian-t-test"><i class="fa fa-check"></i><b>12.2</b> A Bayesian t-test</a></li>
<li class="chapter" data-level="12.3" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#bayes-factors-for-general-linear-models"><i class="fa fa-check"></i><b>12.3</b> Bayes factors for General Linear Models</a></li>
<li class="chapter" data-level="12.4" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#some-objections-to-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>12.4</b> Some objections to null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#the-p-value-is-not-a-proper-measure-of-evidential-support"><i class="fa fa-check"></i><b>12.4.1</b> The <span class="math inline">\(p\)</span>-value is not a proper measure of evidential support</a></li>
<li class="chapter" data-level="12.4.2" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#the-p-value-depends-on-researcher-intentions"><i class="fa fa-check"></i><b>12.4.2</b> The <span class="math inline">\(p\)</span>-value depends on researcher intentions</a></li>
<li class="chapter" data-level="12.4.3" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#results-of-a-nhst-are-often-misinterpreted"><i class="fa fa-check"></i><b>12.4.3</b> Results of a NHST are often misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#to-bayes-or-not-to-bayes-a-pragmatic-view"><i class="fa fa-check"></i><b>12.5</b> To Bayes or not to Bayes? A pragmatic view</a></li>
<li class="chapter" data-level="12.6" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#summary-2"><i class="fa fa-check"></i><b>12.6</b> “Summary”</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html"><i class="fa fa-check"></i><b>13</b> Being a responsible data analyst</a>
<ul>
<li class="chapter" data-level="13.1" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#consider-analysis-before-data-collection"><i class="fa fa-check"></i><b>13.1</b> Consider analysis before data collection</a></li>
<li class="chapter" data-level="13.2" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#explore-the-data"><i class="fa fa-check"></i><b>13.2</b> Explore the data</a></li>
<li class="chapter" data-level="13.3" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#evaluate-the-assumptions-underlying-your-analyses"><i class="fa fa-check"></i><b>13.3</b> Evaluate the assumptions underlying your analyses</a></li>
<li class="chapter" data-level="13.4" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#distinguish-between-confirmatory-and-exploratory-analyses"><i class="fa fa-check"></i><b>13.4</b> Distinguish between confirmatory and exploratory analyses</a></li>
<li class="chapter" data-level="13.5" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#aim-for-openness-and-reproducibility"><i class="fa fa-check"></i><b>13.5</b> Aim for openness and reproducibility</a></li>
<li class="chapter" data-level="13.6" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#communicate-clearly-and-concisely"><i class="fa fa-check"></i><b>13.6</b> Communicate clearly and concisely</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#example-of-reporting-a-multiple-regression-analysis"><i class="fa fa-check"></i><b>13.6.1</b> Example of reporting a multiple regression analysis</a></li>
<li class="chapter" data-level="13.6.2" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#example-of-reporting-a-factorial-anova"><i class="fa fa-check"></i><b>13.6.2</b> Example of reporting a factorial ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: Data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch:ANOVA" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> A model of means (ANOVA)</h1>
<p>In this chapter, we will discuss how <em>nominal variables</em>, which often reflect different manipulations within an experiment, can be included in the General Linear Model. This is done by constructing so-called <em>contrast codes</em>, which provide a means to construct metric predictors which encode differences between the levels of the nominal variable (e.g. differences between conditions in an experiment). When a study concerns only a single nominal variable, the interest is generally in determining whether there are differences in the average of the dependent variable between the levels (groups). Traditionally, ANOVA has focused on so-called <em>omnibus tests</em>, which test whether the mean of at least one group mean is different to that of another. Contrast codes allow you to test more informative hypotheses, and we will discuss different ways of constructing these contrast codes, including so-called <em>orthogonal contrast codes</em>. Within the context of a single model, you can only include a limited number of contrast codes, and we end the chapter with considering how you can test more hypotheses, whether these were conceived beforehand (<em>planned comparisons</em>) or afterwards (<em>post-hoc tests</em>).</p>
<div id="can-playing-tetris-reduce-intrusive-memories" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Can playing Tetris reduce intrusive memories?</h2>
<p>After a traumatic experience, some people experience flashbacks, which are intrusive and involuntary memories that involve vivid imagery related to the traumatic event. These intrusive memories can be highly distressing and are a hallmark of acute stress disorder and posttraumatic stress disorder (PTSD). It has been suggested that recalling traumatic memories under certain conditions can reduce their negative impact. Memory consolidation refers to a collection of neural processes which stabilize a memory trace after it is aqcuired. According to reconsolidation theory, when a consolidated memory trace is reactivated (remembered), it again becomes malleable and will require restabilization for it to persist. Disruption of this reconsolidation after recall may then be a way to reduce the strength of traumatic memories, or even allow them to be forgotten.</p>
<p><span class="citation"><a href="references.html#ref-james_computer_2015" role="doc-biblioref">James et al.</a> (<a href="references.html#ref-james_computer_2015" role="doc-biblioref">2015</a>)</span> conducted a study to investigate this idea. They reasoned that because intrusive memories of trauma are often visual in nature, performing a demanding visuospatial task (e.g. playing the computer game Tetris) after recall could interfere with the reconsolidation process and reduce subsequent intrusions of the traumatic memory. In their Experiment 2, they first created traumatic memories by showing their participants a 12-minute film with graphic scenes depicting death and serious injury (e.g. a van hitting a teenage boy while he was using his mobile phone crossing the road). Participants then went home and recorded the number of intrusive memories of the film during the subsequent 24-hour period (Day 0). The next day, they returned to the lab and were randomly assigned to one of four conditions:</p>
<ol style="list-style-type: decimal">
<li>No-task control: participants in this condition (<span class="math inline">\(n=18\)</span>) completed a 10-minute music filler task, rating excerpts of classical music for pleasantness.</li>
<li>Tetris+Reactivation: participants in this condition (<span class="math inline">\(n=18\)</span>) were shown a series of images from the scenes in the trauma film to reactivate the memories of the scenes. After this reactivation task, they completed the 10-minute music filler task, and then played the video game Tetris for 12 minutes.</li>
<li>Tetris-Only: participants in this condition (<span class="math inline">\(n=18\)</span>) performed the music filler task and then played Tetris for 12 minutes, but did not complete the reactivation task.</li>
<li>Reactivation Only: participants in this condition (<span class="math inline">\(n=18\)</span>) completed the reactivation and music filler task, but did not play Tetris.</li>
</ol>
<p>All participants then went home and were asked to record the number of intrusive memories they experienced over the next seven days (Day 1 to 7). After this week passed, participants returned to the lab and completed an Intrusion-Provocation Task, in which they
were shown blurred images from the trauma film and asked to indicate whether each of these triggered an intrusive memory.</p>
<p>The sample means and standard deviations of the number of intrusions in each condition are:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\overline{Y}\)</span></th>
<th align="right"><span class="math inline">\(S_Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Control</td>
<td align="right">5.11</td>
<td align="right">4.11</td>
</tr>
<tr class="even">
<td align="left">Tetris+Reactivation</td>
<td align="right">1.89</td>
<td align="right">1.70</td>
</tr>
<tr class="odd">
<td align="left">Tetris-Only</td>
<td align="right">3.89</td>
<td align="right">2.81</td>
</tr>
<tr class="even">
<td align="left">Reactivation-Only</td>
<td align="right">4.83</td>
<td align="right">3.24</td>
</tr>
</tbody>
</table>
<p>Boxplots for each condition are provided in Figure <a href="ch-ANOVA.html#fig:tetris-intrusive-memories">7.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:tetris-intrusive-memories"></span>
<img src="_main_files/figure-html/tetris-intrusive-memories-1.svg" alt="Number of intrusive memories during days 1 to 7 for each condition of Experiment 2 of James et al. (2015)." width="95%" />
<p class="caption">
Figure 7.1: Number of intrusive memories during days 1 to 7 for each condition of Experiment 2 of James et al. (2015).
</p>
</div>
</div>
<div id="sec:06-two-groups" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Comparing two groups</h2>
<p>To start with a relatively straightforward example, let’s first focus only on the data from the Tetris+Reactivation and the Reactivation-Only conditions. We are interested in whether playing Tetris during reactivation reduces the number of memory intrusions on later days, in comparison to when the traumatic memory was reactivated only. To investigate this, we will extend the simple model of Chapter <a href="ch-simple-GLM.html#ch:simple-GLM">3</a>. In particular, we will assume that the number of intrusions is Normal-distributed with a mean that depends on the experimental condition participants were assigned to (Tetris+Reactivation or Reactivation-Only). We can write this model in two equivalent ways as:
<span class="math display">\[Y_i \sim \mathbf{Normal}(\mu_{\text{con}},\sigma)\]</span>
and
<span class="math display" id="eq:two-group-model-mu">\[\begin{equation}
Y_i = \mu_\text{con} + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma)
\tag{7.1}
\end{equation}\]</span>
where <span class="math inline">\(\mu_\text{con}\)</span> is a placeholder for the mean of a particular condition. Specifying that mean explicitly, we can also write the model as</p>
<p><span class="math display">\[Y_{i} = \begin{cases}  \mu_\text{t+r} + \epsilon_{i} \hspace{2em} \text{if condition = Tetris+Reactivation} \\ \mu_\text{react} + \epsilon_{i} \hspace{2em} \text{if condition = Reactivation-Only}  \end{cases} \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)\]</span>
Note that the model assumes that while the mean can differ between the conditions, deviations of the observations around the mean are assumed to have the same standard deviance <span class="math inline">\(\sigma_\epsilon\)</span>. An example of what the model might look like is given in Figure <a href="ch-ANOVA.html#fig:two-group-Normal-densities">7.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:two-group-Normal-densities"></span>
<img src="_main_files/figure-html/two-group-Normal-densities-1.svg" alt="Two Normal density functions with a different mean but identical standard deviation." width="80%" />
<p class="caption">
Figure 7.2: Two Normal density functions with a different mean but identical standard deviation.
</p>
</div>
<p>Can we estimate such a model, using the tools we have already learned about? Yes! It is actually quite straightforward to construct a linear model to represent the model with means depending on condition. Condition is a nominal variable, which we can’t simply include “as is” in a linear model. Linear models need <em>metric</em> predictors. But we can construct a new predictor <span class="math inline">\(X\)</span>, which has the value 0 for participants in the Tetris+Reactivation condition, and the value 1 for participants in the Reactivation-Only condition. This predictor <span class="math inline">\(X\)</span>, with values 0 and 1 referring to different groups, is commonly referred to as a <strong>dummy coding</strong> variable. With this dummy predictor, the model</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_i + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma)\]</span>
is formally equivalent to the model of Equation <a href="ch-ANOVA.html#eq:two-group-model-mu">(7.1)</a>. To see why, it is important to realize that linear regression concerns the conditional means of the dependent variable given the values of the predictor variable(s). If we give the model enough flexibility (and I will discuss more in detail what that means later), then the model will be able to represent those conditional means accurately. Let’s write out the structural part of the model (excluding the error term <span class="math inline">\(\epsilon_i\)</span>) for a participant from each condition. For <em>everyone</em> in the Tetris+Reactivation condition, <span class="math inline">\(X_i = 0\)</span>, so the model predictions are
<span class="math display">\[\begin{aligned}
\hat{Y}_i &amp;= \beta_0 + \beta_1 \times 0 \\
&amp;= \beta_0
\end{aligned}\]</span>
For <em>everyone</em> in the Reactivation-Only condition, <span class="math inline">\(X_i = 1\)</span>, so the model are
<span class="math display">\[\begin{aligned}
\hat{Y}_i &amp;= \beta_0 + \beta_1 \times 1 \\
&amp;= \beta_0 + \beta_1
\end{aligned}\]</span></p>
<p>For cases in the Tetris+Reactivation condition, the model predictions are a constant: <span class="math inline">\(\beta_0\)</span>. If we “rename” <span class="math inline">\(\beta_0 = \mu_\text{t+r}\)</span>, then we have the the model we wanted for this condition. For cases in the Reactivation-Only condition, the prediction consists of the sum of two constants, <span class="math inline">\(\beta_0 + \beta_1\)</span>. A sum of two constants is itself a constant. Moreover, because we have already “renamed” <span class="math inline">\(\beta_0 = \mu_\text{tetr}\)</span>, we can also write the sum as <span class="math inline">\(\mu_\text{react} = \mu_\text{t+r} + \beta_1\)</span>, which if we move <span class="math inline">\(\mu_\text{t+r}\)</span> to the left-hand side, indicates that <span class="math inline">\(\beta_1 = \mu_\text{react} - \mu_\text{t+r}\)</span>. In words, the slope of our predictor equals the difference between the mean of the the Reactivation-Only and the mean of Tetris+Reactivation condition.</p>
<p>Remember that the linear model represents the conditional mean of the dependent variable for each set of values of the predictor variables. We have just constructed a predictor with a different value for cases in each condition, and we could write the resulting model predictions in terms of two parameters (the intercept and the slope) which we could relate to the means in each condition. In Chapter <a href="ch-simple-GLM.html#ch:simple-GLM">3</a>, we showed that the maximum likelihood estimate of the mean <span class="math inline">\(\mu\)</span> of a Normal-distributed variable is the sample mean <span class="math inline">\(\overline{Y}\)</span>. In a model with multiple means, the maximum likelihood estimates are the respective sample means in each group (condition). In the model we just constructed, the parameter estimates are a function of the sample means in each group. More precisely, the estimate of the intercept equals the mean in the Tetris+Reactivation condition:
<span class="math display">\[\hat{\beta}_0 = \overline{Y}_\text{t+r}\]</span>
and the estimate of the slope equals the difference between the sample mean of the the Reactivation-Only and the sample mean of Tetris+Reactivation condition:
<span class="math display">\[\hat{\beta}_1 = \overline{Y}_\text{react} - \overline{Y}_\text{t+r}\]</span>
The average number of intrusive memories in the Tetris+Reactivation condition was <span class="math inline">\(\overline{Y}_\text{t+r} = 1.89\)</span>, and the corresponding mean in the Reactivation condition was <span class="math inline">\(\overline{Y}_\text{react} = 4.83\)</span>. Estimating the regression model provides the following estimates
<span class="math display">\[\texttt{intrusions}_i = 1.89 + 2.94 \times \texttt{dummy}_i  + \hat{\epsilon}_i \quad \quad \hat{\epsilon}_i \sim \mathbf{Normal}(0, 2.66)\]</span>
As you can see, the intercept equals <span class="math inline">\(\overline{Y}_\text{t+r}\)</span>, and the slope of <span class="math inline">\(\texttt{dummy}\)</span> equals <span class="math inline">\(\overline{Y}_\text{react} - \overline{Y}_\text{t+r} = 4.83 - 1.89 = 2.94\)</span>. The results of the hypothesis tests that the intercept and slope equal 0 are given in Table <a href="ch-ANOVA.html#tab:tetris-intrusions-dummy-ANOVA">7.1</a>. The test result for the intercept indicates that the average number of intrusions in the Tetris+Reactivation condition is different than zero. More interestingly, the test result for the slope of <span class="math inline">\(\texttt{dummy}\)</span> indicates that the difference between the Tetris+Reactivation and Reactivation-Only condition is not equal to 0. In other words, there is a <em>difference</em> between the conditions in the average number of intrusions. Playing Tetris after memory reactivation seems to reduce the number of subsequent intrusions.</p>
<table>
<caption><span id="tab:tetris-intrusions-dummy-ANOVA">Table 7.1: </span>Linear model predicting number of intrusions by a dummy predictor.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SS}\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(F\)</span></th>
<th align="right"><span class="math inline">\(P(\geq \lvert F \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">1.89</td>
<td align="right">64.2</td>
<td align="right">1</td>
<td align="right">9.09</td>
<td align="right">0.005</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\texttt{dummy}\)</span></td>
<td align="right">2.94</td>
<td align="right">78.0</td>
<td align="right">1</td>
<td align="right">11.04</td>
<td align="right">0.002</td>
</tr>
<tr class="odd">
<td align="left">Error</td>
<td align="right"></td>
<td align="right">240.3</td>
<td align="right">34</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>A test comparing the means of two Normal-distributed variables is also known as an <strong>independent samples t-test</strong>. It involves an extension of the one-sample <span class="math inline">\(t\)</span> test discussed in Chapter <a href="ch-simple-GLM.html#ch:simple-GLM">3</a> and is based on the sampling distribution of the difference between two sample means. As the mathematical details of that test do not provide any new insights, we omit them here. A main thing to realise is that this test will provide identical results to the test of the slope of <span class="math inline">\(\texttt{dummy}\)</span>. If you’d take the square-root of the <span class="math inline">\(F\)</span> statistic for this test, you obtain the value of the <span class="math inline">\(t\)</span> statistic of the independent samples t-test.</p>
<!-- 
$$t = \frac{\overline{Y}_1 - \overline{Y}_2}{\hat{\sigma}_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$

where $\hat{\sigma}_p$ is a so-called _pooled_ estimate of the standard deviation in each group
$$\hat{\sigma}_p = \sqrt{\frac{(n_1 - 1) S^2_1 + (n_2 - 1) S^2_2}{n_1 + n_2 - 2}}$$

-->
</div>
<div id="the-anova-model" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> The ANOVA model</h2>
<p>The dummy coding procedure above can be generalized to the situation in which you want to compare more than two groups. In the Tetris study, there were four conditions. To allow a linear model to represent the means in all four conditions, you need to use more than one dummy-coding predictor. In fact, you would need 3 dummy-coding predictors. In such a model, the intercept represents the mean of one condition, which we can call the reference group. The slope of each predictor represents the difference between the mean of the remaining groups and the reference group. Whilst dummy coding is simple and provides interpretable parameters, there are alternative coding schemes that may provide more interesting tests.</p>
<p>Before going into such alternative coding schemes, we will take a <em>slight</em> detour and consider the traditional model for the case of Normal-distributed variables in multiple groups with (potentially) different means, but the same standard deviation. This model is also called the oneway ANOVA model, and can be stated as follows:
<span class="math display" id="eq:glm-ANOVA-model">\[\begin{equation}
Y_{j,i} = \mu + \tau_j + \epsilon_{j,i} \quad \quad \epsilon_{j,i} \sim \textbf{Normal}(0, \sigma_\epsilon)
\tag{7.2}
\end{equation}\]</span>
Here, <span class="math inline">\(Y_{j,i}\)</span> denotes the value of the dependent variable for case <span class="math inline">\(i = 1,\ldots, n_j\)</span> in group <span class="math inline">\(j = 1, \ldots, g\)</span>. So <span class="math inline">\(g\)</span> denotes the total number of groups (i.e. <span class="math inline">\(g=4\)</span> in the Tetris study), and <span class="math inline">\(n_j\)</span> the number of cases in group <span class="math inline">\(j\)</span> (i.e. <span class="math inline">\(n_j = 18\)</span> in the Tetris study for all <span class="math inline">\(j=1,\ldots,4\)</span>). <span class="math inline">\(\epsilon_{j,i}\)</span> is the corresponding error term for participant <span class="math inline">\(i\)</span> in group <span class="math inline">\(j\)</span>. The mean <span class="math inline">\(\mu\)</span> is the so-called “grand mean,” which is the overall mean of the dependent variable, the mean of any observation that could be produced by the Data Generating Process, regardless of group. It is generally defined as the average of the means in all conditions, i.e. <span class="math inline">\(\mu = \frac{\sum_{j=1}^g \mu_j}{g}\)</span>. This is identical to the mean over all observations if all the conditions have an equal number of observations. The term <span class="math inline">\(\tau_j\)</span> represents the so-called “treatment effect” of group <span class="math inline">\(j\)</span>, defined as the difference between the mean of group <span class="math inline">\(j\)</span> and the grand mean:
<span class="math display">\[\begin{equation}
\tau_j = \mu_j - \mu
\end{equation}\]</span></p>
<p>The traditional goal of an ANOVA is to determine whether there is any treatment effect. That is, to test the null-hypothesis <span class="math inline">\(H_0: \tau_j = 0 \text{ for all } j = 1, \ldots, g\)</span>. Note that the hypothesis states that <span class="math inline">\(\tau_j = 0\)</span> for all groups. We could have also stated this hypothesis as <span class="math inline">\(H_0: \tau_1 = \tau_2 = \ldots = \tau_g = 0\)</span>, if you find that clearer. As the hypothesis states that the difference between the group-specific mean <span class="math inline">\(\mu_j\)</span> and the grand mean <span class="math inline">\(\mu\)</span> is 0 for every group, the implication is that <span class="math inline">\(\mu_j = \mu\)</span> for all groups (i.e. all groups have an identical mean <span class="math inline">\(\mu\)</span>). The test statistic for this null-hypothesis is the <span class="math inline">\(F\)</span> statistic, calculated as a ratio of the (estimated) sample variance of the treatment effects and the (estimated) variance of residual error terms. As we will see, we can perform this hypothesis test with the General Linear Model through an overall model test (comparing a “full” MODEL G to an intercept-only MODEL R). In addition, we can also test more specific hypotheses regarding differences between the group means and the overall mean, or specific hypotheses regarding differences between particular (combinations of) groups. In contrast to traditional ANOVA, the GLM approach also deals naturally with situations in which the groups have unequal sizes (i.e. <span class="math inline">\(n_j\)</span> differs between the groups). This is not straightforward with the traditional ANOVA test.</p>
</div>
<div id="contrast-coding" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Contrast coding</h2>
<!-- The dummy coding procedure above can be generalized to the situation in which you want to compare more than two groups. In the Tetris study, there were four conditions. To allow a linear model to represent the means in all four conditions, you need to use more than one dummy-coding predictor. In fact, you would need 3 dummy-coding predictors. In such a model, the intercept represents the mean of one condition, which we can call the reference group. The slope of each predictor represents the difference between the mean of the remaining groups and the reference group. Whilst dummy coding is simple and provides interpretable parameters, there are alternative coding schemes that may provide more interesting tests. <!-- we will in the remainder of this book focus mostly on other forms of coding. -->
<!-- a different way of coding, which also provides interpretable parameters, but has the benefit that it (in the case of equally-sized groups) provides __independent predictors__. If the predictors in a linear model are all independent, there is no redundancy between them (i.e. no multicollinearity). This increases the reliability of parameter estimates and with that the power of the tests of those parameters.-->
<p>As in the case of two groups, the approach to testing group differences in the GLM is to construct new predictor variables, which we might call <strong>contrast-coding predictors</strong>, that represent differences between groups.</p>
<p>To illustrate the general concepts of contrast coding in a hopefully intuitive manner, let’s first consider a game in which you ask someone to pick a random number between 1 and 8 and your job is to determine the number they picked by asking questions which can be answered by “yes” or “no.” There are different ways in which you can play this game, and some of these are more efficient than others. For instance, you can ask “Is the number 1?” “Is the number 2?” “Is the number 3?” etc. If they picked the number 1, then you would have needed just a single question, but if they picked the number 8, you would have needed a total of 7 questions. After having asked “Is the number 7?” you would not need to ask whether the number was 8, because that would be the only remaining possibility.</p>
<p>To uniquely determine any number between 1 and 8, your playing strategy will always consist of 7 questions. You would not need to ask all of these questions in any given play of the game, but over many plays with the same question strategy, you would ask a total of 7 unique questions. The strategy above is a bit like dummy coding. The final number 8 is the reference group, and each question such as “Is the number 1?” is implicitly the same as “Is the number 1 <em>and not 8</em>?” A different way to play the game is by a strategy which guarantees you to always correctly “guess” the number in three questions. This strategy is depicted in Figure <a href="ch-ANOVA.html#fig:number-guessing-game-contrast-coding">7.3</a>. In this strategy, each question halves the number of remaining options. If the first question “Is the number larger than 4?” is answered as “yes,” the number can only be 5, 6, 7, or 8. Subsequently asking “Is the number larger than 6?” would reduce the remaining options by half again. If the answer to this question was “yes,” then the number would have to be either 7 or 8. Asking “Is the number 7?” would then allow you to finish the game (if the answer is “yes,” the number is 7, if “no,” it would have to be 8). Just like the earlier strategy, this strategy consists of a total of 7 questions (and in any given game, you would ask 3 of these).</p>
<div class="figure" style="text-align: center"><span id="fig:number-guessing-game-contrast-coding"></span>
<img src="_main_files/figure-html/number-guessing-game-contrast-coding-1.svg" alt="Questions to guess a random number between 1 and 8" width="60%" />
<p class="caption">
Figure 7.3: Questions to guess a random number between 1 and 8
</p>
</div>
<p>Constructing contrast codes can be seen as a more complicated version of the game above. Suppose that instead of guessing a single number, the other person can assign numbers to 8 letters <span class="math inline">\(A, B, \ldots, H\)</span>, and your job is to determine what all these numbers are. You are given one hint, and then you can ask questions only about what the <em>differences</em> are between the number(s) assigned to (combinations of) the letters. In the case of our analysis situation, the letters represent the groups, and the numbers the averages of those groups. This game is less fun to play than the earlier version, but its more interesting from a data analysis perspective.</p>
<p>Let’s get back to the Tetris study. There were four conditions: the no-task Control condition, Tetris+Reactivation, Tetris-Only, and Reactivation-Only. Remember, your job is not to determine which condition someone was in. This can be answered with a total of two yes-no questions (“Did the condition involve playing Tetris?” and “Did the condition involve memory reactivation?”). Your job is now to determine the average number of intrusions in each condition by asking questions about differences between the averages. To start, I will give you a hint, which is that the average over all conditions (i.e. the grand mean) is <span class="math inline">\(\hat{\mu} = 3.93\)</span>.</p>
<p>One set of questions you could ask is the following:</p>
<ol style="list-style-type: decimal">
<li>What is the difference between the mean of the Control condition and the grand mean?</li>
<li>What is the difference between the mean of the Tetris+Reactivation condition and the grand mean?</li>
<li>What is the difference between the mean of the Tetris-Only condition and the grand mean?</li>
</ol>
<p>The answers to these questions are</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\hat{\mu}_\text{contr} - \hat{\mu} = 5.11 - 3.93 = 1.18\)</span>.</li>
<li><span class="math inline">\(\hat{\mu}_\text{t+r} - \hat{\mu} = 1.89 - 3.93 = -2.04\)</span>.</li>
<li><span class="math inline">\(\hat{\mu}_\text{tetr} - \hat{\mu} = 3.89 - 3.93 = -0.0417\)</span>.</li>
</ol>
<p>Because I have told you the value of <span class="math inline">\(\hat{\mu}\)</span> already, you can simply add the value of this “hint” to the answer of each question to determine the mean of a condition. You do not need to ask a fourth question (“What is the difference between the mean of the reactivation-Only condition and the grand mean?”), because with the information provided, you would be able to determine this. Firstly, I should point out that <span class="math display">\[\begin{aligned}
\hat{\mu} &amp;= \frac{\hat{\mu}_\text{contr} + \hat{\mu}_\text{t+r} + \hat{\mu}_\text{tetr} + \hat{\mu}_\text{react}}{4} \\
&amp;= \frac{\overline{Y}_\text{contr} + \overline{Y}_\text{t+r} + \overline{Y}_\text{tetr} + \overline{Y}_\text{react}}{4}
\end{aligned}\]</span>
i.e., the estimate of the grand mean is a “mean of means.” From this, it directly follows that
<span class="math display">\[\hat{\mu}_\text{react} = 4 \times \mu - \hat{\mu}_\text{contr} - \hat{\mu}_\text{t+r} - \hat{\mu}_\text{tetr}\]</span>
and the treatment effect can also be determined as
<span class="math display">\[\hat{\mu}_\text{react} - \hat{\mu} = -1 \times\left( (\hat{\mu}_\text{contr} - \hat{\mu}) + (\hat{\mu}_\text{t+r} - \hat{\mu}) + (\hat{\mu}_\text{tetr} - \hat{\mu}) \right)\]</span>
i.e. as minus one times the sum of the treatment effect of the other conditions. It is precisely for dependencies like these that when constructing contrast codes for a nominal variable representing group membership, you need <span class="math inline">\(g-1\)</span> (the number of groups minus 1) contrast codes.</p>
<div id="effect-coding" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Effect coding</h3>
<p>The questions about deviations between group means and the grand mean (the <em>treatment effects</em>) correspond to the following three contrast codes <span class="math inline">\(c_j\)</span>:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(c_1\)</span></th>
<th align="right"><span class="math inline">\(c_2\)</span></th>
<th align="right"><span class="math inline">\(c_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Control</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">Tetris+Reactivation</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">Tetris-Only</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">Reactivation-Only</td>
<td align="right">-1</td>
<td align="right">-1</td>
<td align="right">-1</td>
</tr>
</tbody>
</table>
Just like for the dummy coding example discussed above, the idea is to construct a new predictor for each of these contrast codes. The contrast codes have values for each condition. The corresponding contrast-coding predictor has a value for each case <span class="math inline">\(i\)</span> in the data, where we give all cases in a condition the corresponding value of <span class="math inline">\(c_j\)</span> for that condition. For example, the first predictor <span class="math inline">\(X_1\)</span>, which corresponds to the first contrast code <span class="math inline">\(c_1\)</span>, would have the value <span class="math inline">\(X_{1,i} = 1\)</span> if case <span class="math inline">\(i\)</span> is in the Control condition, the value <span class="math inline">\(X_{1,i} = 0\)</span> if case <span class="math inline">\(i\)</span> is in the Tetris+Reactivation or Tetris-only condition, and the value <span class="math inline">\(X_{1,i} = -1\)</span> if case <span class="math inline">\(i\)</span> is in the Reactivation-Only condition. Similarly, the second predictor <span class="math inline">\(X_2\)</span>, which corresponds to the second contrast code <span class="math inline">\(c_2\)</span>, would have the value <span class="math inline">\(X_{2,i} = 0\)</span> if case <span class="math inline">\(i\)</span> is in the Control condition, the value <span class="math inline">\(X_{2,i} = 1\)</span> if case <span class="math inline">\(i\)</span> is in the Tetris+Reactivation condition, the value <span class="math inline">\(X_{2,i} = 0\)</span> if case <span class="math inline">\(i\)</span> is in the Tetris-only condition, and the value <span class="math inline">\(X_{2,i} = -1\)</span> if case <span class="math inline">\(i\)</span> is in the Reactivation-Only condition. Having defined three contrast-coding predictors, <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span> in this manner, we can then estimate the linear (regression) model
<span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \beta_3 \times X_{3,i} + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)\]</span>
This model is treated as any other regression model. So after re-coding a nominal variable “condition” with four levels (Control, Tetris+Reactivation, Tetris-Only, and Reactivation-Only) with three contrast codes <span class="math inline">\(c_1\)</span>, <span class="math inline">\(c_2\)</span>, and <span class="math inline">\(c_3\)</span>, each with a corresponding predictor <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>, we end up with a linear model that is effectively like any other multiple regression model. Estimating the model gives:
<span class="math display">\[\texttt{intrusions}_i = 3.93 + 1.18 \times \texttt{}X_1\texttt{}_i  - 2.04 \times \texttt{}X_2\texttt{}_i  - 0.0417 \times \texttt{}X_3\texttt{}_i  + \hat{\epsilon}_i \quad \quad \hat{\epsilon}_i \sim \mathbf{Normal}(0, 3.18)\]</span>
Figure <a href="ch-ANOVA.html#fig:tetris-effect-coding-parameter-plot">7.4</a> shows how the parameters of the model are related to the average number of intrusions in each condition.
<div class="figure" style="text-align: center"><span id="fig:tetris-effect-coding-parameter-plot"></span>
<img src="_main_files/figure-html/tetris-effect-coding-parameter-plot-1.svg" alt="Average number of instrusions in the four conditions in the Tetris study and how they are related to the parameters of the effect-coding model. The intercept $\hat{\beta}_0 = \hat{\mu}$ is the grand mean (dotted line). The slopes reflect treatment effects, which are deviations from the average intrusions in a condition and the grand mean. The averages of the first three conditions are equal to $\beta_0 + \beta_j$, the sum of the intercept and the slope of the effect-coding predictor $X_j$ representing the treatment effect of that condition. The mean of the last condition is the intercept minus the sum of the slopes of the effect-coding predictors." width="95%" />
<p class="caption">
Figure 7.4: Average number of instrusions in the four conditions in the Tetris study and how they are related to the parameters of the effect-coding model. The intercept <span class="math inline">\(\hat{\beta}_0 = \hat{\mu}\)</span> is the grand mean (dotted line). The slopes reflect treatment effects, which are deviations from the average intrusions in a condition and the grand mean. The averages of the first three conditions are equal to <span class="math inline">\(\beta_0 + \beta_j\)</span>, the sum of the intercept and the slope of the effect-coding predictor <span class="math inline">\(X_j\)</span> representing the treatment effect of that condition. The mean of the last condition is the intercept minus the sum of the slopes of the effect-coding predictors.
</p>
</div>
<p>Test results for the parameters of the model are given in Table <a href="ch-ANOVA.html#tab:tetris-effect-coding-ANOVA">7.2</a>. As can be seen there, the test of the intercept is significant. In this model, the intercept represents the “grand mean” <span class="math inline">\(\mu\)</span> (the average of the means in each condition). The test indicates that the true value of the grand mean is unlikely to be 0. In addition, the slope of <span class="math inline">\(X_2\)</span> is significant. This slope is equal to the <em>treatment effect</em> of the Tetris+Reactivation condition (i.e. <span class="math inline">\(\hat{\beta}_2 = \hat{\mu}_{t+r} - \hat{\mu}\)</span>). The test is a test of the null-hypothesis <span class="math inline">\(H_0: \beta_2 = 0\)</span>, and this test involves a comparison of the models
<span class="math display">\[\begin{aligned}
\text{MODEL G}: &amp;&amp; Y_i &amp;= \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \beta_3 \times X_{3,i} + \epsilon_i \\
\text{MODEL R}: &amp;&amp; Y_i &amp;= \beta_0 + \beta_1 \times X_{1,i} + \beta_3 \times X_{3,i} + \epsilon_i
\end{aligned}\]</span>
The comparison indicates that fixing the slope <span class="math inline">\(\beta_2 = 0\)</span> in MODEL R results is a substantial increase in the Sum of Squared Error of MODEL R compared to MODEL G. As such, there is evidence that the true value of this slope does not equal 0, and with that, that the true treatment effect of the Tetris+Reactivation condition does not equal 0, i.e. that <span class="math inline">\(\mu_{t+r} - \mu \neq 0\)</span>. Furthermore, the estimate of the treatment effect is <span class="math inline">\(\hat{\beta}_2 = \hat{\mu}_{t+r} - \hat{\mu} = \overline{Y}_{t+r} - \frac{\overline{Y}_\text{contr} + \overline{Y}_\text{t+r} + \overline{Y}_\text{tetr} + \overline{Y}_\text{react}}{4} = -2.042\)</span>, which indicates that the number of intrusions in this condition is <em>lower</em> than the grand mean. As such, we would conclude that playing Tetris after memory reactivation <em>reduces</em> the subsequent memory intrusions. The tests of the slopes of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> are not significant. We therefore do not have sufficient evidence that the treatment effect of the Control condition, or of the Tetris-only condition, are different to 0.</p>
<table>
<caption><span id="tab:tetris-effect-coding-ANOVA">Table 7.2: </span>Linear model predicting number of intrusions by three effect-coded predictors.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SS}\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(F\)</span></th>
<th align="right"><span class="math inline">\(P(\geq \lvert F \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">3.931</td>
<td align="right">1112.347</td>
<td align="right">1</td>
<td align="right">110.289</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">Condition</td>
<td align="right"></td>
<td align="right">114.819</td>
<td align="right">3</td>
<td align="right">3.795</td>
<td align="right">0.014</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\quad X_1\)</span></td>
<td align="right">1.181</td>
<td align="right">33.449</td>
<td align="right">1</td>
<td align="right">3.316</td>
<td align="right">0.073</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\quad X_2\)</span></td>
<td align="right">-2.042</td>
<td align="right">100.042</td>
<td align="right">1</td>
<td align="right">9.919</td>
<td align="right">0.002</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\quad X_3\)</span></td>
<td align="right">-0.042</td>
<td align="right">0.042</td>
<td align="right">1</td>
<td align="right">0.004</td>
<td align="right">0.949</td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="right"></td>
<td align="right">685.833</td>
<td align="right">68</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>Table <a href="ch-ANOVA.html#tab:tetris-effect-coding-ANOVA">7.2</a> also includes a row labelled “Condition.” This is a test of the hypothesis that <em>all</em> of the slopes of the contrast-coding predictors for Condition are equal to 0:
<span class="math display">\[H_0\!: \beta_1 = \beta_2 = \beta_3 = 0\]</span>
This hypothesis test is based on comparing the following two models:
<span class="math display">\[\begin{aligned}
\text{MODEL G}: &amp;&amp; Y_i &amp;= \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \beta_3 \times X_{3,i} + \epsilon_i \\
\text{MODEL R}: &amp;&amp; Y_i &amp;= \beta_0 + \epsilon_i
\end{aligned}\]</span>
i.e., it is a “whole model test.” The result of this so-called <strong>omnibus test</strong> (a simultaneous test of multiple parameters) is significant, indicating that it is unlikely that the true values of the slopes are all equal to 0. In MODEL R above, there is only a single parameter <span class="math inline">\(\beta_0\)</span>. As such, this model predicts that all conditions have the same mean <span class="math inline">\(\mu\)</span>. This test is thus a test of the null-hypothesis that <em>all</em> treatment effects for Condition are equal to 0:
<span class="math display">\[H_0\!: (\mu_\text{contr} - \mu) = (\mu_\text{t+r} - \mu) = (\mu_\text{tetr} - \mu) = (\mu_\text{react} - \mu) = 0\]</span>
which is also equivalent to the test that the means of all conditions are equal to eachother:
<span class="math display">\[H_0\!: \mu_\text{contr} = \mu_\text{t+r} = \mu_\text{tetr} = \mu_\text{react} \]</span></p>
<p>The result of the omnibus test indicates that there is at least one treatment effect which is different from 0. Omnibus tests for all slopes reflecting treatment effects are what is traditionally focused on in an ANOVA. But these omnibus tests are not always that informative. We’d generally like to know more than “there is at least one treatment that is likely to have an effect.” It seems inherently more interesting to know <em>which</em> conditions are associated with a treatment effect. This is where the tests of the individual slopes come in handy. Assessing the effect of the three effect-coding predictors, we can conclude that only the treatment effect of the Tetris+Reactivation condition is significant. As such, we only have sufficient evidence that a combination of memory reactivation and playing Tetris changes the number of subsequent memory intrusions.</p>
<p>At this point, I’d like to make some important remarks. Firstly, the absence of sufficient evidence that any of the other treatment effects differs from 0 should <em>not</em> be taken as direct evidence that the true treatment effects equal 0. A non-significant result indicates a lack of evidence <em>against</em> the null-hypothesis, but not an abundance of evidence <em>for</em> the null hypothesis. You can think of this as follows: that a suspect in a murder trial is not able to provide evidence that she is innocent is in itself not sufficient evidence that she is guilty. You might also think of black swans. While the empirical statement “All swans are white” is impossible to prove conclusively without checking the colour of all swans that have and will ever grace this world, finding a single black swan disproves that statement immediately <span class="citation">(<a href="references.html#ref-popper1959logic" role="doc-biblioref">Popper, 1959</a>)</span>. Although significance testing does not provide <em>conclusive evidence</em> for or against the null-hypothesis<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>, the analogy can be described as follows: a significant test is like spotting a swan that is “off-white” enough for you to decide it is not exactly white. But not having spotted such a swan could either mean that such a swan does not exist (the null hypothesis is true), or that you have not searched hard enough (the null hypothesis is false, but your test lacked power).</p>
<p>Secondly, the effect-coding predictors only reflect three out of a total of four treatment effects. The treatment effect of the reactivation-Only condition follows directly as minus the sum of these three treatment effects. As such, it is redundant. But we don’t have a hypothesis test for this redundant treatment effect. If we had chosen a slightly different coding scheme, where we would have estimated the treatment effects of the Control, Tetris-Only, and Reactivation-Only condition, such that the treatment effect of the Tetris+Reactivation condition is redundant, then none of the significance tests of the slopes of this different model would have been significant. The omnibus test would give exactly the same results, thus indicating that at least one treatment effect does not equal 0, but we couldn’t have easily spotted which treatment effect(s) these were. Whilst the estimates and slopes of contrast-coding predictors are often more specific and informative than an omnibus test, because we can only use <span class="math inline">\(g-1\)</span> of such predictors (one less than the number of groups), we can’t test for all treatment effects in a single model, nor test all hypotheses that we might be interested in. We will come back to this when we discuss “multiple testing” approaches. For now, a main thing to realise is that a significant omnibus test indicates that at least two groups differ in their means. If none of the tests of the slopes of the contrast-coding predictors in a linear model are significant, but you have obtained a significant omnibus test, that indicates that none of these predictors encoded that specific difference. Whilst some authors bemoan the use of omnibus tests, they have a role to play in the inference process, for instance in spotting whether you have missed a potentially important effect. Other authors put too much emphasis on omnibus tests, for instance requiring a significant omnibus tests before you might consider tests of the individual slopes that comprise this omnibus test. It is perfectly possible for an omnibus test to be non-significant, whilst a slope for one (or more) contrast-coding predictors is significant. For instance, if you’d conduct an experiment with 10 conditions, and only one has an actual treatment effect, the omnibus test might be non-significant because it effectively assigns that one treatment effect to nine parameters (slopes for nine contrast-coding predictors). Let’s take an extreme example, where only <span class="math inline">\(X_1\)</span> (the first contrast-coding predictor) reduces the SSE with <span class="math inline">\(\text{SSR}(X_1) &gt; 0\)</span>, whilst the other predictors provide no reduction in the SSE whatsoever (i.e. <span class="math inline">\(\text{SSR}(X_j) = 0\)</span> for <span class="math inline">\(j=2, \ldots, 9\)</span>). Then the <span class="math inline">\(F\)</span> statistic of the omnibus test might be
<span class="math display">\[\begin{aligned}
F &amp;= \frac{\frac{\text{SSE}(R) - \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar}(G)}} \\
&amp;= \frac{\text{SSR}(X_1)/9}{\text{SSE}(G)/(n-10)}
\end{aligned}\]</span>
whilst the <span class="math inline">\(F\)</span> statistic for the slope of <span class="math inline">\(X_1\)</span> would be
<span class="math display">\[\begin{aligned}
F &amp;= \frac{\frac{\text{SSE}(R) - \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar}(G)}} \\
&amp;= \frac{\text{SSR}(X_1)/1}{\text{SSE}(G)/(n-10)}
\end{aligned}\]</span>
which, with a higher value and smaller <span class="math inline">\(\text{df}_1\)</span>, would be more likely to be significant.</p>
<p>Thirdly, when you look at the value of <span class="math inline">\(\text{SSR}(\text{Condition}) = 115\)</span> in Table <a href="ch-ANOVA.html#tab:tetris-effect-coding-ANOVA">7.2</a>, you can see that it is smaller than the sum of the SSR terms corresponding to the three predictors. This indicates that there is redundancy between the predictors <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span> (i.e., some form of multicollinearity). Although Venn diagrams such as Figure <a href="ch-multiple-regression.html#fig:sse-partition-multicollinearity">5.9</a> imply that the SSR of the full model would be larger than the sum of the unique SSR terms attributable to each predictor, the opposite can also be true! This situation is commonly referred to as <strong>suppression</strong> and is rather difficult to explain here without a lengthy and detailed detour. For now, we will therefore leave it to the interested reader to consult other sources on this, and simply not that if the sum of the SSR terms of the predictors does not equal the “whole model” SSR, this indicates redundancy between the predictors.</p>
</div>
<div id="orthogonal-contrast-codes" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Orthogonal contrast codes</h3>
<p>Whilst redundancy is not necessarily a problem, it is in some sense preferable if all the predictors in a linear model are independent, as we can then neatly distribute the total SSR to each predictor. When the number of cases in each group (<span class="math inline">\(n_j\)</span>) is equal for all groups, there are contrast coding schemes that ensure that the resulting contrast-coding predictors are non-redundant (i.e., independent). Such coding schemes are called <strong>orthogonal contrast codes</strong>. There are some benefits to employing orthogonal contrast codes, although these benefits are sometimes overstated. Firstly, using orthogonal contrast codes ensures that the model predictions will exactly equal to (sample) averages in the conditions – as long as no additional predictors are included in the model; we will discuss such additional predictors in the context of Analysis of Covariance (ANCOVA). Whilst redundant coding schemes such as dummy coding or effect coding (amongst others) also ensure this, when you start defining your own contrast coding schemes, it might be difficult to check whether this is the case, and then resorting to orthogonal contrast codes may provide useful guidance. Secondly, as already mentioned, using independent predictors will ensure that the whole model SSR is completely distributed over the SSR terms for the individual predictors. This makes it somewhat less likely that you will miss a difference between the groups in the tests of individual parameters that would be identified in the whole model test. Thirdly, using orthogonal contrast codes provides a general formula to state the estimate of the slopes of contrast-coding predictors in terms of the averages of the groups. If you use a set of orthogonal contrast codes, then the estimated slope of each predictor <span class="math inline">\(X_j\)</span> corresponding to contrast code <span class="math inline">\(c_j\)</span> can be expressed as the following function of the values of <span class="math inline">\(c_{j,k}\)</span> contrast code <span class="math inline">\(c_j\)</span> for group <span class="math inline">\(k\)</span> and the sample means <span class="math inline">\(\overline{Y}_k\)</span> of the dependent variable in group <span class="math inline">\(k\)</span> as:
<span class="math display" id="eq:estimate-slope-orthogonal-contrast-codes">\[\begin{equation}
\hat{\beta}_j = \frac{\sum_{k=1}^{g} c_{j,k} \overline{Y}_k}{\sum_{k=1} c_{j,k}^2}
\tag{7.3}
\end{equation}\]</span>
i.e. as the sum of the sample means <span class="math inline">\(\overline{Y}_k\)</span> multiplied by the group-wise values of contrast code <span class="math inline">\(c_j\)</span>, divided by the sum of <span class="math inline">\(c_{j,k}^2\)</span>, the squared group-wise values of contrast <span class="math inline">\(c_j\)</span>. Note that this formula does <em>not</em> hold for the effect-coding scheme discussed previously. For instance, if you’d fill in the values of <span class="math inline">\(c_1\)</span>, you’d get
<span class="math display">\[\begin{aligned}
\hat{\beta}_1 &amp;= \frac{1 \times \overline{Y}_\text{contr} + 0 \times \overline{Y}_\text{t+r} + 0 \times \overline{Y}_\text{tetr} - 1 \times \overline{Y}_\text{react}}{(1)^2 + (0)^2 + (0)^2 + (-1)^2} \\
&amp;= \frac{\overline{Y}_\text{contr} - \overline{Y}_\text{react}}{2}
\end{aligned}\]</span>
which is obviously <em>not</em> the same as the treatment effect the estimated slope actually reflects:
<span class="math display">\[\hat{\beta}_1 = \overline{Y}_\text{contr} - \frac{\overline{Y}_\text{contr} + \overline{Y}_\text{t+r} + \overline{Y}_\text{tetr} + \overline{Y}_\text{react}}{4}\]</span>
Nevertheless, the slopes of the effect-coding scheme gave us interpretable parameters, hence using non-orthogonal contrast codes does not prohibit interpretable parameter estimates. With that said, if you can define comparisons you would like to make in terms of a set of orthogonal contrast codes, that would be preferable. If you can’t, there are ways to deal with that too, and you shouldn’t worry too much.</p>
<p>There are two rules you can follow to create orthogonal contrast codes:</p>
<ol style="list-style-type: decimal">
<li>For all contrast codes <span class="math inline">\(c_j\)</span>, <span class="math inline">\(j = 1, \ldots, g-1\)</span>, <span class="math inline">\(\sum_{k=1}^g c_{j,k} = 0\)</span>. The sum of the values of each contrast code <span class="math inline">\(j\)</span> is zero.</li>
<li>For all pairs of contrast codes <span class="math inline">\(c_j\)</span> and <span class="math inline">\(c_l\)</span>, <span class="math inline">\(j \neq l\)</span>, <span class="math inline">\(\sum_{k=1}^g c_{j,k} \times c_{l,k} = 0\)</span>. The sum of the cross-products of each pair of contrast codes <span class="math inline">\(j\)</span> and <span class="math inline">\(l\)</span> is equal to zero.</li>
</ol>
<p>In the statements above, <span class="math inline">\(c_{j,k}\)</span> refers to the value of a contrast code <span class="math inline">\(c_j\)</span> for group <span class="math inline">\(k\)</span>, and similarly <span class="math inline">\(c_{l,k}\)</span> refers to the value of a contrast code <span class="math inline">\(c_l\)</span> for group <span class="math inline">\(k\)</span>, whilst <span class="math inline">\(g\)</span> refers to the total number of groups. Only the second rule is strictly necessary, but the first one is a useful one to follow as well, as –like in the effect coding scheme– it ensures that the intercept represents the grand mean. If we check the two rules for the effect coding scheme, we can see that the first requirement is adhered to:</p>
<p><span class="math display">\[\begin{aligned}
\sum_{k=1}^g c_{1,k} &amp;= 1 + 0 + 0 + (-1) = 0 \\
\sum_{k=1}^g c_{2,k} &amp;= 0 + 1 + 0 + (-1) = 0 \\
\sum_{k=1}^g c_{3,k} &amp;= 0 + 0 + 1 + (-1) = 0
\end{aligned}\]</span>
However, the second is <em>not</em>:
<span class="math display">\[\begin{aligned}
\sum_{k=1}^g c_{1,k} \times c_{2,k} &amp;= 1 \times 0 + 0 \times 1 + 0 \times 0 + (-1) \times (-1) = 1 \\
\sum_{k=1}^g c_{1,k} \times c_{3,k} &amp;= 1 \times 0 + 0 \times 0 + 0 \times 1 + (-1) \times (-1) = 1 \\
\sum_{k=1}^g c_{2,k} \times c_{3,k} &amp;= 0 \times 0 + 0 \times 1 + 0 \times 1 + (-1) \times (-1) = 1 
\end{aligned}\]</span></p>
<p>One set of questions you could ask, which will result in orthogonal contrast codes, is the following:</p>
<ol style="list-style-type: decimal">
<li>What is the difference between doing nothing (the Control condition) and doing something (the three remaining conditions)?</li>
<li>What is the difference between playing Tetris (the Tetris-Only condition) and a procedure involving memory reactivation (the Tetris+Reactivation and reactivation-Only condition)?</li>
<li>What is the difference between memory reactivation without playing Tetris (the Reactivation-only condition) and the playing Tetris after reactivation (the Tetris+Reactivation condition)?</li>
</ol>
<!-- With answers to these three questions, and knowing the overall mean, you would be able to calculate the average in each condition with complete accuracy. How that works in practice we'll leave for the moment. Instead, let's focus on these three questions. -->
<p>To define out contrast codes, we need to define values for the contrast codes such that the slopes of the predictors based on them provide the answers to these questions. The first question refers to the difference between the Control condition and the three other conditions. As the answer to the question should be a single deviation, it makes sense to compare the mean of the Control condition to the average of the three remaining conditions combined. In this combination, we would not distinguish between those three conditions, and hence we should give each of those three conditions the same value. In addition, remember that a slope reflects an in- or decrease in the dependent variable for a one-unit increase in the predictor. It then makes sense to let the difference between the value we assign to the Control condition and the values we assign to the three other conditions equal 1. Finally, we would like to use values such that their sum over the groups equals 0. Combining these three ideas, to answer the first question, we could define a contrast code <span class="math inline">\(c_1\)</span> with value <span class="math inline">\(\tfrac{3}{4}\)</span> for the control condition, and the value <span class="math inline">\(-\tfrac{1}{4}\)</span> for the Tetris+Reactivation, Tetris-Only, and Reactivation-Only condition. In particular, note that these values sum to 0, and that the distance between <span class="math inline">\(\tfrac{3}{4}\)</span> and <span class="math inline">\(-\tfrac{1}{4}\)</span> equals 1. As such, a one-unit increase for this contrast code is equal to going from one of the three experimental conditions to the Control condition.</p>
<p>For the second question, we would like to compare the Tetris-Only condition to the Tetris+Reactivation and Reactivation-Only condition combined. In this comparison, we ignore the Control condition. By giving this condition a value of 0 in the contrast code, it will not be entered in the comparison. Additionally using similar reasoning as before, the suggested second contrast code <span class="math inline">\(c_2\)</span> has values <span class="math inline">\(0, -\tfrac{1}{3}, \tfrac{2}{3}, -\tfrac{1}{3}\)</span> for the Control, Tetris+Reactivation, Tetris-Only, and Reactivation-Only conditions respectively. Again, the difference between <span class="math inline">\(\tfrac{2}{3}\)</span> and <span class="math inline">\(-\tfrac{1}{3}\)</span> equals 1, so that a one-unit increase in this contrast code is equal to going from one of the reactivation conditions to the Tetris-Only condition.</p>
<p>For the third question, finally, we would like to compare the Reactivation-Only condition to the Tetris+Reactivation condition, ignoring the other two conditions. The suggested contrast code <span class="math inline">\(c_3\)</span> then has values <span class="math inline">\(0, -\tfrac{1}{2}, 0, \tfrac{1}{2}\)</span> for the four conditions respectively. Here, a one-unit increase is equal to going from the Tetris+Reactivation condition to the Reactivation-Only condition.</p>
<p>The values of the three contrast codes <span class="math inline">\(c_1\)</span>, <span class="math inline">\(c_2\)</span>, and <span class="math inline">\(c_3\)</span> (for questions 1 to 3 respectively) for each of the four conditions, are summarized given in the table below:</p>
<table>
<caption><span id="tab:tetris-orthogonal-cotrast-codes">Table 7.3: </span>A set of orthogonal contrast codes.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(c_1\)</span></th>
<th align="right"><span class="math inline">\(c_2\)</span></th>
<th align="right"><span class="math inline">\(c_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Control</td>
<td align="right"><span class="math inline">\(\tfrac{3}{4}\)</span></td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">Tetris+Reactivation</td>
<td align="right"><span class="math inline">\(-\tfrac{1}{4}\)</span></td>
<td align="right"><span class="math inline">\(-\tfrac{1}{3}\)</span></td>
<td align="right"><span class="math inline">\(-\tfrac{1}{2}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Tetris-Only</td>
<td align="right"><span class="math inline">\(-\tfrac{1}{4}\)</span></td>
<td align="right"><span class="math inline">\(\tfrac{2}{3}\)</span></td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">Reactivation-Only</td>
<td align="right"><span class="math inline">\(-\tfrac{1}{4}\)</span></td>
<td align="right"><span class="math inline">\(-\tfrac{1}{3}\)</span></td>
<td align="right"><span class="math inline">\(\tfrac{1}{2}\)</span></td>
</tr>
</tbody>
</table>
<p>Let’s check whether these three contrast codes are orthogonal. The first requirements (values sum to 0) holds:
<span class="math display">\[\begin{aligned}
\sum_{k=1}^g c_{1,k} &amp;= \tfrac{3}{4} - \tfrac{1}{4} - \tfrac{1}{4} - \tfrac{1}{4} = 0 \\
\sum_{k=1}^g c_{2,k} &amp;= 0 - \tfrac{1}{3} + \tfrac{2}{3} - \tfrac{1}{3} = 0 \\
\sum_{k=1}^g c_{3,k} &amp;= 0 - \tfrac{1}{2} + 0 + \tfrac{1}{2} = 0
\end{aligned}\]</span>
And indeed the second requirement (pairwise products sum to 0) also holds:
<span class="math display">\[\begin{aligned}
\sum_{k=1}^g c_{1,k} \times c_{2,k} &amp;= \tfrac{3}{4} \times 0 + (- \tfrac{1}{4}) \times (- \tfrac{1}{3}) + (-\tfrac{1}{4}) \times \tfrac{2}{3} + (- \tfrac{1}{4}) \times (- \tfrac{1}{3}) \\ &amp;= \tfrac{1}{12} - \tfrac{2}{12} + \tfrac{1}{12} = 0 \\
\sum_{k=1}^g c_{1,k} \times c_{3,k} &amp;= \tfrac{3}{4} \times 0 + (- \tfrac{1}{4}) \times (- \tfrac{1}{2}) + (-\tfrac{1}{4}) \times 0 + (- \tfrac{1}{4}) \times \tfrac{1}{2} \\ &amp;= \tfrac{1}{8} - \tfrac{1}{8} = 0 \\
\sum_{k=1}^g c_{2,k} \times c_{3,k} &amp;= 0 \times 0 + (-\tfrac{1}{3}) \times (- \tfrac{1}{2}) + \tfrac{2}{3} \times 0 + (-\tfrac{1}{3}) \times \tfrac{1}{2} \\ &amp;=  \tfrac{1}{6} - \tfrac{1}{6} = 0
\end{aligned}\]</span></p>
<p>Because the contrast codes <span class="math inline">\(c_1\)</span>, <span class="math inline">\(c_2\)</span>, and <span class="math inline">\(c_3\)</span> are orthogonal, we can use Equation <a href="ch-ANOVA.html#eq:estimate-slope-orthogonal-contrast-codes">(7.3)</a> to determine the estimates of the slopes. The estimated slope of the contrast-coding predictor <span class="math inline">\(X_1\)</span> which takes its values from <span class="math inline">\(c_1\)</span> will be:
<span class="math display">\[
\begin{aligned}
\hat{\beta}_1 &amp;= \frac{ \tfrac{3}{4} \times \overline{Y}_\text{contr} - \tfrac{1}{4} \times \overline{Y}_\text{t+r} - \tfrac{1}{4} \times \overline{Y}_\text{tetr} - \frac{1}{4} \times  \overline{Y}_\text{react} }{ (\frac{3}{4})^2 + (-\frac{1}{4})^2 + (-\frac{1}{4})^2 + (-\frac{1}{4})^2 }  \\
&amp;= \frac{ \tfrac{3}{4} \times \overline{Y}_\text{contr} - \tfrac{1}{4} \times \overline{Y}_\text{t+r} - \tfrac{1}{4} \times \overline{Y}_\text{tetr} - \frac{1}{4} \times  \overline{Y}_\text{react} }{ \tfrac{3^2}{4^2} + \tfrac{1^2}{4^2} + \tfrac{1^2}{4^2} + \tfrac{1^2}{4^2} }
\end{aligned}
\]</span>
The denominator evaluates to <span class="math inline">\(\tfrac{12}{16} = \tfrac{3}{4}\)</span>, and dividing both the numerator and denominator by <span class="math inline">\(\tfrac{3}{4}\)</span> gives<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a>
<span class="math display">\[\hat{\beta}_1 = \overline{Y}_\text{contr} - \frac{\overline{Y}_\text{t+r} + \overline{Y}_\text{tetr} + \overline{Y}_\text{react}}{3}\]</span>
i.e. the slope of the first predictor is equal to the difference between the average intrusions in the Control condition, and the average of the average number of intrusions in the three other conditions. When we fill in the actual sample averages, we get:
<span class="math display">\[
\begin{aligned}
\hat{\beta}_1 &amp;= \frac{ \tfrac{3}{4} \times 5.11 - \tfrac{1}{4} \times 1.89 - \tfrac{1}{4} \times 3.89 - \frac{1}{4} \times  4.83 }{ \tfrac{3}{4}}  \\
&amp;= \frac{1.181}{0.75} = 1.574
\end{aligned}
\]</span></p>
<p>The estimated slope of the contrast-coding predictor <span class="math inline">\(X_2\)</span> which takes its values from <span class="math inline">\(c_2\)</span> will be:
<span class="math display">\[
\begin{aligned}
\hat{\beta}_2 &amp;= \frac{ 0 \times \overline{Y}_\text{contr} - \tfrac{1}{3} \times \overline{Y}_\text{t+r} + \tfrac{2}{3} \times \overline{Y}_\text{tetr} - \frac{1}{3} \times  \overline{Y}_\text{react} }{ (0)^2 + (-\frac{1}{3})^2 + (\frac{2}{3})^2 + (-\frac{1}{3})^2 }  \\
&amp;= \frac{ \tfrac{2}{3} \times \overline{Y}_\text{tetr} - \tfrac{1}{3} \times \overline{Y}_\text{t+r}  - \frac{1}{3} \times  \overline{Y}_\text{react} }{ \tfrac{6}{9}} \\
&amp;= \overline{Y}_\text{tetr} - \frac{\overline{Y}_\text{t+r} + \overline{Y}_\text{react}}{2}
\end{aligned}
\]</span>
The slope of the second predictor is thus equal to the difference between the average number of intrusions in the Tetris-Only condition, and the average of the average number of intrusions in the Tetris+Reactivation condition and the Reactivation-Only condition. Filling in the actual averages then gives.
<span class="math display">\[
\begin{aligned}
\hat{\beta}_2 &amp;= \frac{ 0 \times 5.11 - \tfrac{1}{3} \times 1.89 + \tfrac{2}{3} \times 3.89 - \frac{1}{3} \times  4.83 }{ \tfrac{2}{3} }  \\
&amp;= \frac{0.352}{0.667} = 0.528
\end{aligned}
\]</span>
Finally, the estimated slope of the contrast-coding predictor <span class="math inline">\(X_3\)</span> which takes its values from <span class="math inline">\(c_3\)</span> will be:
<span class="math display">\[
\begin{aligned}
\hat{\beta}_3 &amp;= \frac{ 0 \times \overline{Y}_\text{contr}  - \tfrac{1}{2} \times \overline{Y}_\text{t+r} + 0 \times \overline{Y}_\text{tetr} + \frac{1}{2} \times  \overline{Y}_\text{react} }{ (0)^2 + (-\frac{1}{2})^2 + (0)^2 + (\frac{1}{2})^2 }  \\
&amp;= \frac{\frac{1}{2} \times  \overline{Y}_\text{react} - \tfrac{1}{2} \times \overline{Y}_\text{t+r} }{ \tfrac{2}{4} } \\
&amp;= \overline{Y}_\text{react} - \overline{Y}_\text{t+r}
\end{aligned}
\]</span>
i.e. the difference between the average intrusions in the Reactivation-Only condition and the average intrusions in Tetris+Reactivation condition. Filling in the actual averages gives:
<span class="math display">\[
\begin{aligned}
\hat{\beta}_3 &amp;= \frac{ 0 \times 5.11 - \tfrac{1}{2} \times 1.89 + 0 \times 3.89 + \frac{1}{2} \times  4.83 }{\tfrac{1}{2}}  \\
&amp;= \frac{1.472}{0.5} = 2.944
\end{aligned}
\]</span></p>
<p>While it is important to understand what each slope reflects in terms of differences between the group means, you would not normally use these equations to actually estimate the slopes. It is much easier to use statistical software for that. This gives us the following estimated model:</p>
<span class="math display">\[\texttt{intrusions}_i = 3.93 + 1.57 \times \texttt{}X_1\texttt{}_i  + 0.528 \times \texttt{}X_2\texttt{}_i  + 2.94 \times \texttt{}X_3\texttt{}_i  + \hat{\epsilon}_i \quad \quad \hat{\epsilon}_i \sim \mathbf{Normal}(0, 3.18)\]</span>
As you can see, the parameter estimates are identical to those worked out from the sample means. Figure <a href="ch-ANOVA.html#fig:tetris-contrast-coding-parameter-plot">7.5</a> shows how the parameters of the model are related to the average number of intrusions in each condition.
<div class="figure" style="text-align: center"><span id="fig:tetris-contrast-coding-parameter-plot"></span>
<img src="_main_files/figure-html/tetris-contrast-coding-parameter-plot-1.svg" alt="Average number of instrusions in the four conditions in the Tetris study and how they are related to the parameters of the orthogonal contrast-coding model. The intercept $\hat{\beta}_0 = \hat{\mu}$ is the grand mean (dotted line). The slopes reflect deviations from the average intrusions in a condition and combinations of other conditions." width="95%" />
<p class="caption">
Figure 7.5: Average number of instrusions in the four conditions in the Tetris study and how they are related to the parameters of the orthogonal contrast-coding model. The intercept <span class="math inline">\(\hat{\beta}_0 = \hat{\mu}\)</span> is the grand mean (dotted line). The slopes reflect deviations from the average intrusions in a condition and combinations of other conditions.
</p>
</div>
<p>Hypothesis tests for all the parameters, as well as the omnibus test for Condition, are provided in Table <a href="ch-ANOVA.html#tab:tetris-contrast-coding-ANOVA">7.4</a>. Again, we find a significant overall effect for Condition, which indicates that at least one of the group means differs from another one. The tests for the first two contrasts are not significant. As such, we have no evidence that the Control condition differs from the other three conditions combined, or that the Tetris-Only condition differs from the Tetris+Reactivation or Reactivation-Only condition combined. The test of the third contrast is significant, however, indicating a difference between the Tetris+Reactivation and Reactivation-Only condition. The slope of this comparison is positive, indicating that, as expected, there are more memory intrusions in the Reactivation-Only condition compared to the Tetris+Reactivation condition.</p>
<table>
<caption><span id="tab:tetris-contrast-coding-ANOVA">Table 7.4: </span>Linear model predicting number of intrusions by three contrast-coded predictors.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SS}\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(F\)</span></th>
<th align="right"><span class="math inline">\(P(\geq \lvert F \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">3.931</td>
<td align="right">1112.35</td>
<td align="right">1</td>
<td align="right">110.289</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">Condition</td>
<td align="right"></td>
<td align="right">114.82</td>
<td align="right">3</td>
<td align="right">3.795</td>
<td align="right">0.014</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\quad X_1\)</span></td>
<td align="right">1.574</td>
<td align="right">33.45</td>
<td align="right">1</td>
<td align="right">3.316</td>
<td align="right">0.073</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\quad X_2\)</span></td>
<td align="right">0.528</td>
<td align="right">3.34</td>
<td align="right">1</td>
<td align="right">0.331</td>
<td align="right">0.567</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\quad X_3\)</span></td>
<td align="right">2.944</td>
<td align="right">78.03</td>
<td align="right">1</td>
<td align="right">7.736</td>
<td align="right">0.007</td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="right"></td>
<td align="right">685.83</td>
<td align="right">68</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>Comparing the results in Table <a href="ch-ANOVA.html#tab:tetris-contrast-coding-ANOVA">7.4</a> to those in Table <a href="ch-ANOVA.html#tab:tetris-effect-coding-ANOVA">7.2</a>, there are a few things to note. Firstly, the result of the omnibus test is <em>exactly the same</em>, whether you use effect coding or orthogonal contrast coding. Because both ways of coding in the end make the same prediction <span class="math inline">\(\hat{Y}_i\)</span> (namely that <span class="math inline">\(\hat{Y}_i\)</span> equals the group mean of the condition that case <span class="math inline">\(i\)</span> belongs to), the model as a whole has the same SSE for both coding schemes. And as the omnibus test involves the same MODEL R (an intercept-only model), the SSR term of the omnibus Condition effect is the same for effect-coding and orthogonal coding. Indeed, for any form of contrast coding which results in model predictions that equal the group means, the omnibus test gives exactly the same results! The model with orthogonal contrast coding, however, separates the whole model SSR term neatly into into different SSR terms for the contrast-coding predictors. If you add up the SSR terms for the three predictors (in the column labelled SS), you get exactly the SSR term given for Condition. This shows that when you use orthogonal contrast coding, the variance explained by the model as a whole is separated exactly into three independent parts reflecting the unique part of the variance explained due to each predictor.</p>
</div>
<div id="defining-your-own-orthogonal-contrasts" class="section level3" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Defining your own (orthogonal) contrasts</h3>
<p>Initially, coming up with orthogonal contrast codes (or for that matter, any coding scheme which allows the model predictions to equal all the group means exactly) will not be easy, especially for nominal variables with more than 3 levels (i.e. more than 3 groups). With practice, you should become better at this, though. Ideally, the contrast codes reflect at least some of the theoretically important research questions you want to ask (e.g., relevant comparisons of the effects of experimental manipulations). In an experimental design with <span class="math inline">\(g\)</span> groups, you need to define <span class="math inline">\(g-1\)</span> contrast codes which correspond to such questions. When designing your own contrast codes, you should start with the most important question you want to ask, For instance, if you want to compare the conditions with memory reactivation to those without memory reactivation, because you expect more memory intrusions after memory reactivation than without this reactivation, your initial contrast could be <span class="math inline">\(c_1 = (-\tfrac{1}{2}, \tfrac{1}{2}, -\tfrac{1}{2}, \tfrac{1}{2})\)</span>. Note that I’m giving the Control and Tetris-Only condition a negative value of <span class="math inline">\(-\tfrac{1}{2}\)</span> and the Tetris+Reactivation and Reactivation-Only condition a positive value of <span class="math inline">\(\tfrac{1}{2}\)</span> because the resulting slope would be positive if the expectation of more memory intrusions after reactivation holds true. For expectations which involve not only a difference, but also a direction of that difference, I find it handy to assign values which are in line with those expectations. Negative slopes in the model then indicate that my expectations did not hold true.</p>
<p>Having defined this initial contrast, we have two more to go. Perhaps I’m also interested in assessing the effect of Tetris within the conditions that involve memory reactivation, because I expect playing Tetris to reduce the number of memory intrusions. This is a comparison between the Reactivation-Only and Tetris+Reactivation conditions. Because we gave these conditions the same value in the first contrast <span class="math inline">\(c_1\)</span> (i.e. we did not differentiate between them in the first comparison), this new comparison will be independent of (orthogonal to) the first comparison. We can use the same contrast code for this as the one used earlier, so <span class="math inline">\(c_2 = (0,-\tfrac{1}{2}, 0, \tfrac{1}{2})\)</span>. It is easy to check that this contrast is indeed orthogonal to the first one
<span class="math display">\[\sum_{k=1}^g c_{1,k} \times c_{2,k} = -\tfrac{1}{2} \times 0 + \tfrac{1}{2} \times (-\tfrac{1}{2}) + (-\tfrac{1}{2}) \times 0 + \tfrac{1}{2} \times \tfrac{1}{2} = 0\]</span>
We now have one contrast left to specify. Perhaps we don’t really have any more important questions to ask, so this contrast might be arbitrary from a theoretical viewpoint. However, to have a model which is able to fit the group means exactly, we would need a contrast code that differentiates between the Control condition and the Tetris-Only condition, as these conditions received the same value on <span class="math inline">\(c_1\)</span> (both <span class="math inline">\(-\tfrac{1}{2}\)</span>) <em>and</em> <span class="math inline">\(c_2\)</span> (both 0). If our last contrast code would not differentiate between these two conditions, then there would be no way in which the model can make different predictions for those two conditions. The model then involves a quite strong assumption that means of the Control and Tetris-Only condition are exactly identical. I don’t see a reason why such equivalence between the conditions would necessarily hold. So the final contrast code will be <span class="math inline">\(c_3 = (\tfrac{1}{2}, 0, -\tfrac{1}{2}, 0)\)</span>. Note that if I had used different values for the Reactivation-Only and Tetris+Reactivation conditions, for instance a contrast code of <span class="math inline">\((\tfrac{1}{2}, -\tfrac{1}{2}, -\tfrac{1}{2}, \tfrac{1}{2})\)</span>, then the contrast code would correlate with <span class="math inline">\(c_2\)</span> (or with <span class="math inline">\(c_1\)</span> if I had used different values). So the final contrast code needs to give the same value (0) to the Reactivation-Only and Tetris+Reactivation conditions to give us a set of orthogonal contrast codes. So the final set of orthogonal contrast codes is</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(c_1\)</span></th>
<th align="right"><span class="math inline">\(c_2\)</span></th>
<th align="right"><span class="math inline">\(c_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Control</td>
<td align="right"><span class="math inline">\(-\tfrac{1}{2}\)</span></td>
<td align="right">0</td>
<td align="right"><span class="math inline">\(\tfrac{1}{2}\)</span></td>
</tr>
<tr class="even">
<td align="left">Tetris+Reactivation</td>
<td align="right"><span class="math inline">\(\tfrac{1}{2}\)</span></td>
<td align="right"><span class="math inline">\(-\tfrac{1}{2}\)</span></td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">Tetris-Only</td>
<td align="right"><span class="math inline">\(-\tfrac{1}{2}\)</span></td>
<td align="right">0</td>
<td align="right"><span class="math inline">\(-\tfrac{1}{2}\)</span></td>
</tr>
<tr class="even">
<td align="left">Reactivation-Only</td>
<td align="right"><span class="math inline">\(\tfrac{1}{2}\)</span></td>
<td align="right"><span class="math inline">\(\tfrac{1}{2}\)</span></td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>I will leave checking the orthogonality, and estimating and testing the parameters of the resulting model, as an exercise to the reader. Note that this strategy of splitting conditions in halves to compare, and then splitting these halves in other halves, can generally be applied. And the question strategy in Figure <a href="ch-ANOVA.html#fig:number-guessing-game-contrast-coding">7.3</a> is an example of this.</p>
</div>
</div>
<div id="default-orthogonal-coding-schemes" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Default orthogonal coding schemes</h2>
<p>While orthogonal contrasts provide benefits in terms of straightforward interpretation of the parameters, and, in the case of equally sized groups, independent predictors and and a neat division of the whole model <span class="math inline">\(\text{SSR}\)</span> to the <span class="math inline">\(\text{SSR}\)</span> terms for the predictors, orthogonality is not a strict requirement. For instance, effect coding and dummy coding also provide interpretable parameters, although the predictors are not independent. The most important thing is that the contrast codes can answer questions about the Data Generating Process which are of interest to the researcher.</p>
<p>In the absence of any comparisons of interest, you shouldn’t really be conducting hypothesis tests in the first place. That said, there might be times where you would like to resort to a “default” way of orthogonal contrast coding. Such default coding schemes can also provide inspiration for defining your own contrast codes. Two default orthogonal contrast codes are Helmert coding, and polynomial contrast coding.</p>
<p><strong>Helmert coding</strong> involves a set of contrast codes in which each code compares a group mean to the average of all group means that come before it. You can find these contrast, for situations with a group size of <span class="math inline">\(g=1, 2, \ldots, 6\)</span>, in Table <a href="ch-ANOVA.html#tab:Helmert-contrasts">7.5</a>. Using Helmert contrasts will always provide a set of orthogonal contrasts. And if you reorder the conditions in Table <a href="ch-ANOVA.html#tab:tetris-orthogonal-cotrast-codes">7.3</a> as Tetris+Reactivation, Reactivation-Only, tetris-Only, Control, you should recognize that this was actually an example of Helmert coding for <span class="math inline">\(g=4\)</span>.</p>
<table>
<caption><span id="tab:Helmert-contrasts">Table 7.5: </span>Helmert contrast codes for situations with two to six groups.</caption>
<thead>
<tr class="header">
<th align="right">group</th>
<th align="right"><span class="math inline">\(c_1\)</span></th>
<th align="right"><span class="math inline">\(c_2\)</span></th>
<th align="right"><span class="math inline">\(c_3\)</span></th>
<th align="right"><span class="math inline">\(c_4\)</span></th>
<th align="right"><span class="math inline">\(c_5\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-0.5</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">0.5</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">-0.5</td>
<td align="right">-0.333</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.5</td>
<td align="right">-0.333</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.0</td>
<td align="right">0.667</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">-0.5</td>
<td align="right">-0.333</td>
<td align="right">-0.25</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.5</td>
<td align="right">-0.333</td>
<td align="right">-0.25</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.0</td>
<td align="right">0.667</td>
<td align="right">-0.25</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.0</td>
<td align="right">0.000</td>
<td align="right">0.75</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-0.5</td>
<td align="right">-0.333</td>
<td align="right">-0.25</td>
<td align="right">-0.2</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">0.5</td>
<td align="right">-0.333</td>
<td align="right">-0.25</td>
<td align="right">-0.2</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.0</td>
<td align="right">0.667</td>
<td align="right">-0.25</td>
<td align="right">-0.2</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.0</td>
<td align="right">0.000</td>
<td align="right">0.75</td>
<td align="right">-0.2</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.0</td>
<td align="right">0.000</td>
<td align="right">0.00</td>
<td align="right">0.8</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-0.5</td>
<td align="right">-0.333</td>
<td align="right">-0.25</td>
<td align="right">-0.2</td>
<td align="right">-0.167</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">0.5</td>
<td align="right">-0.333</td>
<td align="right">-0.25</td>
<td align="right">-0.2</td>
<td align="right">-0.167</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.0</td>
<td align="right">0.667</td>
<td align="right">-0.25</td>
<td align="right">-0.2</td>
<td align="right">-0.167</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.0</td>
<td align="right">0.000</td>
<td align="right">0.75</td>
<td align="right">-0.2</td>
<td align="right">-0.167</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.0</td>
<td align="right">0.000</td>
<td align="right">0.00</td>
<td align="right">0.8</td>
<td align="right">-0.167</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.0</td>
<td align="right">0.000</td>
<td align="right">0.00</td>
<td align="right">0.0</td>
<td align="right">0.833</td>
</tr>
</tbody>
</table>
<p><strong>Polynomial contrast codes</strong> tend to be used when the groups can be ordered in a meaningful way. For instance, a study might involve different age groups, e.g. “18-30 year old,” “31-40 year old,” and “41-50 year old.” Using polynomial contrast coding, you can then determine a linear trend over these age groups with a <em>linear</em> contrast <span class="math inline">\(c_1 = (-1,0,1)\)</span>, as well as whether the average of the middle age group is lower (or higher) than would be expected from the linear contrast through a <em>quadratic</em> contrast <span class="math inline">\(c_2 = (1,-2,1)\)</span>. Examples of polynomial contrast codes for situations with between 2 and 6 groups are provided in Table <a href="ch-ANOVA.html#tab:polynomial-contrasts">7.6</a>.</p>
<table>
<caption><span id="tab:polynomial-contrasts">Table 7.6: </span>Polynomial contrast codes for situations with two to six groups.</caption>
<thead>
<tr class="header">
<th align="right">group</th>
<th align="right"><span class="math inline">\(c_1\)</span></th>
<th align="right"><span class="math inline">\(c_2\)</span></th>
<th align="right"><span class="math inline">\(c_3\)</span></th>
<th align="right"><span class="math inline">\(c_4\)</span></th>
<th align="right"><span class="math inline">\(c_5\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">-1</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0</td>
<td align="right">-2</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">-3</td>
<td align="right">1</td>
<td align="right">-1</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">-1</td>
<td align="right">-1</td>
<td align="right">3</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">-1</td>
<td align="right">-3</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">2</td>
<td align="right">-1</td>
<td align="right">1</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">-1</td>
<td align="right">-1</td>
<td align="right">2</td>
<td align="right">-4</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0</td>
<td align="right">-2</td>
<td align="right">0</td>
<td align="right">6</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">-1</td>
<td align="right">-2</td>
<td align="right">-4</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">-5</td>
<td align="right">5</td>
<td align="right">-5</td>
<td align="right">1</td>
<td align="right">-1</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">-3</td>
<td align="right">-1</td>
<td align="right">7</td>
<td align="right">-3</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">-1</td>
<td align="right">-4</td>
<td align="right">4</td>
<td align="right">2</td>
<td align="right">-10</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">-4</td>
<td align="right">-4</td>
<td align="right">2</td>
<td align="right">10</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">3</td>
<td align="right">-1</td>
<td align="right">-7</td>
<td align="right">-3</td>
<td align="right">-5</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
</div>
<div id="multiple-testing-and-post-hoc-tests" class="section level2" number="7.6">
<h2><span class="header-section-number">7.6</span> Multiple testing and post-hoc tests</h2>
<p>Using contrast codes, you can test a total of <span class="math inline">\(g-1\)</span> comparisons within a single analysis. At times, this may suffice to test all hypotheses of interest. At other times, you would like to test more hypotheses.</p>
<p>For example, you might want to test for differences in the means of all pairs of conditions. In a design with 4 conditions, there are a total of <span class="math inline">\((4-1)! = 6\)</span> pairwise comparisons between the conditions. Obtaining all these pairwise tests is actually pretty simple: you can just perform multiple analyses with different contrast codes. If you are interested in pairwise differences, then dummy coding is the most obvious choice. In any model with dummy coding, you would obtain all pairwise tests comparing each condition to a reference condition. If you perform three analyses, and in each change the reference condition, you would obtain all pairwise tests. Case closed?</p>
<p>Well, maybe not entirely. Although this is a straightforward way to obtain the tests, you will end up performing quite a lot of these. The number of tests increases when you have more conditions. For instance, with 5 conditions, you would need to perform <span class="math inline">\((5-1)! = 24\)</span> tests, and with 6 conditions you would perform <span class="math inline">\((6-1)! = 120\)</span> tests. For each test, we allow a Type 1 error rate of <span class="math inline">\(\alpha\)</span>, usually <span class="math inline">\(\alpha = .05\)</span>. Say that we perform 120 tests. When there are no differences between the conditions, then we would still expect <span class="math inline">\(.05 \times 120 = 6\)</span> significant test results. This <strong>inflation of Type 1 error</strong> can be dealt with, though. One (perhaps too) simple solution is to apply the so-called <strong>Bonferroni correction</strong> to the significance level of each test, such that the overall Type 1 error rate over all tests is kept at a desired level.</p>
<p>The overall Type 1 error rate in a set of tests is conventionally called the <strong>family-wise Type 1 error rate</strong>, or <span class="math inline">\(\alpha_\text{FW}\)</span>. This is equal to the probability of making at least one Type 1 error within a set of hypothesis tests. This probability is equal to one minus the probability of making no Type 1 error at all. If all tests are independent, then the probability of making no Type 1 error in a total of <span class="math inline">\(q\)</span> tests is <span class="math inline">\((1-\alpha)^q\)</span>. Hence, the family-wise error rate is
<span class="math display">\[\alpha_\text{FW} = 1 - (1-\alpha)^q\]</span>
If we use the conventional <span class="math inline">\(\alpha = .05\)</span> for each test, and <span class="math inline">\(q = 120\)</span>, then the family-wise error rate would be
<span class="math display">\[\alpha_\text{FW} = 1 - .95^{120} = 0.998\]</span>
which shows we are virtually guaranteed to make at least one Type 1 error. The Bonferroni correction is simply to choose a desired value for <span class="math inline">\(\alpha_\text{FW}\)</span>, e.g. <span class="math inline">\(\alpha_\text{FW} = .05\)</span>, and then to set the significance level of the individual tests to <span class="math inline">\(\alpha = \frac{\alpha_\text{FW}}{q}\)</span>. Doing so, the family-wise error rate is never above what we want it to be. For example:
<span class="math display">\[\alpha_\text{FW} = 1 - \left(1 - \frac{.05}{120}\right)^{120} = 0.049\]</span>
So, following the Bonferroni correction, we would use a significance level of <span class="math inline">\(.05/120 = 0.000417\)</span> for each test of a pairwise difference. While this correction ensures that the family-wise error rate is kept within bounds, it is also rather conservative. The difference between each pair of conditions would have to be rather large to pass such a high bar. Luckily, clever statisticians have devised alternatives which obtain the same objective (limiting the family-wise error rate), whilst being less conservative. There are a <em>lot</em> of these, and I will only mention a few here.</p>
<p>The <strong>Holm correction</strong> requires you to perform all tests first. You then need to order the resulting <span class="math inline">\(p\)</span>-values, <span class="math inline">\(p_j\)</span>, <span class="math inline">\(j=1, \ldots,q\)</span> for each test from smallest to largest. The smallest <span class="math inline">\(p\)</span>-value is assigned <span class="math inline">\(\text{rank}(p_j) = 1\)</span>, and the largest <span class="math inline">\(\text{rank}(p_j) = q\)</span>. The Holm procedure is to compare each <span class="math inline">\(p\)</span>-value to a significance level of
<span class="math display">\[\alpha(p_j) = \frac{\alpha_\text{FW}}{(q - \text{rank}(p_j) + 1)}\]</span>
For example, if you performed a <span class="math inline">\(q=80\)</span> tests and want to maintain a family-wise significance level of <span class="math inline">\(\alpha_\text{FW} = .05\)</span>, you would compare the 40th lowest <span class="math inline">\(p\)</span>-value to a significance level of <span class="math inline">\(.05/(80 - 40 + 1) = 0.00122\)</span>. Using the Bonferroni correction, the comparison level would be <span class="math inline">\(.05/80 = 0.000625\)</span>. As soon as you obtain a non-significant result (i.e. <span class="math inline">\(p_j &gt; \alpha(p_j)\)</span>), you stop testing. All subsequent tests are declared non-significant. Like the Bonferroni correction, this procedure ensures that the family-wise Type 1 error rate is kept within a desired bound <span class="math inline">\(\alpha_\text{FW}\)</span>, but it is more powerful than the Bonferroni correction (i.e. the rate of Type 2 errors is lower). The Holm correction may be less intuitive and more involved to compute than the Bonferroni correction, but because it reaches the same goal with more power, there is no reason to use the Bonferroni correction instead of the Holm correction.</p>
<p>Another option, relevant within the context of ANOVA, is to use the <strong>Scheffé adjusted</strong> critical <span class="math inline">\(F\)</span>-value:
<span class="math display">\[F_\text{Scheffé} = (g-1)F_{g-1,n-\text{npar}(G);\alpha}\]</span>
where <span class="math inline">\(F_{g-1,n-\text{npar}(G);\alpha_\text{FW}}\)</span> is the critical value for an <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(\text{df}_1 = g-1\)</span> and <span class="math inline">\(\text{df}_2 = n - \text{npar}(G)\)</span> degrees of freedom (note that <span class="math inline">\(g\)</span> refers to the number of groups/conditions in the design, not the number of tests you want to perform), and a desired family-wise error rate <span class="math inline">\(\alpha_\text{FW}\)</span>. Just as for the Holm procedure, the Scheffé testing procedure ensures that the family-wise error rate never exceeds the desired level. It can also be more powerful than the Bonferonni correction, depending on the number of hypotheses you want to test. The Scheffé criterion is independent of the number of contrasts you want to test, and is effectively a correction for <em>any number</em> of possible contrast you want to test, whether this be pairwise comparisons between conditions, or comparisons of combinations of the different conditions. The Holm procedure can also be applied in the case of these more general comparisons. If you have a limited number of hypotheses to test, the Holm procedure will generally be more powerful than the Scheffé procedure.</p>
<p>This is a good point to discuss the difference between <strong>planned comparisons</strong> and <strong>post-hoc tests</strong>. The former refers to a limited set of hypotheses that are of interest to test, and that you planned to test <em>before</em> looking at the data. The latter refers to hypotheses that are inspired by analyses you have already performed on the data. For instance, a significant omnibus test but a non-significant result for an expected difference might inspire you to conduct all pairwise comparisons to see what this significant omnibus test is due to. A danger with such post-hoc tests is that the result that inspired the subsequent tests might itself be a Type 1 error (i.e. a “fluke”). For this reason, more care should be taken with post-hoc tests, and a certain level of conservatism is warranted. Because planned comparisons are, by definition, not the result of prior test results, there is less of a chance of following a trail of spurious results.</p>
<p>We now have to discuss a rather tricky issue. Suppose you are really just interested in testing a small number of interesting comparisons between conditions. Good! That means that you have thought about your study and the information you want to get out of it. The question is now whether you should apply any correction at all. You may have already wondered why, in a linear model with <span class="math inline">\(m\)</span> predictors, and tests of all <span class="math inline">\(m+1\)</span> parameters (the intercept and slopes for the <span class="math inline">\(m\)</span> predictors), we haven’t worried before about an inflation of Type 1 error. Clearly, if we’d included 20 predictors in our model, we’d on average expect one of these tests to be significant, even if none of the predictors have any real effect. That is a very good question, and I don’t have a definite answer to it. The answer really depends on what you think is an allowable error rate. And that is not tied to a single analysis. In your scientific career, you might perform a total of 100,000 hypothesis tests. How many of those would you like to be possible Type 1 errors? Should you apply something like a Bonferonni or Holm correction to all these tests? This is of course a rather silly question to ask of you, but it highlights that recommendations for multiple testing should be considered carefully. They are recommendations, and not strict rules that should be followed blindly. In my humble opinion, as long as you have a limited number of reasonable planned comparisons, you don’t necessarily have to apply any correction. And if you want to be careful, apply a Holm correction, as this is more powerful than a Bonferroni correction. When your tests are exploratory (post-hoc and inspired by previous test results), you should be careful and something like the Scheffé procedure makes sense, because it corrects for all comparisons you might conduct, whether the result of “data-snooping” or otherwise. There is of course still no guarantee that a given test result is not a Type 1 error, but at least you have limited the total number of likely Type 1 errors within the set of possible comparisons.</p>
<p>Finally, if you are just interested in <em>all</em> pairwise comparisons between conditions (whether post-hoc or not), I want to point out the <strong>Tukey Honestly Significant Difference</strong> (HSD) test. It was specifically designed for this purpose, and will often be more powerful than the previously discussed correction methods for these particular pairwise comparisons.</p>
<!-- 
## Assumptions {#sec:06-assumptions}

The assumptions of the oneway ANOVA model are the same as the assumptions of any General Linear Model (see Section \@ref(sec:04b-assumptions)). Thus, we assume the errors (residuals) of the model to be Normal-distributed with a mean of 0, a constant standard deviation $\sigma_\epsilon$, and each error term $\epsilon_i$ to be independent from any other one. However, because a oneway ANOVA makes the same prediction within each group, $\hat{Y} = \overline{Y}_j$, where $\overline{Y}_j$ is the sample mean of the dependent variable in the group the case belongs to.  
-->

</div>
</div>
<div class="footnotes">
<hr />
<ol start="16">
<li id="fn16"><p>Remember that the significance level sets an allowable error rate if the null hypothesis is true, whilst the error rate when the null hypothesis is not true is unknown, but unlikely to be 0.<a href="ch-ANOVA.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>you are allowed to multiply or divide the numerator and denominator by the same constant, as this does not change the fraction, i.e. <span class="math inline">\(\tfrac{a}{b} = \frac{a/c}{b/c}\)</span><a href="ch-ANOVA.html#fnref17" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="moderation-and-mediation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-factorial-ANOVA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
},
"source": null,
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
