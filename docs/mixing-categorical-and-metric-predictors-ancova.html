<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Mixing categorical and metric predictors (ANCOVA) | Statistics: data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Mixing categorical and metric predictors (ANCOVA) | Statistics: data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Mixing categorical and metric predictors (ANCOVA) | Statistics: data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2020-11-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="factorial-anova.html"/>
<link rel="next" href="repeated-measures-anova.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="book_assets/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="book_assets/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.0/anchor-sections.js"></script>
<script src="book_assets/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.6.1/grViz.js"></script>
<script src="book_assets/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-1.52.2/plotly-latest.min.js"></script>
<link href="book_assets/combineWidgetStyle-0.1/combineWidgets.css" rel="stylesheet" />
<script src="book_assets/combineWidgets-binding-0.10.1/combineWidgets.js"></script>
<link href="book_assets/rglwidgetClass-2/rgl.css" rel="stylesheet" />
<script src="book_assets/rglwidgetClass-2/rglClass.src.js"></script>
<script src="book_assets/CanvasMatrix4-2016/CanvasMatrix.src.js"></script>
<script src="book_assets/rglWebGL-binding-0.100.54/rglWebGL.js"></script>
<script src="book_assets/rglPlayer-binding-0.100.54/rglPlayer.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-modelling.html"><a href="statistical-modelling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#what-is-probability"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-modelling.html"><a href="statistical-modelling.html#flipping-a-biased-coin-an-alternative-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-modelling.html"><a href="statistical-modelling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a><ul>
<li class="chapter" data-level="2.4.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="statistical-modelling.html"><a href="statistical-modelling.html#comparing-models-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a><ul>
<li class="chapter" data-level="2.5.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="statistical-modelling.html"><a href="statistical-modelling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-modelling.html"><a href="statistical-modelling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="statistical-modelling.html"><a href="statistical-modelling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="statistical-modelling.html"><a href="statistical-modelling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a><ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-an-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has an specific value</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#confidence-intervals"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.6</b> Assumptions</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.7</b> The Central Limit Theorem</a><ul>
<li class="chapter" data-level="3.7.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.7.1</b> The Central Limit Theorem in action</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a><ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a><ul>
<li class="chapter" data-level="5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#effect-size-and-the-importance-of-predictors"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a><ul>
<li class="chapter" data-level="5.6.1" data-path="multiple-regression.html"><a href="multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="multiple-regression.html"><a href="multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assumptions</a></li>
<li class="chapter" data-level="5.8" data-path="multiple-regression.html"><a href="multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.8</b> Multicollinearity: Redundancy between predictors</a><ul>
<li class="chapter" data-level="5.8.1" data-path="multiple-regression.html"><a href="multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.8.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="multiple-regression.html"><a href="multiple-regression.html#outliers"><i class="fa fa-check"></i><b>5.9</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a><ul>
<li class="chapter" data-level="6.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#dont-forget-about-fun-a-model-with-multiple-interactions"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a><ul>
<li class="chapter" data-level="6.2.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a><ul>
<li class="chapter" data-level="7.1" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#comparing-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a><ul>
<li class="chapter" data-level="7.4.1" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.6</b> Multiple testing and post-hoc tests</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="factorial-anova.html"><a href="factorial-anova.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a><ul>
<li class="chapter" data-level="8.1" data-path="factorial-anova.html"><a href="factorial-anova.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a><ul>
<li class="chapter" data-level="8.1.1" data-path="factorial-anova.html"><a href="factorial-anova.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="factorial-anova.html"><a href="factorial-anova.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a><ul>
<li class="chapter" data-level="8.2.1" data-path="factorial-anova.html"><a href="factorial-anova.html#main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="factorial-anova.html"><a href="factorial-anova.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.3</b> A threeway factorial ANOVA</a><ul>
<li class="chapter" data-level="8.3.1" data-path="factorial-anova.html"><a href="factorial-anova.html#interpreting-interactions"><i class="fa fa-check"></i><b>8.3.1</b> Interpreting interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="factorial-anova.html"><a href="factorial-anova.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.4</b> Orthogonal contrast codes and unequal sample sizes</a><ul>
<li class="chapter" data-level="8.4.1" data-path="factorial-anova.html"><a href="factorial-anova.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.4.1</b> Comparison schemes and SS types</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a><ul>
<li class="chapter" data-level="9.1" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#subjective-feelings-of-power-and-priming"><i class="fa fa-check"></i><b>9.1</b> Subjective feelings of power and priming</a></li>
<li class="chapter" data-level="9.2" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#acounting-for-pre-existing-differences"><i class="fa fa-check"></i><b>9.2</b> Acounting for pre-existing differences</a></li>
<li class="chapter" data-level="9.3" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#slopes-of-contrast-coding-predictors-in-an-ancova-models"><i class="fa fa-check"></i><b>9.3</b> Slopes of contrast-coding predictors in an ANCOVA models</a></li>
<li class="chapter" data-level="9.4" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#homogeneity-of-slopes"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of slopes</a></li>
<li class="chapter" data-level="9.5" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#sec:ANCOVA-power"><i class="fa fa-check"></i><b>9.5</b> Power considerations in ANCOVA</a></li>
<li class="chapter" data-level="9.6" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#models-with-multiple-covariates"><i class="fa fa-check"></i><b>9.6</b> Models with multiple covariates</a></li>
<li class="chapter" data-level="9.7" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#mediation-with-categorical-independent-variables"><i class="fa fa-check"></i><b>9.7</b> Mediation with categorical independent variables</a></li>
<li class="chapter" data-level="9.8" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#ancova-vs-difference-scores"><i class="fa fa-check"></i><b>9.8</b> ANCOVA vs difference scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a></li>
<li class="chapter" data-level="11" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a></li>
<li class="chapter" data-level="12" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html"><i class="fa fa-check"></i><b>12</b> Introduction to Bayesian hypothesis testing</a></li>
<li class="chapter" data-level="13" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html"><i class="fa fa-check"></i><b>13</b> Being a responsible data analyst</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mixing-categorical-and-metric-predictors-ancova" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Mixing categorical and metric predictors (ANCOVA)</h1>
<p>In this chapter, we will consider combining categorical and metric independent variables to predict an outcome variable. Such an analysis is generally called an Analysis of Covariance (ANCOVA). There are a number of reasons why you might want to include metric predictors (or “covariates”) in an ANOVA-type analysis: to statistically control for pre-existing differences between conditions of potential confounders, to reduce prediction error and increase the power of statistical tests, and because they are theoretically interesting as e.g. potential mediators of the effect of experimental manipulations. In the course of this chapter, we will see examples of each of these. We will also discuss the interpretation of contrasts in ANCOVA models as reflecting differences between group means which are adjusted for the effect of the covariate.</p>
<div id="subjective-feelings-of-power-and-priming" class="section level2">
<h2><span class="header-section-number">9.1</span> Subjective feelings of power and priming</h2>
<p>To illustrate the models, we will consider the data from <span class="citation">Gilder &amp; Heerey (<a href="#ref-gilder2018role" role="doc-biblioref">2018</a>)</span> again. As part of the experiment, the researchers measured participants’ subjective feeling of power, both before and after providing them with the power priming task. In the priming task participants were presented with randomly scrambled sentences, and they were asked to reorder the words into grammatically correct sentences. In the high-power prime condition, half of the sentences included words associated with high power (e.g., “dominates,” “commands”), and in the low-power prime condition, half the sentences contained words associated with low power (e.g., “subordinate,” “obeys”). If the priming task had the desired effect, then participants in the high-power condition should report feeling more power after the priming task than participants in the low-power condition. Figure <a href="mixing-categorical-and-metric-predictors-ancova.html#fig:expBelief-power-test-interaction-plots">9.1</a> shows the means of the subjective feeling of power before and after the power priming manipulation. As can be seen there, the task indeed appears to have the desired effect. Before the priming task, differences between the groups are relatively small and there appears to be no real difference between the priming condition. After the priming task, the averages in the low-power prime conditions are lower, and those in the high-power prime conditions higher, than before the priming task.</p>
<div class="figure" style="text-align: center"><span id="fig:expBelief-power-test-interaction-plots"></span>
<img src="_main_files/figure-html/expBelief-power-test-interaction-plots-1.svg" alt="Means and standard errors of subjective feeling of power before (pre-test) and after (post-test) power prime." width="80%" />
<p class="caption">
Figure 9.1: Means and standard errors of subjective feeling of power before (pre-test) and after (post-test) power prime.
</p>
</div>
To determine whether the differences between the conditions after the power prime manipulation are significant, we can use the by-now hopefully familiar procedure of constructing appropriate contrast codes for priming condition and experimenter belief. To code for prime condition, it makes sense to assign a value of <span class="math inline">\(\tfrac{1}{2}\)</span> to the high-power prime condition, and a value of <span class="math inline">\(-\tfrac{1}{2}\)</span> to the lower-power prime condition. As one would expect the high-power prime to increase the subjective feeling of power, one would then expect a positive slope for the associated contrast-coding predictor. Similarly, it makes sense to assign a value of <span class="math inline">\(\tfrac{1}{2}\)</span> to the “experimenter believes high-power” condition, and a value of <span class="math inline">\(-\tfrac{1}{2}\)</span> to the “experimenter believes low-power” condition. If experimenter belief has an effect on feelings of power, one would expect the the subjective feeling of power to be higher when the experimenter believes the participant was given a high-power prime. Hence, one would then expect a positive slope for the contrast-coding predictor associated to this contrast. The contrast code for the interaction is constructed as usual by multiplying the values of these two contrast codes. So our set of (orthogonal) contrast codes is:
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\texttt{P}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\texttt{B}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\texttt{P}\times\texttt{B}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
PL,EL
</td>
<td style="text-align:right;">
<span class="math inline">\(-\tfrac{1}{2}\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(-\tfrac{1}{2}\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(\tfrac{1}{4}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
PL,EH
</td>
<td style="text-align:right;">
<span class="math inline">\(-\tfrac{1}{2}\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(\tfrac{1}{2}\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(-\tfrac{1}{4}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
PH,EL
</td>
<td style="text-align:right;">
<span class="math inline">\(\tfrac{1}{2}\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(-\tfrac{1}{2}\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(-\tfrac{1}{4}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
PH,EH
</td>
<td style="text-align:right;">
<span class="math inline">\(\tfrac{1}{2}\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(\tfrac{1}{2}\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(\tfrac{1}{4}\)</span>
</td>
</tr>
</tbody>
</table>
<p>We can then estimate a linear model predicting <span class="math inline">\(\texttt{post-power}\)</span>, the subjective feeling of power after the priming manipulation, as:
<span class="math display" id="eq:post-power-ANOVA">\[\begin{equation}
\texttt{post-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \beta_{\text{P}\times \text{B}} \times (\texttt{P} \times \texttt{B})_i + \epsilon_i
\tag{9.1}
\end{equation}\]</span>
The estimates and hypothesis tests of the parameters of this model are given in Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-power-post-ANOVA">9.1</a>.</p>
<table>
<caption>
<span id="tab:expBelief-power-post-ANOVA">Table 9.1: </span>Linear model predicting <span class="math inline">\(\texttt{post-power}\)</span> by factorial contrast-coding predictors.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{SS}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(P(\geq \lvert F \rvert)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
57.31
</td>
<td style="text-align:right;">
1.31e+06
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5431.58
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P}\)</span>
</td>
<td style="text-align:right;">
3.88
</td>
<td style="text-align:right;">
1.50e+03
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
6.21
</td>
<td style="text-align:right;">
0.013
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{B}\)</span>
</td>
<td style="text-align:right;">
-0.01
</td>
<td style="text-align:right;">
1.00e-02
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.995
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P} \times \texttt{B}\)</span>
</td>
<td style="text-align:right;">
3.80
</td>
<td style="text-align:right;">
3.61e+02
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.49
</td>
<td style="text-align:right;">
0.222
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
9.58e+04
</td>
<td style="text-align:right;">
396
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
<p>The results show a significant effect of power prime. The slope is estimated as <span class="math inline">\(\hat{\beta}_\text{P} = 3.87\)</span>. Remember that this slope represents a difference between two marginal means (averaging over the levels of experimenter Belief):
<span class="math display">\[
\begin{align}
\hat{\beta}_\text{P} &amp;= \hat{\mu}_{\text{PH},\cdot} - \hat{\mu}_{\text{PL},\cdot} \\
&amp;= \frac{\overline{Y}_\text{PH,EL} + \overline{Y}_\text{PH,EL}}{2} - \frac{\overline{Y}_\text{PL,EL} + \overline{Y}_\text{PL,EL}}{2} \\
&amp;= 3.87
\end{align}
\]</span>
Hence, the power prime manipulation appears to result in a difference of 3.87 points in subjective feeling of power. Neither the main effect of experimenter belief, nor the interaction between priming condition and experimenter belief are significant. We might thus conclude that experimenter belief has no effect on subjective feeling of power, neither directly, nor by moderating the effect of power prime.</p>
<p>While the results are indicative of a successful manipulation of participants’ subjective feeling of power, it could be that these differences between the conditions were already present before the priming manipulation. While random assignment of participants to the conditions makes such pre-existing differences unlikely, they cannot be ruled out a priori. In this case, as we also have a measure of participants’ feeling of power before the priming task, we can test for these pre-existing differences. This can be done with exactly the same model as before, now using <span class="math inline">\(\texttt{pre-power}\)</span> as the dependent variable. The results are provided in Table @(expBelief-power-pre-ANOVA). As only the intercept is significant (which tells us, rather uninterestingly, that the average feeling of power differs from 0), we have no evidence for pre-existing differences, which strengthens our belief that the power manipulation was indeed successful.</p>
<table>
<caption>
<span id="tab:expBelief-power-pre-ANOVA">Table 9.2: </span>Linear model predicting <span class="math inline">\(\texttt{pre-power}\)</span> by factorial contrast-coding predictors.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{SS}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(P(\geq \lvert F \rvert)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
57.287
</td>
<td style="text-align:right;">
1.31e+06
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5742.575
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P}\)</span>
</td>
<td style="text-align:right;">
-0.700
</td>
<td style="text-align:right;">
4.90e+01
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.214
</td>
<td style="text-align:right;">
0.644
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{B}\)</span>
</td>
<td style="text-align:right;">
0.095
</td>
<td style="text-align:right;">
9.04e-01
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.004
</td>
<td style="text-align:right;">
0.950
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P} \times \texttt{B}\)</span>
</td>
<td style="text-align:right;">
3.429
</td>
<td style="text-align:right;">
2.94e+02
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.286
</td>
<td style="text-align:right;">
0.257
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
9.05e+04
</td>
<td style="text-align:right;">
396
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
</div>
<div id="acounting-for-pre-existing-differences" class="section level2">
<h2><span class="header-section-number">9.2</span> Acounting for pre-existing differences</h2>
<p>We have just used two separate analyses to (1) assess differences between the priming conditions in feelings of power after the priming task and (2) rule out that these were due to pre-existing differences. There is an issue with the latter, actually, in that a non-significant test result is not direct evidence for the absence of an effect. Non-significant results can always be due to a lack of power.</p>
<p>There is a better way to reach both objectives: by including <span class="math inline">\(\texttt{pre-power}\)</span> as a predictor in our model of <span class="math inline">\(\texttt{post-power}\)</span>, we can determine the effect of prime and experimenter belief controlling for the effect of <span class="math inline">\(\texttt{pre-power}\)</span>. This is the general idea of ANCOVA, to test for group differences whilst controlling for the effect of <strong>covariates</strong> (metric predictors).</p>
<p>When we add <span class="math inline">\(\texttt{pre-power}\)</span> to the model, it becomes:
<span class="math display" id="eq:post-power-ANCOVA">\[\begin{equation}
\texttt{post-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \beta_{\text{P}\times \text{B}} \times (\texttt{P} \times \texttt{B})_i + \beta_\text{pre} \times \texttt{pre-power}_i + \epsilon_i
\tag{9.2}
\end{equation}\]</span>
Parameter estimates and tests are provided in Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-power-post-ANCOVA">9.3</a>. The results show a significant effect of prime, as well as the pre-test power score (<span class="math inline">\(\texttt{pre-power}\)</span>). The latter effect is expected, and can be interpreted as usual: the positive slope indicates that participants who scored relatively high in the pre-test also score relatively high in the post-test. Comparing the effect of power prime to that obtained earlier (Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-power-post-ANOVA">9.1</a>), we can notice two things: the slope is somewhat higher, and the value of the <span class="math inline">\(F\)</span> statistic is substantially higher. We will consider the interpretation of the slope in the next section, and the reason for the higher <span class="math inline">\(F\)</span> statistic in Section <a href="mixing-categorical-and-metric-predictors-ancova.html#sec:ANCOVA-power">9.5</a>.</p>
<table>
<caption>
<span id="tab:expBelief-power-post-ANCOVA">Table 9.3: </span>Linear model predicting <span class="math inline">\(\texttt{post-power}\)</span> by <span class="math inline">\(\texttt{pre-power}\)</span> and factorial contrast-coding predictors.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{SS}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(P(\geq \lvert F \rvert)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
16.071
</td>
<td style="text-align:right;">
6.66e+03
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
53.857
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P}\)</span>
</td>
<td style="text-align:right;">
4.379
</td>
<td style="text-align:right;">
1.92e+03
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
15.486
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{B}\)</span>
</td>
<td style="text-align:right;">
-0.079
</td>
<td style="text-align:right;">
6.18e-01
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.005
</td>
<td style="text-align:right;">
0.944
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{pre-power}\)</span>
</td>
<td style="text-align:right;">
0.720
</td>
<td style="text-align:right;">
4.69e+04
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
379.192
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P} \times \texttt{B}\)</span>
</td>
<td style="text-align:right;">
1.334
</td>
<td style="text-align:right;">
4.43e+01
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.358
</td>
<td style="text-align:right;">
0.550
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
4.89e+04
</td>
<td style="text-align:right;">
395
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
</div>
<div id="slopes-of-contrast-coding-predictors-in-an-ancova-models" class="section level2">
<h2><span class="header-section-number">9.3</span> Slopes of contrast-coding predictors in an ANCOVA models</h2>
<p>Although a metric predictor in an ANCOVA model is really just like any other predictor in the GLM, it is useful for clarity to (momentarily) denote such a <strong>covariate</strong> with a different symbol, e.g. <span class="math inline">\(Z\)</span>, instead of the usual <span class="math inline">\(X\)</span> we used for predictors. We can then write the model of Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:post-power-ANCOVA">(9.2)</a> more abstractly as:
<span class="math display" id="eq:ANCOVA-one-covariate">\[\begin{equation}
Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \beta_3 \times X_{3,i} + \beta_\text{z} \times Z_i + \epsilon_i
\tag{9.3}
\end{equation}\]</span>
Here, the <span class="math inline">\(X\)</span> variables are the contrast-coding predictors (e.g. <span class="math inline">\(\texttt{P}\)</span>, <span class="math inline">\(\texttt{B}\)</span>, and <span class="math inline">\(\texttt{P}\times\texttt{B}\)</span>), and <span class="math inline">\(Z\)</span> is a single covariate (e.g. <span class="math inline">\(\texttt{pre-power}\)</span>).</p>
<p>As in any General Linear Model, inclusion of additional predictors will likely change the slopes of already included predictors. The slopes of the latter predictors will remain the same only when the additional predictors are <em>completely independent</em> from the already-included predictors. This is extremely unlikely. That means that in a model like that of Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:ANCOVA-one-covariate">(9.3)</a>, we can no longer use Equation <a href="a-model-of-means-anova.html#eq:estimate-slope-orthogonal-contrast-codes">(7.3)</a> to compute the estimated slopes of contrast-coding predictors which are based set of orthogonal contrast codes. Fortunately, that does not mean those slopes no longer have a useful interpretation. In fact, the estimated slopes can be expressed as an adjusted version of Equation <a href="a-model-of-means-anova.html#eq:estimate-slope-orthogonal-contrast-codes">(7.3)</a>:
<span class="math display" id="eq:estimate-slope-orthogonal-contrast-codes-ANCOVA-one-covariate">\[\begin{equation}
\hat{\beta}_j = \frac{\sum_{k=1}^{g} c_{j,k} \overline{Y}_k}{\sum_{k=1} c_{j,k}^2} - \hat{\beta}_z \frac{\sum_{k=1}^{g} c_{j,k} \overline{Z}_k}{\sum_{k=1} c_{j,k}^2}
\tag{9.4}
\end{equation}\]</span>
where <span class="math inline">\(\hat{\beta}_z\)</span> is the slope of the covariate in the full model (Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:ANCOVA-one-covariate">(9.3)</a>). You can think of this as follows. An orthogonal contrast code reflects differences between marginal means. In the presence of a covariate, these differences are adjusted for the same difference in the marginal means of the covariate, weighted by its effect on the dependent variable. If there are differences in the covariate between the groups, and the covariate has an effect on the dependent variable, then any differences between the groups can be explained by differences in the covariate. After the adjustment, the slope of the contrast code then reflects differences between the groups that are not explained by the covariate.</p>
Let’s illustrate how this works. The means of <span class="math inline">\(\texttt{pre-power}\)</span> and <span class="math inline">\(\texttt{post-power}\)</span> are:
<table>
<thead>
<tr>
<th style="text-align:left;">
Prime
</th>
<th style="text-align:left;">
Belief
</th>
<th style="text-align:right;">
pre-power
</th>
<th style="text-align:right;">
post-power
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
PL
</td>
<td style="text-align:left;">
EL
</td>
<td style="text-align:right;">
58.4
</td>
<td style="text-align:right;">
56.3
</td>
</tr>
<tr>
<td style="text-align:left;">
PL
</td>
<td style="text-align:left;">
EH
</td>
<td style="text-align:right;">
56.8
</td>
<td style="text-align:right;">
54.4
</td>
</tr>
<tr>
<td style="text-align:left;">
PH
</td>
<td style="text-align:left;">
EL
</td>
<td style="text-align:right;">
56.0
</td>
<td style="text-align:right;">
58.3
</td>
</tr>
<tr>
<td style="text-align:left;">
PH
</td>
<td style="text-align:left;">
EH
</td>
<td style="text-align:right;">
57.8
</td>
<td style="text-align:right;">
60.2
</td>
</tr>
</tbody>
</table>
<p>The estimated slope of <span class="math inline">\(\texttt{pre-power}\)</span> was <span class="math inline">\(\hat{\beta}_\text{pre} = 0.72\)</span>.</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_\texttt{P} &amp;=&amp;&amp; \frac{\tfrac{1}{2} \times 58.31 + \tfrac{1}{2} \times 60.2 - \tfrac{1}{2} \times 56.33 - \tfrac{1}{2} \times 54.42}{ (\tfrac{1}{2})^2 + (\tfrac{1}{2})^2 + (-\tfrac{1}{2})^2 + (-\tfrac{1}{2})^2} - \\
&amp;&amp;&amp; 0.72 \times \frac{\tfrac{1}{2} \times 56.03 + \tfrac{1}{2} \times 57.84 - \tfrac{1}{2} \times 58.45 - \tfrac{1}{2} \times 56.83}{ (\tfrac{1}{2})^2 + (\tfrac{1}{2})^2 + (-\tfrac{1}{2})^2 + (-\tfrac{1}{2})^2} \\
&amp; = &amp;&amp; \frac{3.875}{1} - 0.72 \times \frac{-0.7}{1} \\
&amp; = &amp;&amp; 4.379
\end{align}\]</span></p>
<p>An alternative way to define the slope of Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:estimate-slope-orthogonal-contrast-codes-ANCOVA-one-covariate">(9.4)</a> is to first adjust the means of <span class="math inline">\(Y\)</span> and then enter the adjusted group means in the usual formula for the slope of a contrast-coding predictor. These adjusted means are computed using a centered version of the covariate (i.e. <span class="math inline">\(Z&#39;_i = Z_i - \overline{Z}\)</span>):
<span class="math display">\[\overline{Y}_k&#39; = \overline{Y}_k - \hat{\beta}_z \times (\overline{Z}_k - \overline{Z})\]</span>
where <span class="math inline">\(\overline{Z}\)</span> is the average of the covariate over all observations (i.e. it is not an “average of averages”). The slope can them be expressed as a function of these adjusted means:
<span class="math display">\[\hat{\beta}_j = \frac{\sum_{k=1}^{g} c_{j,k} \overline{Y}_k&#39;}{\sum_{k=1} c_{j,k}^2}\]</span>
The fact that these two formulations are equivalent again illustrates the main idea of an ANCOVA model, which is to assess group differences that can not be attributed to the covariate.</p>
</div>
<div id="homogeneity-of-slopes" class="section level2">
<h2><span class="header-section-number">9.4</span> Homogeneity of slopes</h2>
<p>The model of Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:post-power-ANOVA">(9.1)</a> does not include any interactions between the covariate and the contrast-coding differences. As such, the model assumes the effect of the covariate on the dependent variable is the same in each condition. The regression lines for the relation between <span class="math inline">\(\texttt{power-pre}\)</span> and <span class="math inline">\(\texttt{power-post}\)</span> in the different conditions are depicted in Figure <a href="mixing-categorical-and-metric-predictors-ancova.html#fig:expBelief-modpostpre-regression-lines">9.2</a>. Because the slopes are assumed to be identical, you can see that the regression lines are parallel. Note that the relative “height” of each regression line (e.g. the intercept) reflects the combined effect of power priming and experimenter belief. As discussed above, you can think of an ANCOVA model as statistically correcting the conditions for any differences in the covariate. After this correction, the conditions effectively have the same value for the covariate. In the context of Figure <a href="mixing-categorical-and-metric-predictors-ancova.html#fig:expBelief-modpostpre-regression-lines">9.2</a>, you can pick any value of the covariate you like, and then think of the contrasts between the conditions as comparisons between the model predictions at that value of the covariate. Because the regression lines for both high-power prime conditions, and for both low-power prime conditions, are almost overlapping, it is clear that experimenter belief has little effect on feelings of power. The separation between the regression lines is clearer between the high-power and low-power conditions, which is also reflected in the significant effect of power prime in the model.</p>
<div class="figure" style="text-align: center"><span id="fig:expBelief-modpostpre-regression-lines"></span>
<img src="_main_files/figure-html/expBelief-modpostpre-regression-lines-1.svg" alt="Regression lines reflecting the relation between $  exttt{power-pre}$ and $ exttt{power-post}$ in the four conditions of the experimenter belief and power priming experiment." width="672" />
<p class="caption">
Figure 9.2: Regression lines reflecting the relation between $ exttt{power-pre}$ and $ exttt{power-post}$ in the four conditions of the experimenter belief and power priming experiment.
</p>
</div>
<p>While the assumption of parallel regression lines (homogeneity of slopes) allows for a straightforward interpretation of the other effects as reflecting differences in adjusted means, like for any assumption, there is always the question whether the assumption holds. Not only does the lack of interactions between the covariate and contrast-coding predictors mean that the effect of the covariate on the dependent variable is the same in all conditions, it also means that the effect of the conditions is the same regardless of the value on the covariate. There may be good reasons to suspect that the effect of the experimental manipulations differ for people who score differently on the covariate. For instance, you might think that a high-power prime is effective in lifting the subjective feeling of power for those who have a relatively low feeling of power to start off with, while it doesn’t do much for those who feel powerful anyway. In a similar vain, the low-power prime might lower the subjective feeling of power for those who have a relatively low feeling of power to start off with, while those with a relatively high feeling of power are immune to it. This implies that the experimental manipulations interact with the covariate, and that we should include interactions between the covariate and the contrast-coding predictors. Allowing for such interactions is straightforward: we just construct new product predictors and include them in the model.</p>
<p>In the model of Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:post-power-ANOVA">(9.1)</a>, we would add three product predictors: <span class="math inline">\((\texttt{P} \times \texttt{power-pre})_i\)</span>, <span class="math inline">\((\texttt{B} \times \texttt{power-pre})_i\)</span>, and <span class="math inline">\((\texttt{P} \times \texttt{B} \times \texttt{power-pre})_i\)</span>. The results of the expanded model are given in Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-power-post-ANCOVA-non-homgeneous">9.4</a>.</p>
<table>
<caption>
<span id="tab:expBelief-power-post-ANCOVA-non-homgeneous">Table 9.4: </span>Linear model predicting <span class="math inline">\(\texttt{post-power}\)</span> by <span class="math inline">\(\texttt{pre-power}\)</span> and factorial contrast-coding predictors, as well as interactions between those.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{SS}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(P(\geq \lvert F \rvert)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
15.945
</td>
<td style="text-align:right;">
6519.6
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
52.440
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P}\)</span>
</td>
<td style="text-align:right;">
6.358
</td>
<td style="text-align:right;">
259.1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2.084
</td>
<td style="text-align:right;">
0.150
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{B}\)</span>
</td>
<td style="text-align:right;">
-2.766
</td>
<td style="text-align:right;">
49.1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.395
</td>
<td style="text-align:right;">
0.530
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{pre-power}\)</span>
</td>
<td style="text-align:right;">
0.722
</td>
<td style="text-align:right;">
46882.4
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
377.091
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P} \times \texttt{B}\)</span>
</td>
<td style="text-align:right;">
-4.101
</td>
<td style="text-align:right;">
27.0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.217
</td>
<td style="text-align:right;">
0.642
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P} \times \texttt{pre-power}\)</span>
</td>
<td style="text-align:right;">
-0.035
</td>
<td style="text-align:right;">
28.0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.225
</td>
<td style="text-align:right;">
0.635
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{B} \times \texttt{pre-power}\)</span>
</td>
<td style="text-align:right;">
0.048
</td>
<td style="text-align:right;">
51.3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.412
</td>
<td style="text-align:right;">
0.521
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P} \times \texttt{B} \times \texttt{pre-power}\)</span>
</td>
<td style="text-align:right;">
0.095
</td>
<td style="text-align:right;">
51.2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.412
</td>
<td style="text-align:right;">
0.521
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
48736.0
</td>
<td style="text-align:right;">
392
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
<p>We can see that none of the interactions between <span class="math inline">\(\texttt{pre-power}\)</span> and the contrast-coding predictors are significant. As such, there is no strong evidence of an interaction between the covariate and the experimental manipulations. Rather than inspecting each interaction separately, it makes sense to perform an omnibus test, comparing the expanded model to the one of Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:post-power-ANOVA">(9.1)</a>. This comparison tests the hypothesis that all the slopes of the additional product-predictors equal 0. If there is no overly strong reason to suspect a moderation within particular conditions, the omnibus test is a reasonable reflection of the more-or-less unspecified hypothesis that the covariate interacts with the experimental manipulations.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> This omnibus test is a straightforward application of the model comparison approach. We need the Sum of Squared Errors of each model, which are <span class="math inline">\(\text{SSE}(G) = 48736\)</span> and <span class="math inline">\(\text{SSE}(R) = 48876\)</span> for the expanded model and the one of Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:post-power-ANOVA">(9.1)</a>, respectively. The <span class="math inline">\(F\)</span> statistic is then computed as usual as</p>
<p><span class="math display">\[\begin{align}
F &amp;= \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar}(G)}} \\
&amp;= \frac{\frac{48876 -  48736}{8 - 5}}{\frac{48736}{400-8}} \\
&amp;= 0.375
\end{align}\]</span>
With <span class="math inline">\(\text{df}_1 = 3\)</span> and <span class="math inline">\(\text{df}_2 = 392\)</span>, the critical value is <span class="math inline">\(F_{3,392, .05} = 2.63\)</span>. Hence, the result is non-significant, and we do not reject the null hypothesis that there are no interactions between the covariate and the contrast-coding predictors. In other words, we have no evidence that the assumption of homogeneous regression slopes is violated.</p>
<p>If the assumption of homogeneous regression slopes is violated, then the model should really include the appropriate interaction terms. In that case, it is important to remember that the “simple slopes” of the contrast-coding predictors then reflect tests of group differences at particular values of the covariate. Group differences at one value of the covariate may be substantial, but absent or even reversed at another. It might make sense to center the covariate, so that you would test for group differences for an average value of the covariate. You could also consider not only centering at the mean, but at a few informative other values. For instance, you could consider testing the slopes at three values of the covariate: the minimum value, mean, and maximum value in the data. You could do this by creating three different “centered” covariates: <span class="math inline">\(Z&#39;_i = Z_i - \overline{Z}\)</span>, <span class="math inline">\(Z&#39;&#39;_i = Z_i - \text{min}(Z)\)</span>, and <span class="math inline">\(Z&#39;&#39;&#39;_i = Z_i - \text{max}(Z)\)</span>, and entering each in a different version of the same model. If the estimates of the group differences have the same direction and the tests significant in each, then you would conclude that there is evidence of group differences in the whole range of the covariate observed in the data. Other strategies are possible, of course. The main thing to realise is that in a model with interactions between the covariate and contrast-codes, group differences depend on the value of the covariate, and statements about group differences should be qualified by for which value of the covariate they hold.</p>
</div>
<div id="sec:ANCOVA-power" class="section level2">
<h2><span class="header-section-number">9.5</span> Power considerations in ANCOVA</h2>
<p>A primary reason for including covariates in models with categorical independent variables is to assess group differences in the dependent variable whilst controlling for any possible differences in the covariates. But there is another reason why the inclusion of covariates can be a good idea: it may increase the power of the tests of group differences. We saw evidence of this when we included <span class="math inline">\(\texttt{pre-power}\)</span> in the model for <span class="math inline">\(\texttt{post-power}\)</span>. Comparing the results in Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-power-post-ANCOVA">9.3</a> to those in Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-power-post-ANOVA">9.1</a>, you can see that the test results for the main effect of priming are stronger after inclusion of the covariate. The reason for this is that the covariate can explain differences in <span class="math inline">\(\texttt{post-power}\)</span> within each condition. Accounting for this within-condition variance reduces the overall error variance of the model, which can subsequently increase the power of the tests. This is particularly the case when the covariate is strongly related to the dependent variable, but independent of the contrast-coding predictors. Such independence implies that the group do not differ in the average value of the covariate.</p>
<p>We can illustrate this with the Venn diagrams in Figure <a href="mixing-categorical-and-metric-predictors-ancova.html#fig:sse-partition-ANCOVA">9.3</a>. Let’s consider first the situation of no redundancy between the covariate <span class="math inline">\(Z\)</span> and the contrast-coding predictors <span class="math inline">\(X\)</span>, which is depicted in the left-hand plot. The SSR associated to the contrast-coding predictors is region <span class="math inline">\(B\)</span>, and the SSR associated to the covariate is region <span class="math inline">\(C\)</span>. The test of group differences (i.e. the test of the contrast-coding predictors) would involve a comparison of the SSR of region <span class="math inline">\(B\)</span> against the unexplained error of region <span class="math inline">\(A\)</span>. In a model without the covariate, the unexplained error would be the sum of region <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. As this error is larger, the <span class="math inline">\(F\)</span> statistic will be smaller, even though the SSR term for <span class="math inline">\(X\)</span> is the same. As such, inclusion of the covariate increases the power of the test of <span class="math inline">\(X\)</span>, and the increase in power is larger the stronger the relation between the covariate <span class="math inline">\(Z\)</span> and the dependent variable <span class="math inline">\(Y\)</span> (i.e. the larger region <span class="math inline">\(B\)</span>). The situation is more complicated when there is redundancy between the covariate and the contrast-coding predictors. This is the situation depicted in the right-hand plot. Whilst inclusion of the covariate again reduces the error – which would be the sum <span class="math inline">\(A+C+D\)</span> for a model without the covariate, but only <span class="math inline">\(A\)</span> for a model with the covariate – it also reduces the unique SSR that can be associated to the contrast-coding predictors. In a model without the covariate, <span class="math inline">\(\text{SSR}(X) = B + D\)</span>, and in a model with the covariate, <span class="math inline">\(\text{SSR}(X) = B\)</span>. The ANCOVA model is now no longer guaranteed to increase the power of the test of <span class="math inline">\(X\)</span>, as this depends on how the reduction in error compares to the reduction of the SSR.</p>
<p>In conclusion, for purposes of increasing power, the covariate is ideally strongly related to the dependent variable, but unrelated to the other predictors in the model.</p>
<div class="figure" style="text-align: center"><span id="fig:sse-partition-ANCOVA"></span>
<img src="_main_files/figure-html/sse-partition-ANCOVA-1.svg" alt="Partitioning the variance in an ANCOVA model. the circle labelled as $Y$ represents the variance of teh dependent variable, circle $X$ represents a (set of) contrast-coding predictor(s), and circle $Z$ the covariate. Overlapping regions represent shared variability (e.g. covariance) between variables." width="60%" />
<p class="caption">
Figure 9.3: Partitioning the variance in an ANCOVA model. the circle labelled as <span class="math inline">\(Y\)</span> represents the variance of teh dependent variable, circle <span class="math inline">\(X\)</span> represents a (set of) contrast-coding predictor(s), and circle <span class="math inline">\(Z\)</span> the covariate. Overlapping regions represent shared variability (e.g. covariance) between variables.
</p>
</div>
</div>
<div id="models-with-multiple-covariates" class="section level2">
<h2><span class="header-section-number">9.6</span> Models with multiple covariates</h2>
<p>The analyses reported above indicate that the power priming task had the desired result of changing participants’ subjective feeling of power. The significant effect of prime on <span class="math inline">\(\texttt{post-power}\)</span> reflects differences between the means of the priming conditions. It is likely that within those conditions, there is variability in the subjective feeling of power. According to the social priming hypothesis, we might expect the approach advantage scores to be higher for those participants with a relatively strong subjective feeling of power, and lower for those with a relatively weak subjective feeling of power.</p>
<p>Given the significant differences in <span class="math inline">\(\texttt{post-power}\)</span> between the priming conditions, but the lack of a significant effect of priming condition on approach advantage scores, a relation between <span class="math inline">\(\texttt{post-power}\)</span> and <span class="math inline">\(\texttt{ApproachAdvantage}\)</span> seems unlikely, but we can include <span class="math inline">\(\texttt{post-power}\)</span> as a predictor of <span class="math inline">\(\texttt{ApproachAdvantage}\)</span> and see. Rather than the final subjective feeling of power, it might also be the case that the approach advantage is related to how much the priming manipulation increased or decreased the feeling of power. To investigate this, we might include the <em>difference</em> <span class="math inline">\(\texttt{post-power} - \texttt{pre-power}\)</span> as a predictor. But actually, we can obtain a similar effect by just including <span class="math inline">\(\texttt{pre-power}\)</span> as a predictor in the model; when the slope of <span class="math inline">\(\texttt{pre-power}\)</span> is negative and the slope of <span class="math inline">\(\texttt{post-power}\)</span> positive, the combined effect on <span class="math inline">\(\texttt{ApproachAdvantage}\)</span> would be similar.
The model thus becomes:
<span class="math display">\[\begin{align}
\texttt{ApproachAdvantage}_i =&amp; \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \beta_{\text{P}\times \text{B}} \times (\texttt{P} \times \texttt{B})_i + \\ &amp; \beta_\text{pre} \times \texttt{pre-power}_i + \beta_\text{post} \times \texttt{post-power}_i + \epsilon_i
\end{align}\]</span>
Parameter estimates and tests are provided in Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-ApproachAdvantage-ANCOVA">9.5</a>. As in the earlier ANOVA (Table <a href="factorial-anova.html#tab:expBelief-oneway-ANOVA-results">8.1</a>), we obtain a significant effect of experimenter belief. None of the other effects are significant. Neither the subjective feeling of power before or after the priming manipulation appears to affect the approach advantage scores. Interestingly, whilst not significant, the slope of <span class="math inline">\(\texttt{post-power}\)</span> is positive, and the slope of <span class="math inline">\(\texttt{pre-power}\)</span> negative, which is what one would expect when the difference between the two is related to the dependent variable.</p>
<table>
<caption>
<span id="tab:expBelief-ApproachAdvantage-ANCOVA">Table 9.5: </span>Linear model predicting <span class="math inline">\(\texttt{ApproachAdvantage}\)</span> by <span class="math inline">\(\texttt{pre-power}\)</span>, <span class="math inline">\(\texttt{post-power}\)</span>, and factorial contrast-coding predictors.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{SS}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(P(\geq \lvert F \rvert)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
28.396
</td>
<td style="text-align:right;">
18309
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.397
</td>
<td style="text-align:right;">
0.529
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P}\)</span>
</td>
<td style="text-align:right;">
9.823
</td>
<td style="text-align:right;">
9279
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.201
</td>
<td style="text-align:right;">
0.654
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{B}\)</span>
</td>
<td style="text-align:right;">
90.569
</td>
<td style="text-align:right;">
820210
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
17.765
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{pre-power}\)</span>
</td>
<td style="text-align:right;">
-0.573
</td>
<td style="text-align:right;">
15190
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.329
</td>
<td style="text-align:right;">
0.567
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{post-power}\)</span>
</td>
<td style="text-align:right;">
0.452
</td>
<td style="text-align:right;">
9998
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.217
</td>
<td style="text-align:right;">
0.642
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P} \times \texttt{B}\)</span>
</td>
<td style="text-align:right;">
37.883
</td>
<td style="text-align:right;">
35727
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.774
</td>
<td style="text-align:right;">
0.380
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
18190490
</td>
<td style="text-align:right;">
394
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
<p>Note that the slopes of the contrast-coding predictors are somewhat different from those in Table <a href="factorial-anova.html#tab:expBelief-oneway-ANOVA-results">8.1</a>. As before, this is because the slopes in the ANCOVA model represent differences between adjusted means. Analogous to Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:estimate-slope-orthogonal-contrast-codes-ANCOVA-one-covariate">(9.4)</a>, when there are a total of <span class="math inline">\(L\)</span> covariates included in the model, which we can denote as <span class="math inline">\(Z_1, Z_2, \ldots, Z_L\)</span>, the estimate of the slope of the contrast-coding predictors can be written as:
<span class="math display" id="eq:estimate-slope-orthogonal-contrast-codes-ANCOVA-multiple-covariates">\[\begin{equation}
\hat{\beta}_j = \frac{\sum_{k=1}^{g} c_{j,k} \overline{Y}_k}{\sum_{k=1} c_{j,k}^2} - \sum_{l=1}^L \hat{\beta}_{z_l} \frac{\sum_{k=1}^{g} c_{j,k} \overline{Z}_{l,k}}{\sum_{k=1} c_{j,k}^2}
\tag{9.5}
\end{equation}\]</span>
i.e. the total adjustment of the slope consists of the sum of adjustments for each covariate.</p>
</div>
<div id="mediation-with-categorical-independent-variables" class="section level2">
<h2><span class="header-section-number">9.7</span> Mediation with categorical independent variables</h2>
<p>The finding that experimenter belief has an effect on participants’ approach advantage indicates that experimenter expectations affect participants behaviour, even though the experimenters in the study of <span class="citation">Gilder &amp; Heerey (<a href="#ref-gilder2018role" role="doc-biblioref">2018</a>)</span> asserted that their knowledge of the condition had not affected their behaviour towards the participants. The question is then how experimenter belief changed their interaction with the participants to change their approach advantage scores. To attempt to answer this question, we assess whether experimenter belief changed participants’ perception of the experimenters, and whether such changes resulted in the difference in approach advantage. Thus, as another example of mixing categorical and metric independent variables, we can consider assessing whether the effect of experimenter belief on approach advantage is mediated by participants’ perceptions of the experimenters.</p>
<p>At the end of the experiment, participants rated how attractive, competent, friendly, and trustworthy they found their experimenter. Preliminary analysis (not shown here for brevity) indicates that when experimenters believed participants were assigned to the high-power condition, they were rated as more attractive, friendly, and trustworthy. If the effect of experimenter belief is mediated by these changes in perception, then after including the ratings of attractiveness, friendliness, and trustworthiness, we would expect the effect of experimenter belief to be reduced. However, the results of the analysis (see Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-ApproachAdvantage-mediation-ANCOVA">9.6</a>) are only slighly suggestive of this. Controlling for attractiveness, friendliness, and trustworthiness, we still obtain a highly significant effect of experimenter belief, with a slope which is a little lower than in a model without these covariates. But none of the covariates appears to be related to the approach advantage.</p>
<table>
<caption>
<span id="tab:expBelief-ApproachAdvantage-mediation-ANCOVA">Table 9.6: </span>Linear model predicting <span class="math inline">\(\texttt{ApproachAdvantage}\)</span> by <span class="math inline">\(\texttt{pre-power}\)</span>, <span class="math inline">\(\texttt{post-power}\)</span>, and factorial contrast-coding predictors.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{SS}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(P(\geq \lvert F \rvert)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
-29.611
</td>
<td style="text-align:right;">
12777
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.277
</td>
<td style="text-align:right;">
0.599
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P}\)</span>
</td>
<td style="text-align:right;">
10.895
</td>
<td style="text-align:right;">
11704
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.253
</td>
<td style="text-align:right;">
0.615
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{B}\)</span>
</td>
<td style="text-align:right;">
85.603
</td>
<td style="text-align:right;">
698114
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
15.120
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{attractive}\)</span>
</td>
<td style="text-align:right;">
-0.792
</td>
<td style="text-align:right;">
561
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.012
</td>
<td style="text-align:right;">
0.912
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{friendly}\)</span>
</td>
<td style="text-align:right;">
10.378
</td>
<td style="text-align:right;">
39004
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.845
</td>
<td style="text-align:right;">
0.359
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{trustworthy}\)</span>
</td>
<td style="text-align:right;">
-0.889
</td>
<td style="text-align:right;">
253
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.005
</td>
<td style="text-align:right;">
0.941
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P} \times \texttt{B}\)</span>
</td>
<td style="text-align:right;">
39.049
</td>
<td style="text-align:right;">
37861
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.820
</td>
<td style="text-align:right;">
0.366
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
18144858
</td>
<td style="text-align:right;">
393
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
</div>
<div id="ancova-vs-difference-scores" class="section level2">
<h2><span class="header-section-number">9.8</span> ANCOVA vs difference scores</h2>
<p>To end the chapter, I want to discuss an alternative method to assess whether the power prime manipulation was effective. Note that the main effect of power prime reported in Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-power-post-ANOVA">9.1</a> just reflects a difference between high-power and low-power priming conditions. That there is a difference between the conditions does not really tell us exactly what the priming task did to participants’ feeling of power. It could be that unscrambling high-power sentences increases feelings of power, and unscrambling low-power sentences decreases feelings of power. But it could also be that the task increased feelings of power in both conditions, but more so in the high-power conditions. Or it could be that the task decreased feelings of power in both conditions, but less so in the high-power conditions. In all these cases, the mean would be higher in the high-power conditions than in the low-power conditions, but the effect of the priming task is rather different. To more directly assess whether the high-power prime <em>increased</em>, and the low-power prime <em>decreased</em> participants’ subjective feeling of power, we can consider using the difference <span class="math inline">\(\texttt{diff-power}_i = \texttt{post-power} - \texttt{pre-power}\)</span> as dependent variable in the model
<span class="math display">\[\texttt{diff-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \epsilon_i\]</span>
The model results are given in Table <a href="mixing-categorical-and-metric-predictors-ancova.html#tab:expBelief-diff-power-ANOVA">9.7</a>. We again find a significant and positive effect of power prime. We also find that the intercept is close to 0 and non-significant. That indicates that averaged over all conditions, the subjective feeling of power seems to neither increase or decrease. However, the significant positive slope of power prime shows that the high-power prime increased, and the low-power prime decreased, subjective feeling of power.</p>
<table>
<caption>
<span id="tab:expBelief-diff-power-ANOVA">Table 9.7: </span>Linear model predicting <span class="math inline">\(\texttt{diff-power}\)</span> by factorial contrast-coding predictors.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{SS}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(P(\geq \lvert F \rvert)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
0.028
</td>
<td style="text-align:right;">
3.21e-01
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.002
</td>
<td style="text-align:right;">
0.962
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P}\)</span>
</td>
<td style="text-align:right;">
4.575
</td>
<td style="text-align:right;">
2.09e+03
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
14.806
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{B}\)</span>
</td>
<td style="text-align:right;">
-0.105
</td>
<td style="text-align:right;">
1.11e+00
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.008
</td>
<td style="text-align:right;">
0.930
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{P} \times \texttt{B}\)</span>
</td>
<td style="text-align:right;">
0.373
</td>
<td style="text-align:right;">
3.48e+00
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.025
</td>
<td style="text-align:right;">
0.875
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
5.60e+04
</td>
<td style="text-align:right;">
396
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
<p>Although a model with a difference score as dependent variable is straightforward to interpret, when we are mainly interested in determining group differences (i.e. the effects of the contrast-coding predictors), there are reasons to prefer an ANCOVA model (e.g. Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:post-power-ANCOVA">(9.2)</a>) to this analysis. If we write out the difference score model as
<span class="math display">\[\texttt{post-power}_i - \texttt{pre-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \epsilon_i\]</span>
we can add <span class="math inline">\(\texttt{pre-power}\)</span> to both sides of the equation to obtain
<span class="math display">\[\texttt{post-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \texttt{pre-power}_i + \epsilon_i\]</span>
Note that this model is the same model as in Equation <a href="mixing-categorical-and-metric-predictors-ancova.html#eq:post-power-ANCOVA">(9.2)</a> if we fix <span class="math inline">\(\beta_\text{pre} = 1\)</span>. In other words, the difference model is a special case of the ANCOVA model, where we assume we know the slope of the covariate equals 1. If this assumption is (approximately) true, then the difference model can be preferred, because it has one parameter less to estimate. If, however, the true slope of the covariate is different from 1, then the difference model is less accurate than the ANCOVA model. If the slope of the covariate is sufficiently different from 1, then the ANCOVA model will provide more powerful tests of the contrasts than the difference model <span class="citation">(Judd et al., <a href="#ref-judd2011data" role="doc-biblioref">2011</a>)</span>. In addition, if one is interested in modelling changes within participants, then approaches such as repeated-measures ANOVA and linear mixed-effects models are a better choice.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-gilder2018role">
<p>Gilder, T. S. E., &amp; Heerey, E. A. (2018). The role of experimenter belief in social priming. <em>Psychological Science</em>, <em>29</em>, 403–417.</p>
</div>
<div id="ref-judd2011data">
<p>Judd, C. M., McClelland, G. H., &amp; Ryan, C. S. (2011). <em>Data analysis: A model comparison approach</em>. Routledge.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="19">
<li id="fn19"><p>Although my reasoning why the power-prime manipulation might have a different effect for those participants with an initial high or low feeling of power might suggest otherwise, this was to illustrate why you <em>might</em> suspect an interaction, it is not a hypothesis I would stand by personally. I’m by no means an expert in the scientific study of social power.<a href="mixing-categorical-and-metric-predictors-ancova.html#fnref19" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="factorial-anova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="repeated-measures-anova.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"source": null,
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
