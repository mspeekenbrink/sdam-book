<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Introduction to Bayesian estimation | Statistics: Data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Introduction to Bayesian estimation | Statistics: Data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Introduction to Bayesian estimation | Statistics: Data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2023-12-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-SEM-latent-variable-models.html"/>
<link rel="next" href="ch-Bayes-factors.html"/>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="book_assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="book_assets/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.10/grViz.js"></script>
<script src="book_assets/plotly-binding-4.10.3/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-2.11.1/plotly-latest.min.js"></script>
<link href="book_assets/combineWidgetStyle-0.1/combineWidgets.css" rel="stylesheet" />
<script src="book_assets/combineWidgets-binding-0.11.1/combineWidgets.js"></script>
<script src="book_assets/rglWebGL-binding-1.2.1/rglWebGL.js"></script>
<link href="book_assets/rglwidgetClass-1.2.1/rgl.css" rel="stylesheet" />
<script src="book_assets/rglwidgetClass-1.2.1/rglClass.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/utils.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/buffer.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/subscenes.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/shaders.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/shadersrc.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/textures.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/projection.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/mouse.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/init.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/pieces.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/draw.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/controls.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/selection.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/rglTimer.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/pretty.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/axes.src.js"></script>
<script src="book_assets/rglwidgetClass-1.2.1/animation.src.js"></script>
<script src="book_assets/CanvasMatrix4-1.2.1/CanvasMatrix.src.js"></script>
<script src="book_assets/rglPlayer-binding-1.2.1/rglPlayer.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ch-intro.html"><a href="ch-intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="ch-intro.html"><a href="ch-intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-modeling.html"><a href="ch-modeling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-probability-definition"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-binomial-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="ch-modeling.html"><a href="ch-modeling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-modeling.html"><a href="ch-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-modeling.html"><a href="ch-modeling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-likelihood-ratio"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-modeling.html"><a href="ch-modeling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-modeling.html"><a href="ch-modeling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-modeling.html"><a href="ch-modeling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-modeling.html"><a href="ch-modeling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="ch-modeling.html"><a href="ch-modeling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="ch-modeling.html"><a href="ch-modeling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-a-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has a specific value</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#ch3-confidence-interval"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#effect-size"><i class="fa fa-check"></i><b>3.6</b> Effect size</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.7</b> Assumptions</a></li>
<li class="chapter" data-level="3.8" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.8</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.8.1</b> The Central Limit Theorem in action</a></li>
<li class="chapter" data-level="3.8.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#bootstrapping-a-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Bootstrapping a statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#in-practice"><i class="fa fa-check"></i><b>3.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>4.4.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#summary-2"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-transformations"><i class="fa fa-check"></i><b>5.7.1</b> Transforming variables</a></li>
<li class="chapter" data-level="5.7.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.7.2</b> Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.8</b> Multicollinearity: Redundancy between predictors</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.8.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-outliers"><i class="fa fa-check"></i><b>5.9</b> Outliers</a></li>
<li class="chapter" data-level="5.10" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#in-practice-1"><i class="fa fa-check"></i><b>5.10</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#sec:05-dont-forget-about-the-fun"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#sec-glm-mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#sec:06-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-size-in-anova"><i class="fa fa-check"></i><b>7.6</b> Effect-size in ANOVA</a></li>
<li class="chapter" data-level="7.7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#assumptions"><i class="fa fa-check"></i><b>7.7</b> Assumptions</a></li>
<li class="chapter" data-level="7.8" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.8</b> Multiple testing and post-hoc tests</a></li>
<li class="chapter" data-level="7.9" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#in-practice-2"><i class="fa fa-check"></i><b>7.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#sec:06-main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#the-factorial-anova-model"><i class="fa fa-check"></i><b>8.3</b> The factorial ANOVA model</a></li>
<li class="chapter" data-level="8.4" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.4</b> A threeway factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#interpreting-interactions"><i class="fa fa-check"></i><b>8.4.1</b> Interpreting interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.5</b> Orthogonal contrast codes and unequal sample sizes</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.5.1</b> Comparison schemes and SS types</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#in-practice-3"><i class="fa fa-check"></i><b>8.6</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#subjective-feelings-of-power-and-priming"><i class="fa fa-check"></i><b>9.1</b> Subjective feelings of power and priming</a></li>
<li class="chapter" data-level="9.2" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#acounting-for-pre-existing-differences"><i class="fa fa-check"></i><b>9.2</b> Acounting for pre-existing differences</a></li>
<li class="chapter" data-level="9.3" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#slopes-of-contrast-coded-predictors-in-ancova-models"><i class="fa fa-check"></i><b>9.3</b> Slopes of contrast-coded predictors in ANCOVA models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#homogeneity-of-slopes"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of slopes</a></li>
<li class="chapter" data-level="9.5" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#sec:ANCOVA-power"><i class="fa fa-check"></i><b>9.5</b> Power considerations in ANCOVA</a></li>
<li class="chapter" data-level="9.6" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#models-with-multiple-covariates"><i class="fa fa-check"></i><b>9.6</b> Models with multiple covariates</a></li>
<li class="chapter" data-level="9.7" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#mediation-with-categorical-independent-variables"><i class="fa fa-check"></i><b>9.7</b> Mediation with categorical independent variables</a></li>
<li class="chapter" data-level="9.8" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#ancova-vs-difference-scores"><i class="fa fa-check"></i><b>9.8</b> ANCOVA vs difference scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#non-independence-in-linear-models"><i class="fa fa-check"></i><b>10.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="10.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#the-cheerleader-effect"><i class="fa fa-check"></i><b>10.2</b> The cheerleader effect</a></li>
<li class="chapter" data-level="10.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#as-a-oneway-anova"><i class="fa fa-check"></i><b>10.3</b> As a oneway ANOVA</a></li>
<li class="chapter" data-level="10.4" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#oneway-repeated-measures-anova"><i class="fa fa-check"></i><b>10.4</b> Oneway repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#within-subjects-composite-scores"><i class="fa fa-check"></i><b>10.4.1</b> Within-subjects composite scores</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-composite-for-between-subjects-effects"><i class="fa fa-check"></i><b>10.4.2</b> A composite for between-subjects effects</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#collecting-all-results-and-omnibus-tests"><i class="fa fa-check"></i><b>10.4.3</b> Collecting all results and omnibus tests</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#partitioning-the-variance"><i class="fa fa-check"></i><b>10.5</b> Partitioning the variance</a></li>
<li class="chapter" data-level="10.6" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-mixed-anova-with-between--and-within-subjects-effects"><i class="fa fa-check"></i><b>10.6</b> A mixed ANOVA with between- and within-subjects effects</a></li>
<li class="chapter" data-level="10.7" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#assumptions-1"><i class="fa fa-check"></i><b>10.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#omnibus-tests-and-sphericity"><i class="fa fa-check"></i><b>10.7.1</b> Omnibus tests and sphericity</a></li>
<li class="chapter" data-level="10.7.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#correcting-for-non-sphericity"><i class="fa fa-check"></i><b>10.7.2</b> Correcting for non-sphericity</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#in-practice-4"><i class="fa fa-check"></i><b>10.8</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#non-independence-in-linear-models-1"><i class="fa fa-check"></i><b>11.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="11.2" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#random-intercept-models"><i class="fa fa-check"></i><b>11.2</b> Random intercept models</a></li>
<li class="chapter" data-level="11.3" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-estimation-1"><i class="fa fa-check"></i><b>11.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="11.4" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-inference"><i class="fa fa-check"></i><b>11.4</b> Parameter inference</a></li>
<li class="chapter" data-level="11.5" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#application-of-the-random-intercepts-model"><i class="fa fa-check"></i><b>11.5</b> Application of the random-intercepts model</a></li>
<li class="chapter" data-level="11.6" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#models-with-random-intercepts-and-slopes"><i class="fa fa-check"></i><b>11.6</b> Models with random intercepts and slopes</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#correlation-between-random-effects"><i class="fa fa-check"></i><b>11.6.1</b> Correlation between random effects</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#crossed-random-effects-dating-partners-in-the-speed-dating-experiment"><i class="fa fa-check"></i><b>11.7</b> Crossed random effects: dating partners in the speed dating experiment</a></li>
<li class="chapter" data-level="11.8" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#choosing-the-random-effects-structure"><i class="fa fa-check"></i><b>11.8</b> Choosing the random effects structure</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html"><i class="fa fa-check"></i><b>12</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#link-functions"><i class="fa fa-check"></i><b>12.1</b> Link functions</a></li>
<li class="chapter" data-level="12.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#estimation-1"><i class="fa fa-check"></i><b>12.2</b> Estimation</a></li>
<li class="chapter" data-level="12.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#inference-in-generalized-linear-models"><i class="fa fa-check"></i><b>12.3</b> Inference in generalized linear models</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#wald-test"><i class="fa fa-check"></i><b>12.3.1</b> Wald test</a></li>
<li class="chapter" data-level="12.3.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>12.3.2</b> Likelihood-ratio test</a></li>
<li class="chapter" data-level="12.3.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#confidence-intervals-1"><i class="fa fa-check"></i><b>12.3.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-generalized-linear-models-overall-model-fit"><i class="fa fa-check"></i><b>12.4</b> Assessing model fit</a></li>
<li class="chapter" data-level="12.5" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#logistic-regression"><i class="fa fa-check"></i><b>12.5</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#parameter-interpretation"><i class="fa fa-check"></i><b>12.5.1</b> Parameter interpretation</a></li>
<li class="chapter" data-level="12.5.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-logistic-regression-metacognition"><i class="fa fa-check"></i><b>12.5.2</b> Example: Metacognition in visual perception</a></li>
<li class="chapter" data-level="12.5.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#using-a-different-link-function-probit-regression"><i class="fa fa-check"></i><b>12.5.3</b> Using a different link function: Probit regression</a></li>
<li class="chapter" data-level="12.5.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#welcome-back-paul"><i class="fa fa-check"></i><b>12.5.4</b> Welcome back Paul!</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>12.6</b> Poisson regression</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-poisson-regression-gestures"><i class="fa fa-check"></i><b>12.6.1</b> Example: Gestures in different social contexts</a></li>
<li class="chapter" data-level="12.6.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#overdispersion"><i class="fa fa-check"></i><b>12.6.2</b> Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#log-linear-models"><i class="fa fa-check"></i><b>12.7</b> Log-linear models</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#example-newspapers-and-voting"><i class="fa fa-check"></i><b>12.7.1</b> Example: Newspapers and voting</a></li>
<li class="chapter" data-level="12.7.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#a-three-way-table-example-rock-paper-scissors"><i class="fa fa-check"></i><b>12.7.2</b> A three-way table example: Rock-Paper-Scissors</a></li>
<li class="chapter" data-level="12.7.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sparse-data-and-empty-cells"><i class="fa fa-check"></i><b>12.7.3</b> Sparse data and empty cells</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.8</b> Multinomial logistic regression</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#baseline-category-logit"><i class="fa fa-check"></i><b>12.8.1</b> Baseline category logit</a></li>
<li class="chapter" data-level="12.8.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#example-rock-paper-scissors"><i class="fa fa-check"></i><b>12.8.2</b> Example: Rock-Paper-Scissors</a></li>
<li class="chapter" data-level="12.8.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#reconstructing-probabilities-of-responses"><i class="fa fa-check"></i><b>12.8.3</b> Reconstructing probabilities of responses</a></li>
<li class="chapter" data-level="12.8.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#alternative-logit-models-for-ordinal-categories"><i class="fa fa-check"></i><b>12.8.4</b> Alternative logit models for ordinal categories</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-glmer"><i class="fa fa-check"></i><b>12.9</b> Generalized linear mixed-effects models</a></li>
<li class="chapter" data-level="12.10" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#in-practice-5"><i class="fa fa-check"></i><b>12.10</b> In practice</a></li>
<li class="chapter" data-level="12.11" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#further-reading"><i class="fa fa-check"></i><b>12.11</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html"><i class="fa fa-check"></i><b>13</b> Path models (SEM 1)</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#graphical-models"><i class="fa fa-check"></i><b>13.1</b> Graphical models</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#exogenous-and-endogenous-variables"><i class="fa fa-check"></i><b>13.1.1</b> Exogenous and endogenous variables</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#regression-models"><i class="fa fa-check"></i><b>13.2</b> Regression models</a></li>
<li class="chapter" data-level="13.3" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#sec-sem-path-model-mediation"><i class="fa fa-check"></i><b>13.3</b> Mediation</a></li>
<li class="chapter" data-level="13.4" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#assumptions-and-estimation"><i class="fa fa-check"></i><b>13.4</b> Assumptions and estimation</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>13.4.1</b> The multivariate Normal distribution</a></li>
<li class="chapter" data-level="13.4.2" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#assumptions-exogenous-vs-endogenous-variables"><i class="fa fa-check"></i><b>13.4.2</b> Assumptions: Exogenous vs endogenous variables</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#model-fit"><i class="fa fa-check"></i><b>13.5</b> Model fit</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#test-of-overall-model-fit"><i class="fa fa-check"></i><b>13.5.1</b> Test of overall model fit</a></li>
<li class="chapter" data-level="13.5.2" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#approximate-fit-indices"><i class="fa fa-check"></i><b>13.5.2</b> Approximate fit indices</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#modification-indices"><i class="fa fa-check"></i><b>13.6</b> Modification indices</a></li>
<li class="chapter" data-level="13.7" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#model-comparison-1"><i class="fa fa-check"></i><b>13.7</b> Model comparison</a></li>
<li class="chapter" data-level="13.8" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#evaluation-and-selection-of-the-mediation-path-models"><i class="fa fa-check"></i><b>13.8</b> Evaluation and selection of the mediation path models</a></li>
<li class="chapter" data-level="13.9" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#a-more-complex-path-model"><i class="fa fa-check"></i><b>13.9</b> A more complex path model</a></li>
<li class="chapter" data-level="13.10" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#principles-in-constructing-path-models"><i class="fa fa-check"></i><b>13.10</b> Principles in constructing path models</a>
<ul>
<li class="chapter" data-level="13.10.1" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#identifiability"><i class="fa fa-check"></i><b>13.10.1</b> Identifiability</a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#model-equivalence"><i class="fa fa-check"></i><b>13.11</b> Model equivalence</a></li>
<li class="chapter" data-level="13.12" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html#correlation-vs-causation"><i class="fa fa-check"></i><b>13.12</b> Correlation vs causation</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html"><i class="fa fa-check"></i><b>14</b> Latent variable models (SEM 2)</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#measurement-of-latent-variables"><i class="fa fa-check"></i><b>14.1</b> Measurement of latent variables</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#scaling-and-identification"><i class="fa fa-check"></i><b>14.1.1</b> Scaling and identification</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#confirmatory-factor-analysis"><i class="fa fa-check"></i><b>14.2</b> Confirmatory factor analysis</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#a-one-factor-model"><i class="fa fa-check"></i><b>14.2.1</b> A one-factor model</a></li>
<li class="chapter" data-level="14.2.2" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#heywood-cases"><i class="fa fa-check"></i><b>14.2.2</b> Heywood cases</a></li>
<li class="chapter" data-level="14.2.3" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#full-hierarchical-factor-model-for-the-bfi-2"><i class="fa fa-check"></i><b>14.2.3</b> Full hierarchical factor model for the BFI-2</a></li>
<li class="chapter" data-level="14.2.4" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#predicting-factor-scores"><i class="fa fa-check"></i><b>14.2.4</b> Predicting factor scores</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#exploratory-factor-and-principal-components-analysis"><i class="fa fa-check"></i><b>14.3</b> Exploratory factor and principal components analysis</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#determining-the-number-of-factors"><i class="fa fa-check"></i><b>14.3.1</b> Determining the number of factors</a></li>
<li class="chapter" data-level="14.3.2" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#factor-rotation"><i class="fa fa-check"></i><b>14.3.2</b> Factor rotation</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html#in-practice-6"><i class="fa fa-check"></i><b>14.4</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html"><i class="fa fa-check"></i><b>15</b> Introduction to Bayesian estimation</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#fundamentals-of-bayesian-inference"><i class="fa fa-check"></i><b>15.1</b> Fundamentals of Bayesian inference</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#probability-in-times-of-covid"><i class="fa fa-check"></i><b>15.1.1</b> Probability in times of Covid</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#bayes-rule"><i class="fa fa-check"></i><b>15.1.2</b> Bayes’ rule</a></li>
<li class="chapter" data-level="15.1.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#we-missed-you-paul"><i class="fa fa-check"></i><b>15.1.3</b> We missed you Paul!</a></li>
<li class="chapter" data-level="15.1.4" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#the-marginal-likelihood-and-prior-predictive-distribution"><i class="fa fa-check"></i><b>15.1.4</b> The marginal likelihood and prior predictive distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#markov-chain-monte-carlo-mcmc"><i class="fa fa-check"></i><b>15.2</b> Markov chain Monte Carlo (MCMC)</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#convergence"><i class="fa fa-check"></i><b>15.2.1</b> Convergence</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#autocorrelation"><i class="fa fa-check"></i><b>15.2.2</b> Autocorrelation</a></li>
<li class="chapter" data-level="15.2.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#effective-sample-size"><i class="fa fa-check"></i><b>15.2.3</b> Effective sample size</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#concerning-prior-distributions"><i class="fa fa-check"></i><b>15.3</b> Concerning prior distributions</a></li>
<li class="chapter" data-level="15.4" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#multiple-regression-example"><i class="fa fa-check"></i><b>15.4</b> Multiple regression example</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html"><i class="fa fa-check"></i><b>16</b> Introduction to Bayesian hypothesis testing</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#hypothesis-testing-relative-evidence-and-the-bayes-factor"><i class="fa fa-check"></i><b>16.1</b> Hypothesis testing, relative evidence, and the Bayes factor</a></li>
<li class="chapter" data-level="16.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#parameter-estimates-and-credible-intervals"><i class="fa fa-check"></i><b>16.2</b> Parameter estimates and credible intervals</a></li>
<li class="chapter" data-level="16.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#a-bayesian-t-test"><i class="fa fa-check"></i><b>16.3</b> A Bayesian t-test</a></li>
<li class="chapter" data-level="16.4" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#bayes-factors-for-general-linear-models"><i class="fa fa-check"></i><b>16.4</b> Bayes factors for General Linear Models</a></li>
<li class="chapter" data-level="16.5" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#some-objections-to-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>16.5</b> Some objections to null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-is-not-a-proper-measure-of-evidential-support"><i class="fa fa-check"></i><b>16.5.1</b> The <span class="math inline">\(p\)</span>-value is not a proper measure of evidential support</a></li>
<li class="chapter" data-level="16.5.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-depends-on-researcher-intentions"><i class="fa fa-check"></i><b>16.5.2</b> The <span class="math inline">\(p\)</span>-value depends on researcher intentions</a></li>
<li class="chapter" data-level="16.5.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#results-of-a-nhst-are-often-misinterpreted"><i class="fa fa-check"></i><b>16.5.3</b> Results of a NHST are often misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#to-bayes-or-not-to-bayes-a-pragmatic-view"><i class="fa fa-check"></i><b>16.6</b> To Bayes or not to Bayes? A pragmatic view</a></li>
<li class="chapter" data-level="16.7" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#in-practice-7"><i class="fa fa-check"></i><b>16.7</b> In practice</a></li>
<li class="chapter" data-level="16.8" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#summary-3"><i class="fa fa-check"></i><b>16.8</b> “Summary”</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html"><i class="fa fa-check"></i><b>17</b> Being a responsible data analyst</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#consider-analysis-before-data-collection"><i class="fa fa-check"></i><b>17.1</b> Consider analysis before data collection</a></li>
<li class="chapter" data-level="17.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#explore-the-data"><i class="fa fa-check"></i><b>17.2</b> Explore the data</a></li>
<li class="chapter" data-level="17.3" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#evaluate-the-assumptions-underlying-your-analyses"><i class="fa fa-check"></i><b>17.3</b> Evaluate the assumptions underlying your analyses</a></li>
<li class="chapter" data-level="17.4" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#distinguish-between-confirmatory-and-exploratory-analyses"><i class="fa fa-check"></i><b>17.4</b> Distinguish between confirmatory and exploratory analyses</a></li>
<li class="chapter" data-level="17.5" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#aim-for-openness-and-reproducibility"><i class="fa fa-check"></i><b>17.5</b> Aim for openness and reproducibility</a></li>
<li class="chapter" data-level="17.6" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#communicate-clearly-and-concisely"><i class="fa fa-check"></i><b>17.6</b> Communicate clearly and concisely</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-multiple-regression-analysis"><i class="fa fa-check"></i><b>17.6.1</b> Example of reporting a multiple regression analysis</a></li>
<li class="chapter" data-level="17.6.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-factorial-anova"><i class="fa fa-check"></i><b>17.6.2</b> Example of reporting a factorial ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: Data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-Bayes-estimation" class="section level1 hasAnchor" number="15">
<h1><span class="header-section-number">Chapter 15</span> Introduction to Bayesian estimation<a href="ch-Bayes-estimation.html#ch-Bayes-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter, we will introduce an alternative to maximum likelihood estimation of statistical models: Bayesian estimation. Bayesian estimation concerns revising beliefs in light of observed data. It does not produce a single parameter estimate, but rather a distribution over the possible parameter values. Prior beliefs about the possible parameter values are encoded in the prior distribution. In light of observed data, this is then updated to the posterior distribution.</p>
<div id="fundamentals-of-bayesian-inference" class="section level2 hasAnchor" number="15.1">
<h2><span class="header-section-number">15.1</span> Fundamentals of Bayesian inference<a href="ch-Bayes-estimation.html#fundamentals-of-bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="probability-in-times-of-covid" class="section level3 hasAnchor" number="15.1.1">
<h3><span class="header-section-number">15.1.1</span> Probability in times of Covid<a href="ch-Bayes-estimation.html#probability-in-times-of-covid" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s start with a classic example in a topical guise. With the aim to move to a new phase in the Covid-19 pandemic, the UK government planned to employ mass testing as part of a wider strategy also involving general vaccination (<a href="https://www.theguardian.com/world/2020/dec/01/funds-for-mass-covid-testing-offered-to-local-authorities-in-tier-3">Guardian, 1 December 2020</a>).<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a> The testing involves lateral flow tests, which are relatively inexpensive and give a result in about 20 minutes. At the time, estimates were that these tests give a positive test result in 76.8% of cases of Covid-19, and a negative test result in 99.68% of cases (<a href="https://www.gov.uk/government/news/oxford-university-and-phe-confirm-high-sensitivity-of-lateral-flow-tests">Department of Health and Social Care, 11 November 2020</a>).<a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a>. The true positive rate (76.8%) is also called the <strong>sensitivity</strong> of a test, and the true negative rate (99.68%) the <strong>specificity</strong>. On 26 November 2020, the <a href="https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/conditionsanddiseases/bulletins/coronaviruscovid19infectionsurveypilot/26november2020">Office of National Statistics</a> estimated the rate of Covid-19 cases in the general population of England to be 1.17%. That implies that about 1 in 85 people carried the virus, which is also called the <strong>base rate</strong>. Suppose someone is tested and the test result is positive. What is the probability that they are actually infected with Covid-19? Perhaps surprisingly, this probability does <em>not</em> equal .768. This is the conditional probability <span class="math inline">\(p(\text{positive test}|\text{Covid-19})\)</span>, i.e. the sensitivity. What we would like to know however is a different conditional probability, namely <span class="math inline">\(p(\text{Covid-19}|\text{positive test})\)</span>. And these are <em>not</em> the same!</p>
<p>We can work out the desired probability using the rules of probability discussed in Section <a href="ch-modeling.html#sec:02-rules-of-probability">2.2.1.1</a>.
<span class="math display">\[p(\text{Covid-19}|\text{positive test}) = \frac{p(\text{Covid-19 and positive test})}{p(\text{positive test})}\]</span>
From the multiplication rule, we know that
<span class="math display">\[p(\text{Covid-19 and positive test}) = p(\text{positive test}|\text{Covid-19}) \times p(\text{Covid-19})\]</span>
To work out <span class="math inline">\(p(\text{positive test})\)</span>, we need to consider all the ways in which someone can obtain a positive test result. In this situation, there are two: the person can carry Covid-19 and have a positive test result, or the person can not carry Covid-19 and obtain a positive test result. Thus
<span class="math display">\[p(\text{positive test}) = p(\text{Covid-19 and positive test}) + p(\text{no Covid-19 and positive test})\]</span>
We have already specified how to calculate <span class="math inline">\(p(\text{Covid-19 and positive test})\)</span>. Similarly, we can compute <span class="math inline">\(p(\text{no Covid-19 and positive test})\)</span> as
<span class="math display">\[p(\text{no Covid-19 and positive test}) = p(\text{positive test}|\text{no Covid-19}) \times p(\text{no Covid-19})\]</span>
Now we are in a position to calculate <span class="math inline">\(p(\text{Covid-19}|\text{positive test})\)</span> from the sensitivity and specificity of the test, and the base rate of Covid-19 infection. We know that <span class="math inline">\(p(\text{Covid-19}) = .0117\)</span>, hence <span class="math inline">\(p(\text{no Covid-19}) = 1 - p(\text{Covid-19}) = 1 - .0117 = .9883\)</span>. Putting all the numbers in a table:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Covid-19 (1.17%)</th>
<th align="right">no Covid-19 (98.83%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">positive test</td>
<td align="right">76.8%</td>
<td align="right">0.32%</td>
</tr>
<tr class="even">
<td align="left">negative test</td>
<td align="right">23.2%</td>
<td align="right">99.68%</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[\begin{aligned}
p(\text{Covid-19}|\text{positive test}) &amp;= \frac{p(\text{Covid-19 and positive test})}{p(\text{positive test})} \\
&amp;= \frac{p(\text{positive test}|\text{Covid-19}) p(\text{Covid-19})}{p(\text{pos. test}|\text{Covid-19}) p(\text{Covid-19}) + p(\text{pos. test}|\text{no Covid-19}) p(\text{no Covid-19})} \\
&amp;= \frac{.768 \times .0117}{.768 \times .0117 + .0032 \times .9883} \\
&amp;= 0.7396
\end{aligned}\]</span>
So just under 3 out of all 4 people that test positive in this scenario would actually carry Covid-19. As a result, 1 out of 4 people might be asked to quarantine without really needing to do so. Although of course very unfortunate for those people, that does not seem like a too-high price to pay to me. But the base-rate is very important here. If the rate of Covid-19 infections is lowered to <span class="math inline">\(p(\text{Covid-19}) = .001\)</span> (i.e. 0.1%), then the result would be <span class="math inline">\(p(\text{Covid-19}|\text{positive test}) = 0.1937\)</span>, which means that only about one in five people who test positive are actually infected by Covid-19! When the base-rate is lowered, massive testing seems like a much less useful procedure.</p>
<p>Perhaps the equations seem a little abstract. Another way to explain the resulting conditional probability is through the tree diagram of Figure <a href="ch-Bayes-estimation.html#fig:covid-19-test-tree">15.1</a>. The tree represents a group of 100,000 people from the general population, of which 1,176 would have Covid-19, and 903 of these would also get a positive test result. Of the 98,824 people without Covid-19, 316 would receive a positive test result. While the change of a false positive is very low, because so many people do not have Covid-19, the actual frequency of people without Covid-19 who obtain a positive test result is not that much smaller than the number of people with Covid-19 who obtain a positive test result. The conditional probability can then be computed simply as
<span class="math display">\[p(\text{Covid-19}|\text{positive test}) = \frac{903}{903 + 316} = .74\]</span>
which is equal to the value computed earlier (up to rounding error resulting from converting probabilities to whole people).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:covid-19-test-tree"></span>
<div class="grViz html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-ff16b0b50c475e8ecba0" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-ff16b0b50c475e8ecba0">{"x":{"diagram":"\n  digraph cvid_tree_plot {\n\n    graph [layout = dot,\n           rankdir = LR]\n\n    node [shape = box] \n    S [label = \"100,000 people\"]\n    \n    node [shape = oval, style = filled]\n    H [label = \"Covid-19\", fillcolor = \"#cc0000\"]\n    T [label = \"no Covid-19\", fillcolor = \"#00cc00\"]\n    HH [label = \"positive test\", fillcolor = \"#ff0000\" ]\n    HT [label = \"negative test\", fillcolor = \"#990000\"]\n    TH [label = \"positive-test\", fillcolor =  \"#00ff00\"]\n    TT [label = \"negative-test\", fillcolor = \"#009900\"]\n    \n    edge [label = \"1,176\"]\n    S -> {H}\n    \n    edge [label = \"98,824\", labelloc=b]\n    S -> {T}\n    \n    edge [label = \"903\"]\n    H -> {HH}\n    \n    edge [label = \"273\", labelloc=b]\n    H -> {HT}\n    \n    edge [label = \"316\" ]\n    T -> {TH}\n    \n    edge [label = \"98,508\", labelloc=b]\n    T -> {TT}\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 15.1: Outcome tree representing mass testing for Covid-19.
</p>
</div>
</div>
<div id="bayes-rule" class="section level3 hasAnchor" number="15.1.2">
<h3><span class="header-section-number">15.1.2</span> Bayes’ rule<a href="ch-Bayes-estimation.html#bayes-rule" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In calculating a conditional probability from other conditional probabilities and base-rates, we have just applied the general rules of probability. That’s nothing special, really. In abstract notation, the formula known as Bayes’ rule is</p>
<p><span class="math display" id="eq:bayes-rule">\[\begin{equation}
p(A|B) = \frac{p(B|A) p(A)}{p(B|A)p(A) + p(B|\neg A) p(\neg A)}
\tag{15.1}
\end{equation}\]</span></p>
<p>Again, there is nothing special about this formula itself, it follows from the rules of probability. These rules were however not clearly specified when Reverend Thomas Bayes defined the rule in an essay which was posthumously published <span class="citation">(<a href="#ref-bayes1763">Bayes, 1763</a>)</span>. More importantly, he used the rule to infer an unknown parameter of a statistical model. According to the Frequentist View (see Section <a href="ch-modeling.html#sec:02-probability-definition">2.2.1</a>), a parameter has a true value, but you cannot assign a probability to it, because it is not a random event that has a long-run frequency. It just has one value: the true value.</p>
<p>In assigning probabilities to parameters, Thomas Bayes can be seen as the founding father of the Subjectivist View of probability. There has been quite a lot of philosophical discussion about probability interpretations. The subjectivist view is that a probability represents a rational degree of belief. This belief can be about anything, whether actual events in the world, or more abstract concepts such as hypotheses or model parameters. Bayesian inference concerns adjusting prior beliefs in light of evidence. The resulting adjusted belief is called the posterior belief. In the previous example, the base-rate of Covid-19 infections can be seen as a rational prior belief that a randomly chosen person from the general population in England has Covid-19. Upon observing a positive test result, this <strong>prior probability</strong> <span class="math inline">\(p(\text{Covid-19})\)</span> can be adjusted to become the <strong>posterior probability</strong> <span class="math inline">\(p(\text{Covid-19}|\text{positive test})\)</span>.</p>
</div>
<div id="we-missed-you-paul" class="section level3 hasAnchor" number="15.1.3">
<h3><span class="header-section-number">15.1.3</span> We missed you Paul!<a href="ch-Bayes-estimation.html#we-missed-you-paul" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Bayesian statistics, we can apply the principles of Bayesian inference to anything we can assign degrees of belief to. For instance, our belief that Paul the Octopus had psychic abilities. In our general model of Paul’s predictions (Section <a href="ch-modeling.html#sec:02-binomial-model">2.3</a>, Equation <a href="ch-modeling.html#eq:definition-binomial-distribution">(2.1)</a>), we assumed there was a probability that he made a correct prediction, which we denoted by <span class="math inline">\(\theta\)</span>. This parameter probability could in principle take any value <span class="math inline">\(0 \leq \theta \leq 1\)</span>. The idea of a prior distribution for such a parameter is to assign to each possible value of <span class="math inline">\(\theta\)</span> a “degree of belief” that this is the true value. These degrees of belief should obey the rules of probability. In the coin-flipping model, which assumed Paul was randomly guessing, there was only one possible value, namely <span class="math inline">\(\theta=.5\)</span>. That means that, if we were to believe this model is true, we would consequently believe that any other value is impossible: <span class="math inline">\(p(\theta \neq .5) = 0\)</span>, which implies <span class="math inline">\(p(\theta = .5) = 1\)</span>. If we don’t believe that Paul is necessarily randomly guessing, then the parameter could have other values as well. Figure <a href="ch-Bayes-estimation.html#fig:prior-distributions-Paul">15.2</a> shows two possible prior distributions. In the plot on the left, the prior assigns an equal probability to any possible value of <span class="math inline">\(\theta\)</span>. This is also called a uniform distribution, and reflects the beliefs of someone who considers that “anything goes” when it comes to Paul’s ability to predict the outcome of football matches. In the plot on the right, the prior distribution reflects the beliefs of someone who quite strongly considers Paul a good predictor of the outcome of football matches.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prior-distributions-Paul"></span>
<img src="15-Bayes-estimation_files/figure-html/prior-distributions-Paul-1.svg" alt="Two different prior distributions for the probability that Paul makes a correct prediction, the normalised likelihood function ($p(Y|\theta)/p(Y)$) and the resulting posterior distributions after observing that Paul made $Y=12$ out of $n=14$ correct predictions. Wherever the normalised likelihood is larger than 1, the posterior probability is larger than the prior probability, while the posterior probability is lower than the prior probability wherever the normalised likelihood is lower than 1. Note that in MODEL 1, the normalised likelihood and posterior distribution are identical and therefore overlapping on the plot." width="90%" />
<p class="caption">
Figure 15.2: Two different prior distributions for the probability that Paul makes a correct prediction, the normalised likelihood function (<span class="math inline">\(p(Y|\theta)/p(Y)\)</span>) and the resulting posterior distributions after observing that Paul made <span class="math inline">\(Y=12\)</span> out of <span class="math inline">\(n=14\)</span> correct predictions. Wherever the normalised likelihood is larger than 1, the posterior probability is larger than the prior probability, while the posterior probability is lower than the prior probability wherever the normalised likelihood is lower than 1. Note that in MODEL 1, the normalised likelihood and posterior distribution are identical and therefore overlapping on the plot.
</p>
</div>
<p>We can write the posterior distribution for the parameter <span class="math inline">\(\theta\)</span>, conditional upon observed data <span class="math inline">\(Y\)</span> (e.g., <span class="math inline">\(Y=12\)</span> out of <span class="math inline">\(n=14\)</span> correct predictions) as:
<span class="math display" id="eq:posterior-parameter-specification">\[\begin{align}
p(\theta|Y) &amp;= \frac{p(Y|\theta) \times p(\theta)}{P(Y)} \\
&amp;= \frac{p(Y|\theta)}{p(Y)} \times p(\theta)
\tag{15.2}
\end{align}\]</span>
Here, <span class="math inline">\(p(Y|\theta)\)</span> is the likelihood function (the probability of the observed data <span class="math inline">\(Y\)</span> given a particular value of <span class="math inline">\(\theta\)</span>), <span class="math inline">\(p(\theta)\)</span> is the prior distribution of the parameter <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(p(Y)\)</span> is the probability of the data over all possible values of <span class="math inline">\(\theta\)</span> (weighted by their prior probability). This is also called the <strong>marginal likelihood</strong>, and technically is defined as
<span class="math display">\[p(Y) = \int p(Y|\theta) p(\theta) d \theta\]</span>
i.e. as the integral of the product of the likelihood function and prior over all possible values of <span class="math inline">\(\theta\)</span>, but you don’t need to worry about this. The formulation on the second line of Equation <a href="ch-Bayes-estimation.html#eq:posterior-parameter-specification">(15.2)</a> is just a rearrangement of the terms, but is meant to show that you can think of the posterior probability as the product of the prior probability and the <strong>normalised likelihood</strong> (the likelihood of the data for a particular value of <span class="math inline">\(\theta\)</span> compared to the marginal likelihood of <span class="math inline">\(Y\)</span> over all possible values of <span class="math inline">\(\theta\)</span>).</p>
<p>In words, we can describe Equation <a href="ch-Bayes-estimation.html#eq:posterior-parameter-specification">(15.2)</a> as:
<span class="math display">\[\begin{aligned}
\text{posterior} &amp;= \frac{\text{likelihood} \times \text{prior}}{\text{marginal likelihood}} \\
&amp;= \frac{\text{likelihood}}{\text{marginal likelihood}} \times \text{prior} \\
&amp;= \text{normalised likelihood} \times \text{prior}
\end{aligned}\]</span>
The values of the normalized likelihood and the resulting posterior distribution are also shown in Figure <a href="ch-Bayes-estimation.html#fig:prior-distributions-Paul">15.2</a>. Because the posterior probabilities are calculated by multiplying the prior probability by the normalised likelihood, the posterior probability will be <em>higher</em> than the prior probability when the normalised likelihood is <em>larger</em> than 1. You can think of the normalised likelihood as an average likelihood over all possible values of <span class="math inline">\(\theta\)</span> (we will discuss this in a little more detail soon). So when a particular value of <span class="math inline">\(\theta\)</span> assigns a higher likelihood to the data than the average likelihood of the data, i.e. <span class="math inline">\(\frac{p(Y|\theta)}{p(Y)} &gt; 1\)</span>, the evidence that this value of <span class="math inline">\(\theta\)</span> is the true value increases. Conversely, the posterior probability will be <em>lower</em> than the prior probability when the normalised likelihood is <em>smaller</em> than 1. So when a particular value of <span class="math inline">\(\theta\)</span> assigns a lower likelihood to the data than average, the evidence that this value of <span class="math inline">\(\theta\)</span> is the true value decreases. This, in a nutshell, is how Bayesian inference of parameters works.</p>
<p>Inspecting the posterior distributions resulting from the two models in Figure <a href="ch-Bayes-estimation.html#fig:prior-distributions-Paul">15.2</a>, you can see that the posterior distribution is mostly located at the higher values of <span class="math inline">\(\theta\)</span>, because small values of <span class="math inline">\(\theta\)</span> are not very likely given Paul’s success rate of 12 out of 14 correct predictions. Comparing the posterior distributions between the two models, you may also see that the posterior distribution for MODEL 2 is less dispersed (i.e. “narrower”) than for MODEL 1. This is a direct consequence of the difference in the prior distributions. The uniform distribution of MODEL 1 does not make very precise predictions regarding the likely values of <span class="math inline">\(\theta\)</span> (“anything goes”). The more dispersed the prior is, the more dispersed the posterior will be. The prior distribution of MODEL 2 is, in some sense, much more daring, indicating that low values of <span class="math inline">\(\theta\)</span> are very unlikely <em>a priori</em>. Because these values are not believed to be true in the first place, they are also relatively less likely after observing the data.</p>
</div>
<div id="the-marginal-likelihood-and-prior-predictive-distribution" class="section level3 hasAnchor" number="15.1.4">
<h3><span class="header-section-number">15.1.4</span> The marginal likelihood and prior predictive distribution<a href="ch-Bayes-estimation.html#the-marginal-likelihood-and-prior-predictive-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A Bayesian statistical model consists of both the prior distribution and the likelihood function. Both are integral parts of a Bayesian model. This is different from statistical models in the Frequentist tradition, which only focus on the likelihood. One way to view a Bayesian model is as a <em>hierarchical model</em>, similar to how linear mixed-effects models can be viewed as hierarchical models. With this formulation, it is straightforward to simulate a Bayesian model, by first sampling a random parameter value from the prior distribution <span class="math inline">\(p(\theta)\)</span>, and then using this to sample a value of the dependent variable from the conditional distribution <span class="math inline">\(p(Y|\theta)\)</span>. The resulting marginal distribution of the dependent variable, <span class="math inline">\(p(Y)\)</span>, is also called the <strong>prior predictive</strong> distribution. The sampling scheme is
<span class="math display">\[\begin{aligned}
Y_j &amp;\sim p(Y|\theta_j) \\
\tilde{\theta}_j &amp;\sim p(\theta)
\end{aligned}\]</span>
Not only can we use each sampled parameter value <span class="math inline">\(\tilde{\theta}_j\)</span> to sample a value of the dependent variable, we can also use each to compute the likelihood of the data actually observed, e.g. <span class="math inline">\(p(Y=12|\tilde{\theta}_j)\)</span>. Figure <a href="ch-Bayes-estimation.html#fig:prior-predictive-samples-binomial-bayesian-models">15.3</a> shows 1000 sampled parameter values (prior samples), corresponding samples of the dependent variable (prior predictive samples), and the likelihood value for Paul’s predictions (sampled likelihood), for both models in Figure <a href="ch-Bayes-estimation.html#fig:prior-distributions-Paul">15.2</a>. Note that all these are based on the same set of sampled parameter values <span class="math inline">\(\tilde{\theta}\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prior-predictive-samples-binomial-bayesian-models"></span>
<img src="15-Bayes-estimation_files/figure-html/prior-predictive-samples-binomial-bayesian-models-1.svg" alt="Number of correct predictions (out of 14) for 1000 simulations from both Bayesian models of Figure \@ref(fig:prior-distributions-Paul)." width="576" /><img src="15-Bayes-estimation_files/figure-html/prior-predictive-samples-binomial-bayesian-models-2.svg" alt="Number of correct predictions (out of 14) for 1000 simulations from both Bayesian models of Figure \@ref(fig:prior-distributions-Paul)." width="576" /><img src="15-Bayes-estimation_files/figure-html/prior-predictive-samples-binomial-bayesian-models-3.svg" alt="Number of correct predictions (out of 14) for 1000 simulations from both Bayesian models of Figure \@ref(fig:prior-distributions-Paul)." width="576" />
<p class="caption">
Figure 15.3: Number of correct predictions (out of 14) for 1000 simulations from both Bayesian models of Figure <a href="ch-Bayes-estimation.html#fig:prior-distributions-Paul">15.2</a>.
</p>
</div>
<p>You can see that the prior predictive distribution looks quite similar in shape to the prior distribution, which is because for this model, there is a close link between the probability of a correct prediction (<span class="math inline">\(\theta\)</span>), and the total number of correct predictions (<span class="math inline">\(Y\)</span>). You can also see that the likelihood value is relatively low for many sampled parameters from MODEL 1. That is because there are many relatively low sampled values <span class="math inline">\(\tilde{\theta}\)</span> in MODEL 1, for which the likelihood of 12 out of 14 correct predictions, <span class="math inline">\(p(Y=12|n=14, \theta = \tilde{\theta})\)</span> is low. Model 2 does better in accounting for the observed data, with generally higher likelihood values for the sampled parameters. This better fit to the observed data can also be seen in the prior predictive distribution of MODEL 2, where 12 correct predictions is a common occurrence. In MODEL 1, all numbers of correct predictions are sampled roughly equally often.</p>
<p>The sampled likelihoods can be used to compute an average likelihood for each model. This average is an estimate of the marginal likelihood, and computed simply as
<span class="math display">\[\hat{p}(Y=12) = \frac{\sum_{j=1}^n p(Y=12|\tilde{\theta}_j)}{n}\]</span>
Because most of the sampled likelihoods are low for MODEL 1, the average is relatively low as well: <span class="math inline">\(\hat{p}(Y=12) = 0.0663\)</span>. For MODEL 3, most of the sampled likelihoods are relatively high, and hence the average is higher than for MODEL 1: <span class="math inline">\(\hat{p}(Y=12) = 0.204\)</span>. Because MODEL 2 on average assigns a higher probability to the observed data, it offers a better account of the observed data than MODEL 1. As we will see in Chapter <a href="ch-Bayes-factors.html#ch-Bayes-factors">16</a>, a Bayesian version of the likelihood ratio called the <em>Bayes factor</em> which reflects the ratio of marginal likelihoods for different models, is central in a Bayesian version of hypothesis testing.</p>
<p>There is a close correspondence between the marginal likelihood and the prior predictive distribution. In fact, the marginal likelihood is just the prior predictive distribution evaluated at the observed data points. The prior predictive distribution is the marginal distribution of the data according to the model, whilst the marginal likelihood is the value of this distribution for particular values of the dependent variable. So in this case, with a single observed value, we can also estimate the marginal likelihood by computing the relative frequency of <span class="math inline">\(Y=12\)</span> in the prior predictive distribution. For MODEL 1, this gives <span class="math inline">\(\hat{p}(Y=12) = 0.076\)</span>, and for MODEL 2, it is <span class="math inline">\(\hat{p}(Y=12) = 0.2\)</span>. These two ways of estimating the marginal likelihood are not exactly the same, because they approach the same quantity via a different route.<a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a> And both are subject to random sampling variation due to simulating a limited number of parameter values. Accurately computing marginal likelihoods is, for most models, a rather complicated thing, but we won’t go into the details here.</p>
</div>
</div>
<div id="markov-chain-monte-carlo-mcmc" class="section level2 hasAnchor" number="15.2">
<h2><span class="header-section-number">15.2</span> Markov chain Monte Carlo (MCMC)<a href="ch-Bayes-estimation.html#markov-chain-monte-carlo-mcmc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For most prior distributions <span class="math inline">\(p(\theta)\)</span>, the posterior distribution <span class="math inline">\(p(\theta|Y_1, \ldots, Y_n)\)</span> does not have a closed-form expression, meaning that it is not possible to calculate the posterior probability of each parameter value analytically.<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a> In these cases, it is possible to obtain an “empirical” estimate of the posterior distribution via sampling. The set of sampled parameter values can be used to compute descriptive statics such as the mean and variance. These are estimates of the posterior mean and variance. The set of samples can also be used to compute a Bayesian version of a confidence interval, which is called the (posterior) highest density interval (HDI). The HDI represents a range of the most plausible parameter values.</p>
<p>We used sampling above to obtain a representation of the prior predictive and sampled likelihood functions. There, we sampled parameters directly from the prior distribution, and then looked at the prior predictive distribution and observed data likelihood for those sampled parameters. In principle, it is possible to use a similar scheme to obtain a representation of the posterior distribution, by weighting each sampled parameter value <span class="math inline">\(\tilde{\theta}\)</span> by its likelihood <span class="math inline">\(p(Y_1, \ldots, Y_n|\tilde{\theta})\)</span>. This is a form of <em>Importance Sampling</em>. It tends to not be very efficient for representing the posterior distribution, however, as many parameter values that may have a high probability in the prior distribution may have a low probability in the posterior distribution. Therefore, many sampled parameters <span class="math inline">\(\tilde{\theta}\)</span> will have such a low probability that they end up having little impact on the posterior distribution. This is why alternative sampling schemes have been proposed, which aim to be more efficient. These sampling schemes are generally <em>sequential</em>, in the sense that a new sampled parameter <span class="math inline">\(\tilde{\theta}_{j}\)</span> depends on the previously sampled value <span class="math inline">\(\tilde{\theta}_{j-1}\)</span>. This dependency is only between the new sample and the last sampled value, not any earlier sampled values. In technical terms, such a dependency is called the first-order <em>Markov condition</em>. Therefore, these sampling techniques are referred to as <strong>Markov Chain Monte Carlo</strong>, where <em>Markov Chain</em> refers to the series of dependent samples, and <em>Monte Carlo</em> refers to the random aspect of sampling.<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a></p>
<p>Two classic methods of MCMC are the <strong>Metropolis-Hastings</strong> algorithm, and <strong>Gibbs sampling</strong>. A relatively new addition is <strong>Hamiltonian Monte Carlo</strong>. We won’t go into the technical details of these sampling schemes here. Modern software has robust implementations of the techniques, requiring little or no contribution from the user in implementing them. However, we will highlight some important issues that affect all these methods.</p>
<p>Markov chains are a probabilistic model of how a variable transitions from one “state” to another. In first-order Markov Chains, the current state depends only on the previous state. A well-behaved Markov Chain is said to be ergodic, which means that it is:</p>
<ol style="list-style-type: decimal">
<li><p><strong>irreducible</strong>: From any state <span class="math inline">\(\tilde{\theta}_j\)</span>, the chain can reach any other possible value <span class="math inline">\(\theta&#39;\)</span> eventually. If the chain is reducible, it is not possible to arrive at a state from some other states. That implies the chain can get “stuck”, never visiting some states which are actually possible.</p></li>
<li><p><strong>aperiodic</strong>: The chain does not have any periodic states. In a periodic chain, some states can only be visited on every <span class="math inline">\(k\)</span>-th iteration.</p></li>
<li><p><strong>recurrent</strong>: The probability of returning to any state <span class="math inline">\(\theta&#39;\)</span>, given sufficient iterations, equals 1. In words, if the chain runs long enough, it is guaranteed to eventually get to the state <span class="math inline">\(\theta&#39;\)</span>.</p></li>
</ol>
<p>The conditions above guarantee that Markov chain will converge to a so-called <em>stationary distribution</em>. Together with some other conditions, this stationary distribution will be equal to the posterior distribution. That implies that after a certain large number of iterations, the samples from the chain can be considered a random (but correlated) draw from the posterior distribution.</p>
<div id="convergence" class="section level3 hasAnchor" number="15.2.1">
<h3><span class="header-section-number">15.2.1</span> Convergence<a href="ch-Bayes-estimation.html#convergence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>MCMC techniques rely on a proof that after a sufficient number of samples, the algorithms provide samples according to the posterior distribution. That means that at some point, the sampled parameter values can be used directly as an empirical measure for the posterior probability. Unfortunately, it is very difficult to determine when this point has been reached. One idea is to use several chains simultaneously and checking whether their distribution of samples overlaps sufficiently to conclude that all are likely to be sampling from the posterior distribution. The measure of this, usually denoted as <span class="math inline">\(\hat{R}\)</span>, is the ratio of the variance of the sampled values <span class="math inline">\(\tilde{\theta}\)</span> within a chain, to the variance between chains. If all chains have converged to the posterior distribution, this ratio should equal 1. Values <span class="math inline">\(\hat{R} \leq 1.04\)</span> are considered satisfactory. If the value is larger, it is unlikely that the chain has converged. This test is by no means guaranteed to prove convergence, but nonetheless can be a useful indicator of non-convergence.</p>
<p>Usually, the first part of the samples is discarded as a “warm-up” or “burn-in” sample, as the initial samples will likely not have been drawn from the posterior distribution (as the chain needs to converge first). Depending on the complexity of the model, this burn-in period may have to be quite long. Apart from computation time, there is no benefit to make the burn-in sample smaller. Hence, the more initial samples that can be discarded, the more likely the subsequent samples are an accurate reflection of the posterior distribution.</p>
</div>
<div id="autocorrelation" class="section level3 hasAnchor" number="15.2.2">
<h3><span class="header-section-number">15.2.2</span> Autocorrelation<a href="ch-Bayes-estimation.html#autocorrelation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Markov chains have inherent dependencies, as each sample parameter <span class="math inline">\(\tilde{\theta}_j\)</span> is dependent on the previous sample <span class="math inline">\(\tilde{\theta}_{j-1}\)</span>. Large correlation between sampled values can bias the overall sample. Therefore, it may help to <strong>thin</strong> the samples by discarding intermediate values (e.g. every second, and/or third, and/or fourth sample, etc).</p>
</div>
<div id="effective-sample-size" class="section level3 hasAnchor" number="15.2.3">
<h3><span class="header-section-number">15.2.3</span> Effective sample size<a href="ch-Bayes-estimation.html#effective-sample-size" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Due to the autocorrelation, each sample provides less information about the posterior distribution than if the samples were independent. The effective sample size is a rough measure of how many independent bits of information is contained in an MCMC sample. Ideally, the effective sample size should equal the number of samples (after burn-in), or at least not be much smaller.</p>
</div>
</div>
<div id="concerning-prior-distributions" class="section level2 hasAnchor" number="15.3">
<h2><span class="header-section-number">15.3</span> Concerning prior distributions<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a><a href="ch-Bayes-estimation.html#concerning-prior-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the most commonly asked questions when one first encounters Bayesian statistics is “how do we choose a prior?” While there is never one “perfect” prior in any situation, we’ll discuss some issues to consider when choosing a prior. But first, here are a few big picture ideas to keep in mind.</p>
<ul>
<li>Bayesian inference is based on the <em>posterior</em> distribution, not the prior. Therefore, the posterior requires much more attention than the prior.</li>
<li>The prior is only one part of the Bayesian model. The likelihood is the other part. And there is the data that is used to fit the model. Choice of prior is just one of many modeling assumptions that should be evaluated and checked.</li>
<li>In many situations, the posterior distribution is not too sensitive to reasonable changes in prior. In these situations, the important question isn’t “what is the prior?” but rather “is there a prior at all”? That is, are you adopting a Bayesian approach, treating parameters as random variables, and quantifying uncertainty about parameters with probability distributions?</li>
<li>One criticism of Bayesian statistics in general and priors in particular is that they are subjective.
However, any statistical analysis is inherently subjective, filled with many assumptions and decisions along the way.
Except in the simplest situations, if you ask five statisticians how to approach a particular problem, you will likely get five different answers.
Priors and Bayesian data analysis are no more inherently subjective than any of the myriad other assumptions made in statistical analysis.</li>
</ul>
<p>Subjectivity is OK, and often beneficial. Choosing a subjective prior allows us to explicitly incorporate a wealth of past experience into our analysis.</p>
<p>Bayesian data analysis treats parameters as random variables with probability distributions.
The prior distribution quantifies the researcher’s uncertainty about parameters <em>before</em> observing data. Some issues to consider when choosing a prior include, in no particular order:</p>
<ul>
<li>The researcher’s prior beliefs! A prior distribution is part of a statistical model, and should be consistent with knowledge about the underlying scientific problem. Researchers are often experts with a wealth of past experience that can be explicitly incorporated into the analysis via the prior distribution. Such a prior is called an informative or weakly informative prior.</li>
<li>Noninformative prior a.k.a., (reference, vague, flat prior). A prior is sought that plays a minimal role in inference so that “the data can speak for itself”.
<!--- A regularizing prior.  A prior which, when tuned properly, reduces overfitting or "overreacting" to the data.--></li>
<li>Mathematical convenience. The prior is chosen so that computation of the posterior is simplified, as in the case for so-called <em>conjugate priors</em>.
<!--- Interpretation.  The posterior is a compromise between the data and prior.  Some priors allow for easy interpretation of the relative contributions of data and prior to the posterior.  For example, think of the "prior successes and prior failures" interpretation in the Beta-Binomial model.--></li>
<li>Prior based on <em>past</em> data. Bayesian updating can be viewed as an iterative process. The posterior distribution obtained from one round of data collection can inform the prior distribution for another round.</li>
</ul>
<p>For those initially sceptical of prior distributions at all, the strategy of always choosing an noninformative or flat prior might be appealing. Flat priors are common, but are rarely ever the best choices from a modelling perspective. Just like you would not want to assume a Normal distribution for the likelihood in every problem, you would not want to use a flat prior in every problem.</p>
<p>Furthermore, there are some subtle issues that arise when attempting to choose a flat or noninformative prior. Flat priors are generally not preserved under transformations of parameters. So a prior that is flat under one parametrization of the problem will generally not be flat under another. For example, when trying to estimate a population SD <span class="math inline">\(\sigma\)</span>, assuming a flat prior for <span class="math inline">\(\sigma\)</span> will result in a non-flat prior for the population variance <span class="math inline">\(\sigma^2\)</span>, and vice versa.</p>
<!--
An *improper* prior distribution is a prior distribution that does not integrate to 1, so is not a proper probability density. However, an improper prior often results in a proper posterior distribution.  Thus, improper prior distributions are sometimes used in practice.

Flat priors are common choices in some situations, but are rarely ever the best choices from a modeling perspective.  Furthermore, flat priors are generally not preserved under transformations of parameters.  So a prior that is flat under one parametrization of the problem will generally not be flat under another. For example, when trying to estimate a population SD $\sigma$, assuming a flat prior for $\sigma$ will result in a non-flat prior for the population variance $\sigma^2$, and vice versa.
-->
<p>It is not possible to provide clear guidelines on choosing prior distributions for all data analysis problems. Ideally, the prior should reflect the beliefs of the analyst. But it can be difficult to formulate beliefs about parameters of statistical models. Without clear beliefs, a noninformative or flat prior is reasonable.</p>
<p>In all cases, it is a good idea to check the sensibility of the prior distribution by inspecting the <em>prior predictive distribution</em>. The prior predictive distribution can be used to check the reasonableness of a prior for a given situation before observing sample data. Do the simulated samples seem consistent with what you might expect of the data based on your background knowledge of the situation? If not, another prior might be more reasonable.</p>
</div>
<div id="multiple-regression-example" class="section level2 hasAnchor" number="15.4">
<h2><span class="header-section-number">15.4</span> Multiple regression example<a href="ch-Bayes-estimation.html#multiple-regression-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As a final example of a Bayesian analysis, we will revisit the data analysed in Chapter <a href="ch-multiple-regression.html#ch-multiple-regression">5</a>, predicting votes for Trump as a function of the number of hate groups and education level.</p>
<p>Our model is like a usual regression model:
<span class="math display">\[\texttt{trump_votes}_i = \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)\]</span>
The regression model above defines the likelihood. For a Bayesian analysis, we need to set priors for all parameters. We use the following prior distributions:
<span class="math display">\[\begin{aligned} \beta_0 &amp;\sim \mathbf{Normal}(0, 100) \\
\beta_1  &amp;\sim \mathbf{Normal}(0, 5) \\
\beta_2  &amp;\sim \mathbf{Normal}(0, 5) \\
\sigma_\epsilon  &amp;\sim \mathbf{half-Normal}(0, 10) \\
\end{aligned}\]</span>
Note that a half-Normal distribution is used for the standard deviation of the model errors, truncated below at 0. This is because a standard deviation can not be negative.</p>
<p>The model was estimated by running four Hamiltonian MCMC chains in parallel, each for 2000 iterations, discarding the first 1000 samples as burn-in. The final 1000 samples of each chain was then left as an estimate of the posterior distribution (so 4000 samples in total).</p>
<p>Figure <a href="ch-Bayes-estimation.html#fig:brms-multiple-regression-model-prior-samples">15.4</a> shows sampled values of the four parameters from the prior distributions. Figure <a href="ch-Bayes-estimation.html#fig:brms-multiple-regression-model-posterior-samples">15.5</a> shows corresponding plots, but now sampling from the posterior distribution. Note that the scales on the x-axis is very different in Figure <a href="ch-Bayes-estimation.html#fig:brms-multiple-regression-model-posterior-samples">15.5</a> compared to Figure <a href="ch-Bayes-estimation.html#fig:brms-multiple-regression-model-prior-samples">15.4</a> The range of parameter values in the approximate posterior distributions is much smaller, which shows how the data constrains the posterior distribution as compared to the prior distributions.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:brms-multiple-regression-model-prior-samples"></span>
<img src="15-Bayes-estimation_files/figure-html/brms-multiple-regression-model-prior-samples-1.svg" alt="Samples from the prior distributions of the parameters of the multiple regression model predicting votes for Trump in 2016 by the number of hate groups and level of education in US states." width="672" />
<p class="caption">
Figure 15.4: Samples from the prior distributions of the parameters of the multiple regression model predicting votes for Trump in 2016 by the number of hate groups and level of education in US states.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:brms-multiple-regression-model-posterior-samples"></span>
<img src="15-Bayes-estimation_files/figure-html/brms-multiple-regression-model-posterior-samples-1.svg" alt="Samples from the posterior distributions of the parameters of the multiple regression model predicting votes for Trump in 2016 by the number of hate groups and level of education in US states." width="672" />
<p class="caption">
Figure 15.5: Samples from the posterior distributions of the parameters of the multiple regression model predicting votes for Trump in 2016 by the number of hate groups and level of education in US states.
</p>
</div>
<p>Table <a href="ch-Bayes-estimation.html#tab:brms-multiple-regression-model-summary">15.1</a> contains a summary measures of the posterior distribution of the main parameters of interest. The <span class="math inline">\(\hat{R}\)</span> values are close to 1, and the effective sample size (ESS) is close to 4000 (the actual number of samples) for all parameters. This indicates a reasonably successful MCMC estimate. Note that unlike maximum likelihood estimation, Bayesian estimation provides a posterior distribution, not a single parameter estimates. But it is common to use the posterior mean as the parameter estimate, insofar as a single number is needed. Comparing the results to those in Table <a href="ch-multiple-regression.html#tab:t-test-results-multiple-regression-table">5.1</a> shows that the posterior means are close, but not identical, to the maximum likelihood parameter estimates. The standard deviation of the posterior distribution can indicate the extent to which the data has restricted the plausible parameter values. The Credible Interval (CI) is used in a similar way to a confidence interval. For all parameters, the CI excludes 0, which indicates that the predictors have an effect on the dependent variable.</p>
<p>There are two main ways to compute a credible interval. The easiest is via quantiles in the posterior sample. This method is also called the <strong>Equal-tailed Interval</strong> (ETI). For example, a 95% ETI credible interval would be defined via the minimum and maximum value in the posterior sample <em>after excluding the 2.5% lowest and 2.5% highest values</em>. The 95% CI reported in Table <a href="ch-Bayes-estimation.html#tab:brms-multiple-regression-model-summary">15.1</a> is based on this method. The second method is the <strong>Highest Density Interval</strong> (HDI), which is a little more involved to compute. The HDI computes a range such that all values within the interval have a higher probability density than values outside the interval. For symmetric posterior distributions, the ETI and HDI provide the same results. For skewed distributions, the ETI and HDI will differ.</p>
<table>
<caption><span id="tab:brms-multiple-regression-model-summary">Table 15.1: </span>Results from a Hamiltonian MCMC estimation of the multiple regression model predicting votes for Trump in 2016 by the number of hate groups and level of education in US states.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">mean</th>
<th align="right">SD</th>
<th align="right">lower 95% CI</th>
<th align="right">upper 95% CI</th>
<th align="right"><span class="math inline">\(\hat{R}\)</span></th>
<th align="right">ESS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">82.09</td>
<td align="right">6.35</td>
<td align="right">69.54</td>
<td align="right">94.70</td>
<td align="right">1</td>
<td align="right">3916</td>
</tr>
<tr class="even">
<td align="left">Hate_groups</td>
<td align="right">1.29</td>
<td align="right">0.51</td>
<td align="right">0.27</td>
<td align="right">2.31</td>
<td align="right">1</td>
<td align="right">3853</td>
</tr>
<tr class="odd">
<td align="left">Education</td>
<td align="right">-1.22</td>
<td align="right">0.19</td>
<td align="right">-1.60</td>
<td align="right">-0.84</td>
<td align="right">1</td>
<td align="right">4075</td>
</tr>
</tbody>
</table>
<p>To get an idea of how much the parameter estimates are affected, we also present results from a model where we use the following prior distributions:</p>
<p><span class="math display">\[\begin{aligned} \beta_0 &amp;\sim \mathbf{Normal}(0, 1) \\
\beta_1  &amp;\sim \mathbf{Normal}(0, 1) \\
\beta_2  &amp;\sim \mathbf{Normal}(0, 1) \\
\sigma_\epsilon  &amp;\sim \mathbf{half-Normal}(0, 1) \\
\end{aligned}\]</span></p>
<p>Note that the standard deviations are much smaller than before. These prior distributions will constrain the parameters to be closer to 0, as very large values are very unlikely in these distributions.</p>
<p>Table <a href="ch-Bayes-estimation.html#tab:brms-multiple-regression-model-2-summary">15.2</a> shows the results are rather different. In this case, the prior distributions were too restrictive, putting too much prior constraint on the parameters. This is particularly evident for the intercept. A normal distribution with a mean of 0 and a standard deviation of 1 makes a value such as 82 very unlikely.</p>
<table>
<caption><span id="tab:brms-multiple-regression-model-2-summary">Table 15.2: </span>Results from a Hamiltonian MCMC estimation of the multiple regression model predicting votes for Trump in 2016 by the number of hate groups and level of education in US states, using Normal priors with smaller standard deviation.</caption>
<colgroup>
<col width="18%" />
<col width="9%" />
<col width="9%" />
<col width="20%" />
<col width="20%" />
<col width="15%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">mean</th>
<th align="right">SD</th>
<th align="right">lower 95% CI</th>
<th align="right">upper 95% CI</th>
<th align="right"><span class="math inline">\(\hat{R}\)</span></th>
<th align="right">ESS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">37.97</td>
<td align="right">13.05</td>
<td align="right">11.73</td>
<td align="right">63.20</td>
<td align="right">1</td>
<td align="right">3709</td>
</tr>
<tr class="even">
<td align="left">Hate_groups</td>
<td align="right">0.56</td>
<td align="right">0.80</td>
<td align="right">-1.02</td>
<td align="right">2.10</td>
<td align="right">1</td>
<td align="right">3798</td>
</tr>
<tr class="odd">
<td align="left">Education</td>
<td align="right">-1.08</td>
<td align="right">0.41</td>
<td align="right">-1.89</td>
<td align="right">-0.24</td>
<td align="right">1</td>
<td align="right">3854</td>
</tr>
</tbody>
</table>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-bayes1763" class="csl-entry">
Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances. <em>Philosophical Transactions of the Royal Society of London</em>, <em>53</em>, 370–418.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="48">
<li id="fn48"><p>I originally wrote most of this chapter in the first week of December 2020. Revising the chapter in the first week of December 2021, not too much has changed.<a href="ch-Bayes-estimation.html#fnref48" class="footnote-back">↩︎</a></p></li>
<li id="fn49"><p>Later evidence indicates that the tests may be more accurate than thought at first (<a href="https://www.bbc.co.uk/news/health-58899612">BBC, 14 October 2021</a>)<a href="ch-Bayes-estimation.html#fnref49" class="footnote-back">↩︎</a></p></li>
<li id="fn50"><p>The first method is generally more reliable.<a href="ch-Bayes-estimation.html#fnref50" class="footnote-back">↩︎</a></p></li>
<li id="fn51"><p>Analytical expressions for the posterior distribution tend to require a so-called <strong>conjugate prior</strong> distribution, which means that the prior distribution is in the same class of distributions as the posterior distribution. For the Binomial likelihood of the Paul example, the conjugate prior is a <em>Beta distribution</em>, which results in a Beta posterior distribution. For a Normal likelihood, the conjugate prior on the mean <span class="math inline">\(\mu\)</span> is a Normal distribution and the conjugate prior for the variance <span class="math inline">\(\sigma^2\)</span> is an inverse-Gamma distribution. The posterior distribution is then a <span class="math inline">\(t\)</span>-distribution. Conjugate prior distributions are mathematically convenient, but generally too constrictive to adequately represent prior beliefs.<a href="ch-Bayes-estimation.html#fnref51" class="footnote-back">↩︎</a></p></li>
<li id="fn52"><p>Monte Carlo being famous for its casino’s with various games of chance.<a href="ch-Bayes-estimation.html#fnref52" class="footnote-back">↩︎</a></p></li>
<li id="fn53"><p>This section contains some material adapted from <a href="https://bookdown.org/kevin_davisross/bayesian-reasoning-and-methods/">An Introduction to Bayesian Reasoning and Methods</a> by Kevin Ross.<a href="ch-Bayes-estimation.html#fnref53" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-SEM-latent-variable-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-Bayes-factors.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"source": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
