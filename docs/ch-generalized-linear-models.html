<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Generalized Linear Models | Statistics: Data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Generalized Linear Models | Statistics: Data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Generalized Linear Models | Statistics: Data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2022-11-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-linear-mixed-effects-models.html"/>
<link rel="next" href="ch-SEM-path-models.html"/>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="book_assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="book_assets/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.9/grViz.js"></script>
<script src="book_assets/plotly-binding-4.10.0/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-2.5.1/plotly-latest.min.js"></script>
<link href="book_assets/combineWidgetStyle-0.1/combineWidgets.css" rel="stylesheet" />
<script src="book_assets/combineWidgets-binding-0.11.1/combineWidgets.js"></script>
<script src="book_assets/rglWebGL-binding-0.110.2/rglWebGL.js"></script>
<link href="book_assets/rglwidgetClass-0.110.2/rgl.css" rel="stylesheet" />
<script src="book_assets/rglwidgetClass-0.110.2/rglClass.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/utils.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/buffer.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/subscenes.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/shaders.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/textures.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/projection.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/mouse.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/init.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/pieces.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/draw.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/controls.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/selection.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/rglTimer.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/pretty.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/axes.src.js"></script>
<script src="book_assets/rglwidgetClass-0.110.2/animation.src.js"></script>
<!--html_preserve--><script type = "text/plain" id = "rgl-vertex-shader">
#line 2 1
// File 1 is the vertex shader
#ifdef GL_ES
#ifdef GL_FRAGMENT_PRECISION_HIGH
precision highp float;
#else
precision mediump float;
#endif
#endif

attribute vec3 aPos;
attribute vec4 aCol;
uniform mat4 mvMatrix;
uniform mat4 prMatrix;
varying vec4 vCol;
varying vec4 vPosition;

#ifdef NEEDS_VNORMAL
attribute vec3 aNorm;
uniform mat4 normMatrix;
varying vec4 vNormal;
#endif

#if defined(HAS_TEXTURE) || defined (IS_TEXT)
attribute vec2 aTexcoord;
varying vec2 vTexcoord;
#endif

#ifdef FIXED_SIZE
uniform vec3 textScale;
#endif

#ifdef FIXED_QUADS
attribute vec3 aOfs;
#endif

#ifdef IS_TWOSIDED
#ifdef HAS_NORMALS
varying float normz;
uniform mat4 invPrMatrix;
#else
attribute vec3 aPos1;
attribute vec3 aPos2;
varying float normz;
#endif
#endif // IS_TWOSIDED

#ifdef FAT_LINES
attribute vec3 aNext;
attribute vec2 aPoint;
varying vec2 vPoint;
varying float vLength;
uniform float uAspect;
uniform float uLwd;
#endif


void main(void) {
  
#ifndef IS_BRUSH
#if defined(NCLIPPLANES) || !defined(FIXED_QUADS) || defined(HAS_FOG)
  vPosition = mvMatrix * vec4(aPos, 1.);
#endif
  
#ifndef FIXED_QUADS
  gl_Position = prMatrix * vPosition;
#endif
#endif // !IS_BRUSH
  
#ifdef IS_POINTS
  gl_PointSize = POINTSIZE;
#endif
  
  vCol = aCol;
  
#ifdef NEEDS_VNORMAL
  vNormal = normMatrix * vec4(-aNorm, dot(aNorm, aPos));
#endif
  
#ifdef IS_TWOSIDED
#ifdef HAS_NORMALS
  /* normz should be calculated *after* projection */
  normz = (invPrMatrix*vNormal).z;
#else
  vec4 pos1 = prMatrix*(mvMatrix*vec4(aPos1, 1.));
  pos1 = pos1/pos1.w - gl_Position/gl_Position.w;
  vec4 pos2 = prMatrix*(mvMatrix*vec4(aPos2, 1.));
  pos2 = pos2/pos2.w - gl_Position/gl_Position.w;
  normz = pos1.x*pos2.y - pos1.y*pos2.x;
#endif
#endif // IS_TWOSIDED
  
#ifdef NEEDS_VNORMAL
  vNormal = vec4(normalize(vNormal.xyz/vNormal.w), 1);
#endif
  
#if defined(HAS_TEXTURE) || defined(IS_TEXT)
  vTexcoord = aTexcoord;
#endif
  
#if defined(FIXED_SIZE) && !defined(ROTATING)
  vec4 pos = prMatrix * mvMatrix * vec4(aPos, 1.);
  pos = pos/pos.w;
  gl_Position = pos + vec4(aOfs*textScale, 0.);
#endif
  
#if defined(IS_SPRITES) && !defined(FIXED_SIZE)
  vec4 pos = mvMatrix * vec4(aPos, 1.);
  pos = pos/pos.w + vec4(aOfs,  0.);
  gl_Position = prMatrix*pos;
#endif
  
#ifdef FAT_LINES
  /* This code was inspired by Matt Deslauriers' code in 
   https://mattdesl.svbtle.com/drawing-lines-is-hard */
  vec2 aspectVec = vec2(uAspect, 1.0);
  mat4 projViewModel = prMatrix * mvMatrix;
  vec4 currentProjected = projViewModel * vec4(aPos, 1.0);
  currentProjected = currentProjected/currentProjected.w;
  vec4 nextProjected = projViewModel * vec4(aNext, 1.0);
  vec2 currentScreen = currentProjected.xy * aspectVec;
  vec2 nextScreen = (nextProjected.xy / nextProjected.w) * aspectVec;
  float len = uLwd;
  vec2 dir = vec2(1.0, 0.0);
  vPoint = aPoint;
  vLength = length(nextScreen - currentScreen)/2.0;
  vLength = vLength/(vLength + len);
  if (vLength > 0.0) {
    dir = normalize(nextScreen - currentScreen);
  }
  vec2 normal = vec2(-dir.y, dir.x);
  dir.x /= uAspect;
  normal.x /= uAspect;
  vec4 offset = vec4(len*(normal*aPoint.x*aPoint.y - dir), 0.0, 0.0);
  gl_Position = currentProjected + offset;
#endif
  
#ifdef IS_BRUSH
  gl_Position = vec4(aPos, 1.);
#endif
}
</script>
<script type = "text/plain" id = "rgl-fragment-shader">
#line 2 2
// File 2 is the fragment shader
#ifdef GL_ES
#ifdef GL_FRAGMENT_PRECISION_HIGH
precision highp float;
#else
precision mediump float;
#endif
#endif
varying vec4 vCol; // carries alpha
varying vec4 vPosition;
#if defined(HAS_TEXTURE) || defined (IS_TEXT)
varying vec2 vTexcoord;
uniform sampler2D uSampler;
#endif

#ifdef HAS_FOG
uniform int uFogMode;
uniform vec3 uFogColor;
uniform vec4 uFogParms;
#endif

#if defined(IS_LIT) && !defined(FIXED_QUADS)
varying vec4 vNormal;
#endif

#if NCLIPPLANES > 0
uniform vec4 vClipplane[NCLIPPLANES];
#endif

#if NLIGHTS > 0
uniform mat4 mvMatrix;
#endif

#ifdef IS_LIT
uniform vec3 emission;
uniform float shininess;
#if NLIGHTS > 0
uniform vec3 ambient[NLIGHTS];
uniform vec3 specular[NLIGHTS]; // light*material
uniform vec3 diffuse[NLIGHTS];
uniform vec3 lightDir[NLIGHTS];
uniform bool viewpoint[NLIGHTS];
uniform bool finite[NLIGHTS];
#endif
#endif // IS_LIT

#ifdef IS_TWOSIDED
uniform bool front;
varying float normz;
#endif

#ifdef FAT_LINES
varying vec2 vPoint;
varying float vLength;
#endif

void main(void) {
  vec4 fragColor;
#ifdef FAT_LINES
  vec2 point = vPoint;
  bool neg = point.y < 0.0;
  point.y = neg ? (point.y + vLength)/(1.0 - vLength) :
                 -(point.y - vLength)/(1.0 - vLength);
#if defined(IS_TRANSPARENT) && defined(IS_LINESTRIP)
  if (neg && length(point) <= 1.0) discard;
#endif
  point.y = min(point.y, 0.0);
  if (length(point) > 1.0) discard;
#endif // FAT_LINES
  
#ifdef ROUND_POINTS
  vec2 coord = gl_PointCoord - vec2(0.5);
  if (length(coord) > 0.5) discard;
#endif
  
#if NCLIPPLANES > 0
  for (int i = 0; i < NCLIPPLANES; i++)
    if (dot(vPosition, vClipplane[i]) < 0.0) discard;
#endif
    
#ifdef FIXED_QUADS
    vec3 n = vec3(0., 0., 1.);
#elif defined(IS_LIT)
    vec3 n = normalize(vNormal.xyz);
#endif
    
#ifdef IS_TWOSIDED
    if ((normz <= 0.) != front) discard;
#endif
    
#ifdef IS_LIT
    vec3 eye = normalize(-vPosition.xyz/vPosition.w);
    vec3 lightdir;
    vec4 colDiff;
    vec3 halfVec;
    vec4 lighteffect = vec4(emission, 0.);
    vec3 col;
    float nDotL;
#ifdef FIXED_QUADS
    n = -faceforward(n, n, eye);
#endif
    
#if NLIGHTS > 0
    for (int i=0;i<NLIGHTS;i++) {
      colDiff = vec4(vCol.rgb * diffuse[i], vCol.a);
      lightdir = lightDir[i];
      if (!viewpoint[i])
        lightdir = (mvMatrix * vec4(lightdir, 1.)).xyz;
      if (!finite[i]) {
        halfVec = normalize(lightdir + eye);
      } else {
        lightdir = normalize(lightdir - vPosition.xyz/vPosition.w);
        halfVec = normalize(lightdir + eye);
      }
      col = ambient[i];
      nDotL = dot(n, lightdir);
      col = col + max(nDotL, 0.) * colDiff.rgb;
      col = col + pow(max(dot(halfVec, n), 0.), shininess) * specular[i];
      lighteffect = lighteffect + vec4(col, colDiff.a);
    }
#endif
    
#else // not IS_LIT
    vec4 colDiff = vCol;
    vec4 lighteffect = colDiff;
#endif
    
#ifdef IS_TEXT
    vec4 textureColor = lighteffect*texture2D(uSampler, vTexcoord);
#endif
    
#ifdef HAS_TEXTURE
#ifdef TEXTURE_rgb
    vec4 textureColor = lighteffect*vec4(texture2D(uSampler, vTexcoord).rgb, 1.);
#endif
    
#ifdef TEXTURE_rgba
    vec4 textureColor = lighteffect*texture2D(uSampler, vTexcoord);
#endif
    
#ifdef TEXTURE_alpha
    vec4 textureColor = texture2D(uSampler, vTexcoord);
    float luminance = dot(vec3(1.,1.,1.), textureColor.rgb)/3.;
    textureColor =  vec4(lighteffect.rgb, lighteffect.a*luminance);
#endif
    
#ifdef TEXTURE_luminance
    vec4 textureColor = vec4(lighteffect.rgb*dot(texture2D(uSampler, vTexcoord).rgb, vec3(1.,1.,1.))/3., lighteffect.a);
#endif
    
#ifdef TEXTURE_luminance_alpha
    vec4 textureColor = texture2D(uSampler, vTexcoord);
    float luminance = dot(vec3(1.,1.,1.),textureColor.rgb)/3.;
    textureColor = vec4(lighteffect.rgb*luminance, lighteffect.a*textureColor.a);
#endif
    
    fragColor = textureColor;

#elif defined(IS_TEXT)
    if (textureColor.a < 0.1)
      discard;
    else
      fragColor = textureColor;
#else
    fragColor = lighteffect;
#endif // HAS_TEXTURE
    
#ifdef HAS_FOG
    // uFogParms elements: x = near, y = far, z = fogscale, w = (1-sin(FOV/2))/(1+sin(FOV/2))
    // In Exp and Exp2: use density = density/far
    // fogF will be the proportion of fog
    // Initialize it to the linear value
    float fogF;
    if (uFogMode > 0) {
      fogF = (uFogParms.y - vPosition.z/vPosition.w)/(uFogParms.y - uFogParms.x);
      if (uFogMode > 1)
        fogF = mix(uFogParms.w, 1.0, fogF);
      fogF = fogF*uFogParms.z;
      if (uFogMode == 2)
        fogF = 1.0 - exp(-fogF);
      // Docs are wrong: use (density*c)^2, not density*c^2
      // https://gitlab.freedesktop.org/mesa/mesa/-/blob/master/src/mesa/swrast/s_fog.c#L58
      else if (uFogMode == 3)
        fogF = 1.0 - exp(-fogF*fogF);
      fogF = clamp(fogF, 0.0, 1.0);
      gl_FragColor = vec4(mix(fragColor.rgb, uFogColor, fogF), fragColor.a);
    } else gl_FragColor = fragColor;
#else
    gl_FragColor = fragColor;
#endif // HAS_FOG
    
}
</script><!--/html_preserve-->
<script src="book_assets/CanvasMatrix4-0.110.2/CanvasMatrix.src.js"></script>
<script src="book_assets/rglPlayer-binding-0.110.2/rglPlayer.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ch-intro.html"><a href="ch-intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="ch-intro.html"><a href="ch-intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-modeling.html"><a href="ch-modeling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-probability-definition"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-binomial-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="ch-modeling.html"><a href="ch-modeling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-modeling.html"><a href="ch-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-modeling.html"><a href="ch-modeling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-likelihood-ratio"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-modeling.html"><a href="ch-modeling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-modeling.html"><a href="ch-modeling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-modeling.html"><a href="ch-modeling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-modeling.html"><a href="ch-modeling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="ch-modeling.html"><a href="ch-modeling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="ch-modeling.html"><a href="ch-modeling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-a-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has a specific value</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#ch3-confidence-interval"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#effect-size"><i class="fa fa-check"></i><b>3.6</b> Effect size</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.7</b> Assumptions</a></li>
<li class="chapter" data-level="3.8" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.8</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.8.1</b> The Central Limit Theorem in action</a></li>
<li class="chapter" data-level="3.8.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#bootstrapping-a-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Bootstrapping a statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#in-practice"><i class="fa fa-check"></i><b>3.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>4.4.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#summary-2"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-transformations"><i class="fa fa-check"></i><b>5.7.1</b> Transforming variables</a></li>
<li class="chapter" data-level="5.7.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.7.2</b> Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.8</b> Multicollinearity: Redundancy between predictors</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.8.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-outliers"><i class="fa fa-check"></i><b>5.9</b> Outliers</a></li>
<li class="chapter" data-level="5.10" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#in-practice-1"><i class="fa fa-check"></i><b>5.10</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#sec:05-dont-forget-about-the-fun"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#sec:06-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-size-in-anova"><i class="fa fa-check"></i><b>7.6</b> Effect-size in ANOVA</a></li>
<li class="chapter" data-level="7.7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#assumptions"><i class="fa fa-check"></i><b>7.7</b> Assumptions</a></li>
<li class="chapter" data-level="7.8" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.8</b> Multiple testing and post-hoc tests</a></li>
<li class="chapter" data-level="7.9" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#in-practice-2"><i class="fa fa-check"></i><b>7.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#sec:06-main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#the-factorial-anova-model"><i class="fa fa-check"></i><b>8.3</b> The factorial ANOVA model</a></li>
<li class="chapter" data-level="8.4" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.4</b> A threeway factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#interpreting-interactions"><i class="fa fa-check"></i><b>8.4.1</b> Interpreting interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.5</b> Orthogonal contrast codes and unequal sample sizes</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.5.1</b> Comparison schemes and SS types</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#in-practice-3"><i class="fa fa-check"></i><b>8.6</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#subjective-feelings-of-power-and-priming"><i class="fa fa-check"></i><b>9.1</b> Subjective feelings of power and priming</a></li>
<li class="chapter" data-level="9.2" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#acounting-for-pre-existing-differences"><i class="fa fa-check"></i><b>9.2</b> Acounting for pre-existing differences</a></li>
<li class="chapter" data-level="9.3" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#slopes-of-contrast-coded-predictors-in-ancova-models"><i class="fa fa-check"></i><b>9.3</b> Slopes of contrast-coded predictors in ANCOVA models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#homogeneity-of-slopes"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of slopes</a></li>
<li class="chapter" data-level="9.5" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#sec:ANCOVA-power"><i class="fa fa-check"></i><b>9.5</b> Power considerations in ANCOVA</a></li>
<li class="chapter" data-level="9.6" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#models-with-multiple-covariates"><i class="fa fa-check"></i><b>9.6</b> Models with multiple covariates</a></li>
<li class="chapter" data-level="9.7" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#mediation-with-categorical-independent-variables"><i class="fa fa-check"></i><b>9.7</b> Mediation with categorical independent variables</a></li>
<li class="chapter" data-level="9.8" data-path="ch-ANCOVA.html"><a href="ch-ANCOVA.html#ancova-vs-difference-scores"><i class="fa fa-check"></i><b>9.8</b> ANCOVA vs difference scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#non-independence-in-linear-models"><i class="fa fa-check"></i><b>10.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="10.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#the-cheerleader-effect"><i class="fa fa-check"></i><b>10.2</b> The cheerleader effect</a></li>
<li class="chapter" data-level="10.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#as-a-oneway-anova"><i class="fa fa-check"></i><b>10.3</b> As a oneway ANOVA</a></li>
<li class="chapter" data-level="10.4" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#oneway-repeated-measures-anova"><i class="fa fa-check"></i><b>10.4</b> Oneway repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#within-subjects-composite-scores"><i class="fa fa-check"></i><b>10.4.1</b> Within-subjects composite scores</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-composite-for-between-subjects-effects"><i class="fa fa-check"></i><b>10.4.2</b> A composite for between-subjects effects</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#collecting-all-results-and-omnibus-tests"><i class="fa fa-check"></i><b>10.4.3</b> Collecting all results and omnibus tests</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#partitioning-the-variance"><i class="fa fa-check"></i><b>10.5</b> Partitioning the variance</a></li>
<li class="chapter" data-level="10.6" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-mixed-anova-with-between--and-within-subjects-effects"><i class="fa fa-check"></i><b>10.6</b> A mixed ANOVA with between- and within-subjects effects</a></li>
<li class="chapter" data-level="10.7" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#assumptions-1"><i class="fa fa-check"></i><b>10.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#omnibus-tests-and-sphericity"><i class="fa fa-check"></i><b>10.7.1</b> Omnibus tests and sphericity</a></li>
<li class="chapter" data-level="10.7.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#correcting-for-non-sphericity"><i class="fa fa-check"></i><b>10.7.2</b> Correcting for non-sphericity</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#in-practice-4"><i class="fa fa-check"></i><b>10.8</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#non-independence-in-linear-models-1"><i class="fa fa-check"></i><b>11.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="11.2" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#random-intercept-models"><i class="fa fa-check"></i><b>11.2</b> Random intercept models</a></li>
<li class="chapter" data-level="11.3" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-estimation-1"><i class="fa fa-check"></i><b>11.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="11.4" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-inference"><i class="fa fa-check"></i><b>11.4</b> Parameter inference</a></li>
<li class="chapter" data-level="11.5" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#application-of-the-random-intercepts-model"><i class="fa fa-check"></i><b>11.5</b> Application of the random-intercepts model</a></li>
<li class="chapter" data-level="11.6" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#models-with-random-intercepts-and-slopes"><i class="fa fa-check"></i><b>11.6</b> Models with random intercepts and slopes</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#correlation-between-random-effects"><i class="fa fa-check"></i><b>11.6.1</b> Correlation between random effects</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#crossed-random-effects-dating-partners-in-the-speed-dating-experiment"><i class="fa fa-check"></i><b>11.7</b> Crossed random effects: dating partners in the speed dating experiment</a></li>
<li class="chapter" data-level="11.8" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#choosing-the-random-effects-structure"><i class="fa fa-check"></i><b>11.8</b> Choosing the random effects structure</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html"><i class="fa fa-check"></i><b>12</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#link-functions"><i class="fa fa-check"></i><b>12.1</b> Link functions</a></li>
<li class="chapter" data-level="12.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#estimation-1"><i class="fa fa-check"></i><b>12.2</b> Estimation</a></li>
<li class="chapter" data-level="12.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#inference-in-generalized-linear-models"><i class="fa fa-check"></i><b>12.3</b> Inference in generalized linear models</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#wald-test"><i class="fa fa-check"></i><b>12.3.1</b> Wald test</a></li>
<li class="chapter" data-level="12.3.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>12.3.2</b> Likelihood-ratio test</a></li>
<li class="chapter" data-level="12.3.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#confidence-intervals-1"><i class="fa fa-check"></i><b>12.3.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#assessing-model-fit"><i class="fa fa-check"></i><b>12.4</b> Assessing model fit</a></li>
<li class="chapter" data-level="12.5" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#logistic-regression"><i class="fa fa-check"></i><b>12.5</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#parameter-interpretation"><i class="fa fa-check"></i><b>12.5.1</b> Parameter interpretation</a></li>
<li class="chapter" data-level="12.5.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-logistic-regression-metacognition"><i class="fa fa-check"></i><b>12.5.2</b> Example: Metacognition in visual perception</a></li>
<li class="chapter" data-level="12.5.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#using-a-different-link-function-probit-regression"><i class="fa fa-check"></i><b>12.5.3</b> Using a different link function: Probit regression</a></li>
<li class="chapter" data-level="12.5.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#welcome-back-paul"><i class="fa fa-check"></i><b>12.5.4</b> Welcome back Paul!</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>12.6</b> Poisson regression</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-poisson-regression-gestures"><i class="fa fa-check"></i><b>12.6.1</b> Example: Gestures in different social contexts</a></li>
<li class="chapter" data-level="12.6.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#overdispersion"><i class="fa fa-check"></i><b>12.6.2</b> Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#log-linear-models"><i class="fa fa-check"></i><b>12.7</b> Log-linear models</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#example-newspapers-and-voting"><i class="fa fa-check"></i><b>12.7.1</b> Example: Newspapers and voting</a></li>
<li class="chapter" data-level="12.7.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#a-three-way-table-example-rock-paper-scissors"><i class="fa fa-check"></i><b>12.7.2</b> A three-way table example: Rock-Paper-Scissors</a></li>
<li class="chapter" data-level="12.7.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sparse-data-and-empty-cells"><i class="fa fa-check"></i><b>12.7.3</b> Sparse data and empty cells</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.8</b> Multinomial logistic regression</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#baseline-category-logit"><i class="fa fa-check"></i><b>12.8.1</b> Baseline category logit</a></li>
<li class="chapter" data-level="12.8.2" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#example-rock-paper-scissors"><i class="fa fa-check"></i><b>12.8.2</b> Example: Rock-Paper-Scissors</a></li>
<li class="chapter" data-level="12.8.3" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#reconstructing-probabilities-of-responses"><i class="fa fa-check"></i><b>12.8.3</b> Reconstructing probabilities of responses</a></li>
<li class="chapter" data-level="12.8.4" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#alternative-logit-models-for-ordinal-categories"><i class="fa fa-check"></i><b>12.8.4</b> Alternative logit models for ordinal categories</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#sec-glmer"><i class="fa fa-check"></i><b>12.9</b> Generalized linear mixed-effects models</a></li>
<li class="chapter" data-level="12.10" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#in-practice-5"><i class="fa fa-check"></i><b>12.10</b> In practice</a></li>
<li class="chapter" data-level="12.11" data-path="ch-generalized-linear-models.html"><a href="ch-generalized-linear-models.html#further-reading"><i class="fa fa-check"></i><b>12.11</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-SEM-path-models.html"><a href="ch-SEM-path-models.html"><i class="fa fa-check"></i><b>13</b> Path models (SEM 1)</a></li>
<li class="chapter" data-level="14" data-path="ch-SEM-latent-variable-models.html"><a href="ch-SEM-latent-variable-models.html"><i class="fa fa-check"></i><b>14</b> Latent variable models (SEM 2)</a></li>
<li class="chapter" data-level="15" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html"><i class="fa fa-check"></i><b>15</b> Introduction to Bayesian estimation</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#fundamentals-of-bayesian-inference"><i class="fa fa-check"></i><b>15.1</b> Fundamentals of Bayesian inference</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#probability-in-times-of-covid"><i class="fa fa-check"></i><b>15.1.1</b> Probability in times of Covid</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#bayes-rule"><i class="fa fa-check"></i><b>15.1.2</b> Bayes’ rule</a></li>
<li class="chapter" data-level="15.1.3" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#we-missed-you-paul"><i class="fa fa-check"></i><b>15.1.3</b> We missed you Paul!</a></li>
<li class="chapter" data-level="15.1.4" data-path="ch-Bayes-estimation.html"><a href="ch-Bayes-estimation.html#the-marginal-likelihood-and-prior-predictive-distribution"><i class="fa fa-check"></i><b>15.1.4</b> The marginal likelihood and prior predictive distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html"><i class="fa fa-check"></i><b>16</b> Introduction to Bayesian hypothesis testing</a>
<ul>
<li class="chapter" data-level="16.0.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#hypothesis-testing-relative-evidence-and-the-bayes-factor"><i class="fa fa-check"></i><b>16.0.1</b> Hypothesis testing, relative evidence, and the Bayes factor</a></li>
<li class="chapter" data-level="16.0.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#parameter-estimates-and-credible-intervals"><i class="fa fa-check"></i><b>16.0.2</b> Parameter estimates and credible intervals</a></li>
<li class="chapter" data-level="16.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#a-bayesian-t-test"><i class="fa fa-check"></i><b>16.1</b> A Bayesian t-test</a></li>
<li class="chapter" data-level="16.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#bayes-factors-for-general-linear-models"><i class="fa fa-check"></i><b>16.2</b> Bayes factors for General Linear Models</a></li>
<li class="chapter" data-level="16.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#some-objections-to-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>16.3</b> Some objections to null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-is-not-a-proper-measure-of-evidential-support"><i class="fa fa-check"></i><b>16.3.1</b> The <span class="math inline">\(p\)</span>-value is not a proper measure of evidential support</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-depends-on-researcher-intentions"><i class="fa fa-check"></i><b>16.3.2</b> The <span class="math inline">\(p\)</span>-value depends on researcher intentions</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#results-of-a-nhst-are-often-misinterpreted"><i class="fa fa-check"></i><b>16.3.3</b> Results of a NHST are often misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#to-bayes-or-not-to-bayes-a-pragmatic-view"><i class="fa fa-check"></i><b>16.4</b> To Bayes or not to Bayes? A pragmatic view</a></li>
<li class="chapter" data-level="16.5" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#in-practice-6"><i class="fa fa-check"></i><b>16.5</b> In practice</a></li>
<li class="chapter" data-level="16.6" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#summary-3"><i class="fa fa-check"></i><b>16.6</b> “Summary”</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html"><i class="fa fa-check"></i><b>17</b> Being a responsible data analyst</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#consider-analysis-before-data-collection"><i class="fa fa-check"></i><b>17.1</b> Consider analysis before data collection</a></li>
<li class="chapter" data-level="17.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#explore-the-data"><i class="fa fa-check"></i><b>17.2</b> Explore the data</a></li>
<li class="chapter" data-level="17.3" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#evaluate-the-assumptions-underlying-your-analyses"><i class="fa fa-check"></i><b>17.3</b> Evaluate the assumptions underlying your analyses</a></li>
<li class="chapter" data-level="17.4" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#distinguish-between-confirmatory-and-exploratory-analyses"><i class="fa fa-check"></i><b>17.4</b> Distinguish between confirmatory and exploratory analyses</a></li>
<li class="chapter" data-level="17.5" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#aim-for-openness-and-reproducibility"><i class="fa fa-check"></i><b>17.5</b> Aim for openness and reproducibility</a></li>
<li class="chapter" data-level="17.6" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#communicate-clearly-and-concisely"><i class="fa fa-check"></i><b>17.6</b> Communicate clearly and concisely</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-multiple-regression-analysis"><i class="fa fa-check"></i><b>17.6.1</b> Example of reporting a multiple regression analysis</a></li>
<li class="chapter" data-level="17.6.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-factorial-anova"><i class="fa fa-check"></i><b>17.6.2</b> Example of reporting a factorial ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: Data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-generalized-linear-models" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Chapter 12</span> Generalized Linear Models<a href="ch-generalized-linear-models.html#ch-generalized-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- References: McCullagh and Nelder, Dobson, Agresti. -->
<p>Up to now, we have considered models for which the residuals (model errors) could be assumed to follow a Normal distribution. This is equivalent to modelling the conditional distribution of a dependent variable <span class="math inline">\(Y\)</span>, given the values of predictor variables <span class="math inline">\(X_j\)</span>, as a Normal distribution. Many of the techniques and ideas of such models can be generalised to situations where we can assume that the distribution of a dependent variable, conditional upon the values of predictor variables, follows a different distribution from a class of probability distributions known as the <em>exponential family</em>. Such models are known as <em>generalized</em> linear models <span class="citation">(<a href="#ref-mccullagh2019generalized" role="doc-biblioref">McCullagh &amp; Nelder, 2019</a>)</span>. This name makes for easy confusion with the <em>general</em> linear model, so make sure you don’t ignore the presence or absence of “-ized”!</p>
<p>The Normal distribution is part of the exponential family of distributions, as are many other distributions. These other distributions include the Binomial, Poisson, and Gamma distributions. Some of these distributions concern non-negative integer variables (i.e. variables with values <span class="math inline">\(0, 1, 2, \ldots\)</span>), others positive continuous variables (i.e. variables with values <span class="math inline">\(&gt;0\)</span>), or unbounded continuous variables (i.e. variables with values between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span>). Exponential-family distributions can all be expressed in the same form, but we won’t go into the details of that here.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> What is important is that there are a wide variety of statistical distributions beyond the Normal distribution. And that there are many variables which are <em>not</em> continuous and unbounded. It generally does not make sense to assume a Normal distribution for such variables. But because exponential-family distributions share important characteristics with the Normal distribution, assuming a different exponential-family distributions allows you to choose a distribution which is more natural for the variable in question, whilst still being able to formulate useful models for it. This is what we can do with generalized linear models. A generalized linear model consists of three components:</p>
<ol style="list-style-type: decimal">
<li>A <em>random</em> part, which describes the variability in the data by means of an exponential-family probability distribution with a conditional mean <span class="math inline">\(\mu_{Y|X_1, \ldots, X_m}\)</span>.</li>
<li>A <em>structural</em> part, which is a linear model of the form <span class="math inline">\(\beta_0 + \sum_{j=1}^m \beta_j \times X_j\)</span>.</li>
<li>A <em>link function</em>, which connects the structural and random part.</li>
</ol>
<p>Recall that the general linear model also consists of a random and structural part. What is new for generalized linear models is the link function.</p>
<p>In the remainder of this Chapter, we will first discuss link functions and the common methods of estimation and inference for generalized linear models. We will then focus on widely used instances of generalized linear models: logistic regression, Poisson regression, and loglinear models. We then discuss two extensions (multinomial logistic regression and generalized linear mixed-effects models).</p>
<div id="link-functions" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Link functions<a href="ch-generalized-linear-models.html#link-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we previously discussed applying the general linear model to “tricky” data, we considered transforming the dependent variable and/or the predictor variables, in order to “force” the data to approximately conform to the assumptions of the GLM (see Section <a href="ch-multiple-regression.html#sec:04b-transformations">5.7.1</a>). Generalized linear models also involve the use transformations. These transformations are performed via the link functions. However, in generalized linear models, these link functions are not applied the variables themselves, but rather to the predictions of a linear model.</p>
<p>In the general linear model, we model the conditional mean of a Normal-distributed variable as a linear function of predictor variables:
<span class="math display">\[\mu_{Y|X_1, \ldots, X_m} = \beta_0 + \sum_{j=1}^m \beta_j \times X_j\]</span>
In a generalized linear model, we model a <em>transformation</em> of the conditional mean of a wider class of distributions as a linear function of predictor variables:
<span class="math display" id="eq:glm-link-function">\[\begin{equation}
g(\mu_{Y|X_1, \ldots, X_K}) = \beta_0 + \sum_{j=1}^m \beta_j \times X_j
\tag{12.1}
\end{equation}\]</span>
where <span class="math inline">\(g\)</span> denotes the <strong>link function</strong>. This link function can take many forms. Common examples are</p>
<ul>
<li>Identity: <span class="math inline">\(g(y) = y\)</span></li>
<li>Log: <span class="math inline">\(g(y) = \log(y)\)</span></li>
<li>Logit <span class="math inline">\(g(y) = \log \left(\frac{y}{1-y}\right)\)</span></li>
</ul>
<p>The key thing is that the right-hand-side of Equation <a href="ch-generalized-linear-models.html#eq:glm-link-function">(12.1)</a> is a <em>linear function</em> that can, in principle, take any value between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span>. For variables that are bounded (e.g., non-negative variables <span class="math inline">\(Y &gt; 0\)</span>), such a linear model could provide predictions that are outside the bounds. An appropriate link function essentially makes such out-of-bounds predictions impossible. This might be easier to understand when we consider the inverse transformation:
<span class="math display" id="eq:glm-inverse-link-function">\[\begin{equation}
\mu_{Y|X_1, \ldots, X_K} = h(\beta_0 + \sum_{j=1}^m \beta_j \times X_j)
\tag{12.2}
\end{equation}\]</span>
where <span class="math inline">\(h\)</span> denotes the so-called <strong>inverse link function</strong>. Inverse link-functions corresponding to the list of common link functions above are:</p>
<ul>
<li>Inverse-Identity: <span class="math inline">\(h(x) = x\)</span></li>
<li>Inverse-Log (or Exponential): <span class="math inline">\(h(x) = \exp(x)\)</span></li>
<li>Inverse-Logit (or Logistic): <span class="math inline">\(h(x) = \frac{\exp(x)}{1+ \exp (x)}\)</span></li>
</ul>
<p>Take for example the inverse-log (or exponential) function above. No matter what the value of <span class="math inline">\(x\)</span>, the value of <span class="math inline">\(\exp(x)\)</span> will never be below 0. The value will approach 0 (from above) when <span class="math inline">\(x\)</span> approaches <span class="math inline">\(-\infty\)</span> (in other words, for very large negative values <span class="math inline">\(x\)</span>, <span class="math inline">\(\exp(x)\)</span> will be be a positive value very close to 0). So this inverse-link function allows one to transform the unbounded predictions from a linear model into bounded predictions suitable for a non-negative continuous variable.</p>
<p>Intuitively, you can think of a link function and the inverse-link function as follows: Suppose you have ten British pounds and need to convert this into US dollars. The conversion rate (at the time of writing this) is £1 = $1.08.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a> So if the function <span class="math inline">\(g\)</span> converts pounds (<span class="math inline">\(x\)</span>) into dollars (<span class="math inline">\(y\)</span>), we can write this as
<span class="math display">\[y = g(x) = 1.08 \times x\]</span>
So <span class="math inline">\(g\)</span> is a function that multiplies its argument by 1.08. If we were to convert dollars back to pounds, we would need a different function, which is the inverse (it “reverses” the effect) of our function <span class="math inline">\(g\)</span>. Calling this <span class="math inline">\(h\)</span>, we would need
<span class="math display">\[x = h(y) = \frac{1}{1.08} \times y \approx 0.926 \times y\]</span>
Such transformation functions <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are also called <em>one-to-one-mappings</em>: for every amount in pounds, there is one corresponding amount in dollars, and vice versa. If I know the amount in one currency, there is only a single amount in the second currency that this refers to. The link function needs to be such a one-to-one mapping. But otherwise, we are free to choose it. For a given link function, there is a unique inverse-link function. In the currency-exchange example, the link function is a linear function, but we can also choose a nonlinear function, e.g.
<span class="math display">\[y = g(x) = \exp x\]</span>
This function has the unique inverse-link function:
<span class="math display">\[x = h(y) = \log y\]</span>
You can also think of the inverse link function as “undoing” the link function, in the sense that applying the inverse link function to the link function itself, we get the values put into the link function:
<span class="math display">\[h(g(x)) = x\]</span>
Whilst the choice of a link function is essentially up to the analyst, it is common to use <strong>canonical link functions</strong>. Canonical link functions are, roughly put, link functions <span class="math inline">\(g\)</span> that are the most direct way to express <span class="math inline">\(h(\mu_{Y|X_1,\ldots,X_m})\)</span> for the particular distribution in question.<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a></p>
</div>
<div id="estimation-1" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Estimation<a href="ch-generalized-linear-models.html#estimation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The important parameters of a generalized linear model (Equation <a href="ch-generalized-linear-models.html#eq:glm-link-function">(12.1)</a>) are the intercept and slopes <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=0,\ldots,m\)</span>. As before, we will focus on maximum likelihood estimation of the model parameters. Unlike for the parameters of the general linear model, analytical solutions for maximum likelihood parameters of generalized linear models are usually not available. However, parameter estimates can be obtained by an iterative numerical procedure called <em>Iteratively Reweighted Least Squares</em> (IRLS). You don’t need to understand the details of this. For present purposes, it is enough to know that this method starts with a “guess” of the parameter estimates, and then updates this guess repeatedly to increase the likelihood of the estimated model, until no further improvement is obtained, at which point the algorithm is said to have <em>converged</em>. Whilst modern statistical software generally has robust implementations of this procedure, estimation difficulties may arise in cases (e.g. when using non-standard link functions). This may be due to the model being a poor representation of the data, but potentially also because of a poor initial guess for the parameter values.</p>
</div>
<div id="inference-in-generalized-linear-models" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Inference in generalized linear models<a href="ch-generalized-linear-models.html#inference-in-generalized-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As usual, we are generally not just interested in the estimates of the model parameters. We know data is noisy and parameter estimates are likely to differ from the true parameter values of the Data Generating Process. As for the general linear model, there are different ways to test whether a parameter <span class="math inline">\(\beta\)</span> differs from a hypothesised value <span class="math inline">\(\underline{\beta}\)</span>. A first method focusses on the sampling distribution of parameter estimates, assuming the true value is <span class="math inline">\(\underline{\beta}\)</span>. Alternatively, we can compare a MODEL R where we restrict the parameter to take the hypothesised value, to a MODEL G where we assume the value of the parameter is unknown, requiring us to estimate that parameter. These two methods are implemented via the Wald test and likelihood-ratio test, respectively.</p>
<div id="wald-test" class="section level3 hasAnchor" number="12.3.1">
<h3><span class="header-section-number">12.3.1</span> Wald test<a href="ch-generalized-linear-models.html#wald-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For large samples, the test statistic
<span class="math display">\[\begin{equation}
z = \frac{\hat{\beta_j} - \underline{\beta}}{\text{SE}(\hat{\beta}_j)}
\end{equation}\]</span>
approximately follows a standard Normal distribution (i.e. a Normal distribution with mean <span class="math inline">\(\mu=0\)</span> and standard deviation <span class="math inline">\(\sigma=1\)</span>). Note that this <span class="math inline">\(z\)</span> statistic is similar to the <span class="math inline">\(t\)</span>-statistic for the General Linear Model. What is missing however are the degrees of freedom. For large samples, as the number of observations <span class="math inline">\(n\)</span> approaches infinity, the <span class="math inline">\(t\)</span>-distribution converges to the standard Normal distribution. For large-enough <span class="math inline">\(n\)</span>, the difference between the <span class="math inline">\(t\)</span>-distribution and the Normal distribution is so small that there is little difference when using one or the other to compute things like <span class="math inline">\(p\)</span>-values (but technically, the use of a Normal distribution is only truly correct when the number of observations is infinite).</p>
<p>Sometimes, the Wald test is presented in an alternative form:
<span class="math display">\[\begin{equation}
z^2 = \left(\frac{\hat{\beta_j}}{\text{SE}(\beta_j)}\right)^2
\end{equation}\]</span>
which approximately (for large sample sizes) follows a Chi-squared distribution with <span class="math inline">\(\text{df} = 1\)</span>. This is due to a mathematical fact that if a variable <span class="math inline">\(z\)</span> follows a standard Normal distribution, then <span class="math inline">\(z^2\)</span> follows a Chi-squared distribution with <span class="math inline">\(\text{df}=1\)</span> degrees of freedom. Therefore, both statistics give entirely equivalent results. However, the <span class="math inline">\(z\)</span>-statistic can be used to obtain one-sided tests, whilst the <span class="math inline">\(z^2\)</span> can only be used to obtain two-sided tests. Given the additional flexibility of the <span class="math inline">\(z\)</span>-statistics, this is what we will use for our Wald tests. But remember: both statistics are approximate and only exact for an infinite sample size. The Wald test (in either form) requires more data than the tests discussed below.</p>
</div>
<div id="likelihood-ratio-test" class="section level3 hasAnchor" number="12.3.2">
<h3><span class="header-section-number">12.3.2</span> Likelihood-ratio test<a href="ch-generalized-linear-models.html#likelihood-ratio-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Wald test is easy to compute and works well for large-enough samples. A more powerful and reliable test for smaller samples is the likelihood-ratio test. To test whether a parameter differs from a hypothesized value, this involves a comparison between a MODEL R where the parameter is fixed to that hypothesised value, and a MODEL G where the parameter is estimated. For general linear models, the <span class="math inline">\(F\)</span>-statistic could be used to perform such likelihood ratio tests. Unfortunately, the wider-class of distributions covered by generalized linear models does not allow for these precise tests. Instead, we will need to rely on the the approximate Chi-squared test that we introduced in the context of linear mixed-effects models.</p>
<p>The test statistic used for a likelihood-ratio test is, as in Chapter <a href="ch-linear-mixed-effects-models.html#ch-linear-mixed-effects-models">11</a>, equal to minus two times the log likelihood ratio:
<span class="math display">\[\begin{aligned}
G^2 = -2 \log \left(\text{likelihood-ratio}\right) &amp;= -2 \log \left( \frac{p(\text{DATA}|\text{MODEL R})}{p(\text{DATA}|\text{MODEL G})} \right) \\ &amp;= -2 \log p(\text{DATA}|\text{MODEL R}) - (-2 \log p(\text{DATA}|\text{MODEL G}))
\end{aligned}\]</span>
Note that we denote the “minus 2 log-likelihood ratio” with the symbol <span class="math inline">\(G^2\)</span>, as this is commonly used in the literature. Note that the letter <span class="math inline">\(G\)</span> in this notation does <em>not</em> stand for our MODEL G! For large-enough samples, the sampling distribution of this statistic is approximately equal to a Chi-squared distribution:
<span class="math display">\[G^2 \sim \mathbf{chi-squared}(\text{npar}(G) - \text{npar}(R))\]</span>
with degrees-of-freedom equal to the difference in the number of estimated parameters (<span class="math inline">\(\text{npar}(G) - \text{npar}(R)\)</span>) between the models.</p>
</div>
<div id="confidence-intervals-1" class="section level3 hasAnchor" number="12.3.3">
<h3><span class="header-section-number">12.3.3</span> Confidence intervals<a href="ch-generalized-linear-models.html#confidence-intervals-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another option for inference is to compute confidence intervals. A straightforward way to compute (approximate) confidence intervals is to rely on the asymptotically Normal distribution of parameter estimates (as in the <span class="math inline">\(z\)</span>-statistic for testing parameter values). The confidence interval is computed by adding or subtracting a multiple of the standard error of the estimate from the estimate:
<span class="math display" id="eq:glm-confidence-interval-normal-approximation">\[\begin{equation}
\hat{\beta}_j \pm z_{1-\tfrac{1}{2}\alpha} \times \text{SE}(\hat{\beta}_j)
\tag{12.3}
\end{equation}\]</span>
Here, <span class="math inline">\(z_{1-\tfrac{1}{2}\alpha}\)</span> is the <span class="math inline">\(1 - \tfrac{1}{2}\alpha\)</span> quantile of the standard Normal distribution (e.g., the value <span class="math inline">\(z\)</span> such that <span class="math inline">\(p(Z \leq z) = 1 - \tfrac{1}{2}\alpha\)</span>). For example, for a 95% confidence interval, we use <span class="math inline">\(\alpha = .05\)</span> and the computation would be
<span class="math display">\[\hat{\beta}_j \pm z_{.975} \times \text{SE}(\hat{\beta}_j) \approx \hat{\beta}_j \pm 1.96 \times \text{SE}(\hat{\beta}_j)\]</span>
More accurate (although still approximate) confidence intervals for small samples can be computed through the <strong>profile likelihood</strong> <span class="citation">(<a href="#ref-venzon1988method" role="doc-biblioref">Venzon &amp; Moolgavkar, 1988</a>)</span>.^[Roughly, this method computes a confidence interval for one parameter, e.g. <span class="math inline">\(\beta_j\)</span>, by constructing a profile log-likelihood function
<span class="math display">\[l_p(\beta_j) = \log p(\text{DATA}|\beta_j, \hat{\theta}_{\neg j|\beta_j})\]</span>
where <span class="math inline">\(\beta_j\)</span> is free to vary, and the remaining parameters of the model, denoted as <span class="math inline">\(\hat{\theta}_{\neg j|\beta_j}\)</span>, are replaced by their maximum likelihood estimates given <span class="math inline">\(\beta_j\)</span>. We can alternatively write this as
<span class="math display">\[l_p(\beta_j) = \max_{\theta_{\neg j}} \left(\log p(\text{DATA}|\beta_j, \theta_{\neg j})\right)\]</span></p>
<p>This profile log likelihood function can then be used to compute possible likelihood ratio tests, comparing MODEL G where all parameters are estimated by maximum likelihood, to a model where the parameter of interest <span class="math inline">\(\beta_j\)</span> is fixed to some value:
<span class="math display">\[\begin{aligned} -2 \log (\text{profile-likelihood-ratio}) &amp;= -2 \log l_p(\beta_j) - (-2 \log l(\hat{\theta})) \\
&amp;= 2 \left(l(\hat{\theta}) - l_p(\beta_j) \right) \end{aligned}\]</span>
where <span class="math inline">\(l(\hat{\theta}) = \log p(\text{DATA}|\hat{\theta})\)</span> denotes the maximum of the log likelihood over all parameters (including <span class="math inline">\(\beta_j\)</span>). As the likelihood-ratio statistic is approximately Chi-squared distributed, we can use the Chi-square distribution (with <span class="math inline">\(\text{df} = 1\)</span> degree of freedom) to determine a critical value, e.g. <span class="math inline">\(\chi^2_{1; 1 - \alpha}\)</span>. We can then finally find the left and right bounds of the confidence interval by finding values <span class="math inline">\(\beta_\text{left}\)</span> and <span class="math inline">\(\beta_\text{right}\)</span> such that
<span class="math display">\[2 \left(l(\hat{\theta}) - l_p(\beta_\text{left}) \right) = 2 \left(l(\hat{\theta}) - l_p(\beta_\text{right}) \right) = \chi^2_{1; 1 - \alpha}\]</span>
] In contrast to the confidence interval based on the Normal approximation (Equation <a href="ch-generalized-linear-models.html#eq:glm-confidence-interval-normal-approximation">(12.3)</a>), confidence intervals based on the profile-likelihood can be non-symmetrical (i.e. the left- and right-bound don’t necessarily have the same distance from the estimate). For non-linear models such as generalized linear models, confidence intervals will often not be symmetrical, unless the number of observations in large. As such, confidence intervals based on the profile likelihood are likely to be more accurate. Yet another way to compute confidence intervals is by a <strong>parametric bootstrap</strong>.<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a></p>
</div>
</div>
<div id="assessing-model-fit" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> Assessing model fit<a href="ch-generalized-linear-models.html#assessing-model-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For the General Linear Model, the <span class="math inline">\(R^2\)</span> statistic reflects the “proportion of variance explained” and provides a useful way to assess the fit of the model to the data. For generalized linear models, a straightforward measure of “proportion of variance explained” is not available.<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a></p>
<p>For generalized linear models, instead of computing something like an <span class="math inline">\(R^2\)</span> measure, it is more common to test overall model fit. This is done by comparing an estimated MODEL R to a <em>saturated</em> MODEL G. A <strong>saturated model</strong>, which we will denote as MODEL S, has as many parameters as unique patterns in the data, and therefore fits the data perfectly. Whilst a model that always fits the data perfectly is not very interesting from a modelling point-of-view, it provides a useful criterion to compare less complex models to. The question asked in this comparison can be stated as: <em>how well does my model perform compared to a perfect model of the Data Generating Process?</em>.</p>
<p>The <span class="math inline">\(-2 \log(\text{likelihood-ratio})\)</span> comparing an estimated MODEL R to the saturated MODEL S is called the <strong>residual deviance</strong> of MODEL R:
<span class="math display" id="eq:glm-deviance-definition">\[\begin{equation}
D_R = -2 \log \frac{p(\text{DATA}|\text{MODEL R})}{p(\text{DATA}|\text{MODEL S})}
\tag{12.4}
\end{equation}\]</span>
Note that the deviance is effectively a <span class="math inline">\(G^2\)</span> statistic (the difference is that for the deviance the MODEL G in question is always the saturated MODEL S). As such, under the null-hypothesis that MODEL R is the “true model”, the deviance approximately follows a Chi-Squared distribution with degrees of freedom <span class="math inline">\(\text{df} = \text{npar}(S) - \text{npar}(R)\)</span>. As the number of parameters of the saturated model is often equal to the number of observations, this often is identical to <span class="math inline">\(\text{df} = n-\text{npar}(G)\)</span>. This equivalence does not always hold however: For some models (e.g. loglinear models), the minimum number of parameters needed to acquire perfect fit may be less than the number of observations. A deviance test of overall model fit concerns the null-hypothesis that MODEL R is the true model. Therefore, a significant test results indicate that MODEL R is not the true model, and that a more complex model will fit the data better.</p>
<p>You can think of the residual deviance similarly to the Sum of Squared Errors (SSE) of a model: the higher the deviance, the lesser the fit of a model to the data. And just like the SSE, the residual deviance can also be written as a sum of the deviance for individual data points.<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a>
Whilst the residual deviance values for individual observations can be used to indicate potential outliers, there are other forms of residuals more useful for this purpose. One of the most widely-used residuals for generalized linear models are so-called <strong>standardized Pearson residuals</strong>.<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a> If the model fits the data well, then the standardized Pearson residuals are approximately standard-Normal-distributed. As such, a standardized Pearson residual with an absolute value larger than 2 may indicate a potential outlier or otherwise problematic data point.</p>
<p>All in all, the residual deviance is to generalized linear models what the SSE is to general linear models. We can actually rewrite the likelihood-ratio test statistics to compare MODEL R to MODEL G in terms of the residual deviance of both models:
<span class="math display">\[G^2 = D_R - D_G\]</span>
Why? Well, because we can divide both the numerator and denominator in the likelihood ratio by the same term (<span class="math inline">\(p(\text{DATA}|\text{MODEL S})\)</span>) without changing its value:
<span class="math display">\[\begin{aligned}
G^2 &amp;=-2 \log \left( \frac{p(\text{DATA}|\text{MODEL R})}{p(\text{DATA}|\text{MODEL G})} \right) \\
&amp;= -2 \log \left( \frac{p(\text{DATA}|\text{MODEL R})/p(\text{DATA}|\text{MODEL S})}{p(\text{DATA}|\text{MODEL G})/p(\text{DATA}|\text{MODEL S})} \right) \\
&amp;= -2 \log \left(p(\text{DATA}|\text{MODEL R})/p(\text{DATA}|\text{MODEL S})\right) \\
&amp;\quad - (-2 \log \left( p(\text{DATA}|\text{MODEL G})/p(\text{DATA}|\text{MODEL S})) \right) \\
&amp;= D_R - D_G
\end{aligned}\]</span></p>
<!-- TODO Hosmer-Lemeshow? -->
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> Logistic regression<a href="ch-generalized-linear-models.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Logistic regression is used for <em>dichotomous</em> or <em>binary</em> dependent variables. Such variables can take one of two possible values. An example is a variable indicating whether a question is answered correctly (<span class="math inline">\(Y = 1\)</span>) or incorrectly (<span class="math inline">\(Y=0\)</span>). As a binary dependent variable can take only two values, assuming it follows a Normal distribution is not sensible. The same can be said for assuming the model errors (residuals) follow a Normal distribution.</p>
<p>Binary data can be assumed to follow a <strong>Bernoulli distribution</strong>:
<span class="math display" id="eq:Bernoulli-distribution">\[\begin{equation}
p(Y = k) = \theta^k \times (1-\theta)^{1-k}
\tag{12.5}
\end{equation}\]</span>
with <span class="math inline">\(k=0, 1\)</span>. Note that this is a simple case of the Binomial distribution we encountered in the Chapter <a href="ch-modeling.html#ch-modeling">2</a>.<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> Also note that as <span class="math inline">\(\theta^0 = 1\)</span>, and as <span class="math inline">\(k\)</span> can only be 0 or 1, Equation <a href="ch-generalized-linear-models.html#eq:Bernoulli-distribution">(12.5)</a>) simply states that
<span class="math display">\[\begin{aligned}
p(Y = 1) &amp;= \theta^1 \times (1-\theta)^{0} = \theta \times 1 \\ &amp;= \theta \\
p(Y = 0) &amp;= \theta^0 \times (1-\theta)^{1} = 1 \times (1 - \theta) \\ &amp;= 1 - \theta
\end{aligned}\]</span></p>
<p>The mean of a Bernoulli-distributed variable <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mu_Y = p(Y=1) = \theta\)</span>. The variance is <span class="math inline">\(\text{Var}(Y) = \theta \times (1-\theta)\)</span>. Note that, in contrast to the Normal distribution, where the mean and variance are determined by separate parameters (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, respectively), the Bernoulli distribution is determined by a single parameter (<span class="math inline">\(\theta\)</span>), and there is a direct relation between the mean (<span class="math inline">\(\theta\)</span>) and the variance (<span class="math inline">\(\theta \times (1-\theta)\)</span>). In many generalized linear models, there is a relation between the mean and the variance. That implies there is no homogeneity of variance. However, as long as the variance depends on the same parameter as the mean, this is no problem, as the heteroscedastic variance is determined by the mean, and hence known when the mean is known.</p>
<p>Modelling the mean with predictor variables <span class="math inline">\(X_j\)</span>, the conditional mean of a Bernoulli-distributed variable can be written as:
<span class="math display">\[\begin{aligned}
\mu_{Y|X_1, \ldots, X_m} &amp;= p(Y=1|X_1, \ldots, X_m) = \pi_1
\end{aligned}\]</span>
<!--As above, due to the binary nature of $Y$, we also know that
$$\begin{aligned}
p(Y=0|X_1, \ldots, X_m) &= 1 - p(Y=1|X_1, \ldots, X_m) \\
&= 1 - \mu_{Y|X_1, \ldots, X_m} \\
\pi_0 &= 1-\pi_1
\end{aligned}$$
This means that the probability distribution for $Y$ is completely determined by $\mu_{Y|X_1, \ldots, X_m} = \pi_1$, and our model can focus on just this.-->
Note that we introduced the shorthand <span class="math inline">\(\pi_1\)</span> to denote the (conditional) probability that <span class="math inline">\(Y=1\)</span>.</p>
<p>The canonical link function for Bernoulli-distributed data is the so-called <strong>logit link</strong> function:
<span class="math display" id="eq:glm-logit-link-function-mu">\[\begin{equation}
g(\mu_{Y|X_1, \ldots, X_m}) = \log \left( \frac{\mu_{Y|X_1, \ldots, X_K}}{1-\mu_{Y|X_1, \ldots, X_K}} \right)
\tag{12.6}
\end{equation}\]</span>
This may be a bit easier to read after using our shorthand notation <span class="math inline">\(\pi_1 = \mu_{Y|X_1, \ldots, X_m}\)</span>:
<span class="math display" id="eq:glm-logit-link-function">\[\begin{equation}
g(\pi_1) = \log \left( \frac{\pi_1}{1-\pi_1} \right)
\tag{12.7}
\end{equation}\]</span>
The logistic regression model is then
<span class="math display" id="eq:glm-logistic-regression-function">\[\begin{equation}
\log \left( \frac{\pi_1}{1-\pi_1} \right) = \beta_0 + \beta_1 \times X_1 + \ldots + \beta_m \times X_m
\tag{12.8}
\end{equation}\]</span></p>
<p>The inverse link function is called the inverse-logit or <strong>logistic</strong> function:
<span class="math display" id="eq:glm-inverse-logit-link-function">\[\begin{equation}
h(\beta_0 + \sum_{j=1}^m \beta_j) = \frac{\exp \{ \beta_0 + \sum_{j=1}^m \beta_j X_{j} \}}{1 + \exp \left( \beta_0 + \sum_{j=1}^m \beta_j X_{j} \right)}
\tag{12.9}
\end{equation}\]</span>
So the inverse of Equation <a href="ch-generalized-linear-models.html#eq:glm-logit-link-function-mu">(12.6)</a> is:
<span class="math display">\[\begin{equation}
\mu_{Y|X_1,\ldots,X_m} = \pi_1 = \frac{\exp \left( \beta_0 + \sum_{j=1}^m \beta_j X_{j} \right)}{1 + \exp \left( \beta_0 + \sum_{j=1}^m \beta_j X_{j} \right)}
\end{equation}\]</span></p>
<p>Some examples of the relation between a single predictor and different values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are shown in Figure <a href="ch-generalized-linear-models.html#fig:logit-link-plot">12.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logit-link-plot"></span>
<img src="12-GenLM_files/figure-html/logit-link-plot-1.svg" alt="Examples of the logistic link function for a logistic regression model with a single predictor, and different values for $\beta_0$ and $\beta_1$" width="672" />
<p class="caption">
Figure 12.1: Examples of the logistic link function for a logistic regression model with a single predictor, and different values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>
</p>
</div>
<p>As usual, the intercept <span class="math inline">\(\beta_0\)</span> reflects the model prediction when all predictors have the value <span class="math inline">\(X_j = 0\)</span>. It is important to keep in mind that generalized linear models make predictions not on the scale of the dependent variable <span class="math inline">\(Y\)</span>, but on the scale of the link function <span class="math inline">\(g(Y)\)</span>. We can use the inverse link-function to assess the predicted value on the scale of the dependent variable. In the case of logistic regression, this is done via the logistic function, <span class="math inline">\(\frac{\exp(\beta_0)}{1+\exp(\beta_0)}\)</span>. For a single-predictor model, the value <span class="math inline">\(-\frac{\beta_0}{\beta_1}\)</span> is equal to value of the predictor at which <span class="math inline">\(\pi_1 = 0.5\)</span>.<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a> This is also the point at which the logistic curve is the steepest. The slope <span class="math inline">\(\beta_1\)</span> reflects the maximum steepness of the curve, as well as its direction. Positive values of <span class="math inline">\(\beta_1\)</span> indicate that the probability <span class="math inline">\(\pi_1\)</span> increases with the value of <span class="math inline">\(x\)</span>, whilst negative values of <span class="math inline">\(\beta_1\)</span> indicate it decreases. The higher the absolute value of <span class="math inline">\(\beta_1\)</span>, the more quickly the probability increases or decreases (i.e. the steeper the logistic curve).</p>
<div id="parameter-interpretation" class="section level3 hasAnchor" number="12.5.1">
<h3><span class="header-section-number">12.5.1</span> Parameter interpretation<a href="ch-generalized-linear-models.html#parameter-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the general linear model, <span class="math inline">\(\beta_j\)</span>, the slope of a predictor variable <span class="math inline">\(X_j\)</span>, reflects the predicted change in the dependent variable for a one-unit increase in <span class="math inline">\(X_j\)</span>. Because the model is linear, the predicted change is the same no matter what starting value of <span class="math inline">\(X_j\)</span> we choose for this one-unit increase. The dependent variable <span class="math inline">\(Y\)</span> is predicted to change by <span class="math inline">\(\beta_j\)</span>, whether we increase <span class="math inline">\(X_j\)</span> from 0 to 1, or from 100 to 101. This is not the case when we consider the predicted change in <span class="math inline">\(p(Y=1|X_j)\)</span> for different values of <span class="math inline">\(X_j\)</span>. For example, for a model with <span class="math inline">\(\beta_0 = 0\)</span> and <span class="math inline">\(\beta_1 = 1\)</span>, Figure <a href="ch-generalized-linear-models.html#fig:logit-link-plot">12.1</a> shows that th probability increases more rapidly when we move from <span class="math inline">\(X_1 = 0\)</span> to <span class="math inline">\(X_1 = 1\)</span> than when we move from <span class="math inline">\(X_1 = 3\)</span> to <span class="math inline">\(X_1 = 4\)</span>.</p>
<p>A slope <span class="math inline">\(\beta_j\)</span> in a logistic regression model reflects a change in the <em>logit</em> <span class="math inline">\(\log \left( \frac{\pi_1}{1-\pi_1}\right)\)</span> for a one-unit increase in a predictor value. This change in the logit is the same, not matter the starting value of <span class="math inline">\(X_j\)</span> we choose. But one the scale of <span class="math inline">\(\pi_1\)</span>, the change in <span class="math inline">\(\pi_1\)</span> due to a one-unit increase in <span class="math inline">\(X_j\)</span> is different for different starting values of <span class="math inline">\(X_j\)</span>. As such, the slope <span class="math inline">\(\beta_j\)</span> does not have a straightforward interpretation on the scale of <span class="math inline">\(\pi_1\)</span>. But an exponential transformation of this slope does, to some extent. More precisely, the slope has a constant effect on the <strong>odds</strong> <span class="math inline">\(\frac{\pi_1}{1-\pi_1}\)</span>. To see how this works, we will make use of the following properties of exponents:
<span class="math display">\[\begin{aligned}
a^{(b + c)} &amp;= a^b \times a^c \\
a^{(b \times c)} &amp;= \left(a^{b}\right)^c = \left(a^{c}\right)^b .
\end{aligned}\]</span>
We also need to remember that <span class="math inline">\(\exp (a) = e^a\)</span>, where <span class="math inline">\(e\)</span> denotes the natural number, and <span class="math inline">\(\log \left(\exp(a)\right) = a\)</span>. Using these properties, we can rewrite the model predictions as
<span class="math display">\[\begin{aligned} \log\left(\frac{\pi_1}{1-\pi_1}\right) &amp;= \beta_0 + \beta_1 \times X \\
\exp \left( \log\left(\frac{\pi_1}{1-\pi_1}\right)\right) &amp;= \exp\left(\beta_0 + \beta_1 \times X\right) \\
\frac{\pi_1}{1-\pi_1} &amp;= \exp\left(\beta_0 + \beta_1 \times X\right) \\
&amp;= \exp(\beta_0) \times \exp\left(\beta_1 \times X\right) \\
&amp;= \exp(\beta_0) \times \left(\exp(\beta_1) \right)^{X} \end{aligned}\]</span>
In the final line of the above derivation, you can see that every one-unit increase in <span class="math inline">\(X_1\)</span> has the effect of <em>multiplying</em> the odds by <span class="math inline">\(\exp(\beta_1)\)</span>. For example, the odds at <span class="math inline">\(X_1 = 1\)</span> is <span class="math display">\[\exp(\beta_0) \times \left(\exp(\beta_1) \right)^{1} = \exp(\beta_0) \times \exp(\beta_1)\]</span>
and at <span class="math inline">\(X_1 = 2\)</span> it is
<span class="math display">\[\exp(\beta_0) \times \left(\exp(\beta_1) \right)^{2} = \exp(\beta_0) \times \exp(\beta_1) \times \exp(\beta_1)\]</span>
So the odds at <span class="math inline">\(X = 2\)</span> is the odds at <span class="math inline">\(X = 1\)</span> multiplied by <span class="math inline">\(\exp(\beta_1)\)</span>. Similarly, the odds at <span class="math inline">\(X = 2\)</span> is the odds at <span class="math inline">\(X = 1\)</span> multiplied by <span class="math inline">\(\exp(\beta_1)\)</span>.</p>
<p>On the scale of log-odds or logits, every one-unit increase in <span class="math inline">\(X\)</span> <em>adds</em> <span class="math inline">\(\beta_1\)</span> to the log-odds. On the scale of odds, every one-unit increase in <span class="math inline">\(X\)</span> <em>multiplies</em> the odds by <span class="math inline">\(\exp(\beta_1)\)</span>. In terms of interpreting the effect of <span class="math inline">\(X\)</span>, you can choose whether you find this easier to do in the scale of log-odds, or in the scale of odds. The most natural scale is probably the scale of the (probability of) response <span class="math inline">\(\pi_1 = p(Y=1|X)\)</span> itself. Unfortunately, the non-linear effect of <span class="math inline">\(X\)</span> on the response prevents a straightforward interpretation on this scale.</p>
</div>
<div id="sec-logistic-regression-metacognition" class="section level3 hasAnchor" number="12.5.2">
<h3><span class="header-section-number">12.5.2</span> Example: Metacognition in visual perception<a href="ch-generalized-linear-models.html#sec-logistic-regression-metacognition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Metacognition broadly refers to awareness of our thought processes. One aspect of metacognition is the ability to ascertain how (un)certain we are in our inferences about the world. For example, <span class="citation">Rausch &amp; Zehetleitner (<a href="#ref-rausch2016visibility" role="doc-biblioref">2016</a>)</span> asked participants to judge whether visually presented stimuli (so-called grating patterns) had a horizontal to vertical direction. To make this task difficult for their participants, they presented each stimulus only briefly (200 ms) and on a variety of relatively low-contrast displays. They also asked participants to indicate their confidence (on a scale from 0 to 100) that their judgement was correct. Participants each completed a total of 378 trials. If metacognition is accurate, then participants should be more confident when they made more accurate responses.</p>
<p>For two participants in the experiment, Figure <a href="ch-generalized-linear-models.html#fig:metacognition-confidence-correct-plot">12.2</a> shows the proportion of correct responses as a function of different levels of stated confidence. For Participant 1, there appears to be quite a good correspondence between confidence and correctness, with accuracy increasing for higher levels of confidence. For Participant 2, the relation does not look so neat, with average correctness higher in the lowest range of confidence, and then decreasing in the next two ranges, before rapidly increasing to almost 100% for the highest two confidence ranges. However, as is evident from the width of the confidence bands, this participant gave relatively few confidence ratings between 20 and 60.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:metacognition-confidence-correct-plot"></span>
<img src="12-GenLM_files/figure-html/metacognition-confidence-correct-plot-1.svg" alt="Confidence and proportion correct for two participants in the study by @rausch2016visibility on metacognition." width="768" />
<p class="caption">
Figure 12.2: Confidence and proportion correct for two participants in the study by <span class="citation">Rausch &amp; Zehetleitner (<a href="#ref-rausch2016visibility" role="doc-biblioref">2016</a>)</span> on metacognition.
</p>
</div>
<p>As the relation between confidence and accuracy likely differs between participants (e.g. participants might interpret the confidence scale differently, or have different levels of metacognition), estimating a single logistic regression model for both participants seems inappropriate. Hence, we will estimate separate logistic regression models for the two participants. In our model, we will allow the effect of confidence to be moderated by contrast (i.e., we allow metacognition to be affected by contrast). For easier interpretation, we first center both variables, and then estimate the following logistic regression model:</p>
<p><span class="math display">\[\log \left( \frac{p(Y_i = \text{correct})}{p(Y_i = \text{incorrect})} \right) = \beta_0 + \beta_\texttt{conf} \times \texttt{conf}_i + \beta_\text{contr} \times \texttt{contr}_i + \beta_{\texttt{cc}} \times (\texttt{conf} \times \texttt{contr})_i\]</span></p>
<p>Table <a href="ch-generalized-linear-models.html#tab:metacognition-logistic-regression-results-z-and-LR-participant-1">12.1</a> shows the results for Participant 1. Each participant completed 378 trials, so the number of observations in this subset of data equals <span class="math inline">\(n=378\)</span>. For didactic purposes, we present results from both the Wald (<span class="math inline">\(z\)</span>) and likelihood-ratio (<span class="math inline">\(G^2\)</span>) tests. You would usually just provide one of these.</p>
<table>
<caption><span id="tab:metacognition-logistic-regression-results-z-and-LR-participant-1">Table 12.1: </span>Results of a logistic regression model of correct identification by confidence for Participant 1.</caption>
<colgroup>
<col width="20%" />
<col width="9%" />
<col width="17%" />
<col width="3%" />
<col width="17%" />
<col width="4%" />
<col width="8%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SE}(\hat{\beta})\)</span></th>
<th align="right"><span class="math inline">\(z\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert z \rvert)\)</span></th>
<th align="right"><span class="math inline">\(G^2\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert G^2 \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">1.851</td>
<td align="right">0.234</td>
<td align="right">7.92</td>
<td align="right">&lt; .001</td>
<td align="right">94.43</td>
<td align="right">1</td>
<td align="right">&lt; .001</td>
</tr>
<tr class="even">
<td align="left">Confidence</td>
<td align="right">0.038</td>
<td align="right">0.007</td>
<td align="right">5.11</td>
<td align="right">&lt; .001</td>
<td align="right">29.82</td>
<td align="right">1</td>
<td align="right">&lt; .001</td>
</tr>
<tr class="odd">
<td align="left">Contrast</td>
<td align="right">0.503</td>
<td align="right">0.108</td>
<td align="right">4.65</td>
<td align="right">&lt; .001</td>
<td align="right">24.04</td>
<td align="right">1</td>
<td align="right">&lt; .001</td>
</tr>
<tr class="even">
<td align="left">Confidence <span class="math inline">\(\times\)</span> Contrast</td>
<td align="right">0.008</td>
<td align="right">0.003</td>
<td align="right">2.52</td>
<td align="right">.012</td>
<td align="right">6.65</td>
<td align="right">1</td>
<td align="right">.010</td>
</tr>
</tbody>
</table>
<p>Regardless of whether we rely on the Wald or likelihood-ratio test, the results show evidence for a positive interaction between confidence and contrast. We also find evidence of positive “simple effects” for confidence and contrast. As we centered the predictors, the simple effect of confidence reflects its effect at the average contrast value. Vice versa, the simple effect of contrast reflects its effect at the average confidence value. We can therefore conclude that, at average levels of confidence, higher levels of contrast result in more accurate responses. We can also conclude that, at average levels of contrast, higher levels of confidence result in more accurate responses. This is thus evidence for accurate metacognition. The interaction indicates that meta-cognition might be more pronounced for higher levels of contrast (i.e., the relation between confidence and accuracy is stronger for higher values of contrast). Finally, the intercept is also significant. This indicates that, for average values of confidence and contrast (such that centered <span class="math inline">\(\texttt{conf}\)</span> and <span class="math inline">\(\texttt{contr}\)</span> are both equal to 0), the predicted probability of a correct response is different from .5. The estimated probability is:
<span class="math display">\[\begin{aligned}
p(Y_i = \text{correct}) &amp;= \frac{\exp (1.851)}{1 + \exp (1.851)} \\
&amp;= 0.864\end{aligned}\]</span>
As for the general linear model, tests of the intercept are usually not of interest.</p>
<p>The residual deviance of the model is <span class="math inline">\(D_R = 273.664\)</span>. We can use the deviance to test whether the model is equal to the true model, using a Chi-Squared distribution with <span class="math inline">\(n-\text{npar}(R)\)</span> degrees of freedom (here, the saturated model needs <span class="math inline">\(n\)</span> parameters to fit the data perfectly). Hence, the test of overall model fit is <span class="math inline">\(X^2(374) = 273.664\)</span>, <span class="math inline">\(p = 1.00\)</span>. As this is not significant, the model can be said to fit the data well.</p>
<p>Table <a href="ch-generalized-linear-models.html#tab:metacognition-logistic-regression-results-z-and-LR-participant-2">12.2</a> shows results from the same analysis applied to the data from Participant 2. Whilst the data seemed less neat than the data of Participant 1, we get rather similar results: significant and positive simple effects of confidence and contrast, as well as a significant positive interaction between them.</p>
<table>
<caption><span id="tab:metacognition-logistic-regression-results-z-and-LR-participant-2">Table 12.2: </span>Results of a logistic regression model of correct identification by confidence for Participant 2.</caption>
<colgroup>
<col width="20%" />
<col width="9%" />
<col width="17%" />
<col width="3%" />
<col width="17%" />
<col width="4%" />
<col width="8%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SE}(\hat{\beta})\)</span></th>
<th align="right"><span class="math inline">\(z\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert z \rvert)\)</span></th>
<th align="right"><span class="math inline">\(G^2\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert G^2 \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">1.871</td>
<td align="right">0.253</td>
<td align="right">7.38</td>
<td align="right">&lt; .001</td>
<td align="right">74.03</td>
<td align="right">1</td>
<td align="right">&lt; .001</td>
</tr>
<tr class="even">
<td align="left">Confidence</td>
<td align="right">0.021</td>
<td align="right">0.005</td>
<td align="right">4.03</td>
<td align="right">&lt; .001</td>
<td align="right">19.71</td>
<td align="right">1</td>
<td align="right">&lt; .001</td>
</tr>
<tr class="odd">
<td align="left">Contrast</td>
<td align="right">0.384</td>
<td align="right">0.121</td>
<td align="right">3.16</td>
<td align="right">.002</td>
<td align="right">8.66</td>
<td align="right">1</td>
<td align="right">.003</td>
</tr>
<tr class="even">
<td align="left">Confidence <span class="math inline">\(\times\)</span> Contrast</td>
<td align="right">0.005</td>
<td align="right">0.002</td>
<td align="right">2.13</td>
<td align="right">.033</td>
<td align="right">4.24</td>
<td align="right">1</td>
<td align="right">.039</td>
</tr>
</tbody>
</table>
<p>The test of the deviance of the model for Participant 2 is <span class="math inline">\(X^2(374) = 281.013\)</span>, <span class="math inline">\(p = 1.00\)</span>. As this is again not significant, the model can be said to fit the data well.</p>
<p>To get a better idea of the relation between confidence and accuracy, Figure <a href="ch-generalized-linear-models.html#fig:metacognition-data-and-logistic-fit-plot">12.3</a> depicts the observed data and model predictions for the logistic regression models of Table <a href="ch-generalized-linear-models.html#tab:metacognition-logistic-regression-results-z-and-LR-participant-1">12.1</a> and <a href="ch-generalized-linear-models.html#tab:metacognition-logistic-regression-results-z-and-LR-participant-2">12.2</a>. Note that the observed data are binary values indicating correct (1) or incorrect (0) responses. The model predictions however are probabilities of a correct (<span class="math inline">\(Y=1\)</span>) response. Both the data and model predictions indicate more correct responses for higher levels of contrast. For lower levels of contrast, we can see that low confidence generally implies a low probability of a correct response, whilst higher confidence a higher probability of a correct response. This patterns seems more pronounced for Participant 1. Participant 2 tends to indicate low confidence for low contrast displays, and high confidence for high contrast displays. For this participant, contrast appears a much stronger predictor of correctness. Nevertheless, Table <a href="ch-generalized-linear-models.html#tab:metacognition-logistic-regression-results-z-and-LR-participant-2">12.2</a> indicates that, for an average level of contrast, confidence has a significant unique effect on the probability of a correct response. As such, this participant appears able to recognize when they made an incorrect response.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:metacognition-data-and-logistic-fit-plot"></span>
<img src="12-GenLM_files/figure-html/metacognition-data-and-logistic-fit-plot-1.svg" alt="Observed and model-predicted response accuracy as a function of confidence and contrast, for two participants in the study by @rausch2016visibility on metacognition." width="768" />
<p class="caption">
Figure 12.3: Observed and model-predicted response accuracy as a function of confidence and contrast, for two participants in the study by <span class="citation">Rausch &amp; Zehetleitner (<a href="#ref-rausch2016visibility" role="doc-biblioref">2016</a>)</span> on metacognition.
</p>
</div>
</div>
<div id="using-a-different-link-function-probit-regression" class="section level3 hasAnchor" number="12.5.3">
<h3><span class="header-section-number">12.5.3</span> Using a different link function: Probit regression<a href="ch-generalized-linear-models.html#using-a-different-link-function-probit-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
The logit link is the canonical link function for the Bernoulli and Binomial distribution. But you don’t necessarily have to use the canonical link function. In principle, for binary data, you can pick any (monotone and one-to-one) function <span class="math inline">\(g(Y)\)</span> that transforms variable <span class="math inline">\(Y\)</span> which is bounded between 0 and 1 to an unbounded variable with possible values between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span>. For example, a common alternative to the logit link is the so-called <strong>Probit link function</strong>:
<span class="math display">\[g(y) = \text{standard-Normal-quantile}(y)\]</span>
The Probit link function returns the <span class="math inline">\(y\)</span>-th quantile of the standard Normal distribution. Recall that a <span class="math inline">\(y\)</span>-th quantile is a distribution-dependent value <span class="math inline">\(x\)</span> such that a random variable <span class="math inline">\(Z\)</span> has a probability being equal to or smaller than it which equals <span class="math inline">\(y\)</span>. Quantiles rely on the cumulative probability distribution (see Figure for an example involving the standard normal distribution).
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cumulative-normal-quantile-plot-Probit"></span>
<img src="12-GenLM_files/figure-html/cumulative-normal-quantile-plot-Probit-1.svg" alt="The Probit link function equals the standard Normal cumulative distribution function. THe solid dark line shows the standard Normal cumulative distribution function, with arrows depicting the .05, .5, .7, and .975 quantiles." width="672" />
<p class="caption">
Figure 12.4: The Probit link function equals the standard Normal cumulative distribution function. THe solid dark line shows the standard Normal cumulative distribution function, with arrows depicting the .05, .5, .7, and .975 quantiles.
</p>
</div>
<p>The inverse-link function for the Probit link function can be written as
<span class="math display">\[\begin{aligned} h(x) = p(Z \leq x) &amp;&amp; Z \sim \mathbf{Normal}(\mu = 0, \sigma = 1) \end{aligned}\]</span>
i.e., the inverse link function is, for a value <span class="math inline">\(x\)</span>, the probability that a standard Normal variable has a value equal or less than it.</p>
<p>The Probit link function is an S-shaped function that looks rather similar to the logistic link function. Figure <a href="ch-generalized-linear-models.html#fig:probit-link-plot">12.5</a> shows best matching Probit link function to logistic ones. As can be seen, the functions are very similar, although the Probit function converges a little more quickly to probabilities of 0 and 1. The best match between the functions is obtained when the parameters for the Probit and logistic models are related as
<span class="math display">\[\beta_j^{(\text{Probit})} \approx 0.588 \times \beta_j^{(\text{logistic})}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:probit-link-plot"></span>
<img src="12-GenLM_files/figure-html/probit-link-plot-1.svg" alt="A comparison of logistic and Probit link functions for a model with a single predictor, and best matching values for $\beta_0$ and $\beta_1$" width="576" />
<p class="caption">
Figure 12.5: A comparison of logistic and Probit link functions for a model with a single predictor, and best matching values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>
</p>
</div>

<div class="advancedtext">
<p>Whilst using a logit or Probit link function will generally provide very similar results, the Probit link function may have a more natural justification than the logit link. This is because the Probit link function can be justified in terms of a “hidden” or latent Normal-distributed variable. For example, suppose that someone is presented with two lines, and has to judge whether the one on the right is longer than the one on the left. Denote the true lengths as <span class="math inline">\(\tau_l\)</span> and <span class="math inline">\(\tau_r\)</span> for the line on the left and right respectively. Due to perceptual noise, vision is not perfect. Assume that the perceived length of the lines, denoted as <span class="math inline">\(P_l\)</span> and <span class="math inline">\(P_r\)</span>, are both independent and Normal-distributed around the true line length:
<span class="math display">\[
\begin{aligned} P_l &amp;= \tau_l + \epsilon_r &amp;&amp; \epsilon_l \sim \mathbf{Normal}(0,\sigma_\epsilon) \\
P_r &amp;= \tau_r + \epsilon_r &amp;&amp; \epsilon_r \sim \mathbf{Normal}(0,\sigma_\epsilon) \end{aligned}
\]</span>
Also assume the person decides the right line is longer whenever the perceived line on the right is longer than the perceived line on the left: <span class="math inline">\(P_r &gt; P_l\)</span>. Depending on the difference between the true lengths, <span class="math inline">\(\tau_r - \tau_l\)</span>, and the level of perceptual noise <span class="math inline">\(\sigma_\epsilon\)</span>, the shorter line may actually look longer than the longer line. As the perceptual noise terms <span class="math inline">\(\epsilon_l\)</span> and <span class="math inline">\(\epsilon_r\)</span> are independent and Normal-distributed, the perceived difference between the line-lengths is also a Normal-distributed variable:
<span class="math display">\[
P_r - P_l  \sim \textbf{Normal}(\tau_r - \tau_l, \sqrt{2} \sigma_{\epsilon})
\]</span>
We can easily turn this perceived difference into a standard-Normal variable
<span class="math display">\[
\frac{(P_r - P_l) -  (\tau_r - \tau_l)}{\sqrt{2} \sigma_{\epsilon}} \sim \textbf{Normal}(0, 1)
\]</span>
This allows us to use a Probit function to determine the probability that the person decides the right line is larger than the left, as a function of the true difference between the line-lengths. Letting <span class="math inline">\(X = \tau_r - \tau_l\)</span>, and the model <span class="math inline">\(\text{Probit}\left(p(\text{right})\right) = \beta_0 + \beta_1 \times X\)</span>, this would imply the following true parameters: <span class="math inline">\(\beta_0 = 0\)</span> and <span class="math inline">\(\beta_1 = \frac{1}{\sqrt{2}\sigma_\epsilon}\)</span>. As such, the slope of this Probit regression model would allow us to estimate the perceptual noise <span class="math inline">\(\sigma_\epsilon\)</span>. The model also allows to estimate a possible response bias (e.g., where person is biased by being more likely to respond right, even if the true line lengths are equal) via the intercept <span class="math inline">\(\beta_0\)</span>. A construction like this links the model to widely-used frameworks like Signal Detection Theory <span class="citation">(<a href="#ref-green1966signal" role="doc-biblioref">Green &amp; Swets, 1966</a>)</span>. It should be noted that the logit link function can be provided with a similar latent variable justification, if it is assumed that the difference in errors <span class="math inline">\(\epsilon_l - \epsilon_r\)</span> is logistic-distributed <span class="citation">(<a href="#ref-mcfadden1973conditional" role="doc-biblioref">McFadden, 1973</a>)</span></p>
</div>
<p>Table <a href="ch-generalized-linear-models.html#tab:metacognition-probit-regression-results-z-and-LR-participant-1">12.3</a> shows the results of applying a Probit regression to the data of Participant 1. The estimated parameters are around 0.588 times those of the logistic regression model, and while the test results are not identical to those in Table <a href="ch-generalized-linear-models.html#tab:metacognition-logistic-regression-results-z-and-LR-participant-1">12.1</a>, they are mostly similar. The test of the deviance of this model is <span class="math inline">\(X^2(374) = 272.289\)</span>, <span class="math inline">\(p = 1.00\)</span>, indicating a good fit of the model. The main purpose of this additional example is to illustrate that other link functions can be used. The logit and Probit models imply different Data Generating Processes, and so their distinction may be interesting from a theoretical viewpoint. From a data-analytic perspective, however, they are very similar and as such the choice may seem somewhat arbitrary and up to personal preference.</p>
<table>
<caption><span id="tab:metacognition-probit-regression-results-z-and-LR-participant-1">Table 12.3: </span>Results of a Probit regression model of correct identification by confidence for Participant 1.</caption>
<colgroup>
<col width="20%" />
<col width="9%" />
<col width="17%" />
<col width="3%" />
<col width="17%" />
<col width="4%" />
<col width="8%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SE}(\hat{\beta})\)</span></th>
<th align="right"><span class="math inline">\(z\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert z \rvert)\)</span></th>
<th align="right"><span class="math inline">\(G^2\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert G^2 \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">1.041</td>
<td align="right">0.120</td>
<td align="right">8.64</td>
<td align="right">&lt; .001</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Confidence</td>
<td align="right">0.020</td>
<td align="right">0.004</td>
<td align="right">5.02</td>
<td align="right">&lt; .001</td>
<td align="right">27.28</td>
<td align="right">1</td>
<td align="right">&lt; .001</td>
</tr>
<tr class="odd">
<td align="left">Contrast</td>
<td align="right">0.280</td>
<td align="right">0.061</td>
<td align="right">4.58</td>
<td align="right">&lt; .001</td>
<td align="right">23.44</td>
<td align="right">1</td>
<td align="right">&lt; .001</td>
</tr>
<tr class="even">
<td align="left">Confidence <span class="math inline">\(\times\)</span> Contrast</td>
<td align="right">0.004</td>
<td align="right">0.002</td>
<td align="right">2.26</td>
<td align="right">.024</td>
<td align="right">5.28</td>
<td align="right">1</td>
<td align="right">.022</td>
</tr>
</tbody>
</table>
</div>
<div id="welcome-back-paul" class="section level3 hasAnchor" number="12.5.4">
<h3><span class="header-section-number">12.5.4</span> Welcome back Paul!<a href="ch-generalized-linear-models.html#welcome-back-paul" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As a final example of logistic regression, I can’t resist getting Paul back to the stage. We can also use logistic regression to test the hypothesis test from Chapter <a href="ch-modeling.html#ch-modeling">2</a> that Paul was randomly guessing, that is <span class="math inline">\(H_0: p(\text{correct}) = .5\)</span>. In this case, we have a simple model without a predictor. But we can test whether the intercept equals <span class="math inline">\(\beta_0 = 0\)</span>, as this value implies that <span class="math inline">\(p(\text{correct}) = .5\)</span>. For this model comparison, we will compare the following models:
<span class="math display">\[\begin{aligned} \text{MODEL R:}&amp;&amp; \log\left(\frac{\pi_1}{1-\pi_1}\right) &amp;= 0 \\
\text{MODEL G:} &amp;&amp; \log\left(\frac{\pi_1}{1-\pi_1}\right) &amp;= \beta_0
\end{aligned}\]</span>
For the data at hand (12 correct and 2 incorrect predictions by Paul), estimate of the intercept is <span class="math inline">\(\hat{\beta}_0 = 1.946\)</span> for MODEL G, which corresponds to a predicted probability of <span class="math inline">\(\hat{\pi}_1 = \frac{\exp(1.946)}{1 + \exp(1.946)} = 0.875\)</span>. The test result of the model comparison is <span class="math inline">\(G^2(1) = 10.124\)</span>, <span class="math inline">\(p = .001\)</span>. Hence, in correspondence with the results from Chapter <a href="ch-modeling.html#ch-modeling">2</a>, we can reject the null-hypothesis that Paul was randomly guessing. <!-- You were truly amazing Paul (even after applying a generalized linear model ;-).--></p>
</div>
</div>
<div id="poisson-regression" class="section level2 hasAnchor" number="12.6">
<h2><span class="header-section-number">12.6</span> Poisson regression<a href="ch-generalized-linear-models.html#poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- https://online.ucpress.edu/collabra/article/7/1/27242/118591/Poisson-Regressions-A-Little-Fishy -->
<!-- https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12439 -->
<p>Poisson regression is useful for count data where the total number of possible occurrences of an event is not given. For example, suppose you give participants one hour to solve as many algebra problems as they can. Some participants may be able to (on average) solve a problem correctly every minute, whilst others may need longer (e.g. 5 minutes per problem, on average), or shorter. In this case, the total number of correctly solved algebra problems depends on the (average) rate at which a person can solve algebra problems. If the rate is unknown and can, in principle, take any possible value, then there is no theoretical maximum to the number of algebra problems that can be solved in an hour. Only the minimum is known (which is 0). This is a rather different situation than those to which the Binomial distribution applies, which counts the number of correct responses <span class="math inline">\(k\)</span> out of a known maximum <span class="math inline">\(n\)</span>.</p>
<p>The <strong>Poisson distribution</strong> is defined as:
<span class="math display" id="eq:Poisson-distribution">\[\begin{equation}
p(Y=k) = \frac{\lambda^k \exp (-\lambda)}{k!}
\tag{12.10}
\end{equation}\]</span>
with <span class="math inline">\(k=0, 1, 2, \ldots\)</span> denoting the number of occurrences (the count), which is a non-negative integer, and ! denoting the factorial function (i.e. <span class="math inline">\(x! = x \times (x-1) \times \ldots \times 1\)</span>). The parameter <span class="math inline">\(\lambda\)</span> is equal to the mean of <span class="math inline">\(Y\)</span>, so <span class="math inline">\(\mu_Y = \lambda\)</span>. The variance of this distribution is equal to the mean, so <span class="math inline">\(\text{Var}(Y) = \lambda\)</span>. As for the Bernoulli distribution in logistic regression, the Poisson distribution used in Poisson regression does not include an additional parameter for the spread of the distribution.</p>
<p>The canonical link function for Poisson-distributed data is the <strong>log</strong> link function:
<span class="math display">\[g(\mu_{Y|X_1, \ldots, X_K}) = \log\left(\mu_{Y|X_1, \ldots, X_K}\right)\]</span>
The Poisson regression model is thus
<span class="math display">\[\log\left(\mu_{Y|X_1, \ldots, X_K}\right) = \beta_0 + \sum_{j=1}^m \beta_j \times X_{j}\]</span>
The inverse link function is the inverse-log or <strong>exponential</strong> function:
<span class="math display">\[h\left(\beta_0 + \sum_{j=1}^m \beta_j \times X_{j}\right) = \exp \left( \beta_0 + \sum_{j=1}^m \beta_j \times X_{j} \right)\]</span></p>
<div id="sec-poisson-regression-gestures" class="section level3 hasAnchor" number="12.6.1">
<h3><span class="header-section-number">12.6.1</span> Example: Gestures in different social contexts<a href="ch-generalized-linear-models.html#sec-poisson-regression-gestures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As an example of Poisson regression, we will consider data also analysed by <span class="citation">Winter &amp; Bürkner (<a href="#ref-winter2021poisson" role="doc-biblioref">2021</a>)</span>. The data is from a study on the use of hand gestures by 27 participants (Catalan or Korean speakers) each conversing with either a friend (equal-status person) or a professor (higher-status person). The duration of the conversation was free and not experimentally controlled. The variable of interest is the count of the number of gestures used by the participants throughout the conversation Figure <a href="ch-generalized-linear-models.html#fig:gestures-histogram">12.6</a> shows the numbers of gestures for the different conditions in the experiment. This Figure doesn’t immediately provide a clear idea of differences between the conditions.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gestures-histogram"></span>
<img src="12-GenLM_files/figure-html/gestures-histogram-1.svg" alt="Counts of the number of hand gestures during conversations with a friend or professor for 14 Catalan and 13 Korean speakers." width="672" />
<p class="caption">
Figure 12.6: Counts of the number of hand gestures during conversations with a friend or professor for 14 Catalan and 13 Korean speakers.
</p>
</div>
<p>To analyse the data with a Poisson regression model, we need to take into account that the conversations had different durations. If, for example, conversations with friends generally last longer than conversations with professors, we could expect less gestures in total when participants converse with professors. But this might be entirely due to the length of the conversations, rather than participants’ tendency to use gestures during their conversation. What we are really interested in is the <em>rate</em> of gesture use (e.g. the number of gestures per minute). If we let <span class="math inline">\(r\)</span> denote the average number of gestures per minute, and <span class="math inline">\(t_i\)</span> the duration (in minutes) of a conversation <span class="math inline">\(i\)</span>, then the expected number of gestures in conversation <span class="math inline">\(i\)</span> would be <span class="math inline">\(\mu_i = r \times t_i\)</span>. A Poisson regression model predicts <span class="math inline">\(\log(\mu)\)</span>, the logarithm of the number of occurrences, which we can write in terms of the rate <span class="math inline">\(r\)</span> and duration <span class="math inline">\(t_i\)</span> as:
<span class="math display">\[\begin{aligned}
\log (\mu_i) &amp;= \log\left(r \times t_i\right) \\
&amp;= \log(r) + \log(t_i)
\end{aligned}\]</span>
In the study, we are interested in how social context and language affect the rate (<span class="math inline">\(r\)</span>) of gestures. So ideally we would like to model the (log) rate <span class="math inline">\(\log(r)\)</span>, and not the expected number of gestures <span class="math inline">\(\log(\mu_i)\)</span>, as a linear function of the predictors of interest. We can do this by replacing <span class="math inline">\(\log(r)\)</span> above with the linear model predictions. We then have the following model for the expected counts:
<span class="math display">\[\log (\mu_i) = \log(t_i) + \beta_0 + \sum_{j=1}^m \beta_j X_{j,i}\]</span>
Note that the term <span class="math inline">\(\log(t_i)\)</span> above has no slope. You can think of <span class="math inline">\(\log(t_i)\)</span> as a predictor with a fixed slope of 1. Such a term added to a model (without needing to estimate a corresponding parameter) is also called an <strong>offset</strong>. Here, the offset is a vital component of the model to allow us to model counts with different durations.</p>
<p>Apart from the choice of link function and distribution, and the resulting implications for parameter interpretation, Poisson regression is very similar to other generalized linear models. Table <a href="ch-generalized-linear-models.html#tab:gestures-pois-glm-results">12.4</a> shows the results of a Poisson regression, using effect coded predictors for Context (friend = 1, professor = <span class="math inline">\(-1\)</span>), Language (Catalan = 1, Korean = <span class="math inline">\(-1\)</span>), and Gender (Female = 1, Male = <span class="math inline">\(-1\)</span>). It should be noted that this analysis does not account for the fact that each participant contributes two data points (one for each Context). Later in this chapter we will provide results from a generalized linear mixed-effects model, which does so.</p>
<table style="width:100%;">
<caption><span id="tab:gestures-pois-glm-results">Table 12.4: </span>Results of a Poisson regression model for the number of gestures in conversations.</caption>
<colgroup>
<col width="26%" />
<col width="8%" />
<col width="15%" />
<col width="4%" />
<col width="16%" />
<col width="3%" />
<col width="7%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SE}(\hat{\beta})\)</span></th>
<th align="right"><span class="math inline">\(z\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert z \rvert)\)</span></th>
<th align="right"><span class="math inline">\(G^2\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert G^2 \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">-0.966</td>
<td align="right">0.02</td>
<td align="right">-48.70</td>
<td align="right">&lt; .001</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Context</td>
<td align="right">0.046</td>
<td align="right">0.02</td>
<td align="right">2.32</td>
<td align="right">.020</td>
<td align="right">5.39</td>
<td align="right">1</td>
<td align="right">.020</td>
</tr>
<tr class="odd">
<td align="left">Language</td>
<td align="right">0.029</td>
<td align="right">0.02</td>
<td align="right">1.45</td>
<td align="right">.147</td>
<td align="right">2.11</td>
<td align="right">1</td>
<td align="right">.146</td>
</tr>
<tr class="even">
<td align="left">Gender</td>
<td align="right">0.062</td>
<td align="right">0.02</td>
<td align="right">3.14</td>
<td align="right">.002</td>
<td align="right">9.88</td>
<td align="right">1</td>
<td align="right">.002</td>
</tr>
<tr class="odd">
<td align="left">Context <span class="math inline">\(\times\)</span> Language</td>
<td align="right">-0.036</td>
<td align="right">0.02</td>
<td align="right">-1.82</td>
<td align="right">.069</td>
<td align="right">3.32</td>
<td align="right">1</td>
<td align="right">.068</td>
</tr>
<tr class="even">
<td align="left">Context <span class="math inline">\(\times\)</span> Gender</td>
<td align="right">-0.026</td>
<td align="right">0.02</td>
<td align="right">-1.31</td>
<td align="right">.190</td>
<td align="right">1.72</td>
<td align="right">1</td>
<td align="right">.190</td>
</tr>
<tr class="odd">
<td align="left">Language <span class="math inline">\(\times\)</span> Gender</td>
<td align="right">-0.042</td>
<td align="right">0.02</td>
<td align="right">-2.10</td>
<td align="right">.035</td>
<td align="right">4.44</td>
<td align="right">1</td>
<td align="right">.035</td>
</tr>
<tr class="even">
<td align="left">Context <span class="math inline">\(\times\)</span> Language <span class="math inline">\(\times\)</span> Gender</td>
<td align="right">0.037</td>
<td align="right">0.02</td>
<td align="right">1.88</td>
<td align="right">.060</td>
<td align="right">3.53</td>
<td align="right">1</td>
<td align="right">.060</td>
</tr>
</tbody>
</table>
<p>As before, we provide results for both the Wald and likelihood-ratio tests, whilst you would normally just report one of these. Given the relatively small size of the dataset, the use of the likelihood-ratio test is recommended over the Wald test. Nevertheless, the results are very similar for both tests: both show a significant and positive effect of Context, indicating more gestures in conversations with friends. There is also a significant and positive effect of Gender, indicating more gestures by females compared to males. Finally, there is a significant and negative interaction between Language and Gender. This indicates that the difference between females and males is reduced for Catalan speakers as compared to Korean speakers. Recall that due to the coding scheme used for (Gender: Female = 1, Male = <span class="math inline">\(-1\)</span>; Language: Catalan = 1, Korean = <span class="math inline">\(-1\)</span>), we can work out the conditional slope of Gender for the two Language conditions as:
<span class="math display">\[\beta_{\text{Gender}|\text{Language}} = \begin{cases}
0.062 + (-0.042) = 0.026 &amp;&amp; \text{Catalan} \\
0.062 - (-0.042) = 0.098 &amp;&amp; \text{Korean}
\end{cases}\]</span>
Due to the log link function, interpretation of the parameters in this Poisson regression model is reasonably straightforward. The value of <span class="math inline">\(\exp (\beta_j)\)</span> reflects the multiplicative increase of the dependent variable (count) for a one-unit increase in predictor <span class="math inline">\(X_j\)</span>. For example, being female would increase the predicted number of gestures by a factor of <span class="math inline">\(\exp(0.062) \approx 1.064\)</span>.</p>
<p>Table <a href="ch-generalized-linear-models.html#tab:gestures-poisson-model-confidence-intervals">12.5</a> shows the 95% confidence intervals, computed by the profile likelihood method, for the parameters of the Poisson regression model. As can be seen there, only the confidence intervals of Context, Gender, and the Language by Gender interaction exclude 0. This leads to similar conclusions as the test results discussed above.</p>
<table>
<caption><span id="tab:gestures-poisson-model-confidence-intervals">Table 12.5: </span>95% profile likelihood confidence intervals for the parameters of the Poisson regression model applied to the gestures data.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">2.5 %</th>
<th align="right">97.5 %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">-1.005</td>
<td align="right">-0.927</td>
</tr>
<tr class="even">
<td align="left">Context</td>
<td align="right">0.007</td>
<td align="right">0.085</td>
</tr>
<tr class="odd">
<td align="left">Language</td>
<td align="right">-0.010</td>
<td align="right">0.068</td>
</tr>
<tr class="even">
<td align="left">Gender</td>
<td align="right">0.023</td>
<td align="right">0.101</td>
</tr>
<tr class="odd">
<td align="left">Context <span class="math inline">\(\times\)</span> Language</td>
<td align="right">-0.075</td>
<td align="right">0.003</td>
</tr>
<tr class="even">
<td align="left">Context <span class="math inline">\(\times\)</span> Gender</td>
<td align="right">-0.065</td>
<td align="right">0.013</td>
</tr>
<tr class="odd">
<td align="left">Language <span class="math inline">\(\times\)</span> Gender</td>
<td align="right">-0.081</td>
<td align="right">-0.003</td>
</tr>
<tr class="even">
<td align="left">Context <span class="math inline">\(\times\)</span> Language <span class="math inline">\(\times\)</span> Gender</td>
<td align="right">-0.002</td>
<td align="right">0.076</td>
</tr>
</tbody>
</table>
<p>The deviance of the model is <span class="math inline">\(D_R = 303.957\)</span>, with residual degrees of freedom equal to <span class="math inline">\(n - \text{npar}(R) = 54 - 8 = 46\)</span>. Hence, the test of overall model fit is <span class="math inline">\(X^2(46) = 303.957\)</span>, <span class="math inline">\(p &lt; .001\)</span>. As this test is significant, the model does not fit the data well. We will explore why in the following section.</p>
</div>
<div id="overdispersion" class="section level3 hasAnchor" number="12.6.2">
<h3><span class="header-section-number">12.6.2</span> Overdispersion<a href="ch-generalized-linear-models.html#overdispersion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The overall model fit test above indicates that we can reject the hypothesis that the model is equal to the “true model” (i.e. the probability distribution provided by the estimated model diverges substantially from the observed distribution in the data). This indicates the model is misspecified. Misspecification can be due to not including all relevant predictors in a model, or other factors. For count data, a common cause of misspecification is <em>overdispersion</em>. In a Poisson distribution, the (conditional) variance is identical to the (conditional) mean. In real data, the (conditional) variance is often <em>larger</em> than the (conditional) mean. When the variance is larger than theoretically expected, this is called <strong>overdispersion</strong>. <em>Underdispersion</em> (where the variance is smaller than expected) is also possible, but much less common in practice.</p>
Overdispersion can lead to large residuals. Figure <a href="ch-generalized-linear-models.html#fig:gestures-poisson-pearson-residuals-histogram">12.7</a> shows a histogram of the standardized Pearson residuals for the Poisson regression model estimated above. Under the null-hypothesis that the model is equal to the true model, these should be standard-Normal-distributed. The overlaid Normal density shows the expected distribution according to the model. Comparing the predicted and observed distribution, it is clear there are more large residuals than expected.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gestures-poisson-pearson-residuals-histogram"></span>
<img src="12-GenLM_files/figure-html/gestures-poisson-pearson-residuals-histogram-1.svg" alt="Hisogram of standardized Pearson residuals for the Poisson regression model" width="672" />
<p class="caption">
Figure 12.7: Hisogram of standardized Pearson residuals for the Poisson regression model
</p>
</div>
<!-- A crude test for overdispersion is based on the sum of squared standardized Pearson residuals, which is approximately Chi-squared distributed with degrees equal to $\text{df} = n - \text{npar}(M)$, where $M$ is the model. In this case, the sum of the squared standardized Pearson residuals is 272.01, and $p(\chisq > 272.01|\text{df} = 46) < .001$. As such, we can reject -->
<p>Overdispersion does not bias the parameter estimates <span class="math inline">\(\hat{\beta}\)</span>. So, on average, the parameter estimates are equal to the true parameters. However, overdispersion makes parameter estimates less reliable (i.e. the variance of the sampling distribution of the parameter estimates is larger than without overdispersion). As a result, the standard errors of the parameter estimates reported in Table <a href="ch-generalized-linear-models.html#tab:gestures-pois-glm-results">12.4</a> are incorrect, as they assume no overdispersion (or underdispersion for that matter). When there is overdispersion, the true standard errors are higher. As a result, the <span class="math inline">\(p\)</span>-values of the Wald test are also incorrect. And as data with overdispersion is not truly Poisson-distributed, the same applies to the <span class="math inline">\(p\)</span>-values of the likelihood ratio tests.</p>
<p>Whilst not exactly straightforward, there are ways to account for overdispersion in generalized linear models. In <strong>quasi-Poisson regression</strong>, it is assumed that the variance is not equal to the mean <span class="math inline">\(\mu\)</span>, but scaled by a dispersion parameter <span class="math inline">\(\phi\)</span>, such that <span class="math inline">\(\text{Var}(Y) = \phi \times \mu\)</span>. Note that usual Poisson regression is nested in this more general model. When <span class="math inline">\(\phi = 1\)</span>, then there is no overdispersion, and we revert to the standard Poisson regression model. If <span class="math inline">\(\phi &gt; 1\)</span>, there is overdispersion. By estimating the dispersion parameter, quasi-Poisson regression allows to account for overdispersion and computation of more accurate standard errors. A downside of quasi-Poisson regression is that the distribution of the dependent variable is no longer a member of the exponential family. In fact, quasi-Poisson regression does not specify a clear probability distribution for the dependent variable at all. As a result, the likelihood of the parameters, <span class="math inline">\(p(\text{DATA}|\theta)\)</span> is not properly defined either. What remains is a <em>quasi-likelihood</em>, where only the conditional means and variances are specified. In the absence of a likelihood function, parameters can not be estimated by maximum likelihood; rather, they are estimated by maximum quasi-likelihood. Nevertheless, maximum quasi-likelihood estimators share many aspects with maximum likelihood estimators. In particular, the quasi-likelihood estimators are unbiased for the regression parameters <span class="math inline">\(\beta_j\)</span>.</p>
<p>For the gestures data, the estimated dispersion parameter is <span class="math inline">\(\hat{\phi} = 5.913\)</span>, which is clearly much larger than 1. Results of the quasi-Poisson regression are provided in Table <a href="ch-generalized-linear-models.html#tab:gestures-quasi-poisson-model-results">12.6</a>. Note that this Table contains Wald and <span class="math inline">\(F\)</span>-tests. Wald tests are justified by the asymptotic properties of quasi-likelihood estimation. The use of <span class="math inline">\(F\)</span>-tests when the dispersion is estimated is recommended by <span class="citation">Dunn &amp; Smyth (<a href="#ref-dunn2018generalized" role="doc-biblioref">2018</a>)</span>. <!-- TODO: chevk that this is appropriate --></p>
<table style="width:100%;">
<caption><span id="tab:gestures-quasi-poisson-model-results">Table 12.6: </span>Results of a quasi-Poisson regression model for the number of gestures in conversations.</caption>
<colgroup>
<col width="24%" />
<col width="8%" />
<col width="14%" />
<col width="4%" />
<col width="14%" />
<col width="3%" />
<col width="8%" />
<col width="8%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SE}(\hat{\beta})\)</span></th>
<th align="right"><span class="math inline">\(z\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert z \rvert)\)</span></th>
<th align="right"><span class="math inline">\(F\)</span></th>
<th align="right"><span class="math inline">\(\text{df}_1\)</span></th>
<th align="right"><span class="math inline">\(\text{df}_2\)</span></th>
<th align="right"><span class="math inline">\(p(\geq \lvert F \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">-0.966</td>
<td align="right">0.048</td>
<td align="right">-20.028</td>
<td align="right">&lt; .001</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Context</td>
<td align="right">0.046</td>
<td align="right">0.048</td>
<td align="right">0.953</td>
<td align="right">.346</td>
<td align="right">0.912</td>
<td align="right">1</td>
<td align="right">46.00</td>
<td align="right">0.345</td>
</tr>
<tr class="odd">
<td align="left">Language</td>
<td align="right">0.029</td>
<td align="right">0.048</td>
<td align="right">0.597</td>
<td align="right">.554</td>
<td align="right">0.357</td>
<td align="right">1</td>
<td align="right">46.00</td>
<td align="right">0.553</td>
</tr>
<tr class="even">
<td align="left">Gender</td>
<td align="right">0.062</td>
<td align="right">0.048</td>
<td align="right">1.291</td>
<td align="right">.203</td>
<td align="right">1.670</td>
<td align="right">1</td>
<td align="right">46.00</td>
<td align="right">0.203</td>
</tr>
<tr class="odd">
<td align="left">Context <span class="math inline">\(\times\)</span> Language</td>
<td align="right">-0.036</td>
<td align="right">0.048</td>
<td align="right">-0.748</td>
<td align="right">.458</td>
<td align="right">0.562</td>
<td align="right">1</td>
<td align="right">46.00</td>
<td align="right">0.457</td>
</tr>
<tr class="even">
<td align="left">Context <span class="math inline">\(\times\)</span> Gender</td>
<td align="right">-0.026</td>
<td align="right">0.048</td>
<td align="right">-0.539</td>
<td align="right">.593</td>
<td align="right">0.291</td>
<td align="right">1</td>
<td align="right">46.00</td>
<td align="right">0.592</td>
</tr>
<tr class="odd">
<td align="left">Language <span class="math inline">\(\times\)</span> Gender</td>
<td align="right">-0.042</td>
<td align="right">0.048</td>
<td align="right">-0.865</td>
<td align="right">.392</td>
<td align="right">0.751</td>
<td align="right">1</td>
<td align="right">46.00</td>
<td align="right">0.391</td>
</tr>
<tr class="even">
<td align="left">Context <span class="math inline">\(\times\)</span> Language <span class="math inline">\(\times\)</span> Gender</td>
<td align="right">0.037</td>
<td align="right">0.048</td>
<td align="right">0.772</td>
<td align="right">.444</td>
<td align="right">0.597</td>
<td align="right">1</td>
<td align="right">46.00</td>
<td align="right">0.444</td>
</tr>
</tbody>
</table>
<p>Table <a href="ch-generalized-linear-models.html#tab:gestures-quasi-poisson-model-results">12.6</a> indicates that none of the effects included in the model are significant. This is essentially because the standard errors of the parameters, corrected for overdispersion, are much larger than under the assumption that the Poisson distribution (without overdispersion) holds. As we will see later in this Chapter, however, the overdispersion is likely due to not properly accounting for the dependencies in the data: as participants contribute to both the friend and professor conditions, the data has repeated measures. Dealing with this properly requires the use of <em>generalized linear mixed-effects models</em>, which we will introduce in Section <a href="ch-generalized-linear-models.html#sec-glmer">12.9</a>.</p>
</div>
</div>
<div id="log-linear-models" class="section level2 hasAnchor" number="12.7">
<h2><span class="header-section-number">12.7</span> Log-linear models<a href="ch-generalized-linear-models.html#log-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Log-linear models are used to analyse dependency relations in multivariate categorical data. Such data can be expressed in the form of <strong>multi-way contingency tables</strong>. These are tables that contain counts for combinations of categorical classifications. For example, we might be interested in whether there is a relation between the newspaper people read and the political party they vote for. To investigate this, we can count how often each combination of newspaper and political party occurs in a random sample of surveyed people. In this case, we would have a two-way contingency table, where the first categorical dimension reflects the newspaper a person reads, and the second the political party they vote for.</p>
<p>To model a two-way contingency table, let <span class="math inline">\(\mu_{i,j}\)</span> denote the expected frequency of the combination where the first categorical variable has value <span class="math inline">\(A = i\)</span> and the second the value <span class="math inline">\(B = j\)</span>. We will denote the probability that this single randomly drawn observation has the values equal to this combination as <span class="math inline">\(\pi_{i,j} = P(A = i, B = j)\)</span>. For a total of <span class="math inline">\(n\)</span> independent observations, the expected frequency is then:
<span class="math display">\[\mu_{i,j} = n \times \pi_{i,j}\]</span>
Now, if the two categorical variables were independent, so that the value of <span class="math inline">\(A\)</span> does not depend on the value of <span class="math inline">\(B\)</span>, we could rewrite the probability of the combination of <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> for a single observation as:
<span class="math display">\[\pi_{i,j} = P(A = i, B = j) = P(A = i) \times P(B=j) = \pi_{i,\cdot} \times \pi_{\cdot,j}\]</span>
where <span class="math inline">\(\pi_{i,\cdot}\)</span> and <span class="math inline">\(\pi_{\cdot,j}\)</span> denote marginal probabilities:
<span class="math display">\[\begin{aligned}
\pi_{i,\cdot} &amp;= \sum_{j} \pi_{i,j} \\
\pi_{\cdot,j} &amp;= \sum_{i} \pi_{i,j}
\end{aligned}\]</span></p>
<p>Furthermore, using a log link function and assuming independence, we can write the logarithm of the expected counts as:
<span class="math display">\[\begin{aligned} \log \mu_{ij} &amp;= \log(n) + \log(\pi_{i,\cdot}) + \log(\pi_{\cdot,j}) \\ &amp;= \lambda + \lambda_i^{(A)} + \lambda_j^{(B)} \end{aligned}\]</span>
where <span class="math inline">\(\lambda = \log(n)\)</span>, <span class="math inline">\(\lambda^{(A)}_i = \log(\pi_{i,\cdot})\)</span>, and <span class="math inline">\(\lambda^{(B)}_j = \log(\pi_{\cdot,j})\)</span>. If independence does not hold, then the above equation is incorrect, and we would need an additional term to correct for this. In that case, we could write
<span class="math display">\[\begin{aligned} \log \mu_{ij} &amp;= \log(n) + \log(\pi_{i,j}) \\ &amp;= \log(n) + \log(\pi_{i,\cdot}) + \log(\pi_{\cdot,j}) + \left( \log(\pi_{i,j} - \log(\pi_{i,\cdot}) - \log(\pi_{\cdot,j})\right) \\ &amp;= \lambda + \lambda_i^{(A)} + \lambda_j^{(B)} + \lambda_{i,j}^{(A,B)}\end{aligned}\]</span>
where <span class="math inline">\(\lambda_{i,j}^{(A,B)} = \left( \log(\pi_{i,j} - \log(\pi_{i,\cdot}) - \log(\pi_{\cdot,j}) \right)\)</span>. Log-linear models use such constructions to test for independence. Under independence, the “correction terms” above would all be equal to <span class="math inline">\(\lambda_{i,j}^{(A,B)} = 0\)</span>.</p>
<p>Log-linear models are similar in spirit to a factorial ANOVA, in the sense that the counts are decomposed into main effects, two-way interactions, three-way interactions, etc. For a two-way contingency table, the counts <span class="math inline">\(\mu_{i,j}\)</span> can always be decomposed exactly as
<span class="math display">\[\log \mu_{i,j} = \lambda + \lambda^{(A)}_i + \lambda^{(B)}_j + \lambda^{(AB)}_{i,j}\]</span>
where <span class="math inline">\(\lambda\)</span> acts as an intercept, <span class="math inline">\(\lambda^{(A)}_i\)</span> as a “main effect” for factor <span class="math inline">\(A\)</span>, <span class="math inline">\(\lambda^{(B)}_j\)</span> as a “main effect” for factor <span class="math inline">\(B\)</span>, and <span class="math inline">\(\lambda^{(AB)}_{i,j}\)</span> as an “interaction effect” between factors <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Just like a factorial ANOVA model can perfectly fit the sample means, this log-linear model can perfectly fit the sample counts. This is hence an example of a <strong>saturated model</strong>, where the total number of parameters equals the total number of possible combinations minus 1.<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a></p>
<p>Although they seem rather different, log-linear models can be formulated in terms of equivalent Poisson regression models (by using contrast-coded predictors to account for the cells or combinations of categories in the multi-way contingency table). As such, they are an instance of generalized linear models, and all the tools available for the latter apply to log-linear models <span class="citation">(<a href="#ref-dobson2018introduction" role="doc-biblioref">Dobson &amp; Barnett, 2018</a>)</span>.</p>
<!--
For a random sample of people from the general population, we can then count how often people who report voting for the Conservative Party also report reading the Daily Mail, and how many report reading the Guardian, etc (see e.g. https://www.statista.com/chart/9897/general-election-voting-by-newspaper-readership/ and https://yougov.co.uk/topics/politics/articles-reports/2017/06/13/how-britain-voted-2017-general-election for an example of such data).

This is an extension of the Chi-squared test for a two-way contingency table.
-->
<div id="example-newspapers-and-voting" class="section level3 hasAnchor" number="12.7.1">
<h3><span class="header-section-number">12.7.1</span> Example: Newspapers and voting<a href="ch-generalized-linear-models.html#example-newspapers-and-voting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After the 2017 UK general elections, a large survey was conducted where people were asked which party they voted for, and amongst other things, the newspaper they most often read. Table <a href="ch-generalized-linear-models.html#tab:papervotes-table">12.7</a> shows the results. In the UK, many newspapers have strong links to political parties. We could ask whether such links also exist for their readers.</p>
<div style="border: 0px;overflow-x: scroll; width:100%; ">
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:papervotes-table">Table 12.7: </span>Party voted for and most-read newspapers for the 2017 UK general election.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Express
</th>
<th style="text-align:right;">
Daily Mail
</th>
<th style="text-align:right;">
Mirror
</th>
<th style="text-align:right;">
Daily Star
</th>
<th style="text-align:right;">
Sun
</th>
<th style="text-align:right;">
Daily Telegraph
</th>
<th style="text-align:right;">
Financial Times
</th>
<th style="text-align:right;">
Guardian
</th>
<th style="text-align:right;">
Independent
</th>
<th style="text-align:right;">
Times
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Did not vote
</td>
<td style="text-align:right;">
212
</td>
<td style="text-align:right;">
1568
</td>
<td style="text-align:right;">
483
</td>
<td style="text-align:right;">
83
</td>
<td style="text-align:right;">
1022
</td>
<td style="text-align:right;">
542
</td>
<td style="text-align:right;">
133
</td>
<td style="text-align:right;">
1772
</td>
<td style="text-align:right;">
474
</td>
<td style="text-align:right;">
632
</td>
</tr>
<tr>
<td style="text-align:left;">
Conservative
</td>
<td style="text-align:right;">
578
</td>
<td style="text-align:right;">
3675
</td>
<td style="text-align:right;">
171
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
557
</td>
<td style="text-align:right;">
1949
</td>
<td style="text-align:right;">
108
</td>
<td style="text-align:right;">
605
</td>
<td style="text-align:right;">
324
</td>
<td style="text-align:right;">
1670
</td>
</tr>
<tr>
<td style="text-align:left;">
Labour
</td>
<td style="text-align:right;">
113
</td>
<td style="text-align:right;">
845
</td>
<td style="text-align:right;">
610
</td>
<td style="text-align:right;">
46
</td>
<td style="text-align:right;">
283
</td>
<td style="text-align:right;">
296
</td>
<td style="text-align:right;">
105
</td>
<td style="text-align:right;">
5513
</td>
<td style="text-align:right;">
1424
</td>
<td style="text-align:right;">
691
</td>
</tr>
<tr>
<td style="text-align:left;">
Liberal Democrat
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
149
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
148
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
907
</td>
<td style="text-align:right;">
259
</td>
<td style="text-align:right;">
404
</td>
</tr>
<tr>
<td style="text-align:left;">
SNP
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
54
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
227
</td>
<td style="text-align:right;">
65
</td>
<td style="text-align:right;">
29
</td>
</tr>
<tr>
<td style="text-align:left;">
UKIP
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
149
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
29
</td>
</tr>
<tr>
<td style="text-align:left;">
Green
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
227
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
58
</td>
</tr>
<tr>
<td style="text-align:left;">
Other
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
76
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
29
</td>
</tr>
</tbody>
</table>
</div>
<p>For each combination of political party <span class="math inline">\(i = 1, \ldots, 8\)</span> (where we include “Did not vote” as a special party) and newspaper <span class="math inline">\(j = 1, \ldots, 10\)</span>, Table <a href="ch-generalized-linear-models.html#tab:papervotes-table">12.7</a> shows the corresponding count <span class="math inline">\(n_{i,j}\)</span>. The total number is <span class="math inline">\(n = \sum_i \sum_j n_{i,j} = 29938\)</span>, reflecting the sample size of the survey. The loglinear model concerns the expected counts for each combination, <span class="math inline">\(\mu_{i,j} = n \times P(\text{party} = i, \text{newspaper} = j)\)</span>. These expected counts can be estimated by the observed counts <span class="math inline">\(\hat{\mu}_{ij} = n_{i,j}\)</span>.</p>
<p>The full model
<span class="math display">\[\log \mu_{i,j} = \lambda + \lambda^{(\text{party})}_i + \lambda^{(\text{newspaper})}_j + \lambda^{(\text{party} \times \text{newspaper})}_{i,j}\]</span>
has <span class="math inline">\(8-1 = 7\)</span> parameters <span class="math inline">\(\lambda^{(\text{party})}_i\)</span> for the “main effect” of party, <span class="math inline">\(10-1 = 9\)</span> parameters <span class="math inline">\(\lambda^{(\text{newspaper})}_j\)</span> for the “main effect” of newspaper, and <span class="math inline">\(7 \times 9 = 63\)</span> parameters <span class="math inline">\(\lambda^{(\text{party} \times \text{newspaper})}_{i,j}\)</span> for the “interaction” between party and newspaper. Including the intercept <span class="math inline">\(\lambda\)</span>, the total number of parameters is thus <span class="math inline">\(1 + 7 + 9 + 63 = 80\)</span>, which equals the number of observed counts (i.e. combinations of party and newspaper). This saturated model therefore will fit the observed counts perfectly. Its main purpose is to serve as a comparison for simpler models. For this example, the comparison model of interest is a model in which political party voted-for is independent of the newspaper read:</p>
<p><span class="math display">\[\log \mu_{i,j} = \lambda + \lambda^{(\text{party})}_i + \lambda^{(\text{newspaper})}_j\]</span></p>
<p>The test for independence consists of a model comparison between the saturated and independence model. The result of the likelihood ratio test is <span class="math inline">\(G^2 = 12211\)</span>, <span class="math inline">\(\text{df} = 63\)</span>, <span class="math inline">\(p &lt; .001\)</span>. This is significant, and hence we reject the null hypothesis that the party voted for is independent of the newspaper most read.</p>
</div>
<div id="a-three-way-table-example-rock-paper-scissors" class="section level3 hasAnchor" number="12.7.2">
<h3><span class="header-section-number">12.7.2</span> A three-way table example: Rock-Paper-Scissors<a href="ch-generalized-linear-models.html#a-three-way-table-example-rock-paper-scissors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="citation">Guennouni &amp; Speekenbrink (<a href="#ref-guennouni2022transfer" role="doc-biblioref">2022</a>)</span> let participants repeatedly play the well-known game of Rock-Paper-Scissors against artificial (AI) opponents. The AI agents were endowed with different human-like strategies. The “level-1” AI player would expect the human player to repeat their last chosen action, and would pick the action that beats this. For example, if the human player had previously chosen “Rock”, then they would expect the human player to play “Rock” again, therefore choosing “Paper” as this is the action that beats “Rock”. The “level-2” AI player would expect the human player to adopt a “level-1” strategy, choosing actions to beat this. For example, if the AI player had previously chosen “Rock”, the human would be expected to next play “Paper”, as this beats “Rock”. The AI player would therefore play “Scissors”, as this beats “Paper” (the expected action of the human player). Results showed that people could pick-up on the strategy of the AI player, using this to their advantage. Moreover, they could transfer this learned strategy to other games.</p>
<p>If we focus on the “level-1” AI player, we can say that their actions depend completely on the previous action of the human player.<a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a> If the human player is sensitive to the AI player’s strategy and able to exploit the weaknesses of the AI player, then the humans’ strategy should also completely depend on their previous actions. For example, they should choose to play “Scissors” after playing “Rock”, as they should expect the AI player to play “Paper”. If this is the case, then we might expect the current action of the human player to be independent of the previous action of the AI player. We can test this hypothesis with log-linear models.</p>
<p>Table <a href="ch-generalized-linear-models.html#tab:rps-table">12.8</a> shows the actions chosen by the human player (in the last half of the game) as a function of the previous actions of the AI and human player.</p>
<table>
<caption>
<span id="tab:rps-table">Table 12.8: </span>Action chosen by human players in Rock-Paper-Scissors by previous actions chosen by both human and the level-1 AI player.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="2">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Current human action
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
Previous human
</th>
<th style="text-align:left;">
Previous AI
</th>
<th style="text-align:right;">
Rock
</th>
<th style="text-align:right;">
Paper
</th>
<th style="text-align:right;">
Scissors
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
rock
</td>
<td style="text-align:left;">
rock
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
15
</td>
</tr>
<tr>
<td style="text-align:left;">
rock
</td>
<td style="text-align:left;">
paper
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
12
</td>
</tr>
<tr>
<td style="text-align:left;">
rock
</td>
<td style="text-align:left;">
scissors
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
107
</td>
</tr>
<tr>
<td style="text-align:left;">
paper
</td>
<td style="text-align:left;">
rock
</td>
<td style="text-align:right;">
94
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
paper
</td>
<td style="text-align:left;">
paper
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
11
</td>
</tr>
<tr>
<td style="text-align:left;">
paper
</td>
<td style="text-align:left;">
scissors
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
8
</td>
</tr>
<tr>
<td style="text-align:left;">
scissors
</td>
<td style="text-align:left;">
rock
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:left;">
scissors
</td>
<td style="text-align:left;">
paper
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
94
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:left;">
scissors
</td>
<td style="text-align:left;">
scissors
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
5
</td>
</tr>
</tbody>
</table>
There are 9 possible models that can be used for this three-way contingency table. One possible model includes only main effects for the previous human action (H), the previous AI action (A) and the current human action (C). Such a model can be denoted as <span class="math inline">\((H, A, C)\)</span>, where elements separated by a comma are assumed independent:
<span class="math display">\[\begin{aligned}(H,A,C)&amp;&amp; \log(\mu_{i,j,k}) = \lambda + \lambda_i^{(H)} + \lambda_j^{(A)} + \lambda_k^{(C)}\end{aligned}\]</span>
If humans played the optimal strategy, then we would expect a dependency between the previous human action and the current human action, but these to be independent from the previous AI action. This corresponds to a model with main effects for <span class="math inline">\(H\)</span>, <span class="math inline">\(A\)</span>, and <span class="math inline">\(C\)</span>, and a two-way interaction between <span class="math inline">\(H\)</span> and <span class="math inline">\(C\)</span>. This model can be denoted as <span class="math inline">\((HC,A)\)</span>, to indicate dependence between <span class="math inline">\(H\)</span> and <span class="math inline">\(C\)</span>, and independence of the factors from <span class="math inline">\(A\)</span>:
<span class="math display">\[\begin{aligned}(HC,A) &amp;&amp; \log(\mu_{ijk}) = \lambda + \lambda_i^{(H)} + \lambda_j^{(A)} + \lambda_k^{(C)} + \lambda_{i,k}^{(H,C)}\end{aligned}\]</span>
Table <a href="ch-generalized-linear-models.html#tab:rps-loglm-results">12.9</a> lists the results of all possible loglinear models for this data.
<table>
<caption>
<span id="tab:rps-loglm-results">Table 12.9: </span>Results from all possible log-linear models for the Rock-Paper-Scissors data of Table <a href="ch-generalized-linear-models.html#tab:rps-table">12.8</a>
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(G^2\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(p(\gt G^2)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\((H, A, C)\)</span>
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
678.4
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\((HA, C)\)</span>
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
417.9
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\((HC,A)\)</span>
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
341.5
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\((AC, H)\)</span>
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
506.2
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\((HA, AC)\)</span>
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
245.7
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\((HA, HC)\)</span>
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
81.0
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\((HC, AC)\)</span>
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
169.3
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\((HA, HC, AC)\)</span>
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
35.6
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\((HAC)\)</span>
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:left;">
1.00
</td>
</tr>
</tbody>
</table>
<p>All the models (apart from the saturated model) are rejected. As such, it seems like the current actions of humans depends both on their previous actions, as well as those of their opponent. That implies they likely have not found the optimal strategy<!-- (although a different analysis shows they were able to reach above change performance against their opponent)-->.</p>
</div>
<div id="sparse-data-and-empty-cells" class="section level3 hasAnchor" number="12.7.3">
<h3><span class="header-section-number">12.7.3</span> Sparse data and empty cells<a href="ch-generalized-linear-models.html#sparse-data-and-empty-cells" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Problems can arise when tables have many small counts. This can happen when the total sample size is small, or when the number of cells in the table is relatively large compared to the total number of observations. A particular problem is when cells are empty (have a count of 0). As <span class="math inline">\(\log(0) = -\infty\)</span> this may lead to problems estimating the parameters (the estimates may become infinite). A <em>structural zero</em> refers to a case where the expected count for a cell is truly zero, because a combination of values can not occur (i.e. the probability of that combination equals 0). Such cells should be ignored (removed) from the analysis. A <em>sampling zero</em> occurs when the expected count is larger than 0, but a particular combination has not been observed in the data due to chance. For such cells, a correction factor may be applied. When zero-count cells are present, <span class="citation">Agresti (<a href="#ref-agresti2018introduction" role="doc-biblioref">2018</a>)</span> recommends to add a value of <span class="math inline">\(\tfrac{1}{2}\)</span> to all cells of saturated models (including cells with non-zero counts). For non-saturated models, this correction is too large however, and <span class="citation">Agresti (<a href="#ref-agresti2018introduction" role="doc-biblioref">2018</a>)</span> recommends to add a much smaller value to each cell (e.g. .00000001, or .0001).</p>
<!-- But even when not zero, cells with much smaller counts than others can also have undue influence on the analysis. -->
</div>
</div>
<div id="multinomial-logistic-regression" class="section level2 hasAnchor" number="12.8">
<h2><span class="header-section-number">12.8</span> Multinomial logistic regression<a href="ch-generalized-linear-models.html#multinomial-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- https://www.bookdown.org/chua/ber642_advanced_regression/multinomial-logistic-regression.html -->
<p>Logistic regression concerns modelling a dichotomous or binary dependent variable. There are many situations in which we would like to model a <em>polytomous</em> (i.e. non-binary) categorical dependent variable. For instance, in a marketing application, we might be interested in determining the factors that underlie consumers’ choice between multiple products (e.g. A, B, C, or D).</p>
<p>A simple, but suboptimal solution would be to treat the data as a set of binary variables. For instance, if we consider four products, we might construct 4 binary variables, where the first one encodes whether product <span class="math inline">\(A\)</span> was chosen (<span class="math inline">\(Y_{1,i} = 1\)</span>) or not (<span class="math inline">\(Y_{1,i} = 0\)</span>), the second one whether product <span class="math inline">\(B\)</span> was chosen (<span class="math inline">\(Y_{2,i} = 1\)</span>) or not (<span class="math inline">\(Y_{2,i} = 0\)</span>), etc. For each of these variables, we might construct a logistic regression model with the same set of predictors. A problem with this approach is that it ignores a fundamental aspect: these variables are not independent! If we know that <span class="math inline">\(Y_{1,i} = 1\)</span>, then all the other variables (<span class="math inline">\(Y_{2,i}, Y_{3,i}, \ldots\)</span>) have to be 0.
In multinomial logistic regression, a set of binary logistic regression models is estimated simultaneously, and the dependencies between these models are taken into account appropriately.</p>
<div id="baseline-category-logit" class="section level3 hasAnchor" number="12.8.1">
<h3><span class="header-section-number">12.8.1</span> Baseline category logit<a href="ch-generalized-linear-models.html#baseline-category-logit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The most common multinomial logistic regression model uses a so-called <em>baseline category logit</em> link function. This approach takes one of the categories (or levels) of the dependent variable as a baseline, and compares all other levels to that baseline category.</p>
<p><span class="math display">\[\log \left( \frac{\pi_j}{\pi_k} \right) = \beta_{j,0} + \beta_{j,1} \times X_1 + \ldots + \beta_{j,m} \times X_m \quad \quad \text{for all } j \neq k\]</span>
We could also parametrise the comparison of the baseline category against itself, i.e. <span class="math inline">\(\log \left( \frac{\pi_k}{\pi_k} \right)\)</span>. But as <span class="math inline">\(\log \left( \frac{\pi_k}{\pi_k} \right) = 0\)</span> always, we can just treat the parameters of this logit function as fixed to 0, i.e <span class="math inline">\(\beta_{k,0} = \beta_{k,1} = \ldots = \beta_{k,m} = 0\)</span>.</p>
</div>
<div id="example-rock-paper-scissors" class="section level3 hasAnchor" number="12.8.2">
<h3><span class="header-section-number">12.8.2</span> Example: Rock-Paper-Scissors<a href="ch-generalized-linear-models.html#example-rock-paper-scissors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As an example, we re-analyse the Rock-Paper-Scissors data, focusing on a model of the human player’s current action as a function of the previous actions of the human and AI player. We choose “paper” as the baseline category, and use effect-coding for the two predictors (human and AI previous action). As each predictor has three possible values, we need two contrast codes for each. We will estimate effects for previous actions “rock” and “scissors” (again, treating “paper” as a baseline). Our two logits are then</p>
<span class="math display">\[\begin{aligned}
\log \left(\frac{p(\text{rock})}{p(\text{paper})} \right) &amp;= \beta_{R, 0} + \beta_{R, HR} \times C_{H: R} +  \beta_{R, HS} \times C_{H: S} + \beta_{R, AR} \times C_{A: R} +  \beta_{R, AS} \times C_{A: S} \\
\log \left(\frac{p(\text{scissors})}{p(\text{paper})} \right) &amp;= \beta_{S, 0} + \beta_{S, HR} \times C_{H: R} +  \beta_{S, HS} \times C_{H: S} + \beta_{S, AR} \times C_{A: R} +  \beta_{S, AS} \times C_{A: S} \end{aligned}\]</span>
Parameter estimates and Wald tests are provided in Table <a href="ch-generalized-linear-models.html#tab:rps-multinomial-logit">12.10</a>.
<table>
<caption>
<span id="tab:rps-multinomial-logit">Table 12.10: </span>Results of a baseline category multinomial logistic regression model for human current actions as a function of human and AI previous actions.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="4">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
C: rock
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="4">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
C: scissors
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{SE}(\hat{\beta})\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(z\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(P(\geq \lvert z \rvert)\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{SE}(\hat{\beta})\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(z\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(P(\geq \lvert z \rvert)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
0.131
</td>
<td style="text-align:right;">
0.084
</td>
<td style="text-align:right;">
1.549
</td>
<td style="text-align:right;">
.121
</td>
<td style="text-align:right;">
-0.090
</td>
<td style="text-align:right;">
0.089
</td>
<td style="text-align:right;">
-1.01
</td>
<td style="text-align:right;">
.311
</td>
</tr>
<tr>
<td style="text-align:left;">
H: rock
</td>
<td style="text-align:right;">
-1.013
</td>
<td style="text-align:right;">
0.127
</td>
<td style="text-align:right;">
-7.991
</td>
<td style="text-align:right;">
&lt; .001
</td>
<td style="text-align:right;">
0.257
</td>
<td style="text-align:right;">
0.114
</td>
<td style="text-align:right;">
2.25
</td>
<td style="text-align:right;">
.024
</td>
</tr>
<tr>
<td style="text-align:left;">
H: scissors
</td>
<td style="text-align:right;">
-0.474
</td>
<td style="text-align:right;">
0.112
</td>
<td style="text-align:right;">
-4.232
</td>
<td style="text-align:right;">
&lt; .001
</td>
<td style="text-align:right;">
-0.922
</td>
<td style="text-align:right;">
0.128
</td>
<td style="text-align:right;">
-7.22
</td>
<td style="text-align:right;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
A: rock
</td>
<td style="text-align:right;">
0.334
</td>
<td style="text-align:right;">
0.117
</td>
<td style="text-align:right;">
2.863
</td>
<td style="text-align:right;">
.004
</td>
<td style="text-align:right;">
-0.129
</td>
<td style="text-align:right;">
0.123
</td>
<td style="text-align:right;">
-1.05
</td>
<td style="text-align:right;">
.294
</td>
</tr>
<tr>
<td style="text-align:left;">
A: scissors
</td>
<td style="text-align:right;">
0.032
</td>
<td style="text-align:right;">
0.124
</td>
<td style="text-align:right;">
0.257
</td>
<td style="text-align:right;">
.797
</td>
<td style="text-align:right;">
0.734
</td>
<td style="text-align:right;">
0.113
</td>
<td style="text-align:right;">
6.52
</td>
<td style="text-align:right;">
&lt; .001
</td>
</tr>
</tbody>
</table>
<p>Instead of the Wald test for the individual coefficients in Table <a href="ch-generalized-linear-models.html#tab:rps-multinomial-logit">12.10</a>, we may wonder whether a predictor has any effect on the dependent variable. For example, if the AI’s previous action has no effect on the human’s current action, that would mean that the null-hypothesis <span class="math display">\[H_0: \beta_{R, AR} = \beta_{R, AS} = \beta_{S, AR} = \beta_{S, AS} = 0\]</span> is true. We can test this hypothesis by a model comparison, comparing the full model to one where we fix these parameters to 0 (effectively removing the AI previous action predictor from the model).
This model comparison provides the following test results:
<span class="math inline">\(-2 \log(\text{likelihood-ratio}) = 252.269\)</span>, <span class="math inline">\(\text{df} = 4\)</span>, <span class="math inline">\(p &lt; .001\)</span>. Hence, we can conclude that the human players’ current actions do depend on the AI’s previous actions. This confirms the results of the log-linear models used earlier.</p>
</div>
<div id="reconstructing-probabilities-of-responses" class="section level3 hasAnchor" number="12.8.3">
<h3><span class="header-section-number">12.8.3</span> Reconstructing probabilities of responses<a href="ch-generalized-linear-models.html#reconstructing-probabilities-of-responses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After estimating a multinomial logistic regression model, we are often interested in the predicted probabilities of each category. These can be derived through the inverse link function:
<span class="math display">\[\hat{\pi_j} = \frac{\exp \left( \beta_{j,0} + \beta_{j,1} \times X_1 + \ldots + \beta_{j,m} \times X_m \right)}{\sum_{l=1}^K \exp \left(\beta_{l,0} + \beta_{l,1} \times X_1 + \ldots + \beta_{l,m} \times X_m \right)}\]</span>
Note that we also sum over the baseline category <span class="math inline">\(k\)</span> in the denominator, setting the corresponding parameters to 0. For example, if the human player previously played “Rock” (<span class="math inline">\(C_{H: R} = 1\)</span>, <span class="math inline">\(C_{H: S} = 0\)</span>) and the AI previously player “Paper” (<span class="math inline">\(C_{A: R} = -1\)</span>, <span class="math inline">\(C_{A: S} = -1\)</span>), we can work out the probability of each possible current action by computing the following quantities:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
action
</th>
<th style="text-align:left;">
linear equation
</th>
<th style="text-align:right;">
value
</th>
<th style="text-align:right;">
exp
</th>
<th style="text-align:right;">
prob
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
C: R
</td>
<td style="text-align:left;">
<span class="math inline">\(0.131 + (-1.013) \times 1 + (-0.474) \times 0 + 0.334\times -1 + 0.032\times -1\)</span>
</td>
<td style="text-align:right;">
-1.248
</td>
<td style="text-align:right;">
0.287
</td>
<td style="text-align:right;">
0.149
</td>
</tr>
<tr>
<td style="text-align:left;">
C: P
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
0.518
</td>
</tr>
<tr>
<td style="text-align:left;">
C: S
</td>
<td style="text-align:left;">
<span class="math inline">\((-0.09) + 0.257 \times 1 + (-0.922) \times 0 + (-0.129) \times -1 + 0.734 \times -1\)</span>
</td>
<td style="text-align:right;">
-0.438
</td>
<td style="text-align:right;">
0.645
</td>
<td style="text-align:right;">
0.334
</td>
</tr>
</tbody>
</table>
<p>For each action, we start with computing the value of the linear equation. These values are then transformed by taking the natural exponent. Finally, we can divide each of these exponentiated values by the sum of all exponentiated values to arrive a the calculated probability. For example, the probability that the action is “Rock” is computed as
<span class="math display">\[\begin{aligned} p(\text{rock}) &amp;= \frac{\exp(-1.248)}{\exp(-1.248) + \exp(0) + \exp(-0.438)} \\
&amp;= 0.149 \end{aligned}\]</span></p>
</div>
<div id="alternative-logit-models-for-ordinal-categories" class="section level3 hasAnchor" number="12.8.4">
<h3><span class="header-section-number">12.8.4</span> Alternative logit models for ordinal categories<a href="ch-generalized-linear-models.html#alternative-logit-models-for-ordinal-categories" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When the categories of the dependent variable can be meaningfully ordered, this ordering can be used to construct a different set of logit models. All these formulations work with odds, but these odds are differentially defined. Some common examples are:</p>
<ul>
<li>Cumulative logit. <span class="math inline">\(\log \left( \frac{\pi_1}{\pi_2 + \pi_3 + \ldots} \right)\)</span>, <span class="math inline">\(\log \left( \frac{\pi + \pi_2}{\pi_3 + \pi_4 + \ldots} \right)\)</span>, <span class="math inline">\(\log \left( \frac{\pi_1 + \pi_2 + \pi_3}{\pi_4 + \ldots} \right)\)</span>, etc.</li>
<li>Adjacent category logit. <span class="math inline">\(\log \left( \frac{\pi_1}{\pi_2} \right)\)</span>, <span class="math inline">\(\log \left( \frac{\pi_2}{\pi_3} \right)\)</span>, <span class="math inline">\(\log \left( \frac{\pi_3}{\pi_4} \right)\)</span>, etc.</li>
<li>Continuation ratio logit. <span class="math inline">\(\log \left( \frac{\pi_1}{\pi_2} \right)\)</span>, <span class="math inline">\(\log \left( \frac{\pi_1 + \pi_2}{\pi_3} \right)\)</span>, <span class="math inline">\(\log \left( \frac{\pi_1 + \pi_2 + \pi_3}{\pi_4} \right)\)</span>, etc.</li>
</ul>
<p>We can choose any set of logits, as long as it is possible to reconstruct the probability of each outcome from the set. In principle, we could also use one of these logit formulations for unordered categories. The model as a whole would fit the data equally well, no matter what valid set of logits is chosen. Similarly to the approach we took in repeated-measures ANOVA, these logits essentially define contrasts on a dependent variable. Any valid contrast will do, but choosing a good set of contrasts will provide a more informative set of parameters.</p>
<p>The main usefulness of alternative logit formulations for ordered categories is when using these in combination with restrictions on the parameters. In particular, assuming that the slope of predictors is identical for each logit. For example, the <strong>Proportional Odds Model</strong> uses the cumulative logit formulation and assumes the effect of a predictor to be identical for each of these cumulative logits:</p>
<p><span class="math display">\[\log \left( \frac{\sum_{k=1}^l \pi_k}{\sum_{k=l+1}^K \pi_k} \right) = \beta_0 + \sum_{j=1}^m \beta_j \times X_j \quad \quad l = 1, \ldots, K\]</span>
This reduces the number of parameters quite drastically compared to an unconstrained multinomial logistic regression model.</p>
</div>
</div>
<div id="sec-glmer" class="section level2 hasAnchor" number="12.9">
<h2><span class="header-section-number">12.9</span> Generalized linear mixed-effects models<a href="ch-generalized-linear-models.html#sec-glmer" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As in the linear model, the generalized linear model can be extended by including random effects. Whilst estimation of such generalized linear mixed-effects models is more difficult than for linear mixed-effects models, conceptually, the models are similar. Again, we can write each intercept and slope as consisting of a fixed and a random part, and assume that each random effect is drawn from a (multivariate) Normal distribution, with mean 0.</p>
<p>Table <a href="ch-generalized-linear-models.html#tab:metacognition-glmer">12.11</a> shows the results for a logistic mixed-effects regression model, using data from all participants of the metacognition experiment (see Section <a href="ch-generalized-linear-models.html#sec-logistic-regression-metacognition">12.5.2</a>). In addition to the fixed effects of Confidence, Contrast, and their interaction, the model includes participant-wise random intercepts and slopes for Confidence and Contrast. Random effects are assumed independent, and to aid estimation, Confidence and Contrast were scaled by a <span class="math inline">\(Z\)</span>-transformation. Scaling predictors can be crucial when estimating generalized linear mixed-effects models.</p>
<table>
<caption>
<span id="tab:metacognition-glmer">Table 12.11: </span>Estimates and tests for the fixed-effects of a logistic mixed-effects model with random intercepts and slopes for Confidence and Contrast. Note: Confidence and Contrast are scaled by a Z-transformation.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(G^2\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(p(\geq G^2)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
1.723
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
45.6
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
Confidence
</td>
<td style="text-align:right;">
1.177
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
62.2
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
Contrast
</td>
<td style="text-align:right;">
1.210
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
49.5
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
Confidence<span class="math inline">\(\times\)</span>Contrast
</td>
<td style="text-align:right;">
0.814
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
255.3
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
</tbody>
</table>
<p>The results show significant and positive effects of Confidence and Contrast, as well as their interaction. This confirms the results obtained for models fitted to the data of the individual participants (see Table <a href="ch-generalized-linear-models.html#tab:metacognition-logistic-regression-results-z-and-LR-participant-1">12.1</a> and <a href="ch-generalized-linear-models.html#tab:metacognition-logistic-regression-results-z-and-LR-participant-2">12.2</a>).</p>
<p>Table <a href="ch-generalized-linear-models.html#tab:gestures-glmer">12.12</a> shows the results for a Poisson mixed-effects model for the gestures data (see Section <a href="ch-generalized-linear-models.html#sec-poisson-regression-gestures">12.6.1</a>). Accounting for the dependence in the data by including random intercepts for each participant, we now only find a significant effect of Context. The effects of Gender and the Gender by Language interaction are no longer significant for this model.</p>
<table>
<caption>
<span id="tab:gestures-glmer">Table 12.12: </span>Estimates and tests for the fixed-effects of a Poisson mixed-effects model with random intercepts for participants.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{\beta}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\text{df}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(G^2\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(p(\geq G^2)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
-1.031
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
59.31
</td>
<td style="text-align:left;">
&lt; .001
</td>
</tr>
<tr>
<td style="text-align:left;">
Context
</td>
<td style="text-align:right;">
0.054
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
7.28
</td>
<td style="text-align:left;">
.007
</td>
</tr>
<tr>
<td style="text-align:left;">
Language
</td>
<td style="text-align:right;">
0.040
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.34
</td>
<td style="text-align:left;">
.561
</td>
</tr>
<tr>
<td style="text-align:left;">
Gender
</td>
<td style="text-align:right;">
0.060
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.74
</td>
<td style="text-align:left;">
.388
</td>
</tr>
<tr>
<td style="text-align:left;">
Context <span class="math inline">\(\times\)</span> Language
</td>
<td style="text-align:right;">
-0.037
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3.43
</td>
<td style="text-align:left;">
.064
</td>
</tr>
<tr>
<td style="text-align:left;">
Context <span class="math inline">\(\times\)</span> Gender
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.94
</td>
<td style="text-align:left;">
.331
</td>
</tr>
<tr>
<td style="text-align:left;">
Language <span class="math inline">\(\times\)</span> Gender
</td>
<td style="text-align:right;">
-0.016
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:left;">
.814
</td>
</tr>
<tr>
<td style="text-align:left;">
Context <span class="math inline">\(\times\)</span> Language <span class="math inline">\(\times\)</span> Gender
</td>
<td style="text-align:right;">
0.034
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2.88
</td>
<td style="text-align:left;">
.090
</td>
</tr>
</tbody>
</table>
<!--
### Item response theory

Item-response theory concerns the measurement of *latent variables* through a set of items that differ in difficulty. Whole books have been written on this topic, and we will only provide a relatively straightforward example here.

The so-called Rasch or 1-parameter logistic IRT model can be stated as a generalized linear mixed-effects model as follows:
$$\begin{aligned}
\text{logit}(p(Y_{i,j} = 1)) &= \beta_j + \gamma_i \\
\gamma_i &\sim \mathbf{Normal}(0,\sigma_{\gamma_i})
\end{aligned}$$
where $p(Y_{i,j} = 1)$ is the probability that person $i$ answers item $j$ correct, $\beta_j$ is a (fixed) effect for item $j$, and $\gamma_i$ a (random) effect for person $i$. 


-->
</div>
<div id="in-practice-5" class="section level2 hasAnchor" number="12.10">
<h2><span class="header-section-number">12.10</span> In practice<a href="ch-generalized-linear-models.html#in-practice-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Generalized linear models are more general than the General Linear Model (GLM), and are useful to analyse a wide variety of data. A linear model is used for the structural part. For this linear model, similar principles apply as for the GLM: We can use suitable contrast codes to incorporate categorical predictors, and use polynomial regression techniques to allow for non-linear relations. Regarding the latter, it is important to keep in mind that generalized linear models already involve a transformation of the model predictions (via the link function). The assumption of linearity applies to the relation between predictors and the link-transformed expectations of the dependent variable.</p>
<p>Steps involved in analysing data with generalized linear models are:</p>
<ol style="list-style-type: decimal">
<li><p>Choose a sensible distribution (family) for the random part of the model. This can usually be based on the nature of the dependent variable. For binary data, a Bernoulli distribution is generally the right choice. For count (frequency) data, if the maximum possible count is fixed by design, a Binomial distribution makes sense. If the maximum possible count is not fixed, a Poisson distribution makes sense. For polytomous dependent variables, a multinomial distribution is sensible. If you are interested in determining dependence relations between (polytomous) categorical variables, log-linear models are a natural choice.</p></li>
<li><p>Choose a useful link function. The link function defines how (a linear function of) the predictors relate to the conditional mean of the dependent variable. Hence, the choice is important. It is often safe to use to the canonical link function. But for specific cases, you may want to choose an alternative or define your own link function.</p></li>
<li><p>If there are repeated measures or other dependencies between data points, random effects can be added to the model to account for this. Random effects can account for the overdispersion that these dependencies give rise to. Overdispersion can also be dealt with by using a quasi-generalized-linear model. But if overdispersion is due to dependencies in the data, it will be better to deal with the added complexity of generalized linear mixed-effects models rather than the that of quasi-generalized-linear models.</p></li>
<li><p>Estimate and assess the fit of the model. Tests of the overall fit are important here, as they may signal overdispersion for instance. When a model does not fit the data well, check if excessive deviance is due to a few cases. If not, then consider a different link function, a different probability distribution, or even a “quasi” distribution (in this order).</p></li>
<li><p>Perform inferential tests for the model parameters. In almost all cases, it is advisable to use likelihood-ratio tests for this purpose, or profile or bootstrap confidence intervals, rather than Wald tests or confidence intervals based on the standard Normal approximation. The latter are easier to compute and useful in exploratory model building, but not so much in confirmatory tests.</p></li>
<li><p>Report the results. In reporting results, make sure you specify (1) the distribution assumed for the (conditional) dependent variable, (2) the link function used, (3) all the predictors used in the model.</p></li>
</ol>
</div>
<div id="further-reading" class="section level2 hasAnchor" number="12.11">
<h2><span class="header-section-number">12.11</span> Further reading<a href="ch-generalized-linear-models.html#further-reading" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter can only provide an introduction to generalized linear models. <!--I did my best in providing the basics and highlighting important issues.--> Whole books have been written on the topic. If you want to read more about generalized linear models, <span class="citation">Agresti (<a href="#ref-agresti2018introduction" role="doc-biblioref">2018</a>)</span> provides a good and relatively readable introduction. <span class="citation">Dobson &amp; Barnett (<a href="#ref-dobson2018introduction" role="doc-biblioref">2018</a>)</span> provides a more succinct and technical text. <span class="citation">Fox (<a href="#ref-fox2015applied" role="doc-biblioref">2015</a>)</span> provides an extensive and unified account of the GLM and extensions into generalized linear models, covering much more ground than I am able to do here. There are also sources covering more specialized topics. <span class="citation">Hosmer, Lemeshow, &amp; Sturdivant (<a href="#ref-hosmer2013applied" role="doc-biblioref">2013</a>)</span> offer a readable and practical book on logistic regression. <span class="citation">Wickens (<a href="#ref-wickens2014multiway" role="doc-biblioref">2014</a>)</span> provides a very extensive book on log-linear models.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-agresti2018introduction" class="csl-entry">
Agresti, A. (2018). <em>An introduction to categorical data analysis</em>. John Wiley &amp; Sons.
</div>
<div id="ref-dobson2018introduction" class="csl-entry">
Dobson, A. J., &amp; Barnett, A. G. (2018). <em>An introduction to generalized linear models</em>. Chapman; Hall/CRC.
</div>
<div id="ref-dunn2018generalized" class="csl-entry">
Dunn, P. K., &amp; Smyth, G. K. (2018). <em>Generalized linear models with examples in <span>R</span></em>. Springer.
</div>
<div id="ref-fox2015applied" class="csl-entry">
Fox, J. (2015). <em>Applied regression analysis and generalized linear models</em>. Sage Publications.
</div>
<div id="ref-green1966signal" class="csl-entry">
Green, D. M., &amp; Swets, J. A. (1966). <em>Signal detection theory and psychophysics</em>. New York: Wiley.
</div>
<div id="ref-guennouni2022transfer" class="csl-entry">
Guennouni, I., &amp; Speekenbrink, M. (2022). Transfer of learned opponent models in zero sum games. <em>Computational Brain &amp; Behavior</em>, <em>5</em>, 326--342.
</div>
<div id="ref-hosmer2013applied" class="csl-entry">
Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. (2013). <em>Applied logistic regression</em>. John Wiley &amp; Sons.
</div>
<div id="ref-mccullagh2019generalized" class="csl-entry">
McCullagh, P., &amp; Nelder, J. A. (2019). <em>Generalized linear models</em>. Routledge.
</div>
<div id="ref-mcfadden1973conditional" class="csl-entry">
McFadden, D. (1973). <em>Conditional logit analysis of qualitative choice behavior</em>.
</div>
<div id="ref-rausch2016visibility" class="csl-entry">
Rausch, M., &amp; Zehetleitner, M. (2016). Visibility is not equivalent to confidence in a low contrast orientation discrimination task. <em>Frontiers in Psychology</em>, <em>7</em>, 591.
</div>
<div id="ref-venzon1988method" class="csl-entry">
Venzon, D., &amp; Moolgavkar, S. (1988). A method for computing profile-likelihood-based confidence intervals. <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em>, <em>37</em>, 87–94.
</div>
<div id="ref-wickens2014multiway" class="csl-entry">
Wickens, T. D. (2014). <em>Multiway contingency tables analysis for the social sciences</em>. Psychology Press.
</div>
<div id="ref-winter2021poisson" class="csl-entry">
Winter, B., &amp; Bürkner, P.-C. (2021). Poisson regression for linguists: A tutorial introduction to modelling count data with brms. <em>Language and Linguistics Compass</em>, <em>15</em>, e12439.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="30">
<li id="fn30"><p>If you really want to know, the probability density function of exponential family models can be written as <span class="math display">\[p(y|\theta, \phi) = \exp \left( \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right),\]</span> where <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> are parameters, and <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> are known functions. The parameter <span class="math inline">\(\theta\)</span> is called the <em>canonical parameter</em>, and can be written as a function of the mean of <span class="math inline">\(Y\)</span>: <span class="math inline">\(\theta = g(\mu)\)</span>. The function <span class="math inline">\(g\)</span> is called the <em>canonical link function</em>. The parameter <span class="math inline">\(\phi &gt; 0\)</span> is the <em>dispersion parameter</em>. For some distributions, such as the Bernoulli and Poisson distribution, the dispersion parameter is fixed to <span class="math inline">\(\phi = 1\)</span>.<a href="ch-generalized-linear-models.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn31"><p>The UK financial market was rather in turmoil on 29 September 2022 after the UK government released its “mini-budget”.<a href="ch-generalized-linear-models.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p>As mentioned in a previous footnote, the canonical link function is the function that relates the canonical parameter of the exponential family distribution in question to the mean, <span class="math inline">\(\theta = g(\mu)\)</span>.<a href="ch-generalized-linear-models.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p>This involves repeatedly simulating data under the null-hypothesis and obtaining parameter estimates for each simulated data set. This leads to a set of parameter estimates, which can be ordered. You can then compute a bootstrap confidence interval as the range of estimates excluding the most extreme estimates. For example, a 95% confidence interval would be computed by excluding the 2.5% lowest and 2.5% highest parameter estimates; the lower confidence bound is then the lowest estimate, and the upper bound the maximum estimate, in the set that remains.<a href="ch-generalized-linear-models.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p>Pseudo-<span class="math inline">\(R^2\)</span> measures have been suggested for generalized linear models, such as McFadden’s, Cox and Snell’s, and Nagelkerke’s. These measures are based on the log likelihood ratio comparing the the fitted model and a null model with only an intercept. Whilst the aim of these measures is to have a statistic with a similar scale as the traditional <span class="math inline">\(R^2\)</span> for linear models, they don’t generally succeed in this. Hence, I don’t recommend their use. If you are interested, you can find more information on these measures <a href="https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/">here</a>.<a href="ch-generalized-linear-models.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p>That is because, under the assumption of conditional independence of the observations:
<span class="math display">\[\begin{aligned} \frac{p(Y_1, \ldots, Y_n|\text{MODEL R})}{p(Y_1, \ldots, Y_n|\text{MODEL S})} &amp;= \frac{\prod_{i=1}^n p(Y_i|\text{MODEL R})}{\prod_{i=1}^n p(Y_i|\text{MODEL S})} \\
&amp;= \prod_{i=1}^n \frac{p(Y_i|\text{MODEL R})}{p(Y_i|\text{MODEL S})}\\
\log\left( \prod_{i=1}^n \frac{p(Y_i|\text{MODEL R})}{p(Y_i|\text{MODEL S})} \right) &amp;= \sum_{i=1}^n \log \left( \frac{p(Y_i|\text{MODEL R})}{p(Y_i|\text{MODEL S})} \right) \\
&amp;= \sum_{i=1}^n D_{R,i} \end{aligned}\]</span><a href="ch-generalized-linear-models.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn36"><p>Standardized Pearson residuals are defined as <span class="math display">\[\begin{equation}
R^\text{Pearson}_i = \frac{Y_i - \hat{\mu}_i}{\sqrt{\hat{V}(Y_i|X_{1,i}, \ldots, X_{m,i})(1-h_i)}}
\end{equation}\]</span>
where <span class="math inline">\(\hat{V}(Y_i|X_{1,i}, \ldots, X_{m,i})\)</span> denotes the (estimated) variance of <span class="math inline">\(Y_i\)</span> for cases with predictor values <span class="math inline">\(X_{1,i}, \ldots, X_{m,i}\)</span>, and <span class="math inline">\(h_i\)</span> is the <em>leverage</em> value.<a href="ch-generalized-linear-models.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p>As we are only considering a single outcome, <span class="math inline">\(n = 1\)</span> and we do not need to worry about how many possible outcomes could constitute <span class="math inline">\(k = 0, 1, 2, \ldots\)</span> correct ones, which is why we don’t need the <span class="math inline">\(\left(\right)\)</span> term.<a href="ch-generalized-linear-models.html#fnref37" class="footnote-back">↩︎</a></p></li>
<li id="fn38"><p>This is because at <span class="math inline">\(p(y=1) = .5\)</span>, <span class="math display">\[\log \frac{p(Y=1)}{p(Y=0)} = \log \left(\frac{.5}{.5} \right) = \log(1) = 0\]</span>. And <span class="math inline">\(\beta_0 + \beta_1 \times X = 0\)</span> when <span class="math inline">\(X = -\frac{\beta_0}{\beta_1}\)</span>. <a href="ch-generalized-linear-models.html#fnref38" class="footnote-back">↩︎</a></p></li>
<li id="fn39"><p>We need one parameter less as the number of cells, as when <span class="math inline">\(n\)</span> is known, the last remaining cell can be filled in as <span class="math inline">\(n\)</span> minus the frequencies in the other cells.<a href="ch-generalized-linear-models.html#fnref39" class="footnote-back">↩︎</a></p></li>
<li id="fn40"><p>In the experiment, some noise was added to the actions of the AI player, who would choose a completely random action on 10% of the rounds.<a href="ch-generalized-linear-models.html#fnref40" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-linear-mixed-effects-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-SEM-path-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"source": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
