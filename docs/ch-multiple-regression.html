<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Multiple regression | Statistics: Data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Multiple regression | Statistics: Data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Multiple regression | Statistics: Data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2021-12-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-simple-regression.html"/>
<link rel="next" href="ch-moderation-mediation.html"/>
<script src="book_assets/header-attrs-2.11/header-attrs.js"></script>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="book_assets/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.6.1/grViz.js"></script>
<script src="book_assets/plotly-binding-4.10.0/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-2.5.1/plotly-latest.min.js"></script>
<link href="book_assets/combineWidgetStyle-0.1/combineWidgets.css" rel="stylesheet" />
<script src="book_assets/combineWidgets-binding-0.11.1/combineWidgets.js"></script>
<script src="book_assets/rglWebGL-binding-0.108.3/rglWebGL.js"></script>
<link href="book_assets/rglwidgetClass-0.108.3/rgl.css" rel="stylesheet" />
<script src="book_assets/rglwidgetClass-0.108.3/rglClass.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/utils.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/buffer.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/subscenes.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/shaders.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/textures.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/projection.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/mouse.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/init.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/pieces.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/draw.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/controls.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/selection.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/rglTimer.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/pretty.src.js"></script>
<script src="book_assets/rglwidgetClass-0.108.3/axes.src.js"></script>
<script src="book_assets/CanvasMatrix4-0.108.3/CanvasMatrix.src.js"></script>
<script src="book_assets/rglPlayer-binding-0.108.3/rglPlayer.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ch-intro.html"><a href="ch-intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="ch-intro.html"><a href="ch-intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-modeling.html"><a href="ch-modeling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-probability-definition"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-binomial-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="ch-modeling.html"><a href="ch-modeling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-modeling.html"><a href="ch-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-modeling.html"><a href="ch-modeling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-likelihood-ratio"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-modeling.html"><a href="ch-modeling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-modeling.html"><a href="ch-modeling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-modeling.html"><a href="ch-modeling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-modeling.html"><a href="ch-modeling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="ch-modeling.html"><a href="ch-modeling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="ch-modeling.html"><a href="ch-modeling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-an-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has an specific value</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#ch3-confidence-interval"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#effect-size"><i class="fa fa-check"></i><b>3.6</b> Effect size</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.7</b> Assumptions</a></li>
<li class="chapter" data-level="3.8" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.8</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.8.1</b> The Central Limit Theorem in action</a></li>
<li class="chapter" data-level="3.8.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#bootstrapping-a-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Bootstrapping a statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#in-practice"><i class="fa fa-check"></i><b>3.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>4.4.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-simple-regression.html"><a href="ch-simple-regression.html#summary-2"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#transforming-variables"><i class="fa fa-check"></i><b>5.7.1</b> Transforming variables</a></li>
<li class="chapter" data-level="5.7.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.7.2</b> Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.8</b> Multicollinearity: Redundancy between predictors</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.8.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-outliers"><i class="fa fa-check"></i><b>5.9</b> Outliers</a></li>
<li class="chapter" data-level="5.10" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#in-practice-1"><i class="fa fa-check"></i><b>5.10</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#sec:05-dont-forget-about-the-fun"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-moderation-mediation.html"><a href="ch-moderation-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#sec:06-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-size-in-anova"><i class="fa fa-check"></i><b>7.6</b> Effect-size in ANOVA</a></li>
<li class="chapter" data-level="7.7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#assumptions"><i class="fa fa-check"></i><b>7.7</b> Assumptions</a></li>
<li class="chapter" data-level="7.8" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.8</b> Multiple testing and post-hoc tests</a></li>
<li class="chapter" data-level="7.9" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#in-practice-2"><i class="fa fa-check"></i><b>7.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#sec:06-main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#the-factorial-anova-model"><i class="fa fa-check"></i><b>8.3</b> The factorial ANOVA model</a></li>
<li class="chapter" data-level="8.4" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.4</b> A threeway factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#interpreting-interactions"><i class="fa fa-check"></i><b>8.4.1</b> Interpreting interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.5</b> Orthogonal contrast codes and unequal sample sizes</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.5.1</b> Comparison schemes and SS types</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#in-practice-3"><i class="fa fa-check"></i><b>8.6</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#subjective-feelings-of-power-and-priming"><i class="fa fa-check"></i><b>9.1</b> Subjective feelings of power and priming</a></li>
<li class="chapter" data-level="9.2" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#acounting-for-pre-existing-differences"><i class="fa fa-check"></i><b>9.2</b> Acounting for pre-existing differences</a></li>
<li class="chapter" data-level="9.3" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#slopes-of-contrast-coded-predictors-in-ancova-models"><i class="fa fa-check"></i><b>9.3</b> Slopes of contrast-coded predictors in ANCOVA models</a></li>
<li class="chapter" data-level="9.4" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#homogeneity-of-slopes"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of slopes</a></li>
<li class="chapter" data-level="9.5" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#sec:ANCOVA-power"><i class="fa fa-check"></i><b>9.5</b> Power considerations in ANCOVA</a></li>
<li class="chapter" data-level="9.6" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#models-with-multiple-covariates"><i class="fa fa-check"></i><b>9.6</b> Models with multiple covariates</a></li>
<li class="chapter" data-level="9.7" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#mediation-with-categorical-independent-variables"><i class="fa fa-check"></i><b>9.7</b> Mediation with categorical independent variables</a></li>
<li class="chapter" data-level="9.8" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#ancova-vs-difference-scores"><i class="fa fa-check"></i><b>9.8</b> ANCOVA vs difference scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#non-independence-in-linear-models"><i class="fa fa-check"></i><b>10.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="10.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#the-cheerleader-effect"><i class="fa fa-check"></i><b>10.2</b> The cheerleader effect</a></li>
<li class="chapter" data-level="10.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#as-a-oneway-anova"><i class="fa fa-check"></i><b>10.3</b> As a oneway ANOVA</a></li>
<li class="chapter" data-level="10.4" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#oneway-repeated-measures-anova"><i class="fa fa-check"></i><b>10.4</b> Oneway repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#within-subjects-composite-scores"><i class="fa fa-check"></i><b>10.4.1</b> Within-subjects composite scores</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-composite-for-between-subjects-effects"><i class="fa fa-check"></i><b>10.4.2</b> A composite for between-subjects effects</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#collecting-all-results-and-omnibus-tests"><i class="fa fa-check"></i><b>10.4.3</b> Collecting all results and omnibus tests</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#partitioning-the-variance"><i class="fa fa-check"></i><b>10.5</b> Partitioning the variance</a></li>
<li class="chapter" data-level="10.6" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-mixed-anova-with-between--and-within-subjects-effects"><i class="fa fa-check"></i><b>10.6</b> A mixed ANOVA with between- and within-subjects effects</a></li>
<li class="chapter" data-level="10.7" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#assumptions-1"><i class="fa fa-check"></i><b>10.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#omnibus-tests-and-sphericity"><i class="fa fa-check"></i><b>10.7.1</b> Omnibus tests and sphericity</a></li>
<li class="chapter" data-level="10.7.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#correcting-for-non-sphericity"><i class="fa fa-check"></i><b>10.7.2</b> Correcting for non-sphericity</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#in-practice-4"><i class="fa fa-check"></i><b>10.8</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#non-independence-in-linear-models-1"><i class="fa fa-check"></i><b>11.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="11.2" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#random-intercept-models"><i class="fa fa-check"></i><b>11.2</b> Random intercept models</a></li>
<li class="chapter" data-level="11.3" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-estimation-1"><i class="fa fa-check"></i><b>11.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="11.4" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#parameter-inference"><i class="fa fa-check"></i><b>11.4</b> Parameter inference</a></li>
<li class="chapter" data-level="11.5" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#application-of-the-random-intercepts-model"><i class="fa fa-check"></i><b>11.5</b> Application of the random-intercepts model</a></li>
<li class="chapter" data-level="11.6" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#models-with-random-intercepts-and-slopes"><i class="fa fa-check"></i><b>11.6</b> Models with random intercepts and slopes</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#correlation-between-random-effects"><i class="fa fa-check"></i><b>11.6.1</b> Correlation between random effects</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#crossed-random-effects-dating-partners-in-the-speed-dating-experiment"><i class="fa fa-check"></i><b>11.7</b> Crossed random effects: dating partners in the speed dating experiment</a></li>
<li class="chapter" data-level="11.8" data-path="ch-linear-mixed-effects-models.html"><a href="ch-linear-mixed-effects-models.html#choosing-the-random-effects-structure"><i class="fa fa-check"></i><b>11.8</b> Choosing the random effects structure</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html"><i class="fa fa-check"></i><b>12</b> Introduction to Bayesian hypothesis testing</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#fundamentals-of-bayesian-inference"><i class="fa fa-check"></i><b>12.1</b> Fundamentals of Bayesian inference</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#probability-in-times-of-covid"><i class="fa fa-check"></i><b>12.1.1</b> Probability in times of Covid</a></li>
<li class="chapter" data-level="12.1.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#bayes-rule"><i class="fa fa-check"></i><b>12.1.2</b> Bayes’ rule</a></li>
<li class="chapter" data-level="12.1.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#we-missed-you-paul"><i class="fa fa-check"></i><b>12.1.3</b> We missed you Paul!</a></li>
<li class="chapter" data-level="12.1.4" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-marginal-likelihood-and-prior-predictive-distribution"><i class="fa fa-check"></i><b>12.1.4</b> The marginal likelihood and prior predictive distribution</a></li>
<li class="chapter" data-level="12.1.5" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#hypothesis-testing-relative-evidence-and-the-bayes-factor"><i class="fa fa-check"></i><b>12.1.5</b> Hypothesis testing, relative evidence, and the Bayes factor</a></li>
<li class="chapter" data-level="12.1.6" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#parameter-estimates-and-credible-intervals"><i class="fa fa-check"></i><b>12.1.6</b> Parameter estimates and credible intervals</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#a-bayesian-t-test"><i class="fa fa-check"></i><b>12.2</b> A Bayesian t-test</a></li>
<li class="chapter" data-level="12.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#bayes-factors-for-general-linear-models"><i class="fa fa-check"></i><b>12.3</b> Bayes factors for General Linear Models</a></li>
<li class="chapter" data-level="12.4" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#some-objections-to-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>12.4</b> Some objections to null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-is-not-a-proper-measure-of-evidential-support"><i class="fa fa-check"></i><b>12.4.1</b> The <span class="math inline">\(p\)</span>-value is not a proper measure of evidential support</a></li>
<li class="chapter" data-level="12.4.2" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#the-p-value-depends-on-researcher-intentions"><i class="fa fa-check"></i><b>12.4.2</b> The <span class="math inline">\(p\)</span>-value depends on researcher intentions</a></li>
<li class="chapter" data-level="12.4.3" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#results-of-a-nhst-are-often-misinterpreted"><i class="fa fa-check"></i><b>12.4.3</b> Results of a NHST are often misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#to-bayes-or-not-to-bayes-a-pragmatic-view"><i class="fa fa-check"></i><b>12.5</b> To Bayes or not to Bayes? A pragmatic view</a></li>
<li class="chapter" data-level="12.6" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#in-practice-5"><i class="fa fa-check"></i><b>12.6</b> In practice</a></li>
<li class="chapter" data-level="12.7" data-path="ch-Bayes-factors.html"><a href="ch-Bayes-factors.html#summary-3"><i class="fa fa-check"></i><b>12.7</b> “Summary”</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html"><i class="fa fa-check"></i><b>13</b> Being a responsible data analyst</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#consider-analysis-before-data-collection"><i class="fa fa-check"></i><b>13.1</b> Consider analysis before data collection</a></li>
<li class="chapter" data-level="13.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#explore-the-data"><i class="fa fa-check"></i><b>13.2</b> Explore the data</a></li>
<li class="chapter" data-level="13.3" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#evaluate-the-assumptions-underlying-your-analyses"><i class="fa fa-check"></i><b>13.3</b> Evaluate the assumptions underlying your analyses</a></li>
<li class="chapter" data-level="13.4" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#distinguish-between-confirmatory-and-exploratory-analyses"><i class="fa fa-check"></i><b>13.4</b> Distinguish between confirmatory and exploratory analyses</a></li>
<li class="chapter" data-level="13.5" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#aim-for-openness-and-reproducibility"><i class="fa fa-check"></i><b>13.5</b> Aim for openness and reproducibility</a></li>
<li class="chapter" data-level="13.6" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#communicate-clearly-and-concisely"><i class="fa fa-check"></i><b>13.6</b> Communicate clearly and concisely</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-multiple-regression-analysis"><i class="fa fa-check"></i><b>13.6.1</b> Example of reporting a multiple regression analysis</a></li>
<li class="chapter" data-level="13.6.2" data-path="ch-responsible-data-analyst.html"><a href="ch-responsible-data-analyst.html#example-of-reporting-a-factorial-anova"><i class="fa fa-check"></i><b>13.6.2</b> Example of reporting a factorial ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: Data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-multiple-regression" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Multiple regression</h1>
<p>In this chapter, we extend the simple regression model to include multiple predictor variables. The slopes of the predictors in the resulting multiple regression model reflect the unique effect of each predictor on the dependent variable, after removing the effect of all other predictors from both the dependent and predictor variable. We will look at what that means in some detail. We then go on to discuss parameter estimation and testing.</p>
<div id="trump-votes-and-hate-groups-again" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Trump, votes, and hate groups (again)</h2>
<p>In the previous chapter, we found evidence for a relation between the number of hate groups and votes for Trump. This relation was found with an observational study, not an experimental one. Because the number of hate groups was not randomly assigned to states, there are possible confounding factors that could account for the result. The well-known phrase “correlation does not imply causation” should be taken seriously. For instance, it could be that hate groups are especially prevalent in places with relatively low levels of education. And it might be that those with a relatively low level of education are also more likely to vote for Trump. In other words, education level might be a <strong>common cause</strong> of both hate groups and Trump votes, whilst there is no direct relation between hate groups and Trump votes. The difference between a direct relation, and a spurious one through a common cause, is depicted in Figure <a href="ch-multiple-regression.html#fig:direct-relation-vs-common-cause">5.1</a>. You can see more, often rather amusing examples of spurious relations on the <a href="https://www.tylervigen.com/spurious-correlations">spurious correlation</a> website.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:direct-relation-vs-common-cause"></span>
<img src="04b-GLM-multiple-regression_files/figure-html/direct-relation-vs-common-cause-1.svg" alt="A direct relation between hate groups and Trump votes vs a spurious relation through a common cause (education)." width="672" />
<p class="caption">
Figure 5.1: A direct relation between hate groups and Trump votes vs a spurious relation through a common cause (education).
</p>
</div>
<p>To assess the relation between hate groups and Trump votes in the possible presence of common causes and other confounds, ideally, we would like to “remove” the effect of such confounds from both the “hate groups” predictor, and the “Trump votes” dependent variable. If, after statistically controlling for confounds in such a way, we still find evidence for a relation between hate groups and Trump votes, that would strengthen our belief that the relation is real, rather than spurious. Multiple regression is a way in which to determine <em>unique effects</em> of predictor variables on the dependent variable. Before we move on to define the multiple regression model, we will work through an indirect way to control for a third variable (e.g. a possible common cause), which will provide insight into what is meant by unique effects in a multiple regression model.</p>
<div id="controlling-for-education-level" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Controlling for education level</h3>
<p>We have indicated that, potentially, the level of education might be a common cause for both the prevalence of hate groups, and voter support for Trump. What if we could assess the relation between hate groups and Trump votes after removing the effect of education level from both? One reasonable way to do this, using the tools we already have, is by means of simple regression models. The idea is reasonably straightforward. In a regression model predicting Trump votes from education level, the error terms (residuals) of that model reflect that part of the variation in Trump votes which can not be predicted from education level. Similarly, in a regression model predicting hate groups from education level, the error terms (residuals) of that model reflect the variation in hate groups which can not be predicted from education level. So, if there is still a relation between the <em>residual</em> Trump vote, and the <em>residual</em> hate groups, then the relation can not be due to education level as a common cause.</p>
<p>When we estimate a regression model predicting Trump votes from education level (defined as the percentage of citizens who obtained a Bachelors degree or higher), we obtain the following estimated model
<span class="math display">\[\texttt{trump_votes}_i = 90.069 - 1.357 \times \texttt{education}_i + \hat{\epsilon}_{\texttt{votes},i}\]</span>
and for the hate groups we obtain
<span class="math display">\[\texttt{hate_groups}_i = 6.146 - 0.105 \times \texttt{education}_i + \hat{\epsilon}_{\texttt{hate},i}\]</span>
Note that I’m labelling the error terms for both models with a subscript (either <span class="math inline">\(\texttt{votes}\)</span> or <span class="math inline">\(\texttt{hate}\)</span>) to denote these are the residual terms for a model predicting Trump votes and hate groups from education levels respectively. Also, I’m adding the hat above them to indicate that these are not the “true errors” (from a model with the true values of the slope and intercept) but rather estimates of these resulting from using estimated parameters. The residuals, as well as the estimated regression lines, are depicted in Figure <a href="ch-multiple-regression.html#fig:regressions-controlling-for-education">5.2</a>. The slope of <span class="math inline">\(\texttt{education}_i\)</span> is significant in both models. In the model for Trump votes, <span class="math inline">\(\hat{\beta}_\texttt{education} = -1.357\)</span>, <span class="math inline">\(t(48) = -7.30\)</span>, <span class="math inline">\(p &lt; .001\)</span>. In the model for hate groups, <span class="math inline">\(\hat{\beta}_\texttt{education} = -0.105\)</span>, <span class="math inline">\(t(48) = -2.11\)</span>, <span class="math inline">\(p = .040\)</span>.
This thus shows a reliable relation between education level and Trump votes, and between education level and hate groups, which is supportive of the idea of a common cause. However, significance is not a prerequisite to statistically control for a third variable.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:regressions-controlling-for-education"></span>
<img src="04b-GLM-multiple-regression_files/figure-html/regressions-controlling-for-education-1.svg" alt="Predicting Trump votes and hate groups by linear regression models with education level as predictor. The residuals of each model are depicted as vertical lines from the observations to the regression line." width="49%" /><img src="04b-GLM-multiple-regression_files/figure-html/regressions-controlling-for-education-2.svg" alt="Predicting Trump votes and hate groups by linear regression models with education level as predictor. The residuals of each model are depicted as vertical lines from the observations to the regression line." width="49%" />
<p class="caption">
Figure 5.2: Predicting Trump votes and hate groups by linear regression models with education level as predictor. The residuals of each model are depicted as vertical lines from the observations to the regression line.
</p>
</div>
<p>The regression line in both models represents the variation in the dependent variable (Trump votes and hate groups) that is dependent on education level. In both models, the error term <span class="math inline">\(\epsilon\)</span> is assumed to be completely independent from the predictor. Hence, these are the aspects of Trump votes and hate groups that cannot be explained by education level. To assess whether there is a relation between these residual terms, which would indicate a relation between Trump votes and hate groups that is independent from education level, we can estimate another linear regression model
<span class="math display" id="eq:slope-hate-groups-from-residuals">\[
\hat{\epsilon}_{\texttt{votes},i}  = 0 + 1.314 \times \hat{\epsilon}_{\texttt{hate},i} + \hat{\epsilon}_i
\tag{5.1}
\]</span>
Note that the intercept is equal to 0. This is necessarily the case, because <strong>the residuals of estimated regression models always have a mean of 0</strong>. If you go back to the estimate of the intercept in Equation <a href="ch-simple-regression.html#eq:simple-regression-intercept-estimate">(4.4)</a>, you will see that if the means are both 0, the result has to be 0 as well:
<span class="math display">\[\begin{aligned}
\hat{\beta}_0 &amp;= \overline{Y} - \hat{\beta}_1 \overline{X} \\
&amp;= 0 - \hat{\beta}_1 \times 0 \\
&amp;= 0
\end{aligned}\]</span></p>
<p>A scatterplot of the dependent variable (the <em>residual</em> Trump votes <span class="math inline">\(\hat{\epsilon}_{\texttt{votes},i}\)</span>) and the predictor (the <em>residual</em> hate groups <span class="math inline">\(\hat{\epsilon}_{\texttt{hate},i}\)</span>), together with the estimated regression line, is provided in Figure <a href="ch-multiple-regression.html#fig:scatterplot-residual-votes-and-hate">5.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterplot-residual-votes-and-hate"></span>
<img src="04b-GLM-multiple-regression_files/figure-html/scatterplot-residual-votes-and-hate-1.svg" alt="Residual Trump votes and hate groups, after statistically removing the effect of education level from both, and the estimated regression line." width="80%" />
<p class="caption">
Figure 5.3: Residual Trump votes and hate groups, after statistically removing the effect of education level from both, and the estimated regression line.
</p>
</div>
<p>We can see that there appears to be a positive relation between residual Trump votes and residual hate groups. If we perform a test of the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> in this model, we find that the result is significant, <span class="math inline">\(t(48) = 2.60\)</span>, <span class="math inline">\(p = .012\)</span>. This thus indicates that there is a relation between Trump votes and hate groups that can not be attributed to education level. While the procedure of removing the effect of education level from both Trump votes and hate groups is appropriate, there however is a slight issue with the hypothesis test we just performed. To remove the effect of education level, we first had to estimate two new models. These estimates are noisy, and this noise may affect the estimates of the third model in which we predicted the residual Trump votes from the residual hate groups. The hypothesis test does not take this additional source of noise into account.</p>
<p>It turns out that we do not need to estimate separate models to remove the effect of education level. By including both education level and hate groups as predictors of Trump votes in a multiple regression model, the estimated slope of hate groups will be <em>exactly equal</em> to the one that we just computed from the residuals. Multiple regression models thus concern the unique effects of each predictor on the dependent variable, removing the effect of all other predictors from that relation.</p>
</div>
</div>
<div id="the-multiple-regression-model" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> The multiple regression model</h2>
<p>The multiple regression model is a straightforward extension of the simple linear regression model, including more than one predictor <span class="math inline">\(X\)</span>:
<span class="math display" id="eq:multiple-regression-model-definition">\[\begin{equation}
Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
\tag{5.2}
\end{equation}\]</span>
Note that we are using <span class="math inline">\(m\)</span> to reflect the total number of predictors <span class="math inline">\(X\)</span> in the model. As for the simple regression model, this model consists of a structural part which reflects the conditional mean of the dependent variable <span class="math inline">\(Y\)</span>, conditional upon all predictors:
<span class="math display">\[\beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_k \times X_{m,i} = \mu_{Y|X_1,\ldots,X_m}\]</span>
and a random part <span class="math inline">\(\epsilon_i\)</span>.</p>
<p>For a model with <span class="math inline">\(m=2\)</span> predictors, such as
<span class="math display" id="eq:multiple-regression-votes-by-hate-education">\[
\texttt{trump_votes}_i = \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
\tag{5.3}
\]</span>
the data can be represented in a three-dimensional space. In this space, the conditional means <span class="math inline">\(\mu_{Y | X_{1,i}, X_{2,i}}\)</span> can be represented as a <strong>regression plane</strong>. A visual representation is given below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:3d-regression-plane"></span>
<div id="htmlwidget-ad2e7a42db534b8311e7" style="width:672px;height:480px;" class="combineWidgets html-widget"></div>
<script type="application/json" data-for="htmlwidget-ad2e7a42db534b8311e7">{"x":{"data":[{"material":{"color":"#000000","alpha":1,"lit":true,"ambient":"#000000","specular":"#FFFFFF","emission":"#000000","shininess":50,"smooth":true,"front":"filled","back":"filled","size":3,"lwd":1,"fog":true,"point_antialias":false,"line_antialias":false,"texture":null,"textype":"rgb","texmipmap":false,"texminfilter":"linear","texmagfilter":"linear","texenvmap":false,"depth_mask":true,"depth_test":"less","isTransparent":false,"polygon_offset":[0,0],"margin":"","floating":false,"tag":""},"rootSubscene":7,"objects":{"13":{"id":13,"type":"spheres","material":{},"vertices":"0","colors":"1","radii":[[0.469553381204605]],"centers":"2","ignoreExtent":false,"fastTransparency":true,"flags":32771},"15":{"id":15,"type":"text","material":{"lit":false,"margin":0,"floating":true,"edge":[0,1,1]},"vertices":"3","colors":"4","texts":[["Hate groups"]],"cex":[[1]],"adj":[[0.5,0.5,0.5]],"centers":"5","family":[["sans"]],"font":[[1]],"ignoreExtent":true,"flags":33808},"16":{"id":16,"type":"text","material":{"lit":false,"margin":1,"floating":true,"edge":[1,1,1]},"vertices":"6","colors":"7","texts":[["% bachelors degree or higher"]],"cex":[[1]],"adj":[[0.5,0.5,0.5]],"centers":"8","family":[["sans"]],"font":[[1]],"ignoreExtent":true,"flags":33808},"17":{"id":17,"type":"text","material":{"lit":false,"margin":2,"floating":true,"edge":[1,1,1]},"vertices":"9","colors":"10","texts":[["% Trump votes"]],"cex":[[1]],"adj":[[0.5,0.5,0.5]],"centers":"11","family":[["sans"]],"font":[[1]],"ignoreExtent":true,"flags":33808},"18":{"id":18,"type":"planes","material":{"alpha":0.498039215803146,"isTransparent":true},"vertices":"12","colors":"14","offsets":[[81.994499206543]],"centers":"15","normals":"13","ignoreExtent":true,"flags":32803},"11":{"id":11,"type":"light","vertices":[[0,0,1]],"colors":[[1,1,1,1],[1,1,1,1],[1,1,1,1]],"viewpoint":true,"finite":false},"10":{"id":10,"type":"background","material":{},"colors":"16","centers":"17","sphere":false,"fogtype":"none","fogscale":1,"flags":32768},"12":{"id":12,"type":"background","material":{"lit":false,"back":"lines"},"colors":"18","centers":"19","sphere":false,"fogtype":"none","fogscale":1,"flags":32768},"14":{"id":14,"type":"bboxdeco","material":{"front":"lines","back":"lines"},"vertices":"20","colors":"21","axes":{"mode":["pretty","pretty","pretty"],"step":[2,5,10],"nticks":[5,5,5],"marklen":[15,15,15],"expand":[1.02999997138977,1.02999997138977,1.02999997138977]},"draw_front":true,"flags":32769},"7":{"id":7,"type":"subscene","par3d":{"antialias":8,"FOV":60,"ignoreExtent":false,"listeners":7,"mouseMode":{"none":"none","left":"trackball","right":"zoom","middle":"fov","wheel":"pull"},"observer":[0,0,63.8703956604004],"modelMatrix":[[0.135689556598663,-1.07825040817261,-0.0036207961384207,30.2068157196045],[0.0853842422366142,-0.00411621667444706,0.702260494232178,-35.4404487609863],[-2.9333393573761,-0.0499938614666462,0.0202746856957674,-49.3974571228027],[0,0,0,1]],"projMatrix":[[1.73205077648163,0,0,0],[0,1.73205077648163,0,0],[0,0,-1.99999988079071,-95.8055877685547],[0,0,-1,0]],"skipRedraw":false,"userMatrix":[[0.04618797,-0.998904467,-0.005153618,0],[0.02906432,-0.003813314,0.999554276,0],[-0.99849236,-0.046314929,0.028857738,0],[0,0,0,1]],"userProjection":[[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],"scale":[2.93776845932007,1.07943296432495,0.702573657035828],"viewport":{"x":0,"y":0,"width":1,"height":1},"zoom":1,"bbox":[-0.159833356738091,9.74983310699463,14.9649991989136,41.935001373291,29.3316669464111,70.7683334350586],"windowRect":[1931,83,2187,339],"family":"sans","font":1,"cex":1,"useFreeType":true,"fontname":"/usr/share/fonts/truetype/msttcorefonts/Arial.ttf","maxClipPlanes":8,"glVersion":4.6,"activeSubscene":0},"embeddings":{"viewport":"replace","projection":"replace","model":"replace","mouse":"replace"},"objects":[12,14,13,15,16,17,18,11],"subscenes":[],"flags":34099}},"crosstalk":{"key":[],"group":[],"id":[],"options":[]},"width":672,"height":480,"context":{"shiny":false,"rmarkdown":null},"buffer":{"accessors":[{"bufferView":0,"componentType":5126,"count":50,"type":"VEC3"},{"bufferView":1,"componentType":5126,"count":1,"type":"VEC4"},{"bufferView":2,"componentType":5126,"count":50,"type":"VEC3"},{"bufferView":3,"componentType":5126,"count":1,"type":"VEC3"},{"bufferView":4,"componentType":5121,"count":1,"type":"VEC4"},{"bufferView":5,"componentType":5126,"count":1,"type":"VEC3"},{"bufferView":6,"componentType":5126,"count":1,"type":"VEC3"},{"bufferView":7,"componentType":5121,"count":1,"type":"VEC4"},{"bufferView":8,"componentType":5126,"count":1,"type":"VEC3"},{"bufferView":9,"componentType":5126,"count":1,"type":"VEC3"},{"bufferView":10,"componentType":5121,"count":1,"type":"VEC4"},{"bufferView":11,"componentType":5126,"count":1,"type":"VEC3"},{"bufferView":12,"componentType":5126,"count":3,"type":"VEC3"},{"bufferView":13,"componentType":5126,"count":1,"type":"VEC3"},{"bufferView":14,"componentType":5126,"count":1,"type":"VEC4"},{"bufferView":15,"componentType":5126,"count":4,"type":"VEC3"},{"bufferView":16,"componentType":5126,"count":1,"type":"VEC4"},{"bufferView":17,"componentType":5121,"count":1,"type":"VEC3"},{"bufferView":18,"componentType":5121,"count":1,"type":"VEC4"},{"bufferView":19,"componentType":5121,"count":1,"type":"VEC3"},{"bufferView":20,"componentType":5126,"count":16,"type":"VEC3"},{"bufferView":21,"componentType":5121,"count":1,"type":"VEC4"}],"bufferViews":[{"buffer":0,"byteLength":600,"byteOffset":0},{"buffer":0,"byteLength":16,"byteOffset":600},{"buffer":0,"byteLength":600,"byteOffset":616},{"buffer":0,"byteLength":12,"byteOffset":1216},{"buffer":0,"byteLength":4,"byteOffset":1228},{"buffer":0,"byteLength":12,"byteOffset":1232},{"buffer":0,"byteLength":12,"byteOffset":1244},{"buffer":0,"byteLength":4,"byteOffset":1256},{"buffer":0,"byteLength":12,"byteOffset":1260},{"buffer":0,"byteLength":12,"byteOffset":1272},{"buffer":0,"byteLength":4,"byteOffset":1284},{"buffer":0,"byteLength":12,"byteOffset":1288},{"buffer":0,"byteLength":36,"byteOffset":1300},{"buffer":0,"byteLength":12,"byteOffset":1336},{"buffer":0,"byteLength":16,"byteOffset":1348},{"buffer":0,"byteLength":48,"byteOffset":1364},{"buffer":0,"byteLength":16,"byteOffset":1412},{"buffer":0,"byteLength":3,"byteOffset":1428},{"buffer":0,"byteLength":4,"byteOffset":1431},{"buffer":0,"byteLength":3,"byteOffset":1435},{"buffer":0,"byteLength":192,"byteOffset":1440},{"buffer":0,"byteLength":4,"byteOffset":1632}],"buffers":[{"byteLength":1636,"bytes":"mpmxQGZmdkGamXtCAAAAAJqZ7UGamVNCZmYmQJqZ3UEAAEZCMzOrQGZmrkGamXFC16MAQDMz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\nAADAfwAAwH8AAIxCAAAAAQ=="}]},"players":"rgl-play44962","webGLoptions":{"preserveDrawingBuffer":true}},{"data":[{"actions":[{"type":"propertySetter","value":0,"values":[0.0461955035819809,0.0290752105451463,-0.998509192536835,-0.99891916438069,-0.00380447990672883,-0.0463252519218584,-0.0051457246128756,0.999569986576425,0.0288680351567914,0.221836160711478,0.0284668303666545,-0.974668331982561,-0.975075785870317,0.0105677154957354,-0.221620249974104,0.00399119157326633,0.99953887516098,0.0301016184240678,0.609308114231042,0.0175720042275883,-0.792738826221875,-0.79264779287128,0.0403360236871847,-0.608344048749879,0.0212861078757755,0.999031646075605,0.0385054768957399,0.911421738684351,-0.00493535073406448,-0.411443868063101,-0.410462375721168,0.0591502839663192,-0.909957077022888,0.0288279789601852,0.998236888829418,0.0518850788689345,0.997703289063174,-0.016387961992787,-0.0657235246639178,-0.0647414719966135,0.0545916623166261,-0.996407693772594,0.0199170478795768,0.998374271056316,0.0534053002662864,0.998919525761669,0.00381157557302116,0.0463168753666998,0.0461869308337188,0.0290741998028452,-0.998509618544551,-0.00515252096012599,0.999569988943983,0.0288667409015995,0.976291000946212,0.0629481953306342,-0.207107233519401,-0.204889653080507,-0.0399197571825176,-0.977970675964795,-0.0698291596111018,0.997218099359967,-0.0260758657939122,0.565603953366101,0.0860101521532933,-0.820179505756639,-0.79828270950267,-0.192501185676361,-0.57069081753807,-0.206970731374542,0.977520100683235,-0.0402190143405437,-0.459773734547547,-0.0472908997678245,-0.88677600543731,-0.847209616636638,-0.275939395339742,0.4539750164689,-0.26616532174309,0.960010948322303,0.0868043812414447,-0.936663703461039,-0.101502808773249,-0.335198875937544,-0.315449071607304,-0.171311140623418,0.933351153811037,-0.152161065456121,0.979974322576589,0.128441960627411,-0.998901478396697,-0.00346004438975842,-0.0467318365754416,-0.0466116308099021,-0.0291241989214193,0.998488426027276,-0.00481584158110081,0.99956981203416,0.0289309270480439,-0.974912731593599,0.199420402329447,-0.0988770393542794,-0.0626186390445859,0.180557907398949,0.9815690236147,0.213597920918978,0.963135683696993,-0.16354076852154,-0.814978146643388,0.39478019599221,-0.424216003170626,-0.190640296021312,0.50864994916419,0.839601993058719,0.547235687822546,0.765129940642515,-0.339277579432779,-0.444452370355725,0.368034497142098,-0.816708454344964,-0.214206215170578,0.841593122002915,0.495819235588619,0.869814800901591,0.395312061428373,-0.295212781264255,-0.0861864334744951,0.14875004285399,-0.985111832959021,-0.0856248700134434,0.984026537953687,0.156077398229411,0.99259270615904,0.0978018269451927,-0.0720730346666161,0.0461951586885845,0.0290650452577342,-0.998509504440445,-0.00515343782505161,0.999570250966594,0.0288575026841193,0.998919140567604,0.00381267973300778,0.046325091267408,-0.0861864334744949,0.14875004285399,-0.985111832959021,-0.0856248700134436,0.984026537953687,0.156077398229411,0.992592706159041,0.0978018269451927,-0.0720730346666159,-0.444452370355724,0.368034497142098,-0.816708454344965,-0.214206215170578,0.841593122002915,0.495819235588619,0.869814800901591,0.395312061428373,-0.295212781264254,-0.814978146643387,0.39478019599221,-0.424216003170627,-0.190640296021312,0.508649949164191,0.839601993058718,0.547235687822547,0.765129940642515,-0.339277579432779,-0.974912731593599,0.199420402329447,-0.0988770393542793,-0.0626186390445858,0.180557907398949,0.9815690236147,0.213597920918978,0.963135683696993,-0.16354076852154,-0.998901478396697,-0.00346004438975842,-0.0467318365754416,-0.0466116308099021,-0.0291241989214193,0.998488426027276,-0.00481584158110081,0.99956981203416,0.0289309270480439,-0.936663703461039,-0.101502808773249,-0.335198875937544,-0.315449071607303,-0.171311140623419,0.933351153811037,-0.152161065456121,0.979974322576589,0.128441960627411,-0.459773734547548,-0.0472908997678249,-0.88677600543731,-0.847209616636638,-0.275939395339743,0.453975016468901,-0.266165321743091,0.960010948322303,0.0868043812414445,0.5656039533661,0.0860101521532933,-0.82017950575664,-0.798282709502671,-0.192501185676362,-0.570690817538069,-0.206970731374542,0.977520100683235,-0.040219014340544,0.976291000946212,0.0629481953306347,-0.207107233519402,-0.204889653080509,-0.0399197571825185,-0.977970675964795,-0.0698291596111024,0.997218099359967,-0.0260758657939129,0.998919525761669,0.0038115755730212,0.0463168753666998,0.0461869308337187,0.0290741998028448,-0.998509618544551,-0.00515252096012601,0.999569988943983,0.0288667409015991,0.997703289063174,-0.0163879619927869,-0.0657235246639177,-0.0647414719966133,0.0545916623166261,-0.996407693772594,0.0199170478795768,0.998374271056316,0.0534053002662864,0.911421738684351,-0.00493535073406465,-0.4114438680631,-0.410462375721168,0.0591502839663191,-0.909957077022888,0.0288279789601852,0.998236888829418,0.0518850788689343,0.609308114231042,0.0175720042275882,-0.792738826221876,-0.792647792871281,0.0403360236871844,-0.608344048749878,0.0212861078757753,0.999031646075605,0.0385054768957396,0.221836160711478,0.0284668303666544,-0.974668331982561,-0.975075785870317,0.0105677154957357,-0.221620249974104,0.00399119157326655,0.99953887516098,0.0301016184240677,0.0461955035819809,0.0290752105451463,-0.998509192536835,-0.99891916438069,-0.00380447990672883,-0.0463252519218584,-0.0051457246128756,0.999569986576425,0.0288680351567914],"entries":[0,1,2,4,5,6,8,9,10],"properties":["userMatrix","userMatrix","userMatrix","userMatrix","userMatrix","userMatrix","userMatrix","userMatrix","userMatrix"],"objids":7,"param":[0,0.15,0.3,0.45,0.6,0.75,0.9,1.05,1.2,1.35,1.5,1.65,1.8,1.95,2.1,2.25,2.4,2.55,2.7,2.85,3,3.15,3.3,3.45,3.6,3.75,3.9,4.05,4.2,4.35,4.5],"interp":true}],"start":0,"stop":4.5,"value":0,"interval":0.05,"rate":0.5,"components":["Reverse","Play","Slower","Faster","Reset","Slider","Label"],"buttonLabels":["Reverse","Play","Slower","Faster","Reset","Slider","Label"],"pause":"Pause","loop":true,"step":0.01,"labels":null,"precision":3,"reinit":null,"sceneId":"interactive-3dplot-multiple-regression","respondTo":null}],"widgetType":"rglPlayer","elementId":"rgl-play44962","html":"<div class=\"cw-container\"><div class=\"cw-subcontainer\"><div class=\"cw-content cw-by-row\"><div class=\"cw-row cw-by-row\" style=\"flex:1;-webkit-flex:1\"><div class=\"cw-col\" style=\"flex:1;-webkit-flex:1\">\n              <div id=\"rgl-play44962\" class=\"rglPlayer cw-widget html-widget-static-bound\" style=\"width:100%;height:100%\"><\/div>\n              <\/div><\/div><\/div><\/div><\/div>","hasCrosstalkInputs":false}],"widgetType":["rglWebGL","combineWidgets"],"elementId":["interactive-3dplot-multiple-regression","widget61234280"],"html":"<div class=\"cw-container\"><div class=\"cw-subcontainer\"><div class=\"cw-content cw-by-row\"><div class=\"cw-row cw-by-row\" style=\"flex:480;-webkit-flex:480\"><div class=\"cw-col\" style=\"flex:1;-webkit-flex:1\">\n              <div id=\"interactive-3dplot-multiple-regression\" class=\"rglWebGL cw-widget html-widget-static-bound\" style=\"width:100%;height:100%\"><\/div>\n              <\/div><\/div><div class=\"cw-row cw-by-row\" style=\"flex:40;-webkit-flex:40\"><div class=\"cw-col\" style=\"flex:1;-webkit-flex:1\">\n              <div id=\"widget61234280\" class=\"combineWidgets cw-widget html-widget-static-bound\" style=\"width:100%;height:100%\"><\/div>\n              <\/div><\/div><\/div><\/div><\/div>","hasCrosstalkInputs":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 5.4: Three-dimensional representation of a regression model predicting Trump votes from hate groups and education level. By clicking on the image and moving your mouse, you should be able to rotate the image to explore it further. You can also click on the ‘Play’ button for an animation of such a rotation.
</p>
</div>
<p>Keeping all predictors constant apart from one, the model becomes a simple linear regression model for the non-constant variable. If you pick a value of “% bachelors degree or higher,” you can think of this as slicing the regression plane at that point. The slice of the plane is a single straight line, which is then a simple regression model. For example, if we focus on states with an education level of 15.4% with a bachelors degree or higher, then the multiple regression model can be written as
<span class="math display">\[\begin{aligned}
\texttt{trump_votes}_i &amp;= \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times 15.4 + \epsilon_i  \\
&amp;= (\beta_0 + \beta_2 \times 15.4) + \beta_1 \times \texttt{hate_groups}_i + \epsilon_i \\
&amp;= \beta_0&#39; + \beta_1 \times \texttt{hate_groups}_i + \epsilon_i
\end{aligned}\]</span>
which is a simple regression model with a new intercept <span class="math inline">\(\beta_0&#39; = \beta_0 + \beta_2 \times 15.4\)</span> composed as the sum of the original intercept and 15.4 times the slope of <span class="math inline">\(\texttt{education\_level}\)</span>, and the same slope <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(\texttt{hate_groups}\)</span>. You can see these simple regression lines, for three different values of education level, in Figure <a href="ch-multiple-regression.html#fig:trump-multiple-regression-scatter-with-multiple-regression-lines">5.5</a>. Notice that the regression lines for different education levels are parallel, because they have the same slope. However, the regression lines are at different heights, because they have a different intercept.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:trump-multiple-regression-scatter-with-multiple-regression-lines"></span>
<img src="04b-GLM-multiple-regression_files/figure-html/trump-multiple-regression-scatter-with-multiple-regression-lines-1.svg" alt="Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens, with regression lines for three different values of education level. The colour of each data point and regression line reflects the value of education level." width="60%" />
<p class="caption">
Figure 5.5: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens, with regression lines for three different values of education level. The colour of each data point and regression line reflects the value of education level.
</p>
</div>
</div>
<div id="sec:04b-estimation" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Estimation</h2>
<p>The estimate of the intercept is a direct generalization of that for a simple regression model:
<span class="math display" id="eq:multiple-regression-intercept-estimate">\[\begin{equation} 
\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \times \overline{X}_1 -  \hat{\beta}_2 \times \overline{X}_2 - \ldots -  \hat{\beta}_m \times \overline{X}_m
\tag{5.4}
\end{equation}\]</span>
Again, it is an adjustment of the mean of the dependent variable, subtracting means multiplied by slopes of each predictor variable.</p>
<p>The estimates of the slopes are straightforward to compute using matrix algebra.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> However, we won’t cover matrix algebra here, so we will leave the estimation of the slopes to statistical software.</p>
<p>The (unbiased) estimate of the error variance is also a direct generalization of the estimate for a simple regression model:
<span class="math display" id="eq:multiple-regression-sd-error-variance-estimate">\[\begin{equation}
\hat{\sigma}^2_\epsilon = \frac{\sum_{i=1}^n (Y_i - \hat{\mu}_{Y|X_1, \ldots, X_m})^2}{n-\text{npar}(M)}
\tag{5.5}
\end{equation}\]</span>
Here, <span class="math inline">\(\text{npar}(M)\)</span> is the number of parameters in the model, excluding the error variance (i.e., it is the number of <span class="math inline">\(\beta_j\)</span> parameter, where <span class="math inline">\(j=0,\ldots,m\)</span>. So <span class="math inline">\(\text{npar}(M) = m + 1\)</span>.</p>
<p>Estimating the model in which we predict Trump votes from hate groups and education level gives the following estimates:
<span class="math display">\[\texttt{trump_votes}_i = 81.99 + 1.314 \times \texttt{hate_groups}_i  - 1.219 \times \texttt{education}_i  + \hat{\epsilon}_i \quad \quad \hat{\epsilon}_i \sim \mathbf{Normal}(0, 6.643)\]</span>
A main thing to note here is that the estimated slope of <span class="math inline">\(\texttt{hate_groups}\)</span> is exactly the same as in Equation <a href="ch-multiple-regression.html#eq:slope-hate-groups-from-residuals">(5.1)</a> when first computing the residual Trump votes and hate groups! This is because the slopes in a multiple regression model are <strong>unique effects</strong> of a predictor on the dependent variable, after removing the effect of all other predictors from both that predictor and the dependent variable. So, equivalently, the slope of <span class="math inline">\(\texttt{education}\)</span> in the model above is identical to the slope we would obtain by first computing the residual Trump votes from
<span class="math display">\[\texttt{trump_votes}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \texttt{hate_groups}_i + \hat{\epsilon}_{\text{vote},i}\]</span></p>
<p>and the residual education from
<span class="math display">\[\texttt{education}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \texttt{hate_groups}_i + \hat{\epsilon}_{\text{education},i}\]</span>
and then determining the slope of the residual <span class="math inline">\(\texttt{education}\)</span> in the model
<span class="math display">\[\hat{\epsilon}_{\text{vote},i} + \beta_0 + \beta_1 \times \hat{\epsilon}_{\text{education},i} + \epsilon_i\]</span></p>
</div>
<div id="sec:04b-inference" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Inference</h2>
<p>Testing hypotheses regarding the parameters of multiple regression models is analogous to testing the parameters of simple regression models. So one approach is to look at the sampling distribution of the estimates under the null hypothesis and compute the appropriate <span class="math inline">\(t\)</span> statistic. In a multiple regression model, the <span class="math inline">\(t\)</span>-statistic follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-(m+1)\)</span> degrees of freedom, where as before <span class="math inline">\(m\)</span> stands for the number of predictors in the model. As we estimate a slope for each predictor, as well as an additional intercept parameter, the total number of estimated parameters is <span class="math inline">\(\text{npar} = m + 1\)</span>. Hence, the degrees of freedom equals <span class="math inline">\(n-\text{npar}\)</span>.</p>
<p>Testing, for each parameter, the null hypothesis <span class="math inline">\(H_0: \beta_j = 0\)</span>, where <span class="math inline">\(j = 0,\ldots,2\)</span>, we obtain the results given in <a href="ch-multiple-regression.html#tab:t-test-results-multiple-regression-table">5.1</a>.</p>
<table>
<caption><span id="tab:t-test-results-multiple-regression-table">Table 5.1: </span>Null-hypothesis significance tests using the t statistic.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SE}(\hat{\beta})\)</span></th>
<th align="right"><span class="math inline">\(t\)</span></th>
<th align="right"><span class="math inline">\(P(\geq \lvert t \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">81.99</td>
<td align="right">6.155</td>
<td align="right">13.32</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">Hate groups per million</td>
<td align="right">1.31</td>
<td align="right">0.510</td>
<td align="right">2.58</td>
<td align="right">0.013</td>
</tr>
<tr class="odd">
<td align="left">% bachelors degree or higher</td>
<td align="right">-1.22</td>
<td align="right">0.184</td>
<td align="right">-6.62</td>
<td align="right">0.000</td>
</tr>
</tbody>
</table>
<p>Alternatively, we can compare models in which we either fix the parameter to the value assumed in the null hypothesis or not. As discussed in the previous chapter, this is a more general approach. When comparing two multiple regression models, we use the <span class="math inline">\(F\)</span>-statistic defined in Equation <a href="ch-simple-regression.html#eq:glm-f-definition">(4.8)</a>:
<span class="math display">\[F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar}(G)}}\]</span>
For instance, to test the null hypothesis <span class="math inline">\(\beta_1 = 0\)</span> in the model defined in Equation <a href="ch-multiple-regression.html#eq:multiple-regression-votes-by-hate-education">(5.3)</a>, we compare this general model
<span class="math display">\[\text{MODEL G: } \quad \texttt{trump_votes}_i = \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i\]</span><br />
to a restricted model where we fix <span class="math inline">\(\beta_1 = 0\)</span>:
<span class="math display">\[\begin{aligned}
\text{MODEL R: } \quad \texttt{trump_votes}_i &amp;= \beta_0 + 0 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \\
&amp;= \beta_0 + \beta_1 \times \texttt{education}_i + \epsilon_i
\end{aligned}\]</span><br />
In other words, we would compare the multiple regression model to a simple regression model where we don’t include <span class="math inline">\(\texttt{hate_groups}\)</span>, as the null hypothesis assumes that this predictor has no <em>unique</em> effect on the dependent variable. If the Sum of Squared Error of MODEL R is not substantially higher than that of MODEL G, then there would be no good evidence that <span class="math inline">\(\beta_1 \neq 0\)</span>. Remember that the Sum of Squared Error of MODEL R can never be lower than that of MODEL G, i.e. <span class="math inline">\(\text{SSE}(R) \geq \text{SSE}(G)\)</span>. This is because <strong>estimating the parameters of MODEL G by maximum likelihood is equivalent to minimizing the Sum of Squared Error</strong>. If <span class="math inline">\(\hat{\beta}_1 = 0\)</span> in MODEL G, that would mean that the SSE of both models is exactly the same. If <span class="math inline">\(\hat{\beta}_1 \neq 0\)</span> in MODEL G, then <span class="math inline">\(\text{SSE}(R) &gt; \text{SSE}(G)\)</span>, because otherwise the estimated MODEL G would not have maximised the likelihood (or equivalently minimised the SSE).</p>
<p>We have already shown results for MODEL G. The Sum of Squared Error for that model is <span class="math inline">\(\text{SSE}(G) = 2074.202\)</span>. Estimating MODEL R gives
<span class="math display">\[\texttt{trump_votes}_i = 90.07 - 1.357 \times \texttt{education}_i  + \hat{\epsilon}_i \quad \quad \hat{\epsilon}_i \sim \mathbf{Normal}(0, 7.022)\]</span>
with <span class="math inline">\(\text{SSE}(R) = 2366.875\)</span>. The <span class="math inline">\(F\)</span> statistic then becomes:
<span class="math display">\[\begin{align}
F &amp;= \frac{\frac{2366.875 -  2074.202}{3 - 2}}{\frac{2074.202}{50 - 3}} \\
&amp;= \frac{292.673}{44.132} \\
&amp;= 6.632
\end{align}\]</span>
To determine whether this value is significant, we need to compare it to an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(\text{df}_1 = 3-2 = 1\)</span> and <span class="math inline">\(\text{df}_2 = 50 - 3 = 47\)</span> degrees of freedom. For this distribution and a significance level of <span class="math inline">\(\alpha = .05\)</span>, the critical value is 4.047, which is lower than the computed <span class="math inline">\(F\)</span> value, and hence the result is significant and we reject the null hypothesis.</p>
<p>Similarly, to test the null-hypothesis <span class="math inline">\(\beta_2 = 0\)</span> in MODEL G as defined above, we compare this model to a <em>new</em> restricted MODEL R where we fix <span class="math inline">\(\beta_2 = 0\)</span>:
<span class="math display">\[
\begin{aligned}
\text{MODEL R: } \quad \texttt{trump_votes}_i &amp;= \beta_0 + \beta_1 \times \texttt{hate_groups}_i + 0 \times \texttt{education}_i + \epsilon_i \\
&amp;= \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \epsilon_i
\end{aligned}
\]</span><br />
This is the model we estimated in the previous chapter, where we determined the estimates as
<span class="math display">\[\texttt{trump_votes}_i = 42.9 + 2.3 \times \texttt{hate_groups}_i  + \hat{\epsilon}_i \quad \quad \hat{\epsilon}_i \sim \mathbf{Normal}(0, 9.142)\]</span>
with an associated <span class="math inline">\(\text{SSE}(R) = 4011.398\)</span>. The <span class="math inline">\(F\)</span> statistic is then computed as
<span class="math display">\[\begin{aligned}
F &amp;= \frac{\frac{4011.398 -  2074.202}{3 - 2}}{\frac{2074.202}{50 - 3}} \\
&amp;= \frac{1937.197}{44.132} \\
&amp;= 43.896
\end{aligned}\]</span>
The critical value is the same as before, and hence we also reject this null hypothesis.</p>
<p>To test whether the intercept <span class="math inline">\(\beta_0 = 0\)</span>, we would compare MODEL G to yet another MODEL R:
<span class="math display">\[
\begin{aligned}
\text{MODEL R: } \quad \texttt{trump_votes}_i &amp;= 0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \\
&amp;=\beta_1 \times \texttt{hate_groups}_i + \beta_1 \times \texttt{education}_i + \epsilon_i
\end{aligned}
\]</span><br />
which is estimated as
<span class="math display">\[\texttt{trump_votes}_i = 4.776 \times \texttt{hate_groups}_i  + 1.13 \times \texttt{education}_i  + \hat{\epsilon}_i \quad \quad \hat{\epsilon}_i \sim \mathbf{Normal}(0, 14.36)\]</span>
with <span class="math inline">\(\text{SSE}(R) = 24.084\)</span>. The <span class="math inline">\(F\)</span> statistic is computed in the same way as before:
<span class="math display">\[\begin{align}
F &amp;= \frac{\frac{9904.891 -  2074.202}{3 - 2}}{\frac{2074.202}{50 - 3}} \\
&amp;= \frac{7830.689}{44.132} \\
&amp;= 177.438
\end{align}\]</span></p>
<p>The results of all three tests are collected in Table <a href="ch-multiple-regression.html#tab:trump-multiple-regression-ANOVA-table">5.2</a>.</p>
<table>
<caption><span id="tab:trump-multiple-regression-ANOVA-table">Table 5.2: </span>Null-hypothesis significance tests by model comparisons and the F statistic.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SS}\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(F\)</span></th>
<th align="right"><span class="math inline">\(P(\geq F)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">81.99</td>
<td align="right">7831</td>
<td align="right">1</td>
<td align="right">177.44</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">Hate groups per million</td>
<td align="right">1.31</td>
<td align="right">293</td>
<td align="right">1</td>
<td align="right">6.63</td>
<td align="right">0.013</td>
</tr>
<tr class="odd">
<td align="left">% bachelors degree or higher</td>
<td align="right">-1.22</td>
<td align="right">1937</td>
<td align="right">1</td>
<td align="right">43.90</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="right"></td>
<td align="right">2074</td>
<td align="right">47</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>When performing such model comparisons, it is very important to remember that what we call MODEL G and MODEL R will depend on the null hypothesis tested. Generally, MODEL G will remain the same within the context of an analysis, but MODEL R will vary from test to test.</p>
<p>There is one more comparison that we could perform, namely to compare MODEL G to a MODEL R without any predictors. This is a so-called <strong>omnibus test</strong> of the null hypothesis about two parameters simultaneously, namely <span class="math inline">\(H_0: \beta_1 = 0 \text { and } \beta_2 = 0\)</span>. This results in another version of the restricted MODEL R:
<span class="math display">\[\begin{aligned}
\text{MODEL R: } \quad \texttt{trump_votes}_i &amp;= \beta_0 +  0 \times \texttt{hate_groups}_i + 0 \times \texttt{education}_i + \epsilon_i \\
&amp;= \beta_0 + \epsilon_i
\end{aligned}\]</span><br />
This is like the alternative model of the one-sample t-test we considered earlier. Here, the model is estimated as
<span class="math display">\[\texttt{trump_votes}_i = 49.86 + \hat{\epsilon}_i \quad \quad \hat{\epsilon}_i \sim \mathbf{Normal}(0, 10.09)\]</span>
with an associated <span class="math inline">\(\text{SSE}(R) = 4992.178\)</span>. The <span class="math inline">\(F\)</span> statistic is then computed as
<span class="math display">\[\begin{align}
F &amp;= \frac{\frac{4992.178 -  2074.202}{3 - 1}}{\frac{2074.202}{50 - 3}} \\
&amp;= \frac{1458.988}{44.132} \\
&amp;= 33.06
\end{align}\]</span>
The critical value is different this time, because the degrees of freedom are now <span class="math inline">\(\text{df}_1 = 3 - 1 = 2\)</span> and <span class="math inline">\(\text{df} = 50 - 3 = 47\)</span>, and equals 3.195. Clearly, the computed <span class="math inline">\(F\)</span> value is well above the critical value, hence we reject the null hypothesis (i.e. reject MODEL R in favour of MODEL G).</p>
</div>
<div id="partitioning-and-explaining-variance" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Partitioning and explaining variance</h2>
<p>Let’s have another look at the formula for the <span class="math inline">\(F\)</span> statistic:</p>
<p><span class="math display">\[F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar(G)}}}\]</span>
The numerator (top part) consists of a difference between two SSE terms, divided by the difference in the number of estimated parameters. Because the comparison is between nested models, and MODEL G contains more estimated parameters than MODEL R, we can view this part as the average reduction of the SSE (per parameter) due to the additional parameters of MODEL G. The numerator of the <span class="math inline">\(F\)</span>-statistic is also referred to as the Mean Square Reduced (MSR):
<span class="math display">\[\text{MSR} = \frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}\]</span>
The denominator (bottom part) in the formula of the <span class="math inline">\(F\)</span>-statistic consists of the SSE of MODEL G divided by <span class="math inline">\(n-\text{npar}(G)\)</span>. This is an unbiased estimate of the error variance of MODEL G, and also referred to as the Mean Squared Error (MSE):
<span class="math display">\[\text{MSE} = \frac{\text{SSE}(G)}{n-\text{npar(G)}}\]</span>
So an alternative definition of the <span class="math inline">\(F\)</span>-statistic is
<span class="math display">\[F = \frac{\text{MSR}}{\text{MSE}}\]</span></p>
<p>The SSE of <em>any</em> linear model <span class="math inline">\(M\)</span> is related to an error variance, as the SSE is the sum of squared deviations between the observations and the model predictions (<span class="math inline">\(\hat{Y} = \mu_{Y|X_1,\ldots,X_m}\)</span>):
<span class="math display">\[\text{SSE}(M) = \sum_{i=1}^n (Y_i - \hat{Y}_{M,i})^2\]</span>
Dividing the SSE by <span class="math inline">\(n - \text{npar}(M)\)</span> would provide us with an unbiased estimate of the error variance of that model. Although this is only done in the denominator of the <span class="math inline">\(F\)</span> statistic, which considers MODEL G, it should be clear there is a close relation between any SSE term and an error variance.</p>
<p>A useful perspective on unique effects in multiple regression models and GLMs in general is to consider them as <strong>partitioning the variance of the dependent variable</strong> in parts that are uniquely attributable to the predictors. If you consider the SSE of a very simple MODEL R
<span class="math display">\[Y_i = \beta_0 + \epsilon_i\]</span>
with estimate <span class="math inline">\(\hat{\beta}_0 = \overline{Y}\)</span>, then for this model
<span class="math display">\[\text{SSE}(R) = \sum_{i=1}^n (Y_i - \hat{\beta}_0)^2 = \sum_{i=1}^n (Y_i - \overline{Y})^2 = n S_Y^2\]</span>
So the SSE of a regression model without predictors is equal to <span class="math inline">\(n\)</span> times the sample variance of the dependent variable. When we add predictors to the model, we know the SSE can only decrease (or remain the same, but that is rather unlikely). The proportion of the SSE that is reduced by adding a predictor to the model can be seen as the proportion of the variance that is “explained” by the predictor. This <strong>proportional reduction in error</strong> is a useful measure of the strength of the relation between a predictor and the dependent variable. As usual, this strength will have a true value for the Data Generating Process, and a value we can compute for a given limited dataset. We’ll denote the true value as <span class="math inline">\(\eta_p^2\)</span>. The sample estimate is:
<span class="math display" id="eq:definition-partial-eta-squared-glm">\[\begin{equation}
\hat{\eta}_p^2 = \frac{\text{SSE}(R) - \text{SSE}(G)}{\text{SSE}(R)}
\tag{5.6}
\end{equation}\]</span>
Because <span class="math inline">\(0 \leq \text{SSE}(G) \leq \text{SSE}(R)\)</span>, the value of <span class="math inline">\(\hat{\eta}_p^2\)</span> is between 0 and 1 (as a proportion should be). Figure <a href="ch-multiple-regression.html#fig:sse-partition">5.6</a> depicts such “proportions of variance explained” for a situation with two predictors (<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>) of a dependent variable <span class="math inline">\(Y\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sse-partition"></span>
<img src="04b-GLM-multiple-regression_files/figure-html/sse-partition-1.svg" alt="Partitioning the variance in a multiple regression model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables. " width="50%" />
<p class="caption">
Figure 5.6: Partitioning the variance in a multiple regression model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables.
</p>
</div>
<p>In this Figure, the variance of <span class="math inline">\(Y\)</span> is represented by the circle labelled <span class="math inline">\(Y\)</span>, and the variance of the predictors is represented by the circles labelled <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> respectively. Overlap between the circles represents shared variance (e.g. covariance) between the variables. First, let’s consider a simple regression model where <span class="math inline">\(X_1\)</span> is the sole predictor of <span class="math inline">\(Y\)</span>. The proportion of variance of <span class="math inline">\(Y\)</span> that this model accounts for is the total shared area between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y\)</span>, which is the sum of the regions labelled as <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span>. Similarly, if we’d consider a simple regression model where <span class="math inline">\(X_2\)</span> is the sole predictor of <span class="math inline">\(Y\)</span>, the proportion of variance of <span class="math inline">\(Y\)</span> that this model accounts for is the total shared area between <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y\)</span>, which is the sum of the regions labelled as <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span>. If we consider a multiple regression model with both <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> as predictors of <span class="math inline">\(Y\)</span>, the proportion of variance of <span class="math inline">\(Y\)</span> that this model accounts for is the total shared area between <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y\)</span>, which is the sum of the regions labelled as <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>, and <span class="math inline">\(D\)</span>. The area labelled as <span class="math inline">\(A\)</span> is variance of <span class="math inline">\(Y\)</span> that cannot be accounted for by <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span>, and is the SSE (the random, unexplainable part) of a multiple regression model which includes both <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> as predictors. When MODEL R is an intercept-only model (i.e. a model without any predictors) and MODEL G a model with predictors, the estimate of the variance accounted for is often denoted by <span class="math inline">\(R^2\)</span>. In this case, <span class="math inline">\(R^2\)</span> reflects the proportion of the variance of the dependent variable <span class="math inline">\(Y\)</span> accounted for, or “explained,” by the model as a whole. <!-- The $\eta^2$ is generally reserved to denote the proportion of variance explained by a single predictor. --></p>
<p>Note that the area labelled as <span class="math inline">\(D\)</span> is “explained” by both <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> when each is used as a sole predictor of <span class="math inline">\(Y\)</span>. In a multiple regression model however, the effects of predictors are <em>unique</em> effects, meaning that we consider only that part of the variance of <span class="math inline">\(Y\)</span> which is solely explained by each predictor. For <span class="math inline">\(X_1\)</span>, this unique proportion of the variance is the area labelled as <span class="math inline">\(B\)</span>, and for <span class="math inline">\(X_2\)</span> it is the area labelled as <span class="math inline">\(C\)</span>. The area labelled as <span class="math inline">\(D\)</span> is not uniquely attributable to either <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span>. Although it is not purely random variability of <span class="math inline">\(Y\)</span> – in principle, it could be explained by <em>either</em> <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span> – because it cannot be uniquely attributed to either <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span>, it will not contribute to a hypothesis test of the effect of <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span>. We will discuss this more when we consider the practical problem of <em>multicollinearity</em> later on.</p>
</div>
<div id="sec:04b-effect-size" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Effect size and the importance of predictors</h2>
<p>In a multiple regression model, you may be interested in which predictors are more important, in the sense that they have a stronger relation with the dependent variable and therefore help you to provide better predictions of the dependent variable than other variables. It is important to realise that this question can <em>not</em> be answered by simply comparing the (absolute) value of the slopes of the predictors. Slopes reflect the change in the conditional mean of the dependent variable for every one-unit increase in the predictor. But what a one-unit increase is depends on the scale of the predictor. For example, I might predict people’s weight by their height and daily calorie intake. Whether I measure their height in meters or centimetres should be arbitrary. If I know someone’s height in centimetres, I also know their height in meters, and the choice of scale (whether centimetres or meters) does not change the relation between height and weight in any meaningful way. But the choice of scale clearly changes the (estimated) slope of the height predictor. If people’s average weight increases by 1 gram for every increase in their height by 1 centimetre, the corresponding increase in weight for an increase in height by 1 meter is 100 grams. In other words, the slope of height in centimetres is 1, but the slope for height in meters is 100. But that obviously doesn’t mean that height in meters is a better or more important predictor of weight than height in centimetres!</p>
<p>Even when predictors are measured on the same scale, their relative importance can <em>not</em> be determined by comparing the (absolute) value of their slopes. For example, we could attempt to predict the impact of a negative life event (such as divorce or a bereavement) by the size of someone’s support network, which could consist of the number of people in their immediate family, and the number of friends. Both these predictors (family and friends) are measured on the same scale (number of people), but they may have rather different ranges (e.g., between 0 and 5 for family, and between 0 and 30 for friends). If for example the slope of friends is smaller than the slope of family, that means that every additional family member may soften the negative impact more than every additional friend. However, because the range of friends is larger, the overall effect of friends on the impact of a negative life events may well be larger than that of family.</p>
<p>To resolve such issues of scale and range, we can attempt to standardize all the predictors. Generally, this is done by the <span class="math inline">\(Z\)</span>-transformation (Equation <a href="ch-simple-GLM.html#eq:standardized-Z-definition">(3.2)</a>), i.e. by subtracting the mean from all values and then dividing these by the standard deviation
<span class="math display">\[Z_{X_j} = \frac{X_{j,i} - \overline{X}_j}{S_{X_j}}\]</span>
Conventionally, the same <span class="math inline">\(Z\)</span>-transformation is also applied to the dependent variable. After this standardization of all (predictor and dependent) variables, we obtain the <strong>standardised slopes</strong>:
<span class="math display" id="eq:standardized-slopes">\[\begin{equation}
\hat{\beta}^\text{std}_{X_j} = \hat{\beta}_j \frac{S_{X_j}}{S_Y}
\tag{5.7}
\end{equation}\]</span>
These standardized slopes<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a> reflect the increase or decrease of the dependent variable (in units of standard deviation <span class="math inline">\(S_Y\)</span>) for every increase in the predictor by one standard deviation <span class="math inline">\(S_{X_j}\)</span>. Although this makes the slopes more comparable, some issues with range may still remain. For one thing, as we are basing the standardisation on sample estimates of the standard deviations, the result will depend on the particulars of the dataset. If, by chance, a dataset includes only people with between 2 and 6 friends, the standardised slope of friends may be much higher than if the dataset included people with between 0 and 20 friends, even if the relation between friends and life-event impact is the same in both datasets.</p>
<p>Determination of the relative importance of predictors is tricky. There is no measure which resolves all issues. But if you take note of potential issues with artificially restricted ranges of predictors, measures of relative importance are not completely useless. They can inform you of the relative size of the effect a predictor has on the dependent variable. When considering whether there is evidence that a predictor has any effect on the dependent variable (i.e., when testing the null-hypothesis <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\beta_j = 0\)</span>), it is often a good idea to also consider a measure of the <strong>effect size</strong>. For large datasets, the power of tests to detect <em>any effect</em> of a predictor is generally large. That means that even if a predictor has a very small effect on the dependent variable (i.e. it only changes the conditional mean by a very small fraction), you would still be able to reliably detect such a small effect and reject the null hypothesis. But that would not imply that a predictor is “practically significant,” in the sense that e.g. persuading someone to make one additional friend might help soften the blow of a negative life event in a subjectively perceivable manner.</p>
<div id="r2-changes-and-the-coefficient-of-semi-partial-determination" class="section level3" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</h3>
<p>The standardised slopes can be viewed as measures of effect size. But it is more common to consider effect size in terms of “proportion of variance explained.” Personally, I like these measures because they have a clear and bounded scale between 0 (no effect) and 1 (a predictor is perfect in accounting for the dependent variable). Let’s consider how we can use this measure to reflect the effect size for the different predictors in a multiple regression model. To frame our discussion, we will consider a situation where the dependent variable <span class="math inline">\(Y\)</span> can be modelled by two predictors, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. In this situation, we can define four different models:</p>
<p><span class="math display">\[\begin{aligned}
\text{MODEL R0}: &amp;&amp; Y_i &amp;= \beta_0 + \epsilon_i \\
\text{MODEL R1}: &amp;&amp; Y_i &amp;= \beta_0 + \beta_1 X_{1i} + \epsilon_i \\
\text{MODEL R2}: &amp;&amp; Y_i &amp;= \beta_0 + \beta_1 X_{2i} + \epsilon_i \\
\text{MODEL G}: &amp;&amp; Y_i &amp;= \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \epsilon_i
\end{aligned}\]</span></p>
<p>Referring back to Figure <a href="ch-multiple-regression.html#fig:sse-partition">5.6</a>, the Sums of Squared Error of each model can be defined as</p>
<p><span class="math display">\[\begin{aligned}
\text{SSE}(R0) &amp;= A + B + C + D = S^2_Y \\
\text{SSE}(R1) &amp;= A + C \\
\text{SSE}(R2) &amp;= A + B \\
\text{SSE}(G) &amp;= A 
\end{aligned}\]</span></p>
<p>Recall that the <span class="math inline">\(R^2\)</span> measure reflects the proportion of variance “explained” by all the predictors in a model, and is the <span class="math inline">\(\hat{\eta}^2\)</span> of a model compared to an intercept-only model (i.e. MODEL R0). For each of the models apart from R0, we can compute the <span class="math inline">\(R^2\)</span> measure as</p>
<p><span class="math display">\[\begin{aligned}
R^2_{R1} &amp;= \frac{\text{SSE}(R0) - \text{SSE}(R1)}{\text{SSE(R0)}} =  \frac{B + D}{A + B + C + D} \\
R^2_{R2} &amp;= \frac{\text{SSE}(R0) - \text{SSE}(R2)}{\text{SSE(R0)}} =  \frac{C + D}{A + B + C + D} \\
R^2_{G} &amp;= \frac{\text{SSE}(R0) - \text{SSE}(G)}{\text{SSE(R0)}} =  \frac{B + C + D}{A + B + C + D}
\end{aligned}\]</span></p>
<p>We can define a measure of effect size by considering the following question: what additional proportion of the variance of <span class="math inline">\(Y\)</span> can be explained by including a predictor <span class="math inline">\(X_j\)</span> in a model. For example, what additional proportion of the variance can be explained by adding the predictor <span class="math inline">\(X_2\)</span> to MODEL R1? This is easily computed as the difference in <span class="math inline">\(R^2\)</span> values: <span class="math display">\[R^2_{G} - R^2_{R1} = \frac{C}{A+B+C+D}\]</span> Similarly, we can define a measure of the effect of <span class="math inline">\(X_1\)</span> by considering what additional proportion of the variance is explained by adding the predictor <span class="math inline">\(X_1\)</span> to MODEL R2, providing <span class="math display">\[R^2_{G} - R^2_{R2} = \frac{B}{A+B+C+D}\]</span> These <span class="math inline">\(R^2\)</span>-change values are also called the <strong>coefficient of semi-partial determination</strong>. They reflect the unique proportion of variance of <span class="math inline">\(Y\)</span> that can be attributed to a predictor. Defining the Sum of Squared Reduced (SSR) – the value of this for each predictor is usually given in the “SS” column of a multiple regression ANOVA table – as follows:
<span class="math display">\[\begin{aligned}
\text{SSR}(X_1) &amp;= \text{SSE}(R2) - \text{SSE}(G) = A + B - A = B \\
\text{SSR}(X_2) &amp;= \text{SSE}(R1) - \text{SSE}(G) = A + C - A = C
\end{aligned}\]</span>
the coefficient of semi-partial determination can be computed as
<span class="math display" id="eq:semi-partial-determination">\[\begin{equation}
\text{Semi-partial determination of }X = \hat{\eta}^2 = \frac{\text{SSR}(X)}{\text{SSE}(R0)}
\tag{5.8}
\end{equation}\]</span>
The main thing to remember is that the coefficient of semi-partial determination reflects the proportion of the variance of <span class="math inline">\(Y\)</span> that can be <em>uniquely</em> attributed to a predictor <span class="math inline">\(X\)</span>.</p>
<p>A different measure of effect size is called the <strong>coefficient of partial determination</strong>. This reflects not the proportion of the total variance of <span class="math inline">\(Y\)</span> that can be uniquely attributed to a predictor <span class="math inline">\(X\)</span>, but rather the proportion of the variance of <span class="math inline">\(Y\)</span> that can not be explained by the other predictors in a model, but which can (uniquely) be explained by predictor <span class="math inline">\(X\)</span>. For instance, with reference to Figure <a href="ch-multiple-regression.html#fig:sse-partition">5.6</a>, MODEL R1 leaves <span class="math inline">\(A + C\)</span> “unexplained.” Of this unexplained variance, predictor <span class="math inline">\(X_2\)</span> can explain part <span class="math inline">\(C\)</span> when added to model R1. So the proportion of previously unexplained variance now explained is <span class="math inline">\(\frac{C}{A+C}\)</span>. This proportion can also be computed from <span class="math inline">\(R^2\)</span> values. As the proportion of unexplained variance for MODEL R1 is simply <span class="math inline">\(1 - R^2_{R1}\)</span>, the proportion of this variance accounted for by <span class="math inline">\(X_2\)</span> is <span class="math inline">\(\frac{R^2_{G} - R^2_{R1}}{1 - R^2_{R1}}\)</span>. Because the numerator is equal to the coefficient of semi-partial determination, and the denominator is between 0 and 1, the coefficient of partial determination is equal to or larger than the coefficient of semi-partial determination. It can also be computed directly from SSE and SSR terms as
<span class="math display" id="eq:partial-determination">\[\begin{equation}
\text{Partial determination of }X = \hat{\eta}_p^2 = \frac{\text{SSR}(X)}{\text{SSE}(G) + \text{SSR}(X)}
\tag{5.9}
\end{equation}\]</span>
The coefficient of partial determination is (in the context of ANOVA models, which we will discuss later) also called the <em>partial eta-squared</em>, or <span class="math inline">\(\eta_p^2\)</span>. We have already come across this measure as the proportional reduction in error (Equation <a href="ch-multiple-regression.html#eq:definition-partial-eta-squared-glm">(5.6)</a>).</p>
<!--


## Controlling for education level and poverty




-->
</div>
</div>
<div id="sec:04b-assumptions" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Assumptions</h2>
<p>The assumptions of the multiple regression model, and with that any of the versions of the General Linear Model we discuss, are all really about the errors, which are assumed to be independently and identically distributed:
<span class="math display">\[\epsilon_i \sim \mathbf{Normal}(0, \sigma)\]</span></p>
<p>In other words, this implies the following assumptions for a multiple regression and/or GLM:</p>
<ol style="list-style-type: decimal">
<li><strong>Normality</strong>: the errors <span class="math inline">\(\epsilon_i\)</span> are Normal-distributed</li>
<li><strong>Unbiasedness</strong>: the mean of <span class="math inline">\(\epsilon_i\)</span> is 0. This means that the conditional means are indeed a linear function of the predictors, and that the model predictions are unbiased.</li>
<li><strong>Homoscedasticity</strong>: the errors <span class="math inline">\(\epsilon_i\)</span> have a constant variance <span class="math inline">\(\sigma^2\)</span> and thus also a constant standard deviation <span class="math inline">\(\sigma\)</span>.</li>
<li><strong>Independence</strong>: any error term <span class="math inline">\(\epsilon_i\)</span> is independent of any other <span class="math inline">\(\epsilon_j\)</span> (for all <span class="math inline">\(i \text{ and } j \neq i\)</span>). Independence here means that <span class="math inline">\(p(\epsilon_j|\epsilon_i) = p(\epsilon_j)\)</span> for all <span class="math inline">\(i \text{ and } j \neq i\)</span></li>
</ol>
<p>It is important to realise that the assumption of Normal-distributed errors does not necessarily translate in assuming that the dependent variable <span class="math inline">\(Y\)</span> is itself Normal-distributed. This will often not be the case. Nor does the model require the predictors to be Normal-distributed. No assumption is made at all about the distribution of the predictors.</p>
<p>In Section <a href="ch-simple-GLM.html#sec:02-assumptions">3.7</a> we discussed some methods to assess the assumption of Normality (i.e. histograms, QQ-plots and statistical tests such as the Shapiro-Wilk test). The assumption of unbiasedness and homoscedasticity is generally assessed visually with a so-called predicted-by-residual plot. This plot (see Figure <a href="ch-multiple-regression.html#fig:trump-predicted-residual">5.7</a>) depicts the residuals as a function of the model predictions (i.e. the estimated conditional means). The unbiasedness assumption implies that for each predicted value <span class="math inline">\(\hat{Y}\)</span> (i.e. each conditional mean), the residual or error terms are scattered around 0. For instance, it should <em>not</em> be the case that for relatively low and high values of the predictions, the errors are generally above 0, while for medium values of the predictions, the errors are generally below 0. Such a pattern would be indicative of biased predictions and a likely <em>non-linear</em> relation between predictors and the dependent variable. The homoscedasticity assumption implies that the spread of the residuals is equal for each predicted value <span class="math inline">\(\hat{Y}\)</span>. Looking at the predicted vs residual plot in Figure <a href="ch-multiple-regression.html#fig:trump-predicted-residual">5.7</a>, there is no clear indication that either assumption is violated. In addition, the QQ-plot indicates that the expected and sample quantiles do not differ radically, at least for the middle quantiles. Note that at the extremes (low and high quantiles), you will generally find more variable results, as quantiles in the tails of a distribution are less reliable.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:trump-predicted-residual"></span>
<img src="04b-GLM-multiple-regression_files/figure-html/trump-predicted-residual-1.svg" alt="Predicted vs residual plot, and a QQ plot, for MODEL G predicting Trump votes from hate groups and education level." width="49%" /><img src="04b-GLM-multiple-regression_files/figure-html/trump-predicted-residual-2.svg" alt="Predicted vs residual plot, and a QQ plot, for MODEL G predicting Trump votes from hate groups and education level." width="49%" />
<p class="caption">
Figure 5.7: Predicted vs residual plot, and a QQ plot, for MODEL G predicting Trump votes from hate groups and education level.
</p>
</div>
For comparison, Figure <a href="ch-multiple-regression.html#fig:predicted-residual-for-violations-of-assumptions">5.8</a> shows two examples of predicted vs residual plots where the assumption of unbiasedness and homoscedasticity do not hold.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:predicted-residual-for-violations-of-assumptions"></span>
<img src="04b-GLM-multiple-regression_files/figure-html/predicted-residual-for-violations-of-assumptions-1.svg" alt="Predicted vs residual plots for two cases where the assumptions of the GLM do not hold. In the plot of the left, the assumption of unbiasedness is violated. There is clearly a nonlinearity in the data, which result in the underprediction (positive residuals) for low and high predicted values, and an overprediction (negative residuals) for medium predicted values. In the plot on the right, the homoscedasticity assumption is violated. The variability of the residuals clearly increases as a function of the predicted values." width="49%" /><img src="04b-GLM-multiple-regression_files/figure-html/predicted-residual-for-violations-of-assumptions-2.svg" alt="Predicted vs residual plots for two cases where the assumptions of the GLM do not hold. In the plot of the left, the assumption of unbiasedness is violated. There is clearly a nonlinearity in the data, which result in the underprediction (positive residuals) for low and high predicted values, and an overprediction (negative residuals) for medium predicted values. In the plot on the right, the homoscedasticity assumption is violated. The variability of the residuals clearly increases as a function of the predicted values." width="49%" />
<p class="caption">
Figure 5.8: Predicted vs residual plots for two cases where the assumptions of the GLM do not hold. In the plot of the left, the assumption of unbiasedness is violated. There is clearly a nonlinearity in the data, which result in the underprediction (positive residuals) for low and high predicted values, and an overprediction (negative residuals) for medium predicted values. In the plot on the right, the homoscedasticity assumption is violated. The variability of the residuals clearly increases as a function of the predicted values.
</p>
</div>
<div id="transforming-variables" class="section level3" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> Transforming variables</h3>
<p>In certain cases where the assumptions of unbiasedness and/or homoscedasticity do not hold, it may be possible to transform the dependent variables and/or predictors and obtain a model with these transformed parameters where the assumptions do hold.</p>

<div class="advancedtext">
<p>For example, the true underlying relation in the left plot of Figure <a href="ch-multiple-regression.html#fig:predicted-residual-for-violations-of-assumptions">5.8</a> was
<span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_{1,i}^2 + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)\]</span>
In this case, a linear model would be obtained by transforming the predictor <span class="math inline">\(X_1\)</span> with a <strong>power-transformation</strong>, raising it to the power of 2 (i.e., using <span class="math inline">\(X_1^2\)</span> rather than <span class="math inline">\(X_1\)</span> as a predictor). The true underlying relation in the right plot of Figure <a href="ch-multiple-regression.html#fig:predicted-residual-for-violations-of-assumptions">5.8</a> was
<span class="math display">\[\begin{aligned} Y_i &amp;= \beta_0 + \beta_1 \times X_{1,i} + \epsilon_{i} \quad \quad \epsilon_i \sim \mathbf{Normal}(0,X_{1,i} \times \sigma_\epsilon) \end{aligned}\]</span>
In this case, the standard deviation of the errors increases with the value of <span class="math inline">\(X_1\)</span>. We could get a constant standard deviation by modelling <span class="math inline">\(\frac{Y_i}{X_i}\)</span> as the dependent variable, instead of <span class="math inline">\(Y_i\)</span>.</p>
<p>You might – rightly! – think: but what if I don’t know the true underlying relation between the predictors and the dependent variable? Well, in that case you might have to try a number of different transformations. As there are an infinite number of possible transformations, you would hopefully be lucky enough to arrive at one that works. Common transformations of the dependent variable, which aim to resolve issues with violations of the homoscedasticity assumption, are provided in Table <a href="ch-multiple-regression.html#tab:common-data-transformations-ch4">5.3</a>.</p>
<table>
<caption><span id="tab:common-data-transformations-ch4">Table 5.3: </span> Some common transformations for the dependent variable in linear models.</caption>
<colgroup>
<col width="41%" />
<col width="16%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Transformation</th>
<th align="left">Name</th>
<th align="left">Usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(Y^2\)</span></td>
<td align="left">square</td>
<td align="left">useful for negatively-skewed data, as it will increase the spread among higher scores compared to lower ones</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\sqrt{Y}\)</span></td>
<td align="left">square-root</td>
<td align="left">useful for positively skewed data, as it will decrease the spread among higher scores compared to lower one</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\log_e{(Y)}\)</span></td>
<td align="left">natural logarithm</td>
<td align="left">useful for positively skewed data, as it will decrease the spread among higher scores compared to lower one. If there are values equal to or smaller than 1, you can use an alternative form <span class="math inline">\(\log_e{(Y + c)}\)</span>, with e.g. <span class="math inline">\(c = \min(1,|\min(Y)|)\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\log_{10}{(Y)}\)</span></td>
<td align="left">base 10 logarithm</td>
<td align="left">useful for highly positively skewed data, as it will decrease the spread among higher scores compared to lower one. If there are values equal to or smaller than 1, you can use a similar adjustment as above.</td>
</tr>
</tbody>
</table>
Of course, you don’t have to try different transformations blindly. Inspecting the relations between the predictors and the dependent variable, and between the residuals and predicted values, can sometimes give reasonable clues to potentially useful transformations. The tricky thing is that non-linear transformations of the dependent or predictor variables can change many aspects of the model. For example, if the model
<span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i\]</span>
provides unbiased predictions, but violates the assumption of homoscedasticity, a transformed model
<span class="math display">\[\log(Y_i) = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i\]</span>
might resolve some of the issues with the homoscedasticity assumption, but lead to biased predictions. In this case, we might want to transform both the dependent variable <em>and</em> the predictor
<span class="math display">\[\log(Y_i) = \beta_0 + \beta_1 \times \log(X_{1.i}) + \epsilon_i\]</span>
to (hopefully) resolve the issue with homoscedasticity and maintain unbiased estimates. And when we have more than one predictor, the situation becomes even more tricky to resolve. Transforming variables to make a model conform to the assumptions of the linear model is, in some sense, perhaps more of an art than a science. It is certainly not easy, and something that should be done with caution.
</div>
<p>A general technique which aims to help you determine the appropriate transformation function is the so-called Box-Cox transformation <span class="citation">(see e.g. <a href="#ref-sakia1992box" role="doc-biblioref">Sakia, 1992</a>)</span>. Describing this procedure in detail is beyond the scope of this book, but knowing of its existence might help you out in the future. And once you become comfortable with linear models, you will become more comfortable with the various ways in which to transform your data as well.</p>
</div>
<div id="polynomial-regression" class="section level3" number="5.7.2">
<h3><span class="header-section-number">5.7.2</span> Polynomial regression</h3>
<p>If the assumption of unbiasedness is violated because the relation between a predictor and the dependent variable is not linear, you may also try a technique called <strong>polynomial regression</strong>. For example, if we go back to analysing the relation between hate groups and votes for Trump considered earlier, you might not expect the relation to be exactly linear. It might be more reasonable to assume that the effect of hate groups on votes for Trump diminishes for very large numbers of hate groups. In other words, you might expect the effect of a one-unit increase in hate groups to be <em>larger</em> for moving from one to two hate groups per million citizens, compared to moving from eight to nine hate groups per million citizens. This means that the relation between hate groups and votes for Trump is nonlinear.</p>
<p>There are various techniques to deal with nonlinear relations. Polynomial regression can allow you to estimate nonlinear relations within the context of a linear model. That seems contradictory at first: how can a linear model provide a nonlinear relation? The key to doing so is to effectively transform the predictor in a variety of ways, and then average the functions relating each transformed predictor to the dependent variable. This “averaging” is done automatically, by estimating regression coefficients for each in a multiple regression model.</p>
<p>The transformations considered in a polynomial regression are all power-transformations of a predictor (i.e. <span class="math inline">\((X_i)^k\)</span>, with <span class="math inline">\(k=1, 2, \ldots\)</span>), up to a certain maximum power, called the degree of the model. For example, a 2nd-degree polynomial model would be</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_{i} + \beta_2 \times X_i^2 + \epsilon_i\]</span>
which is sometimes also called a quadratic model. A 3rd-degree polynomial, also called a cubic model, would be
<span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_{i} + \beta_2 \times X_i^2 + \beta_3 \times X_i^3 + \epsilon_i\]</span>
The power-transformed predictors (e.g., <span class="math inline">\(X^2\)</span> and <span class="math inline">\(X^3\)</span>) are treated as any other predictor. But, because when we know the value of the original predictor <span class="math inline">\(X\)</span>, we also exactly know the value of each transformed predictor, the conditional mean <span class="math inline">\(\mu_{Y|X,X^2,X^3}\)</span> is really just a function of <span class="math inline">\(X\)</span>. Hence, we can unambiguously consider the predictions of the model for each possible value of <span class="math inline">\(X\)</span>.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> These are shown, for a quadratic (second-order) and cubic (third-order) polynomial regression model in Figure <a href="ch-multiple-regression.html#fig:trump-polynomial-regression-plot">5.9</a>. As you can see, both the quadratic and cubic model predict a nonlinear relation between hate groups and Trump votes. The quadratic model indicates that the effect of hate groups diminishes with more hate groups. In the cubic model, the effect even appears to reverse, such that after an initial increase, Trump votes decrease with more hate groups. As such, the quadratic model seems more plausible.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:trump-polynomial-regression-plot"></span>
<img src="04b-GLM-multiple-regression_files/figure-html/trump-polynomial-regression-plot-1.svg" alt="Polynomial regression models for the relation between hate groups and Trump votes." width="672" />
<p class="caption">
Figure 5.9: Polynomial regression models for the relation between hate groups and Trump votes.
</p>
</div>
<p>At this point in time, I would like you to just be aware of the possibility of polynomial regression. We will hopefully return to it at some later stage.</p>
<!-- 

TODO: polynomial regression

One way to arbitrate between these models is to consider the more general  (third-order or cubic model) and determine whether the highest-order effect is significant. 



-->
</div>
</div>
<div id="multicollinearity-redundancy-between-predictors" class="section level2" number="5.8">
<h2><span class="header-section-number">5.8</span> Multicollinearity: Redundancy between predictors</h2>
<p>Multicollinearity is a rather fancy word for interdependence between predictors in a linear model. Interdependence means that the values of a predictor are related to the values of other predictors, such that you can predict one from the others. In Figure <a href="ch-multiple-regression.html#fig:sse-partition">5.6</a>, this is represented by the partial overlap of the circles representing <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. A high covariation between predictors indicates <strong>redundancy</strong> between them, in the sense that variation in one predictor can be accounted for by another. If this shared variation is also shared with the dependent variable (e.g. region <span class="math inline">\(D\)</span> in Figure <a href="ch-multiple-regression.html#fig:sse-partition">5.6</a>, than this can <em>not</em> be attributed uniquely to any of those covarying predictors. While the model as a whole may be able to account for a large proportion in the variation of the dependent variable, the unique variance “explained” by individual predictors could be very small so that the hypothesis tests are not significant for any predictor, even though the “whole model test” (i.e. the comparison between the full model and an intercept-only model) is highly significant. This makes parameter inference tricky, to say the least.</p>
<p>Figure <a href="ch-multiple-regression.html#fig:sse-partition-multicollinearity">5.10</a> illustrates the difference between a model without redundancy between between predictors and with (substantial) redundancy between predictors. In the case of no redundancy, each predictor uniquely accounts for a substantial proportion (regions labelled as <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>) of the variance of the dependent variable. In the case of redundancy between the predictors, while the overlapping region between each predictor and the dependent variable is the same size as before (region <span class="math inline">\(B + D\)</span> for <span class="math inline">\(X_1\)</span>, and region <span class="math inline">\(C + D\)</span> for <span class="math inline">\(X_2\)</span>), because a large part of this (region <span class="math inline">\(D\)</span>) is shared between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, the unique parts (<span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> respectively) are substantially smaller. In the most extreme case, the circles representing <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> would be completely overlapping, meaning that neither accounts for a unique proportion of the variation in <span class="math inline">\(Y\)</span>. <strong>Multicollinearity</strong> is an extension of this concept, referring to dependency between more than two predictors.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sse-partition-multicollinearity"></span>
<img src="04b-GLM-multiple-regression_files/figure-html/sse-partition-multicollinearity-1.svg" alt="Partitioning the variance in a multiple regression model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables. " width="60%" />
<p class="caption">
Figure 5.10: Partitioning the variance in a multiple regression model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables.
</p>
</div>
<p>Another way to see the problem of multicollinearity is to consider the confidence interval of a slope in a multiple regression model. The confidence interval can be computed as
<span class="math display" id="eq:confidence-interval-slope-multiple-regression">\[\begin{equation}
\hat{\beta}_j \pm \sqrt{\frac{F_{1,n - \text{npar}(G);\alpha} \text{MSE}(G)}{n S^2_{X_j} (1-R^2_{X_j})}}
\tag{5.10}
\end{equation}\]</span>
In this rather complicated looking equation, <span class="math inline">\(F_{1,n - \text{npar}(G);\alpha}\)</span> is the critical value for the <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(\text{df}_1 = 1\)</span> and <span class="math inline">\(\text{df}_2 = n - \text{npar}(G)\)</span> degrees of freedom and a significance level <span class="math inline">\(\alpha\)</span> (e.g. <span class="math inline">\(\alpha = .05\)</span> for a 95% confidence interval). <span class="math inline">\(\text{MSE}(G) = \frac{\text{SSE}(G)}{n - \text{npar}(G)}\)</span> is the Mean Squared Error of MODEL G, <span class="math inline">\(S^2_{X_j}\)</span> is the sample variance of <span class="math inline">\(X\)</span> (<em>not</em> the unbiased estimator<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>), and <span class="math inline">\(R^2_{X_j}\)</span> is the <span class="math inline">\(R^2\)</span> of a model predicting <span class="math inline">\(X_j\)</span> from all other predictors
<span class="math display">\[X_{j,i} = \beta_0 + \beta_1 X_{1,i} + \ldots + \beta_{j-1} X_{j-1,i} + \beta_{j+1} X_{j+1,i} + \ldots + \beta_m X_{m,i} + \epsilon_i\]</span>
The value of <span class="math inline">\(R^2_{X_j}\)</span> is a measure of the redundancy of the predictor <span class="math inline">\(X_j\)</span>. For <span class="math inline">\(X_1\)</span>, this is represented in the right-hand plot of Figure <a href="ch-multiple-regression.html#fig:sse-partition-multicollinearity">5.10</a> as the proportion <span class="math inline">\(\frac{D+G}{B+D+E+G}\)</span>. For <span class="math inline">\(X_2\)</span>, the corresponding proportion is <span class="math inline">\(\frac{D+G}{C+D+F+G}\)</span>.</p>
<p>If the value of <span class="math inline">\(X_j\)</span> could be perfectly predicted from the values of all other predictors, then <span class="math inline">\(X_j\)</span> would not be able to provide any information about the dependent variable <span class="math inline">\(Y\)</span> <em>over and above</em> the other predictors. Remember that the slope in a multiple regression model represents an increase or decrease in the conditional mean of <span class="math inline">\(Y\)</span> for a one-unit increase in the predictor. If any increase in <span class="math inline">\(X_j\)</span> can be perfectly predicted by an increase or decrease in the other predictors, then those in- and decreases of the other predictors can also perfectly account for the associated in- or decrease in the conditional mean of <span class="math inline">\(Y\)</span>. Very high values of <span class="math inline">\(R^2_{X_j}\)</span> indicate that a predictor <span class="math inline">\(X_j\)</span> will not be able to uniquely account for much variation in <span class="math inline">\(Y\)</span>. The value of <span class="math inline">\(1 - R^2_{X_j}\)</span> then conversely represents the <em>uniqueness</em> of predictor <span class="math inline">\(X_j\)</span>, and this value is commonly referred to as the “tolerance”:
<span class="math display" id="eq:tolerance">\[\begin{equation}
\text{tolerance}(X_j) = 1 - R^2_{X_j}
\tag{5.11}
\end{equation}\]</span></p>
<p>All else being equal, a decrease in the tolerance will increase the width of the confidence interval. In the extreme where <span class="math inline">\(1 - R^2_{X_j} = 0\)</span>, the denominator would be 0, and the confidence interval would be <span class="math display">\[\hat{\beta}_j \pm \sqrt{\frac{F_{1,n - \text{npar}(G);\alpha} \text{MSE}(G)}{0}} = \hat{\beta}_j \pm \infty\]</span>
An infinite confidence interval means that we would <em>not</em> be able to reject <em>any</em> null-hypothesis regarding the slope of <span class="math inline">\(X_j\)</span>. In other words: anything goes.</p>
<p>A tolerance of 0 and complete redundancy is not common if you are working with real variables. It can certainly happen if you include the same predictor twice in a model, or if you for instance include a number of predictors and also their case-wise mean. For example, in the model
<span class="math display">\[Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + \beta_3 \left( \frac{X_{1,i} + X_{2,i}}{2} \right) + \epsilon_i\]</span>
the third predictor (the case-wise average of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>) is completely redundant. We can rewrite this model in an equivalent form as
<span class="math display">\[\begin{aligned}
Y_i &amp;= \beta_0 + (\beta_1 + \tfrac{1}{2} \beta_3) X_{1,i} + (\beta_2 + \tfrac{1}{2} \beta_3) X_{2,i} + \epsilon_i \\
Y_i &amp;= \beta_0 + \beta_1&#39; X_{1,i} + \beta_2&#39; X_{2,i} + \epsilon_i 
\end{aligned}\]</span>
which would give <em>exactly</em> the same predictions of <span class="math inline">\(Y_i\)</span>. Moreover, while we can estimate the slopes <span class="math inline">\(\beta_1&#39;\)</span> and <span class="math inline">\(\beta_2&#39;\)</span> as usual, we would <em>not</em> be able to determine uniquely which part of <span class="math inline">\(\hat{\beta}_1&#39;\)</span> should be assigned to <span class="math inline">\(\beta_1\)</span>, and which part should be assigned to <span class="math inline">\(\tfrac{1}{2}\beta_3\)</span>. Any such assignment would be arbitrary. For instance, if <span class="math inline">\(\hat{\beta}_1&#39; = 2\)</span>, then we could say that <span class="math inline">\(\beta_1 = 1\)</span> and <span class="math inline">\(\beta_3 = 2\)</span>, but also that <span class="math inline">\(\beta_1 = 2\)</span> and <span class="math inline">\(\beta_3 = 0\)</span>, without changing anything substantial about the relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
<div id="detecting-and-dealing-with-multicollinearity" class="section level3" number="5.8.1">
<h3><span class="header-section-number">5.8.1</span> Detecting and dealing with multicollinearity</h3>
<p>Generally, tolerance values below <span class="math inline">\(1- R^2_{X_j} = .20\)</span> are considered potentially problematic. This is by no means a precise cut-off value, but it can guide you nonetheless. Some statistics programs also report the <strong>variance inflation factor</strong> (VIF), which is defined as
<span class="math display">\[\text{VIF} = \frac{1}{1 - R^2_{X_j}}\]</span>
As this is just a transformation of the tolerance, I don’t really see the point. Although it might be more intuitive as a measure of collinearity in the sense that high values indicate stronger collinearity, the scale of this measure is between 1 and <span class="math inline">\(\infty\)</span>, and when possible, I find a bounded scale more easily interpretable. But, as there is a one-to-one relation between the tolerance and VIF measures, both provide exactly the same information, so either is fine.</p>
<p>Multicollinearity is not a major issue if you are mainly concerned with predicting the dependent variable. The model as a whole will provide the best possible predictions, with or without collinearity. Problems arise mainly when your goal is to infer and test the true values of the parameters as reflecting relations in the Data Generating Process. When there is redundancy between predictors (high multicollinearity), the estimates of the slopes of each predictor become unreliable, and hypothesis tests will have low power. A “quick-and-dirty” solution is to simply remove some of the collinear predictors, keeping a set of predictors which are more independent of each other. The choice of which predictors to remove can be rather arbitrary however, and eliminating a predictor from consideration in an analysis does not mean that this predictor has no theoretical importance. Another option, if possible, is to increase the sample size <span class="math inline">\(n\)</span>, which may reduce the width of the confidence intervals to more agreeable magnitudes. Alternatively, you could consider transforming all predictors into a set of orthogonal (uncorrelated) variables using a technique such as <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Components Analysis</a>, where each new variable is a linear combination of the original predictors. Those new predictors can be difficult to interpret, however. Another option is to use a technique called <em>ridge regression</em> <span class="citation">(<a href="#ref-hoerl1970ridge" role="doc-biblioref">Hoerl &amp; Kennard, 1970</a>)</span>, which allows inclusion of all predictors in the model with a reduced variance of the parameter estimates, but thereby introduces some bias in these estimates.</p>
</div>
</div>
<div id="sec:04b-outliers" class="section level2" number="5.9">
<h2><span class="header-section-number">5.9</span> Outliers</h2>
<p>When we analysed the data regarding Trump votes and hate groups up to now, we excluded the data from the District of Columbia (Washington D.C.), because, while it is not part of any US state, it is itself not a state either. If we had included the data from this district, the results would have been quite different. Figure <a href="ch-multiple-regression.html#fig:scatterplot-trump-votes-hate-groups-all-data">5.11</a> shows the Trump votes and hate groups with the Distric of Columbia included. You can see that the data point for the District of Columnbia is far removed from all other data points. The District of Columbia has both a very high number of hate groups (30.83 per million citizens), and a low percentage of votes for Trump (4.1%). This makes the data District of Columbia rather unusual when compared to the 50 states. The plot also shows the estimated regression line of a simple regression model (with hate groups as a single predictor) for all the data, and the data excluding the District of Columbia. After inclusion of the District of Columbia, the estimated relation goes from positive to negative!</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterplot-trump-votes-hate-groups-all-data"></span>
<div id="htmlwidget-e75d1c994294f04905b3" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-e75d1c994294f04905b3">{"x":{"data":[{"x":[5.55,0,2.6,5.35,2.01,2.89,1.4,4.2,30.83,3.06,3.1,0,7.13,2.5,3.92,1.28,2.41,5.18,2.99,2.25,2.99,1.76,2.82,1.81,6.02,3.94,9.59,2.62,1.36,4.5,1.68,0.96,2.38,3.06,1.32,3.01,1.53,2.69,3.13,0.95,2.42,8.09,5.71,1.97,0.98,1.6,4.64,2.88,2.18,1.56,3.42],"y":[62.9,52.9,49.5,60.4,32.7,44.4,41.2,41.9,4.1,49.1,51.3,30,59.2,39.4,57.2,51.8,57.2,62.5,58.1,45.2,35.3,33.5,47.6,45.4,58.3,57.1,56.5,60.3,45.5,47.2,41.8,40,37.5,50.5,64.1,52.1,65.3,41.1,48.8,39.8,54.9,61.5,61.1,52.6,45.9,32.6,45,38.2,68.7,47.9,70.1],"text":["hate_groups_per_million:  5.55<br />percent_Trump_votes: 62.9<br />state: Alabama","hate_groups_per_million:  0.00<br />percent_Trump_votes: 52.9<br />state: Alaska","hate_groups_per_million:  2.60<br />percent_Trump_votes: 49.5<br />state: Arizona","hate_groups_per_million:  5.35<br />percent_Trump_votes: 60.4<br />state: Arkansas","hate_groups_per_million:  2.01<br />percent_Trump_votes: 32.7<br />state: California","hate_groups_per_million:  2.89<br />percent_Trump_votes: 44.4<br />state: Colorado","hate_groups_per_million:  1.40<br />percent_Trump_votes: 41.2<br />state: Connecticut","hate_groups_per_million:  4.20<br />percent_Trump_votes: 41.9<br />state: Delaware","hate_groups_per_million: 30.83<br />percent_Trump_votes:  4.1<br />state: District of Columbia","hate_groups_per_million:  3.06<br />percent_Trump_votes: 49.1<br />state: Florida","hate_groups_per_million:  3.10<br />percent_Trump_votes: 51.3<br />state: Georgia","hate_groups_per_million:  0.00<br />percent_Trump_votes: 30.0<br />state: Hawaii","hate_groups_per_million:  7.13<br />percent_Trump_votes: 59.2<br />state: Idaho","hate_groups_per_million:  2.50<br />percent_Trump_votes: 39.4<br />state: Illinois","hate_groups_per_million:  3.92<br />percent_Trump_votes: 57.2<br />state: Indiana","hate_groups_per_million:  1.28<br />percent_Trump_votes: 51.8<br />state: Iowa","hate_groups_per_million:  2.41<br />percent_Trump_votes: 57.2<br />state: Kansas","hate_groups_per_million:  5.18<br />percent_Trump_votes: 62.5<br />state: Kentucky","hate_groups_per_million:  2.99<br />percent_Trump_votes: 58.1<br />state: Louisiana","hate_groups_per_million:  2.25<br />percent_Trump_votes: 45.2<br />state: Maine","hate_groups_per_million:  2.99<br />percent_Trump_votes: 35.3<br />state: Maryland","hate_groups_per_million:  1.76<br />percent_Trump_votes: 33.5<br />state: Massachusetts","hate_groups_per_million:  2.82<br />percent_Trump_votes: 47.6<br />state: Michigan","hate_groups_per_million:  1.81<br />percent_Trump_votes: 45.4<br />state: Minnesota","hate_groups_per_million:  6.02<br />percent_Trump_votes: 58.3<br />state: Mississippi","hate_groups_per_million:  3.94<br />percent_Trump_votes: 57.1<br />state: Missouri","hate_groups_per_million:  9.59<br />percent_Trump_votes: 56.5<br />state: Montana","hate_groups_per_million:  2.62<br />percent_Trump_votes: 60.3<br />state: Nebraska","hate_groups_per_million:  1.36<br />percent_Trump_votes: 45.5<br />state: Nevada","hate_groups_per_million:  4.50<br />percent_Trump_votes: 47.2<br />state: New Hampshire","hate_groups_per_million:  1.68<br />percent_Trump_votes: 41.8<br />state: New Jersey","hate_groups_per_million:  0.96<br />percent_Trump_votes: 40.0<br />state: New Mexico","hate_groups_per_million:  2.38<br />percent_Trump_votes: 37.5<br />state: New York","hate_groups_per_million:  3.06<br />percent_Trump_votes: 50.5<br />state: North Carolina","hate_groups_per_million:  1.32<br />percent_Trump_votes: 64.1<br />state: North Dakota","hate_groups_per_million:  3.01<br />percent_Trump_votes: 52.1<br />state: Ohio","hate_groups_per_million:  1.53<br />percent_Trump_votes: 65.3<br />state: Oklahoma","hate_groups_per_million:  2.69<br />percent_Trump_votes: 41.1<br />state: Oregon","hate_groups_per_million:  3.13<br />percent_Trump_votes: 48.8<br />state: Pennsylvania","hate_groups_per_million:  0.95<br />percent_Trump_votes: 39.8<br />state: Rhode Island","hate_groups_per_million:  2.42<br />percent_Trump_votes: 54.9<br />state: South Carolina","hate_groups_per_million:  8.09<br />percent_Trump_votes: 61.5<br />state: South Dakota","hate_groups_per_million:  5.71<br />percent_Trump_votes: 61.1<br />state: Tennessee","hate_groups_per_million:  1.97<br />percent_Trump_votes: 52.6<br />state: Texas","hate_groups_per_million:  0.98<br />percent_Trump_votes: 45.9<br />state: Utah","hate_groups_per_million:  1.60<br />percent_Trump_votes: 32.6<br />state: Vermont","hate_groups_per_million:  4.64<br />percent_Trump_votes: 45.0<br />state: Virginia","hate_groups_per_million:  2.88<br />percent_Trump_votes: 38.2<br />state: Washington","hate_groups_per_million:  2.18<br />percent_Trump_votes: 68.7<br />state: West Virginia","hate_groups_per_million:  1.56<br />percent_Trump_votes: 47.9<br />state: Wisconsin","hate_groups_per_million:  3.42<br />percent_Trump_votes: 70.1<br />state: Wyoming"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[-1.5415,32.3715],"y":[53.4167294326168,23.8961093979782],"text":"","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(68,1,84,0.6)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[-1.5415,32.3715],"y":[39.3507371133099,117.364537537078],"text":"","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(68,1,84,0.6)","dash":"dot"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":37.2602739726027},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-1.5415,32.3715],"tickmode":"array","ticktext":["0","10","20","30"],"tickvals":[0,10,20,30],"categoryorder":"array","categoryarray":["0","10","20","30"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Hate groups per million citizens","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[0.799999999999999,73.4],"tickmode":"array","ticktext":["20","40","60"],"tickvals":[20,40,60],"categoryorder":"array","categoryarray":["20","40","60"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"% votes for Trump","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"source":"A","attrs":{"63ca447a13724":{"x":{},"y":{},"name":{},"type":"scatter"},"63ca466c59616":{"intercept":{},"slope":{}},"63ca448c62ee0":{"intercept":{},"slope":{}}},"cur_data":"63ca447a13724","visdat":{"63ca447a13724":["function (y) ","x"],"63ca466c59616":["function (y) ","x"],"63ca448c62ee0":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 5.11: Percentage of votes for Trump in the 2016 elections for 50 US states and the District of Columbia, as a function of the number of hate groups per 1 million citizens. The solid line is the estimated regression line for all data, and the dotted line the estimated regression line excluding the District of Columbia.
</p>
</div>
<p>When we estimate a multiple regression model for all the data, we obtain the results given in Table <a href="ch-multiple-regression.html#tab:trump-multiple-regression-ANOVA-table-all-data">5.4</a>. Instead of a significant positive relation between hate groups and Trump votes, we now find the slope is estimated close to 0, and not significant. Note that this estimated relation is not negative (as in Figure <a href="ch-multiple-regression.html#fig:scatterplot-trump-votes-hate-groups-all-data">5.11</a>), which illustrates the fact that the unique relations determined in multiple regression models can differ substantially from the bivariate relations of simple regression models.</p>
<table>
<caption><span id="tab:trump-multiple-regression-ANOVA-table-all-data">Table 5.4: </span>Null-hypothesis significance tests by model comparisons and the F statistic.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SS}\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(F\)</span></th>
<th align="right"><span class="math inline">\(P(\geq \lvert F \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">93.517</td>
<td align="right">18525.97</td>
<td align="right">1</td>
<td align="right">367.803</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">Hate groups per million</td>
<td align="right">0.054</td>
<td align="right">2.29</td>
<td align="right">1</td>
<td align="right">0.046</td>
<td align="right">0.832</td>
</tr>
<tr class="odd">
<td align="left">% bachelors degree or higher</td>
<td align="right">-1.484</td>
<td align="right">3912.90</td>
<td align="right">1</td>
<td align="right">77.684</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="right"></td>
<td align="right">2417.72</td>
<td align="right">48</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>The question now is: how reasonable are these new results? When looking at all 50 states (excluding the District of Columbia), there appears to be rather good evidence for a positive relation between hate groups and Trump votes, whether assessing this relation in a simple regression model or in a multiple regression model where we control for the possible confounding effect of education level. That these results change dramatically after inclusion of a single additional data point indicates that this additional data point might be an <strong>outlier</strong>. An outlier can be rather imprecisely defined as an <em>unusual data point that does not follow the general trend of the rest of the data</em>. Clearly, the District of Columbia does not appear to follow the general trend in terms of Trump votes and hate groups.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterplot-trump-votes-education-all-data"></span>
<div id="htmlwidget-fafda3f7b24d2e065456" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-fafda3f7b24d2e065456">{"x":{"data":[{"x":[15.4,29.7,27.7,21.8,32.3,39.2,38.3,30.9,56.7,28.4,29.9,31.4,26,32.9,24.9,26.8,31.7,23.3,23.2,30.1,38.8,41.5,27.8,34.7,20.8,27.8,30.6,30.2,23.6,35.7,37.6,26.5,35,29.4,29.1,26.8,24.6,32.2,29.7,32.7,26.8,27.5,25.7,28.4,31.8,36.9,37,34.2,19.6,28.4,26.2],"y":[62.9,52.9,49.5,60.4,32.7,44.4,41.2,41.9,4.1,49.1,51.3,30,59.2,39.4,57.2,51.8,57.2,62.5,58.1,45.2,35.3,33.5,47.6,45.4,58.3,57.1,56.5,60.3,45.5,47.2,41.8,40,37.5,50.5,64.1,52.1,65.3,41.1,48.8,39.8,54.9,61.5,61.1,52.6,45.9,32.6,45,38.2,68.7,47.9,70.1],"text":["percent_bachelors_degree_or_higher: 15.4<br />percent_Trump_votes: 62.9<br />state: Alabama","percent_bachelors_degree_or_higher: 29.7<br />percent_Trump_votes: 52.9<br />state: Alaska","percent_bachelors_degree_or_higher: 27.7<br />percent_Trump_votes: 49.5<br />state: Arizona","percent_bachelors_degree_or_higher: 21.8<br />percent_Trump_votes: 60.4<br />state: Arkansas","percent_bachelors_degree_or_higher: 32.3<br />percent_Trump_votes: 32.7<br />state: California","percent_bachelors_degree_or_higher: 39.2<br />percent_Trump_votes: 44.4<br />state: Colorado","percent_bachelors_degree_or_higher: 38.3<br />percent_Trump_votes: 41.2<br />state: Connecticut","percent_bachelors_degree_or_higher: 30.9<br />percent_Trump_votes: 41.9<br />state: Delaware","percent_bachelors_degree_or_higher: 56.7<br />percent_Trump_votes:  4.1<br />state: District of Columbia","percent_bachelors_degree_or_higher: 28.4<br />percent_Trump_votes: 49.1<br />state: Florida","percent_bachelors_degree_or_higher: 29.9<br />percent_Trump_votes: 51.3<br />state: Georgia","percent_bachelors_degree_or_higher: 31.4<br />percent_Trump_votes: 30.0<br />state: Hawaii","percent_bachelors_degree_or_higher: 26.0<br />percent_Trump_votes: 59.2<br />state: Idaho","percent_bachelors_degree_or_higher: 32.9<br />percent_Trump_votes: 39.4<br />state: Illinois","percent_bachelors_degree_or_higher: 24.9<br />percent_Trump_votes: 57.2<br />state: Indiana","percent_bachelors_degree_or_higher: 26.8<br />percent_Trump_votes: 51.8<br />state: Iowa","percent_bachelors_degree_or_higher: 31.7<br />percent_Trump_votes: 57.2<br />state: Kansas","percent_bachelors_degree_or_higher: 23.3<br />percent_Trump_votes: 62.5<br />state: Kentucky","percent_bachelors_degree_or_higher: 23.2<br />percent_Trump_votes: 58.1<br />state: Louisiana","percent_bachelors_degree_or_higher: 30.1<br />percent_Trump_votes: 45.2<br />state: Maine","percent_bachelors_degree_or_higher: 38.8<br />percent_Trump_votes: 35.3<br />state: Maryland","percent_bachelors_degree_or_higher: 41.5<br />percent_Trump_votes: 33.5<br />state: Massachusetts","percent_bachelors_degree_or_higher: 27.8<br />percent_Trump_votes: 47.6<br />state: Michigan","percent_bachelors_degree_or_higher: 34.7<br />percent_Trump_votes: 45.4<br />state: Minnesota","percent_bachelors_degree_or_higher: 20.8<br />percent_Trump_votes: 58.3<br />state: Mississippi","percent_bachelors_degree_or_higher: 27.8<br />percent_Trump_votes: 57.1<br />state: Missouri","percent_bachelors_degree_or_higher: 30.6<br />percent_Trump_votes: 56.5<br />state: Montana","percent_bachelors_degree_or_higher: 30.2<br />percent_Trump_votes: 60.3<br />state: Nebraska","percent_bachelors_degree_or_higher: 23.6<br />percent_Trump_votes: 45.5<br />state: Nevada","percent_bachelors_degree_or_higher: 35.7<br />percent_Trump_votes: 47.2<br />state: New Hampshire","percent_bachelors_degree_or_higher: 37.6<br />percent_Trump_votes: 41.8<br />state: New Jersey","percent_bachelors_degree_or_higher: 26.5<br />percent_Trump_votes: 40.0<br />state: New Mexico","percent_bachelors_degree_or_higher: 35.0<br />percent_Trump_votes: 37.5<br />state: New York","percent_bachelors_degree_or_higher: 29.4<br />percent_Trump_votes: 50.5<br />state: North Carolina","percent_bachelors_degree_or_higher: 29.1<br />percent_Trump_votes: 64.1<br />state: North Dakota","percent_bachelors_degree_or_higher: 26.8<br />percent_Trump_votes: 52.1<br />state: Ohio","percent_bachelors_degree_or_higher: 24.6<br />percent_Trump_votes: 65.3<br />state: Oklahoma","percent_bachelors_degree_or_higher: 32.2<br />percent_Trump_votes: 41.1<br />state: Oregon","percent_bachelors_degree_or_higher: 29.7<br />percent_Trump_votes: 48.8<br />state: Pennsylvania","percent_bachelors_degree_or_higher: 32.7<br />percent_Trump_votes: 39.8<br />state: Rhode Island","percent_bachelors_degree_or_higher: 26.8<br />percent_Trump_votes: 54.9<br />state: South Carolina","percent_bachelors_degree_or_higher: 27.5<br />percent_Trump_votes: 61.5<br />state: South Dakota","percent_bachelors_degree_or_higher: 25.7<br />percent_Trump_votes: 61.1<br />state: Tennessee","percent_bachelors_degree_or_higher: 28.4<br />percent_Trump_votes: 52.6<br />state: Texas","percent_bachelors_degree_or_higher: 31.8<br />percent_Trump_votes: 45.9<br />state: Utah","percent_bachelors_degree_or_higher: 36.9<br />percent_Trump_votes: 32.6<br />state: Vermont","percent_bachelors_degree_or_higher: 37.0<br />percent_Trump_votes: 45.0<br />state: Virginia","percent_bachelors_degree_or_higher: 34.2<br />percent_Trump_votes: 38.2<br />state: Washington","percent_bachelors_degree_or_higher: 19.6<br />percent_Trump_votes: 68.7<br />state: West Virginia","percent_bachelors_degree_or_higher: 28.4<br />percent_Trump_votes: 47.9<br />state: Wisconsin","percent_bachelors_degree_or_higher: 26.2<br />percent_Trump_votes: 70.1<br />state: Wyoming"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[13.335,58.765],"y":[73.6772050951969,6.95289564807783],"text":"","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(68,1,84,0.8)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[13.335,58.765],"y":[71.973905546301,10.3265370916551],"text":"","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(68,1,84,0.8)","dash":"dot"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":37.2602739726027},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[13.335,58.765],"tickmode":"array","ticktext":["20","30","40","50"],"tickvals":[20,30,40,50],"categoryorder":"array","categoryarray":["20","30","40","50"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"% bachelors degree or higher","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[0.799999999999999,73.4],"tickmode":"array","ticktext":["20","40","60"],"tickvals":[20,40,60],"categoryorder":"array","categoryarray":["20","40","60"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"% votes for Trump","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"source":"A","attrs":{"63ca479b9fd5b":{"x":{},"y":{},"name":{},"type":"scatter"},"63ca43d7b7496":{"intercept":{},"slope":{}},"63ca439df1d87":{"intercept":{},"slope":{}}},"cur_data":"63ca479b9fd5b","visdat":{"63ca479b9fd5b":["function (y) ","x"],"63ca43d7b7496":["function (y) ","x"],"63ca439df1d87":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 5.12: Percentage of votes for Trump in the 2016 elections for 50 US states and the District of Columbia, as a function of the percentage of citizens with a bachelors degree or higher. The solid line is the estimated regression line for all data, and the dotted line the estimated regression line excluding the District of Columbia.
</p>
</div>
<p>Figure <a href="ch-multiple-regression.html#fig:scatterplot-trump-votes-education-all-data">5.12</a> shows the relation between Trump votes and education level for all the data. While the District of Columbia is again far removed from the other data points, it doesn’t seem to “break” the negative relation between education level and Trump votes. Indeed, the estimated regression lines for a simple regression model predicting Trump votes from education level don’t differ all that much whether we include the District of Columbia or not.</p>
<p>So, sometimes “unusual” data points can have a large influence on model estimates, and sometimes not. Outliers which do not affect the models estimates much are of relatively little concern. However, outliers which have an undue influence on the model estimates are better removed from the dataset. You don’t generally want the results of an analysis to depend largely on a single observation (it just wouldn’t be fair to the other data points). For an observation to have a large effect on the model estimates, it generally has to have values on the predictor variables which are far from the average, and a value on the dependent variable which is far removed from the true regression plane (i.e. the true residual <span class="math inline">\(\epsilon_i\)</span> is large). If an observation has values on the predictor variables which are close to other observations, then those other observations generally reduce the impact of the outlier, because they will have values on the dependent variable which are not unusual (thus “pulling” the model estimates towards them and with that towards the true values). If an observation has a value on the dependent variable which is not far removed from the true regression plane, then it will not bias the solution either.</p>
<p>There are several measures which aim to determine the unusualness and undue influence of observations on the model estimates. Commonly used ones include the <strong>leverage</strong> or <strong>lever</strong> (which assess whether the predictor values are far from the average), <strong>Studentized residuals</strong> (which aim to estimate whether an observation is far from the true regression line), and <strong>Studentized deleted residuals</strong> and <strong>Cook’s distances</strong> (which aim to assess both). Some of these (e.g. Studentized deleted residuals) are effectively hypothesis tests applied to each individual observation. That has the benefit of providing them with a formal criterion to identify observations as outliers or not. On the other hand, by performing a total of <span class="math inline">\(n\)</span> hypothesis tests with a significance level of <span class="math inline">\(\alpha\)</span>, on average <span class="math inline">\(n \times \alpha\)</span> of these tests will involve a Type 1 error (a false rejection of the null hypothesis that an observation is not an outlier). These measures are therefore likely to classify observations as outliers which are not truly outliers. Other measures such as Cook’s distance use more heuristic cut-off values to determine when there is a needs to carefully consider whether an observation is an outlier or not.</p>
<p>At this point, I don’t want to go into the mathematical details of these outlier detection measures. For now, I just want you to be aware of the potential problems that arise from outliers. Often, a thorough visual exploration of the data will indicate potential outliers. If you suspect an outlier, it is useful to repeat an analysis with and without including the potential outlier. When the results differ substantially, you will then need to carefully consider whether there are good reasons to exclude the outlying observation from the analysis. In the case of the District of Columbia, there are several reasons to exclude the District of Columbia. Not only does this electoral district has an unusual high number of hate groups, and an unusual low number of Trump votes, the district is not a state but a single city (Washington D.C.), with consequently a much higher population density than the 50 states. As the political capitol of the United States, it is likely to be different in many aspects related to politics. As such, treating this as a separate entity from the 50 US states seems reasonable.</p>
</div>
<div id="in-practice-1" class="section level2" number="5.10">
<h2><span class="header-section-number">5.10</span> In practice</h2>
<p>Linear regression is the “bread and butter” of data analysis. Whilst the name is generally reserved for models with only metric predictors, we we will see later on, the model we have discussed here is the foundation of the General Linear Model.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Explore the data</strong>. Check the distribution of the dependent variable and each (potential) predictor. Are there outlying or otherwise “strange” observations? Also explore pairwise relations between the (potential) predictors and the dependent variable, as well as between the predictors themselves. This may point to potential issues with the assumption of linearity (although pairwise linearity or nonlinearity is not necessarily reflective of linearity in a multiple regression model).</p></li>
<li><p>Estimate the multiple regression model. Then check for potential issues in the assumptions with e.g. predicted vs residual plots. This may point to potential outliers, but you can also check for overly influential cases with a measure such as Cook’s distance. A value of Cook’s distance larger than 1 is generally considered as a cut-off. If there are clear outliers in the data, remove these, and then re-estimate the model. Also check for issues with multicollinearity.</p></li>
<li><p>Report the results. When reporting the results, make sure that you include all relevant statistics. For example, one way to write the results of the analysis reported in this chapter is as follows:</p></li>
</ol>
<blockquote>
<p>We assessed the relation between votes for Donald Trump and the presence of hate groups in 50 states of the USA with a multiple linear regression model (we excluded data from the District of Columbia, as this was a clear outlier). For each state, we computed the number of hate groups per one million citizen, to correct for differences in population size in the states. As the level of education might be a possible confound, we controlled for education level (measured as the percentage of the population with a Bachelor’s degree or higher) by including this as a second predictor. The model accounted for a large proportion of the variance in Trump votes, <span class="math inline">\(R^2 = .66\)</span>, <span class="math inline">\(F(2, 48) = 45.94\)</span>, <span class="math inline">\(p &lt; .001\)</span>. The analysis showed a significant positive effect of hate groups on votes for Donald Trump, <span class="math inline">\(b = 0.05\)</span>, 95% CI <span class="math inline">\([-0.46\)</span>, <span class="math inline">\(0.56]\)</span>, <span class="math inline">\(t(48) = 0.21\)</span>, <span class="math inline">\(p = .832\)</span>. In addition, we found a significant negative relation between education level and votes for Donald Trump, <span class="math inline">\(b = -1.48\)</span>, 95% CI <span class="math inline">\([-1.82\)</span>, <span class="math inline">\(-1.15]\)</span>, <span class="math inline">\(t(48) = -8.81\)</span>, <span class="math inline">\(p &lt; .001\)</span>.</p>
</blockquote>
<!--
## Summary
-->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-hoerl1970ridge" class="csl-entry">
Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. <em>Technometrics</em>, <em>12</em>, 55–67.
</div>
<div id="ref-sakia1992box" class="csl-entry">
Sakia, R. M. (1992). The box-cox transformation technique: A review. <em>Journal of the Royal Statistical Society: Series D (The Statistician)</em>, <em>41</em>, 169–178.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>A vector with the intercept and slopes can be computed by first constructing a so-called design matrix <span class="math inline">\(\mathbf{X}\)</span>, where the first column contains only 1’s, and the remaining columns contain values for each predictor. The vector with estimates is then computed as <span class="math inline">\(\hat{\boldsymbol{{\beta}}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}\)</span>, where <span class="math inline">\(\mathbf{Y}\)</span> is the vector with the values of the dependent variable, and <span class="math inline">\(^\top\)</span> stands for the matrix transpose, and <span class="math inline">\(\mathbf{X}^{-1}\)</span> for the inverse of matrix <span class="math inline">\(\mathbf{X}\)</span>.<a href="ch-multiple-regression.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>In some of the literature and output of statistical software, these are referred to as beta-coefficients. As we are using beta (<span class="math inline">\(\beta\)</span>) to reflect the true value of a linear model parameter, this may be confusing.<a href="ch-multiple-regression.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>In usual multiple regression models, the values of the other predictors <span class="math inline">\(X_k\)</span> can take a variety of values for any focal predictor <span class="math inline">\(X_j\)</span>, <span class="math inline">\(j \neq k\)</span>. Here there is a perfect–but crucially_nonlinear relation between the predictors.<a href="ch-multiple-regression.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>you could also use that, but then you’d have to replace the preceding <span class="math inline">\(n\)</span> by <span class="math inline">\(n-1\)</span><a href="ch-multiple-regression.html#fnref14" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-simple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-moderation-mediation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"source": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
