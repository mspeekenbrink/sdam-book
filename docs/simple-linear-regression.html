<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Simple linear regression | Statistics: Data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Simple linear regression | Statistics: Data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Simple linear regression | Statistics: Data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2021-11-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-simple-GLM.html"/>
<link rel="next" href="ch-multiple-regression.html"/>
<script src="book_assets/header-attrs-2.11/header-attrs.js"></script>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="book_assets/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.6.1/grViz.js"></script>
<script src="book_assets/plotly-binding-4.10.0/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-2.5.1/plotly-latest.min.js"></script>
<link href="book_assets/combineWidgetStyle-0.1/combineWidgets.css" rel="stylesheet" />
<script src="book_assets/combineWidgets-binding-0.11.1/combineWidgets.js"></script>
<script src="book_assets/rglWebGL-binding-0.107.14/rglWebGL.js"></script>
<link href="book_assets/rglwidgetClass-0.107.14/rgl.css" rel="stylesheet" />
<script src="book_assets/rglwidgetClass-0.107.14/rglClass.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/utils.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/subscenes.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/shaders.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/textures.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/projection.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/mouse.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/init.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/pieces.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/draw.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/controls.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/selection.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/rglTimer.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/pretty.src.js"></script>
<script src="book_assets/rglwidgetClass-0.107.14/axes.src.js"></script>
<script src="book_assets/CanvasMatrix4-0.107.14/CanvasMatrix.src.js"></script>
<script src="book_assets/rglPlayer-binding-0.107.14/rglPlayer.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-modeling.html"><a href="ch-modeling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-probability-definition"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-binomial-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="ch-modeling.html"><a href="ch-modeling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-modeling.html"><a href="ch-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-modeling.html"><a href="ch-modeling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-likelihood-ratio"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-modeling.html"><a href="ch-modeling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-modeling.html"><a href="ch-modeling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-modeling.html"><a href="ch-modeling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-modeling.html"><a href="ch-modeling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="ch-modeling.html"><a href="ch-modeling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="ch-modeling.html"><a href="ch-modeling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-an-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has an specific value</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#ch3-confidence-interval"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#effect-size"><i class="fa fa-check"></i><b>3.6</b> Effect size</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.7</b> Assumptions</a></li>
<li class="chapter" data-level="3.8" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.8</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.8.1</b> The Central Limit Theorem in action</a></li>
<li class="chapter" data-level="3.8.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#bootstrapping-a-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Bootstrapping a statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#in-practice"><i class="fa fa-check"></i><b>3.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>4.4.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary-2"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#transforming-variables"><i class="fa fa-check"></i><b>5.7.1</b> Transforming variables</a></li>
<li class="chapter" data-level="5.7.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.7.2</b> Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.8</b> Multicollinearity: Redundancy between predictors</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.8.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-outliers"><i class="fa fa-check"></i><b>5.9</b> Outliers</a></li>
<li class="chapter" data-level="5.10" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#in-practice-1"><i class="fa fa-check"></i><b>5.10</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#sec:05-dont-forget-about-the-fun"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#sec:06-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-size-in-anova"><i class="fa fa-check"></i><b>7.6</b> Effect-size in ANOVA</a></li>
<li class="chapter" data-level="7.7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#assumptions"><i class="fa fa-check"></i><b>7.7</b> Assumptions</a></li>
<li class="chapter" data-level="7.8" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.8</b> Multiple testing and post-hoc tests</a></li>
<li class="chapter" data-level="7.9" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#in-practice-2"><i class="fa fa-check"></i><b>7.9</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#sec:06-main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#the-factorial-anova-model"><i class="fa fa-check"></i><b>8.3</b> The factorial ANOVA model</a></li>
<li class="chapter" data-level="8.4" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.4</b> A threeway factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#interpreting-interactions"><i class="fa fa-check"></i><b>8.4.1</b> Interpreting interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.5</b> Orthogonal contrast codes and unequal sample sizes</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.5.1</b> Comparison schemes and SS types</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#in-practice-3"><i class="fa fa-check"></i><b>8.6</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#subjective-feelings-of-power-and-priming"><i class="fa fa-check"></i><b>9.1</b> Subjective feelings of power and priming</a></li>
<li class="chapter" data-level="9.2" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#acounting-for-pre-existing-differences"><i class="fa fa-check"></i><b>9.2</b> Acounting for pre-existing differences</a></li>
<li class="chapter" data-level="9.3" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#slopes-of-contrast-coded-predictors-in-ancova-models"><i class="fa fa-check"></i><b>9.3</b> Slopes of contrast-coded predictors in ANCOVA models</a></li>
<li class="chapter" data-level="9.4" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#homogeneity-of-slopes"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of slopes</a></li>
<li class="chapter" data-level="9.5" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#sec:ANCOVA-power"><i class="fa fa-check"></i><b>9.5</b> Power considerations in ANCOVA</a></li>
<li class="chapter" data-level="9.6" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#models-with-multiple-covariates"><i class="fa fa-check"></i><b>9.6</b> Models with multiple covariates</a></li>
<li class="chapter" data-level="9.7" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#mediation-with-categorical-independent-variables"><i class="fa fa-check"></i><b>9.7</b> Mediation with categorical independent variables</a></li>
<li class="chapter" data-level="9.8" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#ancova-vs-difference-scores"><i class="fa fa-check"></i><b>9.8</b> ANCOVA vs difference scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#non-independence-in-linear-models"><i class="fa fa-check"></i><b>10.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="10.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#the-cheerleader-effect"><i class="fa fa-check"></i><b>10.2</b> The cheerleader effect</a></li>
<li class="chapter" data-level="10.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#as-a-oneway-anova"><i class="fa fa-check"></i><b>10.3</b> As a oneway ANOVA</a></li>
<li class="chapter" data-level="10.4" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#oneway-repeated-measures-anova"><i class="fa fa-check"></i><b>10.4</b> Oneway repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#within-subjects-composite-scores"><i class="fa fa-check"></i><b>10.4.1</b> Within-subjects composite scores</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-composite-for-between-subjects-effects"><i class="fa fa-check"></i><b>10.4.2</b> A composite for between-subjects effects</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#collecting-all-results-and-omnibus-tests"><i class="fa fa-check"></i><b>10.4.3</b> Collecting all results and omnibus tests</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#partitioning-the-variance"><i class="fa fa-check"></i><b>10.5</b> Partitioning the variance</a></li>
<li class="chapter" data-level="10.6" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#a-mixed-anova-with-between--and-within-subjects-effects"><i class="fa fa-check"></i><b>10.6</b> A mixed ANOVA with between- and within-subjects effects</a></li>
<li class="chapter" data-level="10.7" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#assumptions-1"><i class="fa fa-check"></i><b>10.7</b> Assumptions</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#omnibus-tests-and-sphericity"><i class="fa fa-check"></i><b>10.7.1</b> Omnibus tests and sphericity</a></li>
<li class="chapter" data-level="10.7.2" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#correcting-for-non-sphericity"><i class="fa fa-check"></i><b>10.7.2</b> Correcting for non-sphericity</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="ch-RM-ANOVA.html"><a href="ch-RM-ANOVA.html#in-practice-4"><i class="fa fa-check"></i><b>10.8</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#non-independence-in-linear-models-1"><i class="fa fa-check"></i><b>11.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="11.2" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#random-intercept-models"><i class="fa fa-check"></i><b>11.2</b> Random intercept models</a></li>
<li class="chapter" data-level="11.3" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#parameter-estimation-1"><i class="fa fa-check"></i><b>11.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="11.4" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#parameter-inference"><i class="fa fa-check"></i><b>11.4</b> Parameter inference</a></li>
<li class="chapter" data-level="11.5" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#application-of-the-random-intercepts-model"><i class="fa fa-check"></i><b>11.5</b> Application of the random-intercepts model</a></li>
<li class="chapter" data-level="11.6" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#models-with-random-intercepts-and-slopes"><i class="fa fa-check"></i><b>11.6</b> Models with random intercepts and slopes</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#correlation-between-random-effects"><i class="fa fa-check"></i><b>11.6.1</b> Correlation between random effects</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#crossed-random-effects-dating-partners-in-the-speed-dating-experiment"><i class="fa fa-check"></i><b>11.7</b> Crossed random effects: dating partners in the speed dating experiment</a></li>
<li class="chapter" data-level="11.8" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#choosing-the-random-effects-structure"><i class="fa fa-check"></i><b>11.8</b> Choosing the random effects structure</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html"><i class="fa fa-check"></i><b>12</b> Introduction to Bayesian hypothesis testing</a>
<ul>
<li class="chapter" data-level="12.1" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#fundamentals-of-bayesian-inference"><i class="fa fa-check"></i><b>12.1</b> Fundamentals of Bayesian inference</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#probability-in-times-of-covid"><i class="fa fa-check"></i><b>12.1.1</b> Probability in times of Covid</a></li>
<li class="chapter" data-level="12.1.2" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#bayes-rule"><i class="fa fa-check"></i><b>12.1.2</b> Bayes’ rule</a></li>
<li class="chapter" data-level="12.1.3" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#we-missed-you-paul"><i class="fa fa-check"></i><b>12.1.3</b> We missed you Paul!</a></li>
<li class="chapter" data-level="12.1.4" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#the-marginal-likelihood-and-prior-predictive-distribution"><i class="fa fa-check"></i><b>12.1.4</b> The marginal likelihood and prior predictive distribution</a></li>
<li class="chapter" data-level="12.1.5" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#hypothesis-testing-relative-evidence-and-the-bayes-factor"><i class="fa fa-check"></i><b>12.1.5</b> Hypothesis testing, relative evidence, and the Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#a-bayesian-t-test"><i class="fa fa-check"></i><b>12.2</b> A Bayesian t-test</a></li>
<li class="chapter" data-level="12.3" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#bayes-factors-for-general-linear-models"><i class="fa fa-check"></i><b>12.3</b> Bayes factors for General Linear Models</a></li>
<li class="chapter" data-level="12.4" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#some-objections-to-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>12.4</b> Some objections to null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#the-p-value-is-not-a-proper-measure-of-evidential-support"><i class="fa fa-check"></i><b>12.4.1</b> The <span class="math inline">\(p\)</span>-value is not a proper measure of evidential support</a></li>
<li class="chapter" data-level="12.4.2" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#the-p-value-depends-on-researcher-intentions"><i class="fa fa-check"></i><b>12.4.2</b> The <span class="math inline">\(p\)</span>-value depends on researcher intentions</a></li>
<li class="chapter" data-level="12.4.3" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#results-of-a-nhst-are-often-misinterpreted"><i class="fa fa-check"></i><b>12.4.3</b> Results of a NHST are often misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#to-bayes-or-not-to-bayes-a-pragmatic-view"><i class="fa fa-check"></i><b>12.5</b> To Bayes or not to Bayes? A pragmatic view</a></li>
<li class="chapter" data-level="12.6" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#summary-3"><i class="fa fa-check"></i><b>12.6</b> “Summary”</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html"><i class="fa fa-check"></i><b>13</b> Being a responsible data analyst</a>
<ul>
<li class="chapter" data-level="13.1" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#consider-analysis-before-data-collection"><i class="fa fa-check"></i><b>13.1</b> Consider analysis before data collection</a></li>
<li class="chapter" data-level="13.2" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#explore-the-data"><i class="fa fa-check"></i><b>13.2</b> Explore the data</a></li>
<li class="chapter" data-level="13.3" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#evaluate-the-assumptions-underlying-your-analyses"><i class="fa fa-check"></i><b>13.3</b> Evaluate the assumptions underlying your analyses</a></li>
<li class="chapter" data-level="13.4" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#distinguish-between-confirmatory-and-exploratory-analyses"><i class="fa fa-check"></i><b>13.4</b> Distinguish between confirmatory and exploratory analyses</a></li>
<li class="chapter" data-level="13.5" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#aim-for-openness-and-reproducibility"><i class="fa fa-check"></i><b>13.5</b> Aim for openness and reproducibility</a></li>
<li class="chapter" data-level="13.6" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#communicate-clearly-and-concisely"><i class="fa fa-check"></i><b>13.6</b> Communicate clearly and concisely</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#example-of-reporting-a-multiple-regression-analysis"><i class="fa fa-check"></i><b>13.6.1</b> Example of reporting a multiple regression analysis</a></li>
<li class="chapter" data-level="13.6.2" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#example-of-reporting-a-factorial-anova"><i class="fa fa-check"></i><b>13.6.2</b> Example of reporting a factorial ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: Data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-regression" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Simple linear regression</h1>
<p>In this chapter, we continue our journey into the General Linear Model by extending the very simple model of the previous chapter to include a predictor. The resulting model assumes there is a linear relation between the dependent variable and the predictor, and is also known as a simple linear regression model. We will look at the parameters of the model in detail, and discuss their estimation, as well as testing whether their values are equal to a priori ones of interest.</p>
<div id="trump-votes-and-hate-groups" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Trump, votes, and hate groups</h2>
<p>Donald Trump was perhaps the most divisive president in American history. The 2016 US elections were mired in controversy. <a href="https://www.theguardian.com/world/2020/mar/18/white-nationalist-hate-groups-southern-poverty-law-center">Some reports</a> indicate the number of white nationalist hate groups have gone up by 55% in the Trump era. One study found a strong relation between Trump support and anti-immigrant sentiment, racism, and sexism <span class="citation">(<a href="#ref-schaffner_understanding_2018" role="doc-biblioref">Schaffner, Macwilliams, &amp; Nteta, 2018</a>)</span>.</p>
<p><a href="https://www.csiwithoutdeadbodies.com/2017/02/hate-groups-and-trumps-vote-predictive.html">Paul Ricci</a> collated data about the number of hate groups in the different US states and votes for Trump. A scatterplot of these two variables is provided in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups">4.1</a>. You can see that in states where there are relatively little hate groups, there appear to also be relatively little votes for Trump. In states where there are a relatively large number of hate groups, the appear relatively many votes for Trump.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterplot-trump-votes-hate-groups"></span>
<div id="htmlwidget-b38adca41469f51a3e50" style="width:95%;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-b38adca41469f51a3e50">{"x":{"data":[{"x":[5.55,0,2.6,5.35,2.01,2.89,1.4,4.2,3.06,3.1,0,7.13,2.5,3.92,1.28,2.41,5.18,2.99,2.25,2.99,1.76,2.82,1.81,6.02,3.94,9.59,2.62,1.36,4.5,1.68,0.96,2.38,3.06,1.32,3.01,1.53,2.69,3.13,0.95,2.42,8.09,5.71,1.97,0.98,1.6,4.64,2.88,2.18,1.56,3.42],"y":[62.9,52.9,49.5,60.4,32.7,44.4,41.2,41.9,49.1,51.3,30,59.2,39.4,57.2,51.8,57.2,62.5,58.1,45.2,35.3,33.5,47.6,45.4,58.3,57.1,56.5,60.3,45.5,47.2,41.8,40,37.5,50.5,64.1,52.1,65.3,41.1,48.8,39.8,54.9,61.5,61.1,52.6,45.9,32.6,45,38.2,68.7,47.9,70.1],"text":["hate_groups_per_million: 5.55<br />percent_Trump_votes: 62.9<br />state: Alabama","hate_groups_per_million: 0.00<br />percent_Trump_votes: 52.9<br />state: Alaska","hate_groups_per_million: 2.60<br />percent_Trump_votes: 49.5<br />state: Arizona","hate_groups_per_million: 5.35<br />percent_Trump_votes: 60.4<br />state: Arkansas","hate_groups_per_million: 2.01<br />percent_Trump_votes: 32.7<br />state: California","hate_groups_per_million: 2.89<br />percent_Trump_votes: 44.4<br />state: Colorado","hate_groups_per_million: 1.40<br />percent_Trump_votes: 41.2<br />state: Connecticut","hate_groups_per_million: 4.20<br />percent_Trump_votes: 41.9<br />state: Delaware","hate_groups_per_million: 3.06<br />percent_Trump_votes: 49.1<br />state: Florida","hate_groups_per_million: 3.10<br />percent_Trump_votes: 51.3<br />state: Georgia","hate_groups_per_million: 0.00<br />percent_Trump_votes: 30.0<br />state: Hawaii","hate_groups_per_million: 7.13<br />percent_Trump_votes: 59.2<br />state: Idaho","hate_groups_per_million: 2.50<br />percent_Trump_votes: 39.4<br />state: Illinois","hate_groups_per_million: 3.92<br />percent_Trump_votes: 57.2<br />state: Indiana","hate_groups_per_million: 1.28<br />percent_Trump_votes: 51.8<br />state: Iowa","hate_groups_per_million: 2.41<br />percent_Trump_votes: 57.2<br />state: Kansas","hate_groups_per_million: 5.18<br />percent_Trump_votes: 62.5<br />state: Kentucky","hate_groups_per_million: 2.99<br />percent_Trump_votes: 58.1<br />state: Louisiana","hate_groups_per_million: 2.25<br />percent_Trump_votes: 45.2<br />state: Maine","hate_groups_per_million: 2.99<br />percent_Trump_votes: 35.3<br />state: Maryland","hate_groups_per_million: 1.76<br />percent_Trump_votes: 33.5<br />state: Massachusetts","hate_groups_per_million: 2.82<br />percent_Trump_votes: 47.6<br />state: Michigan","hate_groups_per_million: 1.81<br />percent_Trump_votes: 45.4<br />state: Minnesota","hate_groups_per_million: 6.02<br />percent_Trump_votes: 58.3<br />state: Mississippi","hate_groups_per_million: 3.94<br />percent_Trump_votes: 57.1<br />state: Missouri","hate_groups_per_million: 9.59<br />percent_Trump_votes: 56.5<br />state: Montana","hate_groups_per_million: 2.62<br />percent_Trump_votes: 60.3<br />state: Nebraska","hate_groups_per_million: 1.36<br />percent_Trump_votes: 45.5<br />state: Nevada","hate_groups_per_million: 4.50<br />percent_Trump_votes: 47.2<br />state: New Hampshire","hate_groups_per_million: 1.68<br />percent_Trump_votes: 41.8<br />state: New Jersey","hate_groups_per_million: 0.96<br />percent_Trump_votes: 40.0<br />state: New Mexico","hate_groups_per_million: 2.38<br />percent_Trump_votes: 37.5<br />state: New York","hate_groups_per_million: 3.06<br />percent_Trump_votes: 50.5<br />state: North Carolina","hate_groups_per_million: 1.32<br />percent_Trump_votes: 64.1<br />state: North Dakota","hate_groups_per_million: 3.01<br />percent_Trump_votes: 52.1<br />state: Ohio","hate_groups_per_million: 1.53<br />percent_Trump_votes: 65.3<br />state: Oklahoma","hate_groups_per_million: 2.69<br />percent_Trump_votes: 41.1<br />state: Oregon","hate_groups_per_million: 3.13<br />percent_Trump_votes: 48.8<br />state: Pennsylvania","hate_groups_per_million: 0.95<br />percent_Trump_votes: 39.8<br />state: Rhode Island","hate_groups_per_million: 2.42<br />percent_Trump_votes: 54.9<br />state: South Carolina","hate_groups_per_million: 8.09<br />percent_Trump_votes: 61.5<br />state: South Dakota","hate_groups_per_million: 5.71<br />percent_Trump_votes: 61.1<br />state: Tennessee","hate_groups_per_million: 1.97<br />percent_Trump_votes: 52.6<br />state: Texas","hate_groups_per_million: 0.98<br />percent_Trump_votes: 45.9<br />state: Utah","hate_groups_per_million: 1.60<br />percent_Trump_votes: 32.6<br />state: Vermont","hate_groups_per_million: 4.64<br />percent_Trump_votes: 45.0<br />state: Virginia","hate_groups_per_million: 2.88<br />percent_Trump_votes: 38.2<br />state: Washington","hate_groups_per_million: 2.18<br />percent_Trump_votes: 68.7<br />state: West Virginia","hate_groups_per_million: 1.56<br />percent_Trump_votes: 47.9<br />state: Wisconsin","hate_groups_per_million: 3.42<br />percent_Trump_votes: 70.1<br />state: Wyoming"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":37.2602739726027},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.4795,10.0695],"tickmode":"array","ticktext":["0.0","2.5","5.0","7.5","10.0"],"tickvals":[0,2.5,5,7.5,10],"categoryorder":"array","categoryarray":["0.0","2.5","5.0","7.5","10.0"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Hate groups per million citizens","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[27.995,72.105],"tickmode":"array","ticktext":["30","40","50","60","70"],"tickvals":[30,40,50,60,70],"categoryorder":"array","categoryarray":["30","40","50","60","70"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"% votes for Trump","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"source":"A","attrs":{"16206f3bc52f65":{"x":{},"y":{},"name":{},"type":"scatter"}},"cur_data":"16206f3bc52f65","visdat":{"16206f3bc52f65":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 4.1: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens
</p>
</div>
<p>To assess the relation between Trump votes and the number of hate groups, we will use a <strong>simple linear regression</strong> model. We are interested in a model which allows us to predict the percentage of votes for Trump from the number of hate groups. In other words, votes for Trump is the dependent variable, and hate groups a <strong>predictor</strong> variable. A predictot variable is sometimes also referred to as an independent variable.</p>
<p>As the name suggests, a linear regression model involves a line, a straight one in fact. This straight line represents the average or expected value of the dependent variable <span class="math inline">\(Y\)</span> (i.e. the percentage of Trump votes) over all cases which have a particular value on the predictor (or independent) variable <span class="math inline">\(X\)</span> (i.e. the number of hate groups). For example, we may aim to infer the average percentage of Trump votes in all states with 5.55 hate groups per 1 million citizens, or the average percentage of Trump votes in states with 0 hate groups. The model allows the values of the dependent variable for individual cases to vary around the average. So the percentage of Trump votes for two different states, each with 5.55 hate groups, may differ from the average as well as each other. The model assumes that such variability can be described through a Normal distribution. Importantly, the model assumes that the variance of this normal distribution is the same, no matter the value of on predictor.</p>
<p>both be different from the average and from each to vary around the average. In particular, it assumes that the deviations between actual and mean percentages are Normal-distributed.</p>
</div>
<div id="the-model" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> The model</h2>
<p>In the previous chapter, we used a simple statistical model:
<span class="math display">\[Y_i \sim \mathbf{Normal}(\mu,\sigma)\]</span>
This model assumes that each observation is independently drawn from a Normal distribution, with a mean <span class="math inline">\(\mu\)</span> and a standard deviation <span class="math inline">\(\sigma\)</span>. We can state this model in an equivalent way as:
<span class="math display" id="eq:simple-model-as-glm">\[\begin{equation}
Y_i = \mu + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma)
\tag{4.1}
\end{equation}\]</span>
In this two-part formulation, the first part (formula on the left side) decomposes each observation <span class="math inline">\(Y_i\)</span> in a <em>structural part</em> (here the mean, <span class="math inline">\(\mu\)</span>), and a <em>random part</em> that we usually call the error (<span class="math inline">\(\epsilon_i\)</span>). We don’t know much about this error. We just assume that is is drawn from a Normal distribution with a mean of 0 and standard deviation <span class="math inline">\(\sigma_\epsilon\)</span>. This assumption about the distribution forms the second part (the formula on the right side of Equation <a href="simple-linear-regression.html#eq:simple-model-as-glm">(4.1)</a>).<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>A model like this doesn’t allow for very precise predictions. As the error term is assumed to be completely random, it is unpredictable. So if you were asked to predict the value of <span class="math inline">\(Y\)</span>, all you could really do is to use <span class="math inline">\(\mu\)</span> as your prediction. If, however, the dependent variable (e.g. votes for Trump) is related to a predictor variable (e.g. number of hate groups), then we should be able to use to use the predictor to make more precise predictions. <strong>Linear regression</strong> allows us to to this in a straightforward way.</p>
<p>The bivariate regression model is depicted in Figure <a href="simple-linear-regression.html#fig:simple-regression-explanation-plot">4.2</a>. More formally, the model can be defined as follows:</p>
<p><span class="math display" id="eq:bivariate-regression-model-definition">\[\begin{equation}
Y_i = \beta_0 + \beta_1 \times X_{i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
\tag{4.2}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(Y_i\)</span> is the value of the <span class="math inline">\(i\)</span>-th observation of the dependent variable (with <span class="math inline">\(i = 1, \ldots, n\)</span>) and <span class="math inline">\(X_i\)</span> the value of the <span class="math inline">\(i\)</span>-th observation of the predictor variable. These are the observable parts of the data. The model contains three important parameters:</p>
<ul>
<li>The <strong>intercept</strong> <span class="math inline">\(\beta_0\)</span>, which is the mean of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X=0\)</span>.</li>
<li>The <strong>slope</strong> <span class="math inline">\(\beta_1\)</span>, which reflects the increase or decrease in the mean of <span class="math inline">\(Y\)</span> for every <em>1-unit increase</em> in the predictor <span class="math inline">\(X\)</span>. By a 1-unit increase, we simply mean that the value of <span class="math inline">\(X\)</span> goes up by 1, e.g. from <span class="math inline">\(X=2\)</span> to <span class="math inline">\(X=3\)</span>, or from <span class="math inline">\(X=12.63\)</span> to <span class="math inline">\(X=13.63\)</span>.</li>
<li>The <strong>standard deviation</strong> <span class="math inline">\(\sigma_\epsilon\)</span> of the <strong>error</strong> or <strong>residual</strong> terms <span class="math inline">\(\epsilon_i\)</span>. The errors are assumed to be drawn independently from the same Normal distribution, with a mean of 0 and standard deviation <span class="math inline">\(\sigma_\epsilon\)</span>.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simple-regression-explanation-plot"></span>
<img src="_main_files/figure-html/simple-regression-explanation-plot-1.svg" alt="The simple regression model. $\textbf{A}$: Simple regression aims to capture the relation between two variables, a dependent variable ($Y$) and a predictor variable ($X$). Each case in a dataset provides a pair of observations on both variables. $\textbf{B}$: The intercept is the mean of $Y$ when $X=0$ and is the point at which the regression line crosses the y-axis. The slope determines the steepness of the regression line and represents the increase (or decrease) in the mean of $Y$ for every 1-unit increase in the predictor $X$. $\textbf{C}$: the error terms or residuals are the vertical distances of each observed Y-value from the regression line. $\textbf{D}$: The errors in a regression model are assumed to follow a Normal distribution around the regression line." width="100%" />
<p class="caption">
Figure 4.2: The simple regression model. <span class="math inline">\(\textbf{A}\)</span>: Simple regression aims to capture the relation between two variables, a dependent variable (<span class="math inline">\(Y\)</span>) and a predictor variable (<span class="math inline">\(X\)</span>). Each case in a dataset provides a pair of observations on both variables. <span class="math inline">\(\textbf{B}\)</span>: The intercept is the mean of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X=0\)</span> and is the point at which the regression line crosses the y-axis. The slope determines the steepness of the regression line and represents the increase (or decrease) in the mean of <span class="math inline">\(Y\)</span> for every 1-unit increase in the predictor <span class="math inline">\(X\)</span>. <span class="math inline">\(\textbf{C}\)</span>: the error terms or residuals are the vertical distances of each observed Y-value from the regression line. <span class="math inline">\(\textbf{D}\)</span>: The errors in a regression model are assumed to follow a Normal distribution around the regression line.
</p>
</div>
<p>Remember, unlike variables, parameters are <em>not</em> observable. We can infer their value from the data by estimation and/or performing hypothesis tests. But we can never be completely sure that such inferences are correct. Let’s consider the structural part of the simple regression model again:
<span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_{i} + \epsilon_i\]</span>
This formula involves both observable variables (<span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>) and unobservable parameters (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\epsilon_i\)</span>). Indeed, the errors <span class="math inline">\(\epsilon_i\)</span> are really parameters as well, because they cannot be directly observed. There are many of these (one for each observation <span class="math inline">\(i = 1, \ldots, n\)</span>), and they are not of primary concern. The other part, <span class="math inline">\(\beta_0 + \beta_1 \times X_i\)</span>, determines the regression line, representing the average value of <span class="math inline">\(Y\)</span> for all cases with a particular value on the predictor variable <span class="math inline">\(X\)</span>. These average values are <strong>conditional means</strong>, they are the mean of <span class="math inline">\(Y\)</span>, conditional upon a particular value of the predictor <span class="math inline">\(X\)</span>. “Conditional upon” here means that we only consider cases with a particular value on the predictor variable. For instance, we may consider all states with 5.55 hate groups. The conditional mean is then just the average of the dependent variable (votes for Trump) in this particular group of states. We can use <span class="math inline">\(\mu_{Y|X}\)</span> to denote the mean of <span class="math inline">\(Y\)</span> conditional upon <span class="math inline">\(X\)</span>. If we then use <span class="math inline">\(\mu_{Y|X_i}\)</span> to denote the mean of <span class="math inline">\(Y\)</span> conditional upon <span class="math inline">\(X\)</span> having the value <span class="math inline">\(X_i\)</span> (i.e. the value of <span class="math inline">\(X\)</span> for case <span class="math inline">\(i\)</span> in the dataset), we can define the conditional mean in terms of the regression model as:
<span class="math display" id="eq:conditional-mean-simple-regression">\[\begin{equation}
\mu_{Y|X_{i}} = \beta_0 + \beta_1 \times X_{i}
\tag{4.3}
\end{equation}\]</span></p>
<p>Now, plugging the conditional mean into Equation <a href="simple-linear-regression.html#eq:bivariate-regression-model-definition">(4.2)</a> we get
<span class="math display">\[Y_i = \mu_{Y|X_{i}} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)\]</span>
which, in many respects, is very similar to the simple model of Equation <a href="simple-linear-regression.html#eq:simple-model-as-glm">(4.1)</a>. The key difference is the use of a conditional mean <span class="math inline">\(\mu_{Y|X_{i}}\)</span> instead of an unconditional (constant) mean <span class="math inline">\(\mu\)</span>. Effectively, within each group of cases with the same value on the predictor variable <span class="math inline">\(X\)</span> (e.g. all states with 5.55 hate groups), there is just a single conditional mean. Within such a group, the model is equivalent to the simple model of model simple model of Equation <a href="simple-linear-regression.html#eq:simple-model-as-glm">(4.1)</a>. What is new here is that we now also take into account differences between different groups. In particular, we assume that these groups differ just in the conditional mean <span class="math inline">\(\mu_{Y|X}\)</span>, whilst the standard deviation of the errors (<span class="math inline">\(\sigma_\epsilon\)</span>) is the same for all groups. We finally assume that there is a simple, linear relation between the value of <span class="math inline">\(X\)</span> and the conditional mean <span class="math inline">\(\mu_{Y|X}\)</span>. So the regression model extends the simple model of the previous chapter by using a straight line to represent the relation between the (conditional) mean of the dependent variable and a predictor variable. <!-- In fact, the model of the previous chapter is a special case of the regression model. --></p>
<!--
If you were asked to predict the percentage of Trump votes for a state with for example 5.55 hate groups, the conditional mean of the percentage Trump votes for all states with 5.55 hate groups would be a good prediction, because without knowing anything else, it will be impossible to predict whether actual the votes for Trump will be above or below the mean for a given state. Using $\hat{Y}$ to denote the predicted value of the dependent variable, we can thus write:
$$\hat{Y}_i = \beta_0 + \beta_1 \times X_{i}$$

$$Y_i = \hat{Y}_i + \epsilon_i$$
This last equation can be written in words as 
$$\text{data} = \text{prediction} + \text{error}$$
This is just a mathematical truism, you can _always_ make this statement, because the error is simply the difference between the observed values (the data) and the predicted values. But perhaps it helps in thinking about what error is. Moreover, if you look at Equation \@(eq:bivariate-regression-model-definition) again, you might notice that the probabilistic part of the model (i.e., the part involving probability distributions) really only concerns the errors $\epsilon_i$. These are, as already indicated, assumed to be independently drawn from a Normal distribution with a mean of 0 and a standard deviation $\sigma$:
$$\epsilon_i \sim \mathbf{Normal}(0,\sigma)$$
-->
</div>
<div id="sec:04-estimation" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Estimation</h2>
<p>It can be shown that for the model specified in Equation <a href="simple-linear-regression.html#eq:bivariate-regression-model-definition">(4.2)</a>, the maximum likelihood estimates of the model parameters are
<span class="math display" id="eq:simple-regression-intercept-estimate">\[\begin{equation} 
\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \times \overline{X}
\tag{4.4}
\end{equation}\]</span>
for the intercept, and
<span class="math display" id="eq:simple-regression-slope-estimate">\[\begin{equation}
\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}
\tag{4.5}
\end{equation}\]</span>
for the slope. Note that to estimate the intercept, we first need the estimate of the slope. So let’s focus on
this one first. The top part of the division (the numerator) contains a sum of
deviations of the predictor values (<span class="math inline">\(X_i\)</span>) from its average (<span class="math inline">\(\overline{X}\)</span>) multiplied by deviations
of the values <span class="math inline">\(Y_i\)</span> of the dependent variable from its average (<span class="math inline">\(\overline{Y}\)</span>). Let’s consider these
multiplied deviations in more detail. Each deviation is positive (larger than 0) when the value is
higher than the average, and negative (smaller than 0), when the value is lower than the average.
So the multiplied deviations are positive whenever both values are larger than their average, and
whenever both values are below their average (a negative value multiplied by another negative value
is positive). If we were to divide the sum of the multiplied deviations by <span class="math inline">\(n\)</span> (the number of observations),
we’d get the average of these multiplied deviations. This average is also called the <strong>covariance</strong> between
<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:
<span class="math display">\[\text{Cov}(X,Y) =  \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{n}\]</span>
Note that, as an estimator, this provides biased estimates of the true covariance. An unbiased estimator
of the true covariance is obtained by dividing by <span class="math inline">\(n-1\)</span> instead of <span class="math inline">\(n\)</span> (just as for the variance). Going back to our example, the covariance between Trump votes and hate groups would be positive whenever states with higher-than-average Trump votes are generally also states with higher-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally also states with lower-than-average hate groups.
The covariance would be negative, on the other hand, whenever states with higher-than-average Trump votes are generally states with lower-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally states with higher-than-average hate groups. A positive or negative covariance is indicative of a relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Indeed, the well-known
Pearson correlation coefficient is a standardized covariance, where the standardization scales the correlation
to always be between -1 and 1 and involves dividing the covariance by the product of the standard deviations
of both variables:
<span class="math display">\[\begin{align}
r_{X,Y} &amp;= \frac{\text{Cov}(X,Y)}{S_X \times S_Y} \\
&amp;= \frac{\frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{n}}{\sqrt{\frac{\sum_{i=1}^n (X_i - \overline{X})^2}{n}} \times \sqrt{\frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}}}
\end{align}\]</span></p>
<p>Going back to the estimate of the slope (Equation <a href="simple-linear-regression.html#eq:simple-regression-slope-estimate">(4.5)</a>), we can view this as a different way of standardizing the covariance. Looking at the bottom part of the division (the denominator), we can see it consists of the sum of squared deviations of the predictor values from its mean. If we were to divide this sum by the number of observations, we’d get the variance of <span class="math inline">\(X\)</span>. As dividing both the top part (numerator) and bottom part (denominator) in a division by the same value does not affect the outcome of the division (i.e. <span class="math inline">\(\frac{a}{b} = \frac{a/c}{b/c}\)</span>), we can choose to divide both by <span class="math inline">\(n\)</span> so the numerator becomes the covariance and the denominator the variance (we could also divide both by <span class="math inline">\(n-1\)</span> so they become unbiased estimators of the covariance and variance). So an alternative way of computing the estimate of the slope is
<span class="math display">\[\hat{\beta}_1 = \frac{\text{Cov}(X,Y)}{S^2_X}\]</span>
Note that the variance of <span class="math inline">\(X\)</span> equals the product of the standard deviation of <span class="math inline">\(X\)</span> and itself, as <span class="math inline">\(S^2_X = S_X \times S_X\)</span>. So the slope estimate looks quite a bit like the correlation coefficient, where instead of the standard deviation of <span class="math inline">\(Y\)</span>, we use the standard deviation of <span class="math inline">\(X\)</span> twice. With a little algebraic manipulation, we can also state the slope estimate in terms of the correlation as
<span class="math display">\[\hat{\beta}_1 = \frac{S_Y}{S_X} r_{X,Y}\]</span>
The reason for going into these alternative formulations is to show you that the slope tells us something about the relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, just like the covariance and correlation do. If the sample correlation is 0, then so is the estimated slope. It is important to realise that we have been discussing the <em>estimate</em> of the slope, not the true value itself. But the same relations hold for the true values. If we denote the true correlation as <span class="math inline">\(\rho_{X,Y}\)</span>, then the true value of the slope can be defined as
<span class="math display">\[\beta_1 = \frac{\sigma_Y}{\sigma_X} \rho_{X,Y}\]</span>
The true value of the slope is 0 when the true correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> equals <span class="math inline">\(\rho_{X,Y} = 0\)</span>. It would also be 0 if the true standard deviation of <span class="math inline">\(Y\)</span> equals <span class="math inline">\(\sigma_Y = 0\)</span>, but this implies that <span class="math inline">\(Y\)</span> is a constant and that is not a very interesting situation.</p>
<p>That was perhaps a little tortuous, and we haven’t even discussed the estimate of the intercept! Remember that the intercept represents the mean value of <span class="math inline">\(Y\)</span> for all cases where <span class="math inline">\(X=0\)</span>. This average is often not so interesting itself, although in our example, we might be interested in what the average percentage of votes for Trump would be in the absence of any hate groups. Equation <a href="simple-linear-regression.html#eq:simple-regression-intercept-estimate">(4.4)</a> shows that we can estimate this value by adjusting the sample average <span class="math inline">\(\overline{Y}\)</span> by subtracting <span class="math inline">\(\hat{\beta}_1 \times \overline{X}\)</span> from it. How come? Well, it can be shown that <strong>the regression line always passes through the point <span class="math inline">\((\overline{X},\overline{Y})\)</span></strong>. The derivation for showing that this has to be the case is not that interesting, so you’ll just have to trust me, or look it up elsewhere. But this implies that <span class="math display">\[\overline{Y} = \beta_0 + \beta_1 \times \overline{X}\]</span>
and then we can simply subtract <span class="math inline">\(\beta_1 \times \overline{X}\)</span> from both sides to get <span class="math inline">\(\overline{Y} - \beta_1 \times \overline{X} = \beta_0\)</span>.</p>
<p>Finally, we can also estimate <span class="math inline">\(\sigma_\epsilon\)</span>, the standard deviation of the error. As usual for variances and standard deviations, the maximum likelihood estimate is biased, so we’ll focus on an unbiased estimator. Like before, an estimate of the variance is computed from a sum of squared deviations from an estimated mean. In this case, we need to use the estimated <em>conditional</em> means
<span class="math display">\[\hat{\mu}_{Y|X_i} = \hat{\beta}_0 + \hat{\beta}_1 \times X_i\]</span>
to compute the following estimate of the error variance:
<span class="math display" id="eq:simple-regression-sd-error-variance-estimate">\[\begin{equation}
\hat{\sigma}^2_\epsilon = \frac{\sum_{i=1}^n (Y_i - \hat{\mu}_{Y|X_i})^2}{n-2}
\tag{4.6}
\end{equation}\]</span>
Note that we are dividing by <span class="math inline">\(n-2\)</span> here, rather than by <span class="math inline">\(n-1\)</span> as we did when estimating the variance of <span class="math inline">\(Y\)</span> in a model without predictors. The reason for this is that we are now using two noisy parameter estimates (i.e. <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>), rather than just one. As usual, to get the estimate of <span class="math inline">\(\hat{\sigma}_\epsilon\)</span>, we can just take the square root of <span class="math inline">\(\hat{\sigma}^2_\epsilon\)</span>.</p>
<div id="estimating-the-relation-between-trump-votes-and-hate-groups" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Estimating the relation between Trump votes and hate groups</h3>
<p>So, what are the estimates of the model predicting Trump votes by hate groups? We can calculate the estimates relatively easily by first computing the sample means of predictor and dependent variable, their sample standard deviations, and the covariance between the predictor and dependent variable:</p>
<table>
<colgroup>
<col width="19%" />
<col width="20%" />
<col width="14%" />
<col width="15%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(\overline{\mathtt{hate}}\)</span></th>
<th align="right"><span class="math inline">\(\overline{\mathtt{votes}}\)</span></th>
<th align="right"><span class="math inline">\(S^2_\mathtt{hate}\)</span></th>
<th align="right"><span class="math inline">\(S^2_\mathtt{votes}\)</span></th>
<th align="right"><span class="math inline">\(\text{Cov}(\mathtt{hate},\mathtt{votes})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">3.03</td>
<td align="right">49.9</td>
<td align="right">3.71</td>
<td align="right">99.8</td>
<td align="right">8.53</td>
</tr>
</tbody>
</table>
<p>The estimate of the slope is then
<span class="math display">\[\hat{\beta}_1 = \frac{8.527}{3.707} = 2.3\]</span>
and the estimate of the intercept is
<span class="math display">\[\hat{\beta}_0 = 49.862 - 2.3 \times 3.028 = 42.897\]</span></p>
<p>Of course, we would not normally bother with calculating these estimates “hy hand” in this way. We would rely on statistical software such as R or JASP to calculate the estimates for us. In any case, we can write the estimated model as:</p>
<p><span class="math display">\[\texttt{votes}_i = 42.897 + 2.3 \times \texttt{hate}_i + e_i\]</span>
According to this model, the average percentage of Trump votes in states without any hate groups is 42.897. For every one additional hate group (per million citizens), the percentage of Trump votes increases by 2.3. You can view the resulting regression line (the conditional means of votes for Trump as predicted by the model) in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups-with-regression">4.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterplot-trump-votes-hate-groups-with-regression"></span>
<div id="htmlwidget-2653f0c7e43d58c47b84" style="width:95%;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-2653f0c7e43d58c47b84">{"x":{"data":[{"x":[5.55,0,2.6,5.35,2.01,2.89,1.4,4.2,3.06,3.1,0,7.13,2.5,3.92,1.28,2.41,5.18,2.99,2.25,2.99,1.76,2.82,1.81,6.02,3.94,9.59,2.62,1.36,4.5,1.68,0.96,2.38,3.06,1.32,3.01,1.53,2.69,3.13,0.95,2.42,8.09,5.71,1.97,0.98,1.6,4.64,2.88,2.18,1.56,3.42],"y":[62.9,52.9,49.5,60.4,32.7,44.4,41.2,41.9,49.1,51.3,30,59.2,39.4,57.2,51.8,57.2,62.5,58.1,45.2,35.3,33.5,47.6,45.4,58.3,57.1,56.5,60.3,45.5,47.2,41.8,40,37.5,50.5,64.1,52.1,65.3,41.1,48.8,39.8,54.9,61.5,61.1,52.6,45.9,32.6,45,38.2,68.7,47.9,70.1],"text":["hate_groups_per_million: 5.55<br />percent_Trump_votes: 62.9<br />state: Alabama","hate_groups_per_million: 0.00<br />percent_Trump_votes: 52.9<br />state: Alaska","hate_groups_per_million: 2.60<br />percent_Trump_votes: 49.5<br />state: Arizona","hate_groups_per_million: 5.35<br />percent_Trump_votes: 60.4<br />state: Arkansas","hate_groups_per_million: 2.01<br />percent_Trump_votes: 32.7<br />state: California","hate_groups_per_million: 2.89<br />percent_Trump_votes: 44.4<br />state: Colorado","hate_groups_per_million: 1.40<br />percent_Trump_votes: 41.2<br />state: Connecticut","hate_groups_per_million: 4.20<br />percent_Trump_votes: 41.9<br />state: Delaware","hate_groups_per_million: 3.06<br />percent_Trump_votes: 49.1<br />state: Florida","hate_groups_per_million: 3.10<br />percent_Trump_votes: 51.3<br />state: Georgia","hate_groups_per_million: 0.00<br />percent_Trump_votes: 30.0<br />state: Hawaii","hate_groups_per_million: 7.13<br />percent_Trump_votes: 59.2<br />state: Idaho","hate_groups_per_million: 2.50<br />percent_Trump_votes: 39.4<br />state: Illinois","hate_groups_per_million: 3.92<br />percent_Trump_votes: 57.2<br />state: Indiana","hate_groups_per_million: 1.28<br />percent_Trump_votes: 51.8<br />state: Iowa","hate_groups_per_million: 2.41<br />percent_Trump_votes: 57.2<br />state: Kansas","hate_groups_per_million: 5.18<br />percent_Trump_votes: 62.5<br />state: Kentucky","hate_groups_per_million: 2.99<br />percent_Trump_votes: 58.1<br />state: Louisiana","hate_groups_per_million: 2.25<br />percent_Trump_votes: 45.2<br />state: Maine","hate_groups_per_million: 2.99<br />percent_Trump_votes: 35.3<br />state: Maryland","hate_groups_per_million: 1.76<br />percent_Trump_votes: 33.5<br />state: Massachusetts","hate_groups_per_million: 2.82<br />percent_Trump_votes: 47.6<br />state: Michigan","hate_groups_per_million: 1.81<br />percent_Trump_votes: 45.4<br />state: Minnesota","hate_groups_per_million: 6.02<br />percent_Trump_votes: 58.3<br />state: Mississippi","hate_groups_per_million: 3.94<br />percent_Trump_votes: 57.1<br />state: Missouri","hate_groups_per_million: 9.59<br />percent_Trump_votes: 56.5<br />state: Montana","hate_groups_per_million: 2.62<br />percent_Trump_votes: 60.3<br />state: Nebraska","hate_groups_per_million: 1.36<br />percent_Trump_votes: 45.5<br />state: Nevada","hate_groups_per_million: 4.50<br />percent_Trump_votes: 47.2<br />state: New Hampshire","hate_groups_per_million: 1.68<br />percent_Trump_votes: 41.8<br />state: New Jersey","hate_groups_per_million: 0.96<br />percent_Trump_votes: 40.0<br />state: New Mexico","hate_groups_per_million: 2.38<br />percent_Trump_votes: 37.5<br />state: New York","hate_groups_per_million: 3.06<br />percent_Trump_votes: 50.5<br />state: North Carolina","hate_groups_per_million: 1.32<br />percent_Trump_votes: 64.1<br />state: North Dakota","hate_groups_per_million: 3.01<br />percent_Trump_votes: 52.1<br />state: Ohio","hate_groups_per_million: 1.53<br />percent_Trump_votes: 65.3<br />state: Oklahoma","hate_groups_per_million: 2.69<br />percent_Trump_votes: 41.1<br />state: Oregon","hate_groups_per_million: 3.13<br />percent_Trump_votes: 48.8<br />state: Pennsylvania","hate_groups_per_million: 0.95<br />percent_Trump_votes: 39.8<br />state: Rhode Island","hate_groups_per_million: 2.42<br />percent_Trump_votes: 54.9<br />state: South Carolina","hate_groups_per_million: 8.09<br />percent_Trump_votes: 61.5<br />state: South Dakota","hate_groups_per_million: 5.71<br />percent_Trump_votes: 61.1<br />state: Tennessee","hate_groups_per_million: 1.97<br />percent_Trump_votes: 52.6<br />state: Texas","hate_groups_per_million: 0.98<br />percent_Trump_votes: 45.9<br />state: Utah","hate_groups_per_million: 1.60<br />percent_Trump_votes: 32.6<br />state: Vermont","hate_groups_per_million: 4.64<br />percent_Trump_votes: 45.0<br />state: Virginia","hate_groups_per_million: 2.88<br />percent_Trump_votes: 38.2<br />state: Washington","hate_groups_per_million: 2.18<br />percent_Trump_votes: 68.7<br />state: West Virginia","hate_groups_per_million: 1.56<br />percent_Trump_votes: 47.9<br />state: Wisconsin","hate_groups_per_million: 3.42<br />percent_Trump_votes: 70.1<br />state: Wyoming"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[-0.4795,10.0695],"y":[41.7937724109846,66.0607962859094],"text":"","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":37.2602739726027},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.4795,10.0695],"tickmode":"array","ticktext":["0.0","2.5","5.0","7.5","10.0"],"tickvals":[0,2.5,5,7.5,10],"categoryorder":"array","categoryarray":["0.0","2.5","5.0","7.5","10.0"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Hate groups per million citizens","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[27.995,72.105],"tickmode":"array","ticktext":["30","40","50","60","70"],"tickvals":[30,40,50,60,70],"categoryorder":"array","categoryarray":["30","40","50","60","70"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"% votes for Trump","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"source":"A","attrs":{"16206f71e1a94c":{"x":{},"y":{},"name":{},"type":"scatter"},"16206f49e15c09":{"intercept":{},"slope":{}}},"cur_data":"16206f71e1a94c","visdat":{"16206f71e1a94c":["function (y) ","x"],"16206f49e15c09":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 4.3: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens with the estimated regression line.
</p>
</div>
</div>
</div>
<div id="hypothesis-testing" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Hypothesis testing</h2>
<p>The estimated model indicates a positive relation between the number of hate groups and votes for Donald Trump. But the slope is an <em>estimated</em> parameter. It might be that, in reality, there is no relation between hate groups and Trump votes. But because we have limited and noisy data, sometimes the estimated slope might be positive, and other times it might be negative. So the question is whether we have enough evidence to reject the null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>.</p>
<p>As before, there are two main ways in which to look at testing whether the parameters of the model are equal to a priori values. The first is to consider how variable the parameter estimates are under the assumption that the true parameter is identical to the a priori value. The second way is to compare two models, one in which the parameter is fixed to the a priori value, and one where it is freely estimated. Both of these ways will provide us with the same outcome. The model comparison approach is more flexible, however, as it also allows you to test multiple parameters simultaneously. We’ll start with discussing the first method, but it will be the last time until a while. After that, we’ll focus on model comparison throughout.</p>
<div id="sampling-distribution-of-estimates" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Sampling distribution of estimates</h3>
<p>Remember, an estimate (whether of the mean or another parameter) is a noisy reflection of the true value of that parameter. The noise comes from having access only to limited data, not <em>all</em> the data the data generating process can produce. Suppose in reality there is no relation between hate groups and Trump votes, so that the true slope is <span class="math inline">\(\beta_1 = 0\)</span>. In that case, the model becomes
<span class="math display">\[\begin{align}
Y_i &amp;= \beta_0 + 0 \times X_i + \epsilon \\
&amp;= \beta_0 + \epsilon_i
\end{align}\]</span>
which is identical to the simple model in Equation, renaming <span class="math inline">\(\beta_0 = \mu\)</span> and <span class="math inline">\(\sigma_\epsilon = \sigma\)</span>. Then, the true value of the intercept would be <span class="math inline">\(\beta_0 = \mu\)</span>. If we’d know the true value of the standard deviation <span class="math inline">\(\sigma_\epsilon\)</span>, we’d have a fully specified model (a Normal distribution) which we can use to generate as many alternative data sets as we’d like. By generating these data sets, and estimating the slope of our model for each, we can get an overview of the variability of the estimates when the true slope equals <span class="math inline">\(\beta_1 = 0\)</span>. An unbiased estimator ensures that on average, these estimates equal the true value. The main thing of interest is then the variability around this value. A consistent estimator ensures that this variability becomes smaller with the larger data sets. But to understand how well we can estimate the parameter for the present data, we would simulate data sets with the same number of observations (so <span class="math inline">\(n=50\)</span>). Unfortunately, we don’t know the true value of <span class="math inline">\(\sigma\)</span>. Our data provides an estimate of <span class="math inline">\(\sigma\)</span>, but we know this estimate is noisy itself. Thinking in the same way about the sampling distribution of <span class="math inline">\(\hat{\sigma}\)</span>, we could first sample a values of <span class="math inline">\(\sigma\)</span>, and then use each of these to generate a data set for which to estimate <span class="math inline">\(\beta_1\)</span>. Doing this many (many many!) times would give us a good overview of the variability of the estimates. As before, we don’t have to actually simulate the data sets. If the model corresponding to the null hypothesis is true, then we can derive that the sampling distribution of the estimates follows a t-distribution. Thus, we should look for the <span class="math inline">\(t\)</span> value our data provides, and evaluate this value within the context of the sampling distribution derived under the model where <span class="math inline">\(\beta_1 = 0\)</span>. For both parameters (intercept and slope), the same logic applies.</p>
<p>Actually, the same procedure can be used to test the hypothesis that a parameter takes any given value, i.e. <span class="math inline">\(H_0: \beta_j = \underline{\beta}_j\)</span>. Often, the chosen null-value is <span class="math inline">\(\underline{\beta}_j = 0\)</span>, but that does not need to be the case. In general then, for parameters <span class="math inline">\(\beta_j\)</span> (where <span class="math inline">\(j = 0\)</span> or 1), the <span class="math inline">\(t\)</span>-statistic to test the null-hypothesis <span class="math inline">\(H_0: \beta_j = \underline{\beta}_j\)</span> is computed as
<span class="math display">\[\begin{equation}
t = \frac{\hat{\beta}_j - \underline{\beta}_j}{\text{SE}(\hat{\beta}_j)} \quad \quad \quad t \sim \mathbf{T}(n-2)
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{T}(n-2)\)</span> denotes a standard Student t-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, and <span class="math inline">\(\text{SE}(\hat{\beta}_j)\)</span> is the standard error of the estimate, which you should remember is the standard deviation of the sampling distribution of the estimates. I won’t bore you with how to compute this standard error; the equations aren’t overly insightful, and statistical software does a good job at computing standard errors. One thing to realise though is that <em>the computed standard error is valid for datasets with exactly the same values on the predictor</em>. In other words, it assumes the predictor values are fixed (a part of the Data Generating Process). If you’d collect a different dataset with different values for e.g. the number of hate groups, the standard error would also be different.</p>
<p>R would for instance provide the following results for this regression model:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Estimate</th>
<th align="right">Std. Error</th>
<th align="right">t value</th>
<th align="right">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">42.9</td>
<td align="right">2.409</td>
<td align="right">17.80</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">hate_groups_per_million</td>
<td align="right">2.3</td>
<td align="right">0.672</td>
<td align="right">3.43</td>
<td align="right">0.001</td>
</tr>
</tbody>
</table>
<p>The values listed for <code>(intercept)</code> concern <span class="math inline">\(\beta_0\)</span>, and the values listed for <code>hate_groups_per_million</code> concern the slope <span class="math inline">\(\beta_1\)</span>. You can see the parameter estimates are identical to those computed earlier (phew, no mistakes there!). You can see that the standard errors for the two parameters are quite different in magnitude. This is not so surprising, as they reflect quite different things, the intercept being a particular value of the dependent variable, while the slope represents an increase in the dependent variable for an increase in the predictor. If you’d change the scale of the predictor (e.g., from hate groups per million to hate groups per 10,000 citizens), the slope would change, as well as the corresponding standard error. Changing the scale in this way would not affect the standard error of the intercept. The values of the <span class="math inline">\(t\)</span>-statistic are those for tests where the null-hypotheses are <span class="math inline">\(\beta_0 = 0\)</span> and <span class="math inline">\(\beta_1 = 0\)</span>. These test statistics are obtained by simply dividing each estimate by the corresponding standard error (for a different test value <span class="math inline">\(\underline{\beta}_j \neq 0\)</span>, you would first subtract this value from the estimate, and then divide this difference by the standard error. <!--) , you obtain the value of the $t$-statistic (just like we did earlier in Equation \@ref(eq:definition-one-sample-t)).--></p>
<p>In a manner analogous to that depicted in Figure <a href="ch-simple-GLM.html#fig:critical-values-t-distribution">3.6</a>, you then determine critical values for the <span class="math inline">\(t\)</span>-statistic (based on the degrees of freedom and the significance level <span class="math inline">\(\alpha\)</span>), and determine whether the value you computed for the parameters lies within the critical range. If it does, the test result is called significant, and you reject the null hypothesis. If not, then the result is called not significant, and you don’t reject the null hypothesis. Now that these are easy to compute with statistical software, it is more common to check the <span class="math inline">\(p\)</span>-values. Remember, the <span class="math inline">\(p\)</span>-value is the probability of the computed <span class="math inline">\(t\)</span>-statistic <em>or a more extreme value</em>, assuming the null hypothesis is true. If the <span class="math inline">\(p\)</span>-value is smaller than the chosen significance level (e.g. <span class="math inline">\(\alpha = .05\)</span>), that means that the test is significant, and the null hypothesis rejected. In the results above, you can see that both <span class="math inline">\(p\)</span>-values are below the significance level <span class="math inline">\(\alpha = .05\)</span>. Hence, we can reject the null hypothesis that <span class="math inline">\(\beta_0 = 0\)</span> and the null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>. In other words, there is good evidence that, in the absence of hate groups, the percentage of people voting for Trump is <em>not</em> equal to 0.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> Also, there is good evidence that there is a relation between the number of hate groups and Trump votes.</p>
<p>Personally, I find the <span class="math inline">\(t\)</span> statistic quite intuitive in the context of a one-sample t-test. Generalizing the concept to a standardized estimate (dividing the estimate by its standard error) is also reasonably intuitive. However, in a multi-parameter model such as here, the sampling distribution of a single parameter is dependent on the estimation of all the other parameters. For instance, the test of the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> (i.e., no effect of hate groups on Trump votes) is based on deriving the sampling distribution of <span class="math inline">\(\hat{\beta}_1\)</span> in a model where <span class="math inline">\(\beta_1 = \underline{\beta}_1\)</span>, but all the other parameters (i.e. <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\sigma_\epsilon\)</span>) are not assumed known. Hence, for each possible sample, these values would need to be estimated. This uncertainty is dealt with similarly as before, resulting in a <span class="math inline">\(t\)</span>-distribution, but now there are two sources of uncertainty (two parameters to estimate). Hence, the degrees of freedom are <span class="math inline">\(n-2\)</span> here.</p>
</div>
<div id="model-comparison" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Model comparison</h3>
<p>The fact that, in a multi-parameter model, a test of one parameter is <em>not</em> conducted in isolation, but rather in the context of all the other parameters in the model, is more explicit in the model comparison approach. When we compare two models, we have to make clear what the parameters are in each: what are the unknown quantities which we will have to estimate, and what are quantities which we can assume a precise value for?</p>
<p>As before, we will consider comparing <em>nested models</em>, in which a restricted MODEL R is a <em>special case</em> of a more general MODEL G. Sticking to simple linear regression models, the most general model we have is that of Equation <a href="simple-linear-regression.html#eq:bivariate-regression-model-definition">(4.2)</a>.</p>
<p>Just like for the simple model of the previous chapter, it turns out we can compute the likelihood of a model as a function of a <em>sum of squared deviations</em>. We will not go through the derivation of this again, as it is rather similar and equally tedious. But, because we are now calling these deviations <em>errors</em>, will start to refer to them as a <strong>Sum of Squared Errors</strong> (SSE). The SSE is an overall measure of model error, whilst the likelihood is an overall measure of model fit. The SSE is inversely related to the likelihood: the higher the SSE, the lower the likelihood of the model.</p>
<p>Let’s call the model of Equation <a href="simple-linear-regression.html#eq:bivariate-regression-model-definition">(4.2)</a> MODEL G. Before we go on, I want to warn you that what we call MODEL G, and what we call MODEL R, can change from situation to situation. Basically, the identity of MODEL G and MODEL R are “local” to the particular model comparison. You can think of MODEL G as a parent, and MODEL R as a child. While the relation between them is similar, within an extended family, someone can be both a parent to one family member, and be the child of another. This is the form of flexibility that you will need when thinking about nested models. A model can be both more general than one model, and more restricted than another. I will come back to this soon.</p>
<p>First, let’s consider what the Sum of Squared Errors of a model is. The easiest way to define this is in terms of the <strong>predictions</strong> of each model. Recall that a regression model has a structural and random part. The structural part defines the conditional mean of the dependent variable, while the random part concerns the random variation of the actual observations around the conditional means. As the random part is by definition unpredictable, there is not much we can do with that in terms of forming predictions. So we’re stuck with the structural part. Although not a universal principle, there are many situations in which it makes sense to predict outcomes by the (conditional) mean. If we do this, then we can predict the outcomes with MODEL G as</p>
<p><span class="math display">\[\hat{Y}_{G,i} = \hat{\beta}_{G,0} + \hat{\beta}_{G,1} \times X_i\]</span>
where <span class="math inline">\(\hat{Y}\)</span> stands for a predicted value. We’re using the same “hat” for this as an <em>estimated value</em>, because it is really also a good estimate of what the value of <span class="math inline">\(Y_i\)</span> might have been if it was another observation with the same value for <span class="math inline">\(X\)</span>. We are also assigning the subscript “G” to all the estimates, to distinguish them from those of a different model. Now let’s consider a MODEL R in which we assume that there is no relation between Trump votes and hate groups, so that <span class="math inline">\(\beta_1 = 0\)</span>. The predictions for this model would be</p>
<p><span class="math display">\[\hat{Y}_{R,i} = \hat{\beta}_{R,0} + 0 \times X_i = \hat{\beta}_{R,0}\]</span>
Now we have two models to make predictions, we can write the corresponding Sum of Squared Error of each as:
<span class="math display" id="eq:SSE-GLM-general">\[\begin{equation}
\text{SSE}(M) = \sum_{i=1}^n \left(Y_i - \hat{Y}_{M,i} \right)^2
\tag{4.7}
\end{equation}\]</span>
where we can replace the general letter <span class="math inline">\(M\)</span> (for Model) with either <span class="math inline">\(G\)</span> of <span class="math inline">\(R\)</span>, to get <span class="math inline">\(\text{SSE}(G)\)</span> or <span class="math inline">\(\text{SSE}(R)\)</span>, respectively. So the Sum of Squares is based on a difference between each observation and the model prediction for that observation. These are thus the <em>prediction errors</em>. If the prediction was equal to the the prediction of the true model (e.g. <span class="math inline">\(\beta_0 + \beta_1 X_{i}\)</span>), then these would be equal to the true error terms <span class="math inline">\(\epsilon_i\)</span>. But because we only have an estimated model, they are effectively estimates of the true errors. You can see these (unsquared) errors for the two models in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison">4.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison"></span>
<img src="_main_files/figure-html/scatterplot-trump-votes-hate-groups-with-regression-model-comparison-1.svg" alt="Estimated regression lines for MODEL G (left) and MODEL R (right) with $\beta_1 = 0$ and the errors." width="100%" />
<p class="caption">
Figure 4.4: Estimated regression lines for MODEL G (left) and MODEL R (right) with <span class="math inline">\(\beta_1 = 0\)</span> and the errors.
</p>
</div>
<p>When you compare the errors between the models, you can see that MODEL G does not provide a better prediction for <em>each</em> observation. Sometimes the distance from an observation to the regression line is larger for MODEL G than for MODEL R. However, MODEL G does appear to provide a better prediction for <em>most</em> observations. This is unsurprising. MODEL G is estimated by maximising the likelihood, and for models with Normal-distributed errors, maximising the likelihood is equivalent to minimising the Sum of Squared Errors. As the likelihood of MODEL G can never be lower than that of MODEL R, the Sum of Squared Errors of MODEL G can never be higher than that of MODEL R. In other words, it is <em>always the case that</em>
<span class="math display">\[\text{SSE}(R) \geq \text{SSE}(G)\]</span>
The Sum of Squared Errors of MODEL G will never be higher than that of MODEL R. So we cannot just select the model with the lowest SSE, as this would mean we’d always select MODEL G. We need to find a way to determine whether <span class="math inline">\(\text{SSE}(G)\)</span> is sufficiently lower than <span class="math inline">\(\text{SSE}(R)\)</span> to make us believe that MODEL G is indeed superior to MODEL R.</p>
<p>In the previous chapter, we discussed that the <span class="math inline">\(t\)</span>-statistic can be used to perform a test which is equivalent to the likelihood ratio test, and that this was useful because the sampling distribution of the <span class="math inline">\(t\)</span>-statistic is known, while the sampling distribution of the likelihood ratio is difficult to determine. For the general linear models, there is a similar argument that leads to a new statistic, which can be viewed as a generalization of the <span class="math inline">\(t\)</span>-statistic. This is the <span class="math inline">\(F\)</span>-statistic. If was given the letter in honour of Sir Ronald A. Fisher, a rather brilliant and very influential statistician, who derived the statistic in the 1920s.</p>
<p>For comparing two linear models, where MODEL R is a special case of MODEL G, we can define the <span class="math inline">\(F\)</span>-statistic as:
<span class="math display" id="eq:glm-f-definition">\[\begin{equation}
F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar(G)}}}
\tag{4.8}
\end{equation}\]</span>
Here, <span class="math inline">\(\text{npar}(G)\)</span> stands for the number of parameters which are estimated in MODEL G, and <span class="math inline">\(\text{npar}(R)\)</span> for the number of parameters which are estimated in MODEL R. In counting the number of estimated parameters, we are excluding the standard deviation of the errors <span class="math inline">\(\sigma_\epsilon\)</span>.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> In the present example, MODEL G then has two estimated parameters: <span class="math inline">\(\hat{\beta}_{G,0}\)</span> and <span class="math inline">\(\hat{\beta}_{G,1}\)</span>, while MODEL R has just one: <span class="math inline">\(\hat{\beta}_{R,0}\)</span>. If we compute the SSE for each model (i.e. taking the vertical lines to the regression line in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison">4.4</a> and squaring them), we get <span class="math inline">\(\text{SSE}(G) = 4011.398\)</span> and <span class="math inline">\(\text{SSE}(R) = 4992.178\)</span>. Putting the values into Equation <a href="simple-linear-regression.html#eq:glm-f-definition">(4.8)</a> gives as the following value for the <span class="math inline">\(F\)</span>-statistic:</p>
<p><span class="math display">\[\begin{aligned}
F &amp;= \frac{\frac{4992.178 -  4011.398}{2 - 1}}{\frac{4011.398}{50 - 2}} \\
&amp;= \frac{980.779}{83.571} \\
&amp;= 11.736
\end{aligned}\]</span></p>
If MODEL R is true, the sampling distribution of the <span class="math inline">\(F\)</span>-statistic follows an <span class="math inline">\(F\)</span>-distribution. This distribution has two parameters, and both are degrees of freedom. We will refer to these as <span class="math inline">\(\text{df}_1\)</span> and <span class="math inline">\(\text{df}_2\)</span>. The first one reflects the difference in the number of estimated parameters between the models
<span class="math display" id="eq:glm-df1-general">\[\begin{equation}
\text{df}_1 = \text{npar}(G) - \text{npar}(R)
\tag{4.9}
\end{equation}\]</span>
and the second one the number of observations minus the number of estimated parameters of the more general model
<span class="math display" id="eq:glm-df2-general">\[\begin{equation}
\text{df}_2 = n - \text{npar}(G)
\tag{4.10}
\end{equation}\]</span>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:f-distribution-plot"></span>
<img src="_main_files/figure-html/f-distribution-plot-1.svg" alt="The F distribution with $\text{df}_1 = 1$ and $\text{df}_2 = 48$, and the critical value for $\alpha = .05$" width="70%" />
<p class="caption">
Figure 4.5: The F distribution with <span class="math inline">\(\text{df}_1 = 1\)</span> and <span class="math inline">\(\text{df}_2 = 48\)</span>, and the critical value for <span class="math inline">\(\alpha = .05\)</span>
</p>
</div>
<p>Note that the value of the <span class="math inline">\(F\)</span>-statistic can never be negative; the distribution is thus defined only over positive values of <span class="math inline">\(F\)</span>. High values of <span class="math inline">\(F\)</span> indicate that the MODEL G has substantially less error than MODEL R. Another thing to note is that, <strong>whenever <span class="math inline">\(\text{df}_1 = 1\)</span>, there is a direct relation between the <span class="math inline">\(F\)</span> statistic and the <span class="math inline">\(t\)</span> statistic</strong>: <span class="math inline">\(F = t^2\)</span>, or conversely, <span class="math inline">\(\sqrt{F} = t\)</span>. It is easy to check that this is indeed the case here: <span class="math inline">\(\sqrt{11.736} = 3.426\)</span>, which is the value of the <span class="math inline">\(t\)</span> statistic we computed earlier.</p>
<p>The critical value of the <span class="math inline">\(F\)</span> statistic, with <span class="math inline">\(\alpha = .05\)</span> and <span class="math inline">\(\text{df}_1 = 2-1 = 1\)</span> and <span class="math inline">\(\text{df}_2 = 50 - 2 = 48\)</span>, is 4.043, and any value of <span class="math inline">\(F\)</span> above this critical value would result in a rejection of the null hypothesis. So, because the <span class="math inline">\(F\)</span> value we computed was 11.736, which is larger than the critical value, we reject the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>, and conclude that there is evidence of a relation between the number of hate groups and Trump votes. Instead of checking whether the <span class="math inline">\(F\)</span> value is larger than a critical value, we can also compute the <span class="math inline">\(p\)</span>-value, which is the probability of an <span class="math inline">\(F\)</span> value equal to the sample value 11.736 in the distribution depicted in Figure <a href="simple-linear-regression.html#fig:f-distribution-plot">4.5</a>. The <span class="math inline">\(p\)</span>-value of this test can be stated as <span class="math inline">\(P(F \geq 11.736 | \text{df}_1 = 1, \text{df}_2 = 48) = 0.00127\)</span>. As this probability is smaller than the significance level <span class="math inline">\(\alpha = .05\)</span>, this again implies we reject the null hypothesis.</p>
We can also compare MODEL G to a different MODEL R, in which we assume the intercept equals <span class="math inline">\(\beta_0 = 0\)</span>, whilst allowing the slope to take any value. This model forces the regression line to go through the (0,0) point. Estimating this MODEL R gives<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>
<span class="math display">\[\begin{aligned}
Y_i &amp;= 0 + \hat{\beta}_{R,1} \times X_i \\
&amp;= 12.389 \times X_i
\end{aligned}\]</span>
You can see the resulting regression line and error terms in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2">4.6</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2"></span>
<img src="_main_files/figure-html/scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2-1.svg" alt="Estimated regression lines for MODEL G (left) and MODEL R (right) with $\beta_1 = 0$ and the errors." width="100%" />
<p class="caption">
Figure 4.6: Estimated regression lines for MODEL G (left) and MODEL R (right) with <span class="math inline">\(\beta_1 = 0\)</span> and the errors.
</p>
</div>
<p>Visually, this alternative MODEL R seems clearly inferior to MODEL G. The SSE of this model is $(R) = $. Computing the <span class="math inline">\(F\)</span>-statistic gives
<span class="math display">\[
\begin{aligned}
F &amp;= \frac{\frac{ -  4011.398}{2 - 1}}{\frac{4011.398}{50 - 2}} \\
&amp;= \frac{}{83.571} \\
&amp;= 316.981
\end{aligned}
\]</span>
Because the test involves the same degrees of freedom, the critical value is the same as before. So we reject the null hypothesis again, which here is <span class="math inline">\(H_0: \beta_0 = 0\)</span>. The <span class="math inline">\(p\)</span>-value now is <span class="math inline">\(P(F \geq 316.981 | \text{df}_1 = 1, \text{df}_2 = 48) &lt;.0001.\)</span></p>
<p>A nice thing about the <span class="math inline">\(F\)</span>-statistic is that it is very general, and can be used to compare any two nested linear models. For instance, we could also compare MODEL G to a model where we assume both <span class="math inline">\(\beta_0 = 0\)</span> <em>and</em> <span class="math inline">\(\beta_1 = 0\)</span>. This model would assume that the dependent variable follows a Normal distribution with a mean of 0. That doesn’t make much sense here, so we won’t compute this test. But the generality of the <span class="math inline">\(F\)</span>-test to allow testing of multiple parameters simultaneously comes in very handy in the later chapters.</p>
<!--
I would like to point out that the discrepancy $Y_i - \hat{Y}_{M,i}$ is  a reasonable estimate of the error term $\epsilon_i$, as it is straightforward to rewrite the model as 
$$\begin{align}
\epsilon_i &= Y_i - (\beta_0 + \beta_1 \times X_i) \\
&= Y_i - \mu_{Y|X{_{i}}}
\end{align}$$
and we have just indicated that $\hat{Y}_i = \hat{\mu}_{Y|X{_{i}}}$. Replacing the conditional mean by its estimate, and realizing that ny using an estimate for this, we than also can't be certain about the true value of $\epsilon$, which would also turn that into an estimate, we then write this formally as:  
$$\hat{\epsilon}_{M,i} = Y_i - \hat{Y}_{M,i}$$
This in turn is a somewhat convoluted way to say that the prediction errors would be a reasonable way to estimate the error variance or standard deviation $\sigma_\epsilon$.
-->
<!--



|                        | Df| Sum Sq| Mean Sq| F value| Pr(>F)|
|:-----------------------|--:|------:|-------:|-------:|------:|
|hate_groups_per_million |  1|    981|   980.8|    11.7|  0.001|
|Residuals               | 48|   4011|    83.6|      NA|     NA|

-->
</div>
<div id="confidence-intervals" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Confidence intervals</h3>
<p>The way to compute and interpret confidence intervals for the parameters of a simple linear regression model is analoguous to that for the one sample <span class="math inline">\(t\)</span>-test (see Section <a href="ch-simple-GLM.html#ch3-confidence-interval">3.5</a>). The formula to compute confidence intervals for the two parameters can be written as:
<span class="math display">\[\hat{\beta}_j \pm t_{n-2; 1-\tfrac{\alpha}{2}} \times \text{SE}(\hat{\beta}_j)\]</span>
where <span class="math inline">\(t_{n-2; 1-\tfrac{\alpha}{2}}\)</span> is the upper critical value in a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom and a significance level of <span class="math inline">\(\alpha\)</span>. Using <span class="math inline">\(\alpha=.05\)</span> gives us the conventional 95%-confidence interval.</p>
<!-- 
## Assumptions and outliers

<div class="figure" style="text-align: center">
<img src="_main_files/figure-html/boxplots-trump-votes-hate-groups-1.svg" alt="Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens" width="50%" /><img src="_main_files/figure-html/boxplots-trump-votes-hate-groups-2.svg" alt="Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens" width="50%" />
<p class="caption">(\#fig:boxplots-trump-votes-hate-groups)Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens</p>
</div>




<div class="figure" style="text-align: center">
<img src="_main_files/figure-html/simple-regression-trump2016-residuals-plots-1.svg" alt="Predicted vs residual plot and a QQ plot of the residuals" width="50%" /><img src="_main_files/figure-html/simple-regression-trump2016-residuals-plots-2.svg" alt="Predicted vs residual plot and a QQ plot of the residuals" width="50%" />
<p class="caption">(\#fig:simple-regression-trump2016-residuals-plots)Predicted vs residual plot and a QQ plot of the residuals</p>
</div>

## Summary

-->
</div>
</div>
<div id="summary-2" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Summary</h2>
<p>A simple linear regression is a model of the relation between two variables: the dependent variable <span class="math inline">\(Y\)</span> and a predictor variable <span class="math inline">\(X\)</span>. The model uses a straight line to associate conditional means, which are expected or average value of the dependent variable, to each possible value of the predictor variable. This straight line has two important parameters. The first one is the intercept (<span class="math inline">\(\beta_0\)</span>), which is the conditional mean (<span class="math inline">\(\mu_{Y|X}\)</span>) of the dependent variable for cases where the predictor variable has the value <span class="math inline">\(X=0\)</span>. The second is the slope <span class="math inline">\(\beta_1\)</span>, which reflects the increase in the conditional mean for every one-unit increase in the dependent variable. This is then related the steepness of the line. The model assumes variability in the values of the dependent variable around these conditional means to follow a Normal distribution, with a mean of 0 and a constant standard deviation (<span class="math inline">\(\sigma_\epsilon\)</span>).</p>
<p>Hypothesis testing for the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> can be based on the sampling distribution of the estimates of these parameters under the null-hypothesis, which leads to a t-test. Alternatively, you can perform these hypothesis tests by model comparison, comparing the Sum of Squared Error of each model with an <span class="math inline">\(F\)</span>-test. This is equivalent to a likelihood ratio test, but Sums of Squared Errors and the resulting <span class="math inline">\(F\)</span>-distribution are easier to work with.</p>
<p>A simple (bivariate) regression model is a special case of a <em>multiple regression model</em>, which we will discuss next. As many things that apply to multiple regression models apply to simple regression models as well, we will discuss things like effect sizes and assessing assumptions, in the next chapter.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-schaffner_understanding_2018" class="csl-entry">
Schaffner, B. F., Macwilliams, M., &amp; Nteta, T. (2018). Understanding white polarization in the 2016 vote for president: The sobering role of racism and sexism. <em>Political Science Quarterly</em>, <em>133</em>, 9–34.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>If you are wondering why this formation provides the same model as in the previous chapter, you can show this using the properties of the Normal distribution. In the second formulation, you can view <span class="math inline">\(Y\)</span> as a linear transformation of the error <span class="math inline">\(\epsilon\)</span>: <span class="math inline">\(Y_i = a + b \epsilon\)</span>, with <span class="math inline">\(a = \mu\)</span> and <span class="math inline">\(b=1\)</span>. The mean of <span class="math inline">\(Y\)</span> is then <span class="math inline">\(a + \mu_\epsilon = \mu + 0 = \mu\)</span>, and the standard deviation of <span class="math inline">\(Y\)</span> equals <span class="math inline">\(\sqrt{|b| \sigma} = 1 \times \sigma = \sigma\)</span>, which are the same parameters as when directly specifying the Normal distribution for <span class="math inline">\(Y\)</span>.<a href="simple-linear-regression.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>It’s probably unwise to mix statistics and politics, but wouldn’t that have been an utopia?<a href="simple-linear-regression.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>Although <span class="math inline">\(\sigma_\epsilon\)</span> is really a parameter in the models, it is generally not of direct interest. It is a so-called “nuisance parameter,” something that we need to take into account, but we’d rather forget about.<a href="simple-linear-regression.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Note that we cannot use Equation <a href="simple-linear-regression.html#eq:simple-regression-slope-estimate">(4.5)</a> to estimate the slope now. There is no simple formula for the slope estimate when you fix the intercept to 0, and so you will have to rely on statistical software to do this.<a href="simple-linear-regression.html#fnref10" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-simple-GLM.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-multiple-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"source": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
