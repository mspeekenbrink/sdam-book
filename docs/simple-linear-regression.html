<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Simple linear regression | Statistics: data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Simple linear regression | Statistics: data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Simple linear regression | Statistics: data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2020-11-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-simple-GLM.html"/>
<link rel="next" href="multiple-regression.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="book_assets/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="book_assets/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.0/anchor-sections.js"></script>
<script src="book_assets/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<script src="book_assets/viz-1.8.2/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.6.1/grViz.js"></script>
<script src="book_assets/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-1.52.2/plotly-latest.min.js"></script>
<link href="book_assets/combineWidgetStyle-0.1/combineWidgets.css" rel="stylesheet" />
<script src="book_assets/combineWidgets-binding-0.10.1/combineWidgets.js"></script>
<link href="book_assets/rglwidgetClass-2/rgl.css" rel="stylesheet" />
<script src="book_assets/rglwidgetClass-2/rglClass.src.js"></script>
<script src="book_assets/CanvasMatrix4-2016/CanvasMatrix.src.js"></script>
<script src="book_assets/rglWebGL-binding-0.100.54/rglWebGL.js"></script>
<script src="book_assets/rglPlayer-binding-0.100.54/rglPlayer.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-modelling.html"><a href="statistical-modelling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#what-is-probability"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-modelling.html"><a href="statistical-modelling.html#flipping-a-biased-coin-an-alternative-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-modelling.html"><a href="statistical-modelling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a><ul>
<li class="chapter" data-level="2.4.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="statistical-modelling.html"><a href="statistical-modelling.html#comparing-models-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a><ul>
<li class="chapter" data-level="2.5.1" data-path="statistical-modelling.html"><a href="statistical-modelling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="statistical-modelling.html"><a href="statistical-modelling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="statistical-modelling.html"><a href="statistical-modelling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-modelling.html"><a href="statistical-modelling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="statistical-modelling.html"><a href="statistical-modelling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="statistical-modelling.html"><a href="statistical-modelling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a><ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-an-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has an specific value</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#confidence-intervals"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.6</b> Assumptions</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.7</b> The Central Limit Theorem</a><ul>
<li class="chapter" data-level="3.7.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.7.1</b> The Central Limit Theorem in action</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a><ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a><ul>
<li class="chapter" data-level="5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#effect-size-and-the-importance-of-predictors"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a><ul>
<li class="chapter" data-level="5.6.1" data-path="multiple-regression.html"><a href="multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="multiple-regression.html"><a href="multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assumptions</a></li>
<li class="chapter" data-level="5.8" data-path="multiple-regression.html"><a href="multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.8</b> Multicollinearity: Redundancy between predictors</a><ul>
<li class="chapter" data-level="5.8.1" data-path="multiple-regression.html"><a href="multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.8.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="multiple-regression.html"><a href="multiple-regression.html#outliers"><i class="fa fa-check"></i><b>5.9</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a><ul>
<li class="chapter" data-level="6.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#dont-forget-about-fun-a-model-with-multiple-interactions"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a><ul>
<li class="chapter" data-level="6.2.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a><ul>
<li class="chapter" data-level="7.1" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#comparing-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a><ul>
<li class="chapter" data-level="7.4.1" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="a-model-of-means-anova.html"><a href="a-model-of-means-anova.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.6</b> Multiple testing and post-hoc tests</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="factorial-anova.html"><a href="factorial-anova.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a><ul>
<li class="chapter" data-level="8.1" data-path="factorial-anova.html"><a href="factorial-anova.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a><ul>
<li class="chapter" data-level="8.1.1" data-path="factorial-anova.html"><a href="factorial-anova.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="factorial-anova.html"><a href="factorial-anova.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a><ul>
<li class="chapter" data-level="8.2.1" data-path="factorial-anova.html"><a href="factorial-anova.html#main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="factorial-anova.html"><a href="factorial-anova.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.3</b> A threeway factorial ANOVA</a></li>
<li class="chapter" data-level="8.4" data-path="factorial-anova.html"><a href="factorial-anova.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.4</b> Orthogonal contrast codes and unequal sample sizes</a><ul>
<li class="chapter" data-level="8.4.1" data-path="factorial-anova.html"><a href="factorial-anova.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.4.1</b> Comparison schemes and SS types</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a></li>
<li class="chapter" data-level="10" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a></li>
<li class="chapter" data-level="11" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a></li>
<li class="chapter" data-level="12" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html"><i class="fa fa-check"></i><b>12</b> Introduction to Bayesian hypothesis testing</a></li>
<li class="chapter" data-level="13" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html"><i class="fa fa-check"></i><b>13</b> Being a responsible data analyst</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Simple linear regression</h1>
<p>In this chapter, we continue our journey into the General Linear Model by extending the very simple model of the previous chapter to include a predictor. The resulting model assumes there is a linear relation between the dependent variable and the predictor, and is also known as a simple linear regression model. We will look at the parameters of the model in detail, and discuss their estimation, as well as testing whether their values are equal to a priori ones of interest.</p>
<div id="trump-votes-and-hate-groups" class="section level2">
<h2><span class="header-section-number">4.1</span> Trump, votes, and hate groups</h2>
<p>Donald Trump is perhaps the most divisive president in American history. The 2016 US elections were mired in controversy. <a href="https://www.theguardian.com/world/2020/mar/18/white-nationalist-hate-groups-southern-poverty-law-center">Some reports</a> indicate the number of white nationalist hate groups have gone up by 55% in the Trump era. One study found a strong relation between Trump support and anti-immigrant sentiment, racism, and sexism <span class="citation">(Schaffner, Macwilliams, &amp; Nteta, <a href="#ref-schaffner_understanding_2018" role="doc-biblioref">2018</a>)</span>.</p>
<p><a href="https://www.csiwithoutdeadbodies.com/2017/02/hate-groups-and-trumps-vote-predictive.html">Paul Ricci</a> collated data about the number of hate groups in the different US states and votes for Trump. A scatterplot of these two variables is provided in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups">4.1</a>. You can see that states with relatively little hate groups appear to also have relatively little votes for Trump, while states with a relatively large number of hate groups have relatively many votes for Trump.</p>
<div class="figure" style="text-align: center"><span id="fig:scatterplot-trump-votes-hate-groups"></span>
<div id="htmlwidget-b38adca41469f51a3e50" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-b38adca41469f51a3e50">{"x":{"data":[{"x":[5.55,0,2.6,5.35,2.01,2.89,1.4,4.2,3.06,3.1,0,7.13,2.5,3.92,1.28,2.41,5.18,2.99,2.25,2.99,1.76,2.82,1.81,6.02,3.94,9.59,2.62,1.36,4.5,1.68,0.96,2.38,3.06,1.32,3.01,1.53,2.69,3.13,0.95,2.42,8.09,5.71,1.97,0.98,1.6,4.64,2.88,2.18,1.56,3.42],"y":[62.9,52.9,49.5,60.4,32.7,44.4,41.2,41.9,49.1,51.3,30,59.2,39.4,57.2,51.8,57.2,62.5,58.1,45.2,35.3,33.5,47.6,45.4,58.3,57.1,56.5,60.3,45.5,47.2,41.8,40,37.5,50.5,64.1,52.1,65.3,41.1,48.8,39.8,54.9,61.5,61.1,52.6,45.9,32.6,45,38.2,68.7,47.9,70.1],"text":["hate_groups_per_million: 5.55<br />percent_Trump_votes: 62.9<br />state: Alabama","hate_groups_per_million: 0.00<br />percent_Trump_votes: 52.9<br />state: Alaska","hate_groups_per_million: 2.60<br />percent_Trump_votes: 49.5<br />state: Arizona","hate_groups_per_million: 5.35<br />percent_Trump_votes: 60.4<br />state: Arkansas","hate_groups_per_million: 2.01<br />percent_Trump_votes: 32.7<br />state: California","hate_groups_per_million: 2.89<br />percent_Trump_votes: 44.4<br />state: Colorado","hate_groups_per_million: 1.40<br />percent_Trump_votes: 41.2<br />state: Connecticut","hate_groups_per_million: 4.20<br />percent_Trump_votes: 41.9<br />state: Delaware","hate_groups_per_million: 3.06<br />percent_Trump_votes: 49.1<br />state: Florida","hate_groups_per_million: 3.10<br />percent_Trump_votes: 51.3<br />state: Georgia","hate_groups_per_million: 0.00<br />percent_Trump_votes: 30.0<br />state: Hawaii","hate_groups_per_million: 7.13<br />percent_Trump_votes: 59.2<br />state: Idaho","hate_groups_per_million: 2.50<br />percent_Trump_votes: 39.4<br />state: Illinois","hate_groups_per_million: 3.92<br />percent_Trump_votes: 57.2<br />state: Indiana","hate_groups_per_million: 1.28<br />percent_Trump_votes: 51.8<br />state: Iowa","hate_groups_per_million: 2.41<br />percent_Trump_votes: 57.2<br />state: Kansas","hate_groups_per_million: 5.18<br />percent_Trump_votes: 62.5<br />state: Kentucky","hate_groups_per_million: 2.99<br />percent_Trump_votes: 58.1<br />state: Louisiana","hate_groups_per_million: 2.25<br />percent_Trump_votes: 45.2<br />state: Maine","hate_groups_per_million: 2.99<br />percent_Trump_votes: 35.3<br />state: Maryland","hate_groups_per_million: 1.76<br />percent_Trump_votes: 33.5<br />state: Massachusetts","hate_groups_per_million: 2.82<br />percent_Trump_votes: 47.6<br />state: Michigan","hate_groups_per_million: 1.81<br />percent_Trump_votes: 45.4<br />state: Minnesota","hate_groups_per_million: 6.02<br />percent_Trump_votes: 58.3<br />state: Mississippi","hate_groups_per_million: 3.94<br />percent_Trump_votes: 57.1<br />state: Missouri","hate_groups_per_million: 9.59<br />percent_Trump_votes: 56.5<br />state: Montana","hate_groups_per_million: 2.62<br />percent_Trump_votes: 60.3<br />state: Nebraska","hate_groups_per_million: 1.36<br />percent_Trump_votes: 45.5<br />state: Nevada","hate_groups_per_million: 4.50<br />percent_Trump_votes: 47.2<br />state: New Hampshire","hate_groups_per_million: 1.68<br />percent_Trump_votes: 41.8<br />state: New Jersey","hate_groups_per_million: 0.96<br />percent_Trump_votes: 40.0<br />state: New Mexico","hate_groups_per_million: 2.38<br />percent_Trump_votes: 37.5<br />state: New York","hate_groups_per_million: 3.06<br />percent_Trump_votes: 50.5<br />state: North Carolina","hate_groups_per_million: 1.32<br />percent_Trump_votes: 64.1<br />state: North Dakota","hate_groups_per_million: 3.01<br />percent_Trump_votes: 52.1<br />state: Ohio","hate_groups_per_million: 1.53<br />percent_Trump_votes: 65.3<br />state: Oklahoma","hate_groups_per_million: 2.69<br />percent_Trump_votes: 41.1<br />state: Oregon","hate_groups_per_million: 3.13<br />percent_Trump_votes: 48.8<br />state: Pennsylvania","hate_groups_per_million: 0.95<br />percent_Trump_votes: 39.8<br />state: Rhode Island","hate_groups_per_million: 2.42<br />percent_Trump_votes: 54.9<br />state: South Carolina","hate_groups_per_million: 8.09<br />percent_Trump_votes: 61.5<br />state: South Dakota","hate_groups_per_million: 5.71<br />percent_Trump_votes: 61.1<br />state: Tennessee","hate_groups_per_million: 1.97<br />percent_Trump_votes: 52.6<br />state: Texas","hate_groups_per_million: 0.98<br />percent_Trump_votes: 45.9<br />state: Utah","hate_groups_per_million: 1.60<br />percent_Trump_votes: 32.6<br />state: Vermont","hate_groups_per_million: 4.64<br />percent_Trump_votes: 45.0<br />state: Virginia","hate_groups_per_million: 2.88<br />percent_Trump_votes: 38.2<br />state: Washington","hate_groups_per_million: 2.18<br />percent_Trump_votes: 68.7<br />state: West Virginia","hate_groups_per_million: 1.56<br />percent_Trump_votes: 47.9<br />state: Wisconsin","hate_groups_per_million: 3.42<br />percent_Trump_votes: 70.1<br />state: Wyoming"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":37.2602739726027},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.4795,10.0695],"tickmode":"array","ticktext":["0.0","2.5","5.0","7.5","10.0"],"tickvals":[0,2.5,5,7.5,10],"categoryorder":"array","categoryarray":["0.0","2.5","5.0","7.5","10.0"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Hate groups per million citizens","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[27.995,72.105],"tickmode":"array","ticktext":["30","40","50","60","70"],"tickvals":[30,40,50,60,70],"categoryorder":"array","categoryarray":["30","40","50","60","70"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"% votes for Trump","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"202db95a07d39d":{"x":{},"y":{},"name":{},"type":"scatter"}},"cur_data":"202db95a07d39d","visdat":{"202db95a07d39d":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 4.1: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens
</p>
</div>
<p>To assess the relation between Trump votes and the number of hate groups, we will use a linear regression model. As the name suggests, this model involves a line, a straight one in fact. This straight line represents the mean value of the dependent variable <span class="math inline">\(Y\)</span> (i.e. the percentage of Trump votes) for cases which have a particular value for the predictor (or independent) variable <span class="math inline">\(X\)</span> (i.e. the number of hate groups). The model concerns e.g. the mean percentage of Trump votes in all states with 5.55 hate groups per 1 million citizens, or all states with 0 hate groups per 1 million citizens. The model allows for variation around the mean of the actual percentages of Trump votes in all states with that particular number of hate groups per 1 million citizens. In particular, it assumes that these actual percentages are Normal-distributed.</p>
</div>
<div id="the-model" class="section level2">
<h2><span class="header-section-number">4.2</span> The model</h2>
<p>In the previous chapter, we used a simple statistical model:
<span class="math display">\[Y_i \sim \mathbf{Normal}(\mu,\sigma)\]</span>
This model assumes that each observation is independently drawn from a Normal distribution, with a mean <span class="math inline">\(\mu\)</span> and a standard deviation <span class="math inline">\(\sigma\)</span>. We can state this model in an equivalent way as:
<span class="math display" id="eq:simple-model-as-glm">\[\begin{equation}
Y_i = \mu + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma)
\tag{4.1}
\end{equation}\]</span>
In this two-part formulation, the formula on the left side decomposes observation in a <em>structural part</em> (here the mean, <span class="math inline">\(\mu\)</span>), and a <em>random part</em> that we usually call the error (<span class="math inline">\(\epsilon_i\)</span>). We don’t know much about this error. We just assume that is is drawn from a Normal distribution with a mean of 0 and standard deviation <span class="math inline">\(\sigma_\epsilon\)</span>. This is stated in the formula on the right.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>A model like this doesn’t allow for very precise predictions. As the error term is assumed to be completely random, it is unpredictable. So if you were asked to predict the value of <span class="math inline">\(Y\)</span>, all you could really do is to use <span class="math inline">\(\mu\)</span> as your prediction. If, however, the dependent variable (e.g. votes for Trump) is related to an independent or predictor variable (e.g. number of hate groups), then we would like to be able to use the predictor to adjust our prediction. <strong>Linear regression</strong> allows us to to this in a straightforward way.</p>
<p>The bivariate regression model is depicted in Figure <a href="simple-linear-regression.html#fig:simple-regression-explanation-plot">4.2</a>. More formally, the model can be defined as follows:</p>
<p><span class="math display" id="eq:bivariate-regression-model-definition">\[\begin{equation}
Y_i = \beta_0 + \beta_1 \times X_{i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
\tag{4.2}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(Y_i\)</span> is the value of the <span class="math inline">\(i\)</span>-th observation of the dependent variable (with <span class="math inline">\(i = 1, \ldots, n\)</span>) and <span class="math inline">\(X_i\)</span> the value of the <span class="math inline">\(i\)</span>-th observation of the predictor variable. These are the observable parts of the data. The model contains three important parameters:</p>
<ul>
<li>The <strong>intercept</strong> <span class="math inline">\(\beta_0\)</span>, which is the mean of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X=0\)</span>.</li>
<li>The <strong>slope</strong> <span class="math inline">\(\beta_1\)</span>, which reflects the increase or decrease in the mean of <span class="math inline">\(Y\)</span> for every <em>1-unit increase</em> in the predictor <span class="math inline">\(X\)</span>. By a 1-unit increase, we simply mean that the value of <span class="math inline">\(X\)</span> goes up by 1, e.g. from <span class="math inline">\(X=2\)</span> to <span class="math inline">\(X=3\)</span>, or from <span class="math inline">\(X=12.63\)</span> to <span class="math inline">\(X=13.63\)</span>.</li>
<li>The <strong>standard deviation</strong> <span class="math inline">\(\sigma_\epsilon\)</span> of the <strong>error</strong> or <strong>residual</strong> terms <span class="math inline">\(\epsilon_i\)</span>. The errors are assumed to be drawn independently from the same Normal distribution, with a mean of 0 and standard deviation <span class="math inline">\(\sigma_\epsilon\)</span>.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:simple-regression-explanation-plot"></span>
<img src="_main_files/figure-html/simple-regression-explanation-plot-1.svg" alt="The simple regression model. __A__: The intercept is the mean of $Y$ when $X=0$ and is the point at which the regression line crosses the y-axis. The slope determines the steepness of the regression line and represents the increase (or decrease) in the mean of $Y$ for every 1-unit increase in the predictor $X$. __B__: the error terms or residuals are the vertical distances of each observed Y-value from the regression line. __C__: The errors in a regression model are assumed to follow a Normal distribution around the regression line." width="100%" />
<p class="caption">
Figure 4.2: The simple regression model. <strong>A</strong>: The intercept is the mean of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X=0\)</span> and is the point at which the regression line crosses the y-axis. The slope determines the steepness of the regression line and represents the increase (or decrease) in the mean of <span class="math inline">\(Y\)</span> for every 1-unit increase in the predictor <span class="math inline">\(X\)</span>. <strong>B</strong>: the error terms or residuals are the vertical distances of each observed Y-value from the regression line. <strong>C</strong>: The errors in a regression model are assumed to follow a Normal distribution around the regression line.
</p>
</div>
<p>Remember, unlike variables, parameters are <em>not</em> observable. We can infer their value from the data by estimation and/or performing hypothesis tests. But we can never be completely sure that such inferences are correct. Let’s consider the main model formula again:
<span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_{i} + \epsilon_i\]</span>
This formula involves both observable variables (<span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>) and unobservable parameters (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\epsilon_i\)</span>). Indeed, the errors <span class="math inline">\(\epsilon_i\)</span> are really parameters as well, because they cannot be directly observed. There are many of these (one for each observation <span class="math inline">\(i = 1, \ldots, n\)</span>), and they are not of primary concern. The other part, <span class="math inline">\(\beta_0 + \beta_1 \times X_i\)</span>, determines the regression line, representing the mean value of <span class="math inline">\(Y\)</span> for each possible value of <span class="math inline">\(X\)</span>. These mean values are <strong>conditional means</strong>, they are the mean of <span class="math inline">\(Y\)</span>, conditional upon a particular value of the predictor <span class="math inline">\(X\)</span>. We can use <span class="math inline">\(\mu_{Y|X_{i}}\)</span> to denote this conditional mean, so
<span class="math display" id="eq:conditional-mean-simple-regression">\[\begin{equation}
\mu_{Y|X_{i}} = \beta_0 + \beta_1 \times X_{i}
\tag{4.3}
\end{equation}\]</span></p>
<p>Now, plugging the conditional mean into Equation <a href="simple-linear-regression.html#eq:bivariate-regression-model-definition">(4.2)</a> we get
<span class="math display">\[Y_i = \mu_{Y|X_{i}} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)\]</span>
which, in many respects, is very similar to the simple model of Equation <a href="simple-linear-regression.html#eq:simple-model-as-glm">(4.1)</a>. The key difference is the use of a conditional mean <span class="math inline">\(\mu_{Y|X_{i}}\)</span> instead of a constant mean <span class="math inline">\(\mu\)</span>. So we can view our new model as an extension of the simple one of the previous chapter, where we now use a straight line to represent the relation between the mean and a predictor variable.</p>
<!--
If you were asked to predict the percentage of Trump votes for a state with for example 5.55 hate groups, the conditional mean of the percentage Trump votes for all states with 5.55 hate groups would be a good prediction, because without knowing anything else, it will be impossible to predict whether actual the votes for Trump will be above or below the mean for a given state. Using $\hat{Y}$ to denote the predicted value of the dependent variable, we can thus write:
$$\hat{Y}_i = \beta_0 + \beta_1 \times X_{i}$$

$$Y_i = \hat{Y}_i + \epsilon_i$$
This last equation can be written in words as 
$$\text{data} = \text{prediction} + \text{error}$$
This is just a mathematical truism, you can _always_ make this statement, because the error is simply the difference between the observed values (the data) and the predicted values. But perhaps it helps in thinking about what error is. Moreover, if you look at Equation \@(eq:bivariate-regression-model-definition) again, you might notice that the probabilistic part of the model (i.e., the part involving probability distributions) really only concerns the errors $\epsilon_i$. These are, as already indicated, assumed to be independently drawn from a Normal distribution with a mean of 0 and a standard deviation $\sigma$:
$$\epsilon_i \sim \mathbf{Normal}(0,\sigma)$$
-->
</div>
<div id="sec:04-estimation" class="section level2">
<h2><span class="header-section-number">4.3</span> Estimation</h2>
<p>The maximum likelihood estimates of the model parameters are
<span class="math display" id="eq:simple-regression-intercept-estimate">\[\begin{equation} 
\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \overline{X}
\tag{4.4}
\end{equation}\]</span>
for the intercept, and
<span class="math display" id="eq:simple-regression-slope-estimate">\[\begin{equation}
\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}
\tag{4.5}
\end{equation}\]</span>
for the slope. Note that to estimate the intercept, we first need the estimate of the slope. So let’s focus on
this one first. The top part of the division (the numerator) contains a sum of
deviations of the predictor values (<span class="math inline">\(X_i\)</span>) from its average (<span class="math inline">\(\overline{X}\)</span>) multiplied by deviations
of the values <span class="math inline">\(Y_i\)</span> of the dependent variable from its average (<span class="math inline">\(\overline{Y}\)</span>). Let’s consider these
multiplied deviations a little further. Each deviation is positive (larger than 0) when the value is
higher than the average, and negative (smaller than 0), when the value is lower than the average.
So the multiplied deviations are positive whenever both values are larger than their average, and
whenever both values are below their average (a negative value multiplied by another negative value
is positive). If we were to divide the sum of the multiplied deviations by <span class="math inline">\(n\)</span> (the number of observations),
we’d get the average of these multiplied deviations. This average is also called the <strong>covariance</strong> between
<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:
<span class="math display">\[\text{Cov}(X,Y) =  \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{n}\]</span>
Note that, as an estimator, this provides biased estimates of the true covariance. An unbiased estimator
of the true covariance is obtained by dividing by <span class="math inline">\(n-1\)</span> instead of <span class="math inline">\(n\)</span> (just as for the variance). Going back to our example, the covariance between Trump votes and hate groups would be positive whenever states with higher-than-average Trump votes are generally also states with higher-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally also states with lower-than-average hate groups.
The covariance would be negative, on the other hand, whenever states with higher-than-average Trump votes are generally states with lower-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally states with higher-than-average hate groups. A positive or negative covariance is indicative of a relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Indeed, the well-known
Pearson correlation coefficient is a standardized covariance, where the standardization scales the correlation
to always be between -1 and 1 and involves dividing the covariance by the product of the standard deviations
of both variables:
<span class="math display">\[\begin{align}
r_{X,Y} &amp;= \frac{\text{Cov}(X,Y)}{S_X \times S_Y} \\
&amp;= \frac{\frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{n}}{\sqrt{\frac{\sum_{i=1}^n (X_i - \overline{X})^2}{n}} \times \sqrt{\frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}}}
\end{align}\]</span></p>
<p>Going back to the estimate of the slope (Equation <a href="simple-linear-regression.html#eq:simple-regression-slope-estimate">(4.5)</a>), we can view this as a different way of standardizing the covariance. Looking at the bottom part of the division (the denominator), we can see it consists of the sum of squared deviations of the predictor values from its mean. If we were to divide this sum by the number of observations, we’d get the variance of <span class="math inline">\(X\)</span>. As dividing both the top part (numerator) and bottom part (denominator) in a division by the same value does not affect the outcome of the division (i.e. <span class="math inline">\(\frac{a}{b} = \frac{a/c}{b/c}\)</span>), we can choose to divide both by <span class="math inline">\(n\)</span> so the numerator becomes the covariance and the denominator the variance (we could also divide both by <span class="math inline">\(n-1\)</span> so they become unbiased estimators of the covariance and variance). So an alternative way of computing the estimate of the slope is
<span class="math display">\[\hat{\beta}_1 = \frac{\text{Cov}(X,Y)}{S^2_X}\]</span>
Note that the variance of <span class="math inline">\(X\)</span> equals the product of the standard deviation of <span class="math inline">\(X\)</span> and itself, as <span class="math inline">\(S^2_X = S_X \times S_X\)</span>. So the slope estimate looks quite a bit like the correlation coefficient, where instead of the standard deviation of <span class="math inline">\(Y\)</span>, we use the standard deviation of <span class="math inline">\(X\)</span> twice. With a little algebraic manipulation, we can also state the slope estimate in terms of the correlation as
<span class="math display">\[\hat{\beta}_1 = \frac{S_Y}{S_X} r_{X,Y}\]</span>
The reason for going into these alternative formulations is not to pain or confuse you with lots of equations. What these alternative formulations show is that the slope tells us something about the relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, just like the covariance and correlation do. If the sample correlation is 0, then so is the estimated slope. It is important to realise that we have been discussing the <em>estimate</em> of the slope, not the true value itself. But the same relations hold for the true values. If we denote the true correlation as <span class="math inline">\(\rho_{X,Y}\)</span>, then the true value of the slope can be defined as
<span class="math display">\[\beta_1 = \frac{\sigma_Y}{\sigma_X} \rho_{X,Y}\]</span>
The true value of the slope is 0 when the true correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> equals <span class="math inline">\(\rho_{X,Y} = 0\)</span>. It would also be 0 if the true standard deviation of <span class="math inline">\(Y\)</span> equals <span class="math inline">\(\sigma_Y = 0\)</span>, but this implies that <span class="math inline">\(Y\)</span> is a constant and that is not a very interesting situation.</p>
<p>That was perhaps a little tortuous, and we haven’t even discussed the estimate of the intercept! Remember that the intercept represents the mean value of <span class="math inline">\(Y\)</span> for all cases where <span class="math inline">\(X=0\)</span>. This average is often not so interesting itself, although in our example, we might be interested in what the average percentage of votes for Trump would be in the absence of any hate groups. Equation <a href="simple-linear-regression.html#eq:simple-regression-intercept-estimate">(4.4)</a> shows that we can estimate this value by adjusting the sample average <span class="math inline">\(\overline{Y}\)</span> by subtracting <span class="math inline">\(\hat{\beta}_1 \times \overline{X}\)</span> from it. How come? Well, it can be shown that <strong>the regression line always passes through the point <span class="math inline">\((\overline{X},\overline{Y})\)</span></strong>. The derivation for showing that this has to be the case is not that interesting, so you’ll just have to trust me, or look it up elsewhere. But this implies that <span class="math display">\[\overline{Y} = \beta_0 + \beta_1 \times \overline{X}\]</span>
and then we can simply subtract <span class="math inline">\(\beta_1 \times \overline{X}\)</span> from both sides to get <span class="math inline">\(\overline{Y} - \beta_1 \times \overline{X} = \beta_0\)</span>.</p>
<p>Finally, we can also estimate <span class="math inline">\(\sigma_\epsilon\)</span>, the standard deviation of the error. As usual for variances and standard deviations, the maximum likelihood estimate is biased, so we’ll focus on an unbiased estimator. Like before, an estimate of the variance is computed from a sum of squared deviations from an estimated mean. In this case, we need to use the estimated <em>conditional</em> means
<span class="math display">\[\hat{\mu}_{Y|X_i} = \hat{\beta}_0 + \hat{\beta}_1 \times X_i\]</span>
to compute the following estimate of the error variance:
<span class="math display" id="eq:simple-regression-sd-error-variance-estimate">\[\begin{equation}
\hat{\sigma}^2_\epsilon = \frac{\sum_{i=1}^n (Y_i - \hat{\mu}_{Y|X_i})^2}{n-2}
\tag{4.6}
\end{equation}\]</span>
Note that we are dividing by <span class="math inline">\(n-2\)</span> here, rather than by <span class="math inline">\(n-1\)</span> as we did when estimating the variance of <span class="math inline">\(Y\)</span> in a model without predictors. The reason for this is that we are now using two noisy parameter estimates (i.e. <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>), rather than just one. As usual, to get the estimate of <span class="math inline">\(\hat{\sigma}_\epsilon\)</span>, we can just take the square root of <span class="math inline">\(\hat{\sigma}^2_\epsilon\)</span>.</p>
<div id="estimating-the-relation-between-trump-votes-and-hate-groups" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Estimating the relation between Trump votes and hate groups</h3>
<p>So, what are the estimates of the model predicting Trump votes by hate groups? We can calculate the estimates relatively easily by first computing the following sample quantities of location, scale, and covariation</p>
<table>
<colgroup>
<col width="19%" />
<col width="20%" />
<col width="14%" />
<col width="15%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(\overline{\mathtt{hate}}\)</span></th>
<th align="right"><span class="math inline">\(\overline{\mathtt{votes}}\)</span></th>
<th align="right"><span class="math inline">\(S^2_\mathtt{hate}\)</span></th>
<th align="right"><span class="math inline">\(S^2_\mathtt{votes}\)</span></th>
<th align="right"><span class="math inline">\(\text{Cov}(\mathtt{hate},\mathtt{votes})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">3.03</td>
<td align="right">49.9</td>
<td align="right">3.71</td>
<td align="right">99.8</td>
<td align="right">8.53</td>
</tr>
</tbody>
</table>
<p>The estimate of the slope is then
<span class="math display">\[\hat{\beta}_1 = \frac{8.527}{3.707} = 2.3\]</span>
and the estimate of the intercept is
<span class="math display">\[\hat{\beta}_0 = 49.862 - 2.3 \times 3.028 = 42.897\]</span></p>
<p>Of course, we would not normally bother with calculating these estimates “hy hand”, but rather rely on statistical software such as R or JASP to calculate these for us. In any case, the estimated model becomes:</p>
<p><span class="math display">\[\texttt{votes}_i = 42.897 + 2.3 \times \texttt{hate} + e_i\]</span>
According to this model, the average percentage of Trump votes in states without any hate groups is 42.897. For every one additional hate group (per million citizens), the percentage of Trump votes increases by 2.3. You can view the resulting regression line (the conditional means of votes for Trump as predicted by the model) in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups-with-regression">4.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:scatterplot-trump-votes-hate-groups-with-regression"></span>
<div id="htmlwidget-2653f0c7e43d58c47b84" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-2653f0c7e43d58c47b84">{"x":{"data":[{"x":[5.55,0,2.6,5.35,2.01,2.89,1.4,4.2,3.06,3.1,0,7.13,2.5,3.92,1.28,2.41,5.18,2.99,2.25,2.99,1.76,2.82,1.81,6.02,3.94,9.59,2.62,1.36,4.5,1.68,0.96,2.38,3.06,1.32,3.01,1.53,2.69,3.13,0.95,2.42,8.09,5.71,1.97,0.98,1.6,4.64,2.88,2.18,1.56,3.42],"y":[62.9,52.9,49.5,60.4,32.7,44.4,41.2,41.9,49.1,51.3,30,59.2,39.4,57.2,51.8,57.2,62.5,58.1,45.2,35.3,33.5,47.6,45.4,58.3,57.1,56.5,60.3,45.5,47.2,41.8,40,37.5,50.5,64.1,52.1,65.3,41.1,48.8,39.8,54.9,61.5,61.1,52.6,45.9,32.6,45,38.2,68.7,47.9,70.1],"text":["hate_groups_per_million: 5.55<br />percent_Trump_votes: 62.9<br />state: Alabama","hate_groups_per_million: 0.00<br />percent_Trump_votes: 52.9<br />state: Alaska","hate_groups_per_million: 2.60<br />percent_Trump_votes: 49.5<br />state: Arizona","hate_groups_per_million: 5.35<br />percent_Trump_votes: 60.4<br />state: Arkansas","hate_groups_per_million: 2.01<br />percent_Trump_votes: 32.7<br />state: California","hate_groups_per_million: 2.89<br />percent_Trump_votes: 44.4<br />state: Colorado","hate_groups_per_million: 1.40<br />percent_Trump_votes: 41.2<br />state: Connecticut","hate_groups_per_million: 4.20<br />percent_Trump_votes: 41.9<br />state: Delaware","hate_groups_per_million: 3.06<br />percent_Trump_votes: 49.1<br />state: Florida","hate_groups_per_million: 3.10<br />percent_Trump_votes: 51.3<br />state: Georgia","hate_groups_per_million: 0.00<br />percent_Trump_votes: 30.0<br />state: Hawaii","hate_groups_per_million: 7.13<br />percent_Trump_votes: 59.2<br />state: Idaho","hate_groups_per_million: 2.50<br />percent_Trump_votes: 39.4<br />state: Illinois","hate_groups_per_million: 3.92<br />percent_Trump_votes: 57.2<br />state: Indiana","hate_groups_per_million: 1.28<br />percent_Trump_votes: 51.8<br />state: Iowa","hate_groups_per_million: 2.41<br />percent_Trump_votes: 57.2<br />state: Kansas","hate_groups_per_million: 5.18<br />percent_Trump_votes: 62.5<br />state: Kentucky","hate_groups_per_million: 2.99<br />percent_Trump_votes: 58.1<br />state: Louisiana","hate_groups_per_million: 2.25<br />percent_Trump_votes: 45.2<br />state: Maine","hate_groups_per_million: 2.99<br />percent_Trump_votes: 35.3<br />state: Maryland","hate_groups_per_million: 1.76<br />percent_Trump_votes: 33.5<br />state: Massachusetts","hate_groups_per_million: 2.82<br />percent_Trump_votes: 47.6<br />state: Michigan","hate_groups_per_million: 1.81<br />percent_Trump_votes: 45.4<br />state: Minnesota","hate_groups_per_million: 6.02<br />percent_Trump_votes: 58.3<br />state: Mississippi","hate_groups_per_million: 3.94<br />percent_Trump_votes: 57.1<br />state: Missouri","hate_groups_per_million: 9.59<br />percent_Trump_votes: 56.5<br />state: Montana","hate_groups_per_million: 2.62<br />percent_Trump_votes: 60.3<br />state: Nebraska","hate_groups_per_million: 1.36<br />percent_Trump_votes: 45.5<br />state: Nevada","hate_groups_per_million: 4.50<br />percent_Trump_votes: 47.2<br />state: New Hampshire","hate_groups_per_million: 1.68<br />percent_Trump_votes: 41.8<br />state: New Jersey","hate_groups_per_million: 0.96<br />percent_Trump_votes: 40.0<br />state: New Mexico","hate_groups_per_million: 2.38<br />percent_Trump_votes: 37.5<br />state: New York","hate_groups_per_million: 3.06<br />percent_Trump_votes: 50.5<br />state: North Carolina","hate_groups_per_million: 1.32<br />percent_Trump_votes: 64.1<br />state: North Dakota","hate_groups_per_million: 3.01<br />percent_Trump_votes: 52.1<br />state: Ohio","hate_groups_per_million: 1.53<br />percent_Trump_votes: 65.3<br />state: Oklahoma","hate_groups_per_million: 2.69<br />percent_Trump_votes: 41.1<br />state: Oregon","hate_groups_per_million: 3.13<br />percent_Trump_votes: 48.8<br />state: Pennsylvania","hate_groups_per_million: 0.95<br />percent_Trump_votes: 39.8<br />state: Rhode Island","hate_groups_per_million: 2.42<br />percent_Trump_votes: 54.9<br />state: South Carolina","hate_groups_per_million: 8.09<br />percent_Trump_votes: 61.5<br />state: South Dakota","hate_groups_per_million: 5.71<br />percent_Trump_votes: 61.1<br />state: Tennessee","hate_groups_per_million: 1.97<br />percent_Trump_votes: 52.6<br />state: Texas","hate_groups_per_million: 0.98<br />percent_Trump_votes: 45.9<br />state: Utah","hate_groups_per_million: 1.60<br />percent_Trump_votes: 32.6<br />state: Vermont","hate_groups_per_million: 4.64<br />percent_Trump_votes: 45.0<br />state: Virginia","hate_groups_per_million: 2.88<br />percent_Trump_votes: 38.2<br />state: Washington","hate_groups_per_million: 2.18<br />percent_Trump_votes: 68.7<br />state: West Virginia","hate_groups_per_million: 1.56<br />percent_Trump_votes: 47.9<br />state: Wisconsin","hate_groups_per_million: 3.42<br />percent_Trump_votes: 70.1<br />state: Wyoming"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[-0.4795,10.0695],"y":[41.7937724109846,66.0607962859094],"text":"","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":37.2602739726027},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.4795,10.0695],"tickmode":"array","ticktext":["0.0","2.5","5.0","7.5","10.0"],"tickvals":[0,2.5,5,7.5,10],"categoryorder":"array","categoryarray":["0.0","2.5","5.0","7.5","10.0"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Hate groups per million citizens","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[27.995,72.105],"tickmode":"array","ticktext":["30","40","50","60","70"],"tickvals":[30,40,50,60,70],"categoryorder":"array","categoryarray":["30","40","50","60","70"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"% votes for Trump","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"202db91b90b39b":{"x":{},"y":{},"name":{},"type":"scatter"},"202db914f304b6":{"intercept":{},"slope":{}}},"cur_data":"202db91b90b39b","visdat":{"202db91b90b39b":["function (y) ","x"],"202db914f304b6":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 4.3: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens with the estimated regression line.
</p>
</div>
</div>
</div>
<div id="hypothesis-testing" class="section level2">
<h2><span class="header-section-number">4.4</span> Hypothesis testing</h2>
<p>The estimated model indicates a positive relation between the number of hate groups and votes for Donald Trump. But the slope is an <em>estimated</em> parameter. It might be that, in reality, there is no relation between hate groups and Trump votes. But because we have limited and noisy data, sometimes the estimated slope might be positive, and other times it might be negative. So the question is whether we have enough evidence to reject the null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>.</p>
<p>As before, there are two main ways in which to look at testing whether the parameters of the model are equal to a priori values. The first is to consider how variable the parameter estimates are under the assumption that the true parameter is identical to the a priori value. The second way is to compare two models, one in which the parameter is fixed to the a priori value, and one where it is freely estimated. Both of these ways will provide us with the same outcome. The model comparison approach is more flexible, however, as it also allows you to test multiple parameters simultaneously. We’ll start with discussing the first method, but it will be the last time until a while. After that, we’ll focus on model comparison throughout.</p>
<div id="sampling-distribution-of-estimates" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Sampling distribution of estimates</h3>
<p>Remember, an estimate (whether of the mean or another parameter) is a noisy reflection of the true value of that parameter. The noise comes from having access only to limited data, not <em>all</em> the data the data generating process can produce. Suppose in reality there is no relation between hate groups and Trump votes, so that the true slope is <span class="math inline">\(\beta_1 = 0\)</span>. In that case, the model becomes
<span class="math display">\[\begin{align}
Y_i &amp;= \beta_0 + 0 \times X_i + \epsilon \\
&amp;= \beta_0 + \epsilon_i
\end{align}\]</span>
which is identical to the simple model in Equation, renaming <span class="math inline">\(\beta_0 = \mu\)</span> and <span class="math inline">\(\sigma_\epsilon = \sigma\)</span>. Then, the true value of the intercept would be <span class="math inline">\(\beta_0 = \mu\)</span>. If we’d know the true value of the standard deviation <span class="math inline">\(\sigma_\epsilon\)</span>, we’d have a fully specified model (a Normal distribution) which we can use to generate as many alternative data sets as we’d like. By generating these data sets, and estimating the slope of our model for each, we can get an overview of the variability of the estimates when the true slope equals <span class="math inline">\(\beta_1 = 0\)</span>. An unbiased estimator ensures that on average, these estimates equal the true value. The main thing of interest is then the variability around this value. A consistent estimator ensures that this variability becomes smaller with the larger data sets. But to understand how well we can estimate the parameter for the present data, we would simulate data sets with the same number of observations (so <span class="math inline">\(n=50\)</span>). Unfortunately, we don’t know the true value of <span class="math inline">\(\sigma\)</span>. Our data provides an estimate of <span class="math inline">\(\sigma\)</span>, but we know this estimate is noisy itself. Thinking in the same way about the sampling distribution of <span class="math inline">\(\hat{\sigma}\)</span>, we could first sample a values of <span class="math inline">\(\sigma\)</span>, and then use each of these to generate a data set for which to estimate <span class="math inline">\(\beta_1\)</span>. Doing this many (many many!) times would give us a good overview of the variability of the estimates. As before, we don’t have to actually simulate the data sets. If the model corresponding to the null hypothesis is true, then we can derive that the sampling distribution of the estimates follows a t-distribution. Thus, we should look for the <span class="math inline">\(t\)</span> value our data provides, and evaluate this value within the context of the sampling distribution derived under the model where <span class="math inline">\(\beta_1 = 0\)</span>. For both parameters (intercept and slope), the same logic applies. In general then, for parameters <span class="math inline">\(\beta_j\)</span> (where <span class="math inline">\(j = 0\)</span> or 1), the <span class="math inline">\(t\)</span>-value of our data is computed as
<span class="math display">\[\begin{equation}
t = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)} \quad \quad \quad t \sim \mathbf{T}(n-2)
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{T}(n-2)\)</span> denotes a standard Student t-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, and <span class="math inline">\(\text{SE}(\hat{\beta}_j)\)</span> is the standard error of the estimate, which you should remember is the standard deviation of the sampling distribution of the estimates. I won’t bore you with how to compute this standard error; statistical software does a good job at this. One thing to realise though is that <em>the computed standard error is valid for datasets with exactly the same values of the predictor</em>. In other words, it assumes the predictor values are fixed (a part of the Data Generating Process). If you’d collect a different dataset with different values for e.g. the number of hate groups, the standard error would also be different.</p>
<p>R would for instance provide the following results for this regression model:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Estimate</th>
<th align="right">Std. Error</th>
<th align="right">t value</th>
<th align="right">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">42.9</td>
<td align="right">2.409</td>
<td align="right">17.80</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">hate_groups_per_million</td>
<td align="right">2.3</td>
<td align="right">0.672</td>
<td align="right">3.43</td>
<td align="right">0.001</td>
</tr>
</tbody>
</table>
<p>The values listed for <code>(intercept)</code> concern <span class="math inline">\(\beta_0\)</span>, and the values listed for <code>hate_groups_per_million</code> concern the slope <span class="math inline">\(\beta_1\)</span>. You can see the estimates are identical to those computed earlier (phew, no mistakes there! :-). You can see that the standard errors for the two parameters are quite different in magnitude. This is not so surprising, as they reflect quite different things, the intercept being a particular value of the dependent variable, while the slope represents an increase in the dependent variable for an increase in the predictor. If you’d change the scale of the predictor (e.g., from hate groups per million to hate groups per 10,000 citizens), the slope would change, as well as the corresponding standard error. Changing the scale in this way would not affect the standard error of the intercept. If you divide each estimate by the standard error, you obtain the value of the <span class="math inline">\(t\)</span>-statistic (just like we did earlier in Equation <a href="ch-simple-GLM.html#eq:definition-one-sample-t">(3.5)</a>). In a manner analogous to that depicted in Figure <a href="ch-simple-GLM.html#fig:critical-values-t-distribution">3.6</a>, you then determine critical values for the t-statistic (based on the degrees of freedom and the significance level <span class="math inline">\(\alpha\)</span>), and determine whether the value you computed for the parameters lies within the critical range. If it does, the test result is called significant, and you reject the null hypothesis. If not, then the result is called not significant, and you don’t reject the null hypothesis. Now that these are easy to compute with statistical software, it is more common to check the <span class="math inline">\(p\)</span>-values. Remember, the <span class="math inline">\(p\)</span>-value is the probability of the computed <span class="math inline">\(t\)</span>-statistic <em>or a more extreme value</em>, assuming the null hypothesis is true. If the <span class="math inline">\(p\)</span>-value is smaller than the chosen significance level (e.g. <span class="math inline">\(\alpha = .05\)</span>), that means that the test is significant, and the null hypothesis rejected. In the results above, you can see that both p-values are below <span class="math inline">\(\alpha = .05\)</span>. Hence, we can reject the null hypothesis that <span class="math inline">\(\beta_0 = 0\)</span> and the null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>. In other words, there is good evidence that, in the absence of hate groups, the percentage of people voting for Trump is not equal to 0.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> Also, there is good evidence that there is a relation between the number of hate groups and Trump votes.</p>
<p>Personally, I find the <span class="math inline">\(t\)</span> statistic quite intuitive in the context of a one-sample t-test. Generalizing the concept to a standardized estimate (dividing the estimate by its standard error) is also reasonably intuitive. However, in a multi-parameter model such as here, the sampling distribution of a single parameter is dependent on the estimation of all the other parameters. For instance, the test of the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> (i.e., no effect of hate groups on Trump votes) is based on deriving the sampling distribution of <span class="math inline">\(\hat{\beta}_1\)</span> in a model where <span class="math inline">\(\beta_1 = 0\)</span>, but all the other parameters (i.e. <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\sigma_\epsilon\)</span>) cannot be assumed known). This uncertainty is dealt with similarly as before, resulting in a t-distribution, but now there are two sources of uncertainty (two parameters to estimate). Hence, the degrees of freedom are <span class="math inline">\(n-2\)</span> here.</p>
</div>
<div id="model-comparison" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Model comparison</h3>
<p>The fact that, in a multi-parameter model, a test of one parameter is <em>not</em> conducted in isolation, but rather in the context of all the other parameters in the model, is more explicit in the model comparison approach. When we compare two models, we have to make clear what the parameters are in each: what are the unknown quantities which we will have to estimate, and what are quantities which we can assume a precise value for?</p>
<p>As before, we will consider comparing <em>nested models</em>, in which a restricted MODEL R is a <em>special case</em> of a more general MODEL G. Sticking to simple linear regression models, the most general model we have is that of Equation <a href="simple-linear-regression.html#eq:bivariate-regression-model-definition">(4.2)</a>.</p>
<p>As for the simple model of the previous chapter, it turns out we can write the likelihood for a model as a function of a <em>sum of squared deviations</em>. We will not go through the derivation of this again, as it is rather similar and equally tedious. But, because we are now calling these deviations <em>errors</em>, will start to refer to them as a <strong>Sum of Squared Errors</strong> (SSE).</p>
<p>Let’s call the model of Equation <a href="simple-linear-regression.html#eq:bivariate-regression-model-definition">(4.2)</a> MODEL G. Before we go on, I want to warn you that what we call MODEL G, and what we call MODEL R, can change from situation to situation. Basically, the identity of MODEL G and MODEL R are “local” to the particular model comparison. You can think of MODEL G as a parent, and MODEL R as a child. While the relation between them is similar, within an extended family, someone can be both a parent to one family member, and be the child of another. This is the form of flexibility that you will need when thinking about nested models. A model can be both more general than one model, and more restricted than another. I will come back to this soon.</p>
<p>First, let’s consider what the Sum of Squared Error of a model is. The easiest way to define this is in terms of the <strong>predictions</strong> of each model. Recall that a regression model has a structural and random part. The structural part defines the conditional mean of the dependent variable, while the random part concerns the random variation of the actual observations around the conditional means. As the random part is by definition unpredictable, there is not much we can do with that in terms of forming predictions. So we’re stuck with the structural part. Although not a universal principle, there are many situations in which it makes sense to predict outcomes by the (conditional) mean. If we do this, then we can predict the outcomes with MODEL G as</p>
<p><span class="math display">\[\hat{Y}_{G,i} = \hat{\beta}_{G,0} + \hat{\beta}_{G,1} \times X_i\]</span>
where <span class="math inline">\(\hat{Y}\)</span> stands for a predicted value. We’re using the same “hat” for this as an <em>estimated value</em>, because it is really also a good estimate of what the value of <span class="math inline">\(Y_i\)</span> might have been if it was another observation with the same value for <span class="math inline">\(X\)</span>. We are also assigning the subscript “G” to all the estimates, to distinguish them from those of a different model. Now let’s consider a MODEL R in which we assume that there is no relation between Trump votes and hate groups, so that <span class="math inline">\(\beta_1 = 0\)</span>. The predictions for this model would be</p>
<p><span class="math display">\[\hat{Y}_{R,i} = \hat{\beta}_{R,0} + 0 \times X_i = \hat{\beta}_{R,0}\]</span>
Now we have two models to make predictions, we can write the corresponding Sum of Squared Error of each as:
<span class="math display" id="eq:SSE-GLM-general">\[\begin{equation}
\text{SSE}(M) = \sum_{i=1}^n \left(Y_i - \hat{Y}_{M,i} \right)^2
\tag{4.7}
\end{equation}\]</span>
where we can replace the general letter <span class="math inline">\(M\)</span> (for Model) with either <span class="math inline">\(G\)</span> of <span class="math inline">\(R\)</span>, to get <span class="math inline">\(\text{SSE}(G)\)</span> or <span class="math inline">\(\text{SSE}(R)\)</span>, respectively. So the Sum of Squares is based on a difference between each observation and the model prediction for that observation. These are thus the <em>prediction errors</em>. If the prediction was equal to the the prediction of the true model (e.g. <span class="math inline">\(\beta_0 + \beta_1 X_{i}\)</span>), then these would be equal to the true error terms <span class="math inline">\(\epsilon_i\)</span>. But because we only have an estimated model, they are effectively estimates of the true errors. You can see these (unsquared) errors for the two models in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison">4.4</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison"></span>
<img src="_main_files/figure-html/scatterplot-trump-votes-hate-groups-with-regression-model-comparison-1.svg" alt="Estimated regression lines for MODEL G (left) and MODEL R (right) with $\beta_1 = 0$ and the errors." width="100%" />
<p class="caption">
Figure 4.4: Estimated regression lines for MODEL G (left) and MODEL R (right) with <span class="math inline">\(\beta_1 = 0\)</span> and the errors.
</p>
</div>
<p>When you compare the errors between the models, you can see that MODEL G does not provide a better prediction for <em>each</em> observation. Sometimes the distance from an observation to the regression line is larger for MODEL G than for MODEL R. However, MODEL G does seem to provide a better prediction for most observations. This is unsurprising, as MODEL G, by maximising the likelihood, should be at least as good as MODEL R. This means that it is <em>always the case that</em>
<span class="math display">\[\text{SSE}(R) \geq \text{SSE}(G)\]</span>
The Sum of Squared Errors of MODEL G will never be higher than that of MODEL R. So we cannot just select the model with the lowest SSE, as this would mean we’d always select MODEL G. We need to find a way to determine whether the SSE of MODEL G is sufficiently lower than that of MODEL R to make us believe that MODEL G is indeed superior to MODEL R.</p>
<p>In the previous chapter, we discussed that the <span class="math inline">\(t\)</span> statistic can be used to perform a test which is equivalent to the likelihood ratio test, and that this was useful because the sampling distribution of the <span class="math inline">\(t\)</span> statistic is known, while the sampling distribution of the likelihood ratio is difficult to determine. For the case of linear models, there is a similar argument that leads to a new statistic, which can be viewed as a generalization of the <span class="math inline">\(t\)</span> statistic. This is the <span class="math inline">\(F\)</span> statistic. If was given the letter in honour of Sir Ronald A. Fisher, a rather brilliant and very influential statistician, who developed it in the 1920s.</p>
<p>For comparing two linear models, where MODEL R is a special case of MODEL G, we can define the <span class="math inline">\(F\)</span> statistic as:
<span class="math display" id="eq:glm-f-definition">\[\begin{equation}
F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar(G)}}}
\tag{4.8}
\end{equation}\]</span>
Here, <span class="math inline">\(\text{npar}(G)\)</span> stands for the number of parameters which are estimated in MODEL G, and <span class="math inline">\(\text{npar}(R)\)</span> for the number of parameters which are estimated in MODEL R. In counting the number of estimated parameters, we are excluding the error variance <span class="math inline">\(\sigma^2_\epsilon\)</span>. Although this is really also a parameter of the models, it is generally not of direct interest. It is a so-called “nuisance parameter”, something that we need to take into account, but we’d rather forget about. In the present example, MODEL G then has two estimated parameters: <span class="math inline">\(\hat{\beta}_{G,0}\)</span> and <span class="math inline">\(\hat{\beta}_{G,1}\)</span>, while MODEL R has just one: <span class="math inline">\(\hat{\beta}_{R,0}\)</span>. If we compute the SSE for each model (i.e. taking the vertical lines to the regression line in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison">4.4</a> and squaring them), we get <span class="math inline">\(\text{SSE}(G) = 4011.398\)</span> and <span class="math inline">\(\text{SSE}(R) = 4992.178\)</span>.</p>
<p><span class="math display">\[\begin{align}
F &amp;= \frac{\frac{4992.178 -  4011.398}{2 - 1}}{\frac{4011.398}{50 - 2}} \\
&amp;= \frac{980.779}{83.571} \\
&amp;= 11.736
\end{align}\]</span></p>
If MODEL R is true, the sampling distribution of the <span class="math inline">\(F\)</span> statistic follows an <span class="math inline">\(F\)</span> distribution. This distribution has two parameters, both degrees of freedom, which we’ll refer to as <span class="math inline">\(\text{df}_1\)</span> and <span class="math inline">\(\text{df}_2\)</span>. The first one reflects the difference in the number of estimated parameters between the models
<span class="math display">\[\text{df}_1 = \text{npar}(G) - \text{npar}(R)\]</span>
and the second one the number of observations minus the number of estimated parameters of the more general model
<span class="math display">\[\text{df}_2 = n - \text{npar}(G)\]</span>
<div class="figure" style="text-align: center"><span id="fig:f-distribution-plot"></span>
<img src="_main_files/figure-html/f-distribution-plot-1.svg" alt="The F distribution with $\text{df}_1 = 1$ and $\text{df}_2 = 48$, and the critical value for $\alpha = .05$" width="70%" />
<p class="caption">
Figure 4.5: The F distribution with <span class="math inline">\(\text{df}_1 = 1\)</span> and <span class="math inline">\(\text{df}_2 = 48\)</span>, and the critical value for <span class="math inline">\(\alpha = .05\)</span>
</p>
</div>
<p>Note that the value of the <span class="math inline">\(F\)</span> statistic can never be negative; the distribution is thus defined only over positive values of <span class="math inline">\(F\)</span>. High values of <span class="math inline">\(F\)</span> indicate that the MODEL G has substantially less error than MODEL R. Another thing to note is that, <strong>whenever <span class="math inline">\(\text{df}_1 = 1\)</span>, there is a direct relation between the <span class="math inline">\(F\)</span> statistic and the <span class="math inline">\(t\)</span> statistic</strong>: <span class="math inline">\(F = t^2\)</span>, or conversely, <span class="math inline">\(\sqrt{F} = t\)</span>. It is easy to check that this is indeed the case here: <span class="math inline">\(\sqrt{11.736} = 3.426\)</span>, which is the value of the <span class="math inline">\(t\)</span> statistic we computed earlier.</p>
<p>The critical value of the <span class="math inline">\(F\)</span> statistic, with <span class="math inline">\(\alpha = .05\)</span> and <span class="math inline">\(\text{df}_1 = 2-1 = 1\)</span> and <span class="math inline">\(\text{df}_2 = 50 - 2 = 48\)</span>, is 4.043, and any value of <span class="math inline">\(F\)</span> above this critical value would result in a rejection of the null hypothesis. So, because the <span class="math inline">\(F\)</span> value we computed was 11.736, which is larger than the critical value, we reject the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>, and conclude that there is evidence of a relation between the number of hate groups and Trump votes. Instead of checking whether the <span class="math inline">\(F\)</span> value is larger than a critical value, we can also compute the p-value, which is the probability of an <span class="math inline">\(F\)</span> value equal to or larger than 11.736 in the distribution depicted in Figure , More formally, the p-value can be defined as <span class="math inline">\(P(F \geq 11.736 | \text{df}_1 = 1, \text{df}_2 = 48) = 0.00127\)</span>. As this probability is smaller than the significance level, this again implies we reject the null hypothesis.</p>
A nice thing about the <span class="math inline">\(F\)</span> statistic is that it is general, and can be used to compare any nested linear models. For instance, we can also compare MODEL G to a different MODEL R in which we assume the intercept equals <span class="math inline">\(\beta_0 = 0\)</span>, whilst allowing the slope to take any value. This model forces the regression line to go through the (0,0) point. Estimating this MODEL R gives<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>
<span class="math display">\[\begin{align}
Y_i &amp;= 0 + \hat{\beta}_{R,1} \times X_i \\
&amp;= 12.389 \times X_i
\end{align}\]</span>
You can see the resulting regression line and error terms in Figure <a href="simple-linear-regression.html#fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2">4.6</a>.
<div class="figure" style="text-align: center"><span id="fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2"></span>
<img src="_main_files/figure-html/scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2-1.svg" alt="Estimated regression lines for MODEL G (left) and MODEL R (right) with $\beta_1 = 0$ and the errors." width="100%" />
<p class="caption">
Figure 4.6: Estimated regression lines for MODEL G (left) and MODEL R (right) with <span class="math inline">\(\beta_1 = 0\)</span> and the errors.
</p>
</div>
<p>This alternative MODEL R seems clearly inferior to MODEL G. The SSE of this model is <span class="math inline">\(\text{SSE}(R) = 3.05\times 10^{4}\)</span>. Computing the <span class="math inline">\(F\)</span> statistic gives
<span class="math display">\[
\begin{align}
F &amp;= \frac{\frac{3.05\times 10^{4} -  4011.398}{2 - 1}}{\frac{4011.398}{50 - 2}} \\
&amp;= \frac{2.649\times 10^{4}}{83.571} \\
&amp;= 316.981
\end{align}
\]</span>
Because the test involves the same degrees of freedom, the critical value is the same as before. So we reject the null hypothesis again, which here is <span class="math inline">\(H_0: \beta_0 = 0\)</span>. The p-value now is <span class="math inline">\(P(F \geq 316.981 | \text{df}_1 = 1, \text{df}_2 = 48) &lt;.0001.\)</span></p>
<!--
I would like to point out that the discrepancy $Y_i - \hat{Y}_{M,i}$ is  a reasonable estimate of the error term $\epsilon_i$, as it is straightforward to rewrite the model as 
$$\begin{align}
\epsilon_i &= Y_i - (\beta_0 + \beta_1 \times X_i) \\
&= Y_i - \mu_{Y|X{_{i}}}
\end{align}$$
and we have just indicated that $\hat{Y}_i = \hat{\mu}_{Y|X{_{i}}}$. Replacing the conditional mean by its estimate, and realizing that ny using an estimate for this, we than also can't be certain about the true value of $\epsilon$, which would also turn that into an estimate, we then write this formally as:  
$$\hat{\epsilon}_{M,i} = Y_i - \hat{Y}_{M,i}$$
This in turn is a somewhat convoluted way to say that the prediction errors would be a reasonable way to estimate the error variance or standard deviation $\sigma_\epsilon$.
-->
<!--



|                        | Df| Sum Sq| Mean Sq| F value| Pr(>F)|
|:-----------------------|--:|------:|-------:|-------:|------:|
|hate_groups_per_million |  1|    981|   980.8|    11.7|  0.001|
|Residuals               | 48|   4011|    83.6|      NA|     NA|

-->
<!--
### Confidence interval

The formula to compute confidence intervals for the two parameters can be written as:
$$\hat{\beta}_j \pm t_{1-\alpha/2,n-2} \times \text{SE}(\hat{\beta}_j)$$
where $t_{1-\alpha/2,n-2}$ is the right-critical value in a t-distribution with $n-2$ degrees of freedom and a significance level of $\alpha$. Using $\alpha=.05$ gives us the conventional 95%-confidence interval. 
-->
<!-- 
## Assumptions and outliers

<div class="figure" style="text-align: center">
<img src="_main_files/figure-html/boxplots-trump-votes-hate-groups-1.svg" alt="Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens" width="50%" /><img src="_main_files/figure-html/boxplots-trump-votes-hate-groups-2.svg" alt="Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens" width="50%" />
<p class="caption">(\#fig:boxplots-trump-votes-hate-groups)Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens</p>
</div>




<div class="figure" style="text-align: center">
<img src="_main_files/figure-html/simple-regression-trump2016-residuals-plots-1.svg" alt="Predicted vs residual plot and a QQ plot of the residuals" width="50%" /><img src="_main_files/figure-html/simple-regression-trump2016-residuals-plots-2.svg" alt="Predicted vs residual plot and a QQ plot of the residuals" width="50%" />
<p class="caption">(\#fig:simple-regression-trump2016-residuals-plots)Predicted vs residual plot and a QQ plot of the residuals</p>
</div>

## Summary

-->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-schaffner_understanding_2018">
<p>Schaffner, B. F., Macwilliams, M., &amp; Nteta, T. (2018). Understanding white polarization in the 2016 vote for president: The sobering role of racism and sexism. <em>Political Science Quarterly</em>, <em>133</em>, 9–34.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>If you are wondering why this formation provides the same model as in the previous chapter, you can show this using the properties of the Normal distribution. In the second formulation, you can view <span class="math inline">\(Y\)</span> as a linear transformation of the error <span class="math inline">\(\epsilon\)</span>: <span class="math inline">\(Y_i = a + b \epsilon\)</span>, with <span class="math inline">\(a = \mu\)</span> and <span class="math inline">\(b=1\)</span>. The mean of <span class="math inline">\(Y\)</span> is then <span class="math inline">\(a + \mu_\epsilon = \mu + 0 = \mu\)</span>, and the standard deviation of <span class="math inline">\(Y\)</span> equals <span class="math inline">\(\sqrt{|b| \sigma} = 1 \times \sigma = \sigma\)</span>, which are the same parameters as when directly specifying the Normal distribution for <span class="math inline">\(Y\)</span>.<a href="simple-linear-regression.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>It’s probably unwise to mix statistics and politics, but wouldn’t that be an utopia?<a href="simple-linear-regression.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Note that we cannot use Equation <a href="simple-linear-regression.html#eq:simple-regression-slope-estimate">(4.5)</a> to estimate the slope now. There is no nice formula for the slope estimate when you fix the intercept to 0, and so you will have to rely on statistical software to do this.<a href="simple-linear-regression.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-simple-GLM.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"source": null,
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
