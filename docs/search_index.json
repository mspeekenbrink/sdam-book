[["a-model-of-means-anova.html", "Chapter 7 A model of means (ANOVA) 7.1 Can playing Tetris reduce intrusive memories? 7.2 Comparing two groups 7.3 The ANOVA model 7.4 Contrast coding 7.5 Default orthogonal coding schemes 7.6 Multiple testing and post-hoc tests", " Chapter 7 A model of means (ANOVA) 7.1 Can playing Tetris reduce intrusive memories? After a traumatic experience, some people experience flashbacks, which are intrusive and involuntary memories that involve vivid imagery related to the traumatic event. These intrusive memories can be highly distressing and are a hallmark of acute stress disorder and posttraumatic stress disorder (PTSD). It has been suggested that recalling traumatic memories under certain conditions can reduce their negative impact. Memory consolidation refers to a collection of neural processes which stabilize a memory trace after it is aqcuired. According to reconsolidation theory, when a consolidated memory trace is reactivated (remembered), it again becomes malleable and will require restabilization for it to persist. Disruption of this reconsolidation after recall may then be a way to reduce the strength of traumatic memories, or even allow them to be forgotten. James et al. (2015) conducted a study to investigate this idea. They reasoned that because intrusive memories of trauma are often visual in nature, performing a demanding visuospatial task (e.g. playing the computer game Tetris) after recall could interfere with the reconsolidation process and reduce subsequent intrusions of the traumatic memory. In their Experiment 2, they first created traumatic memories by showing their participants an 12-minute film with graphic scenes depicting death and serious injury (e.g. a van hitting a teenage boy while he was using his mobile phone crossing the road). Participants then went home and recorded the number of intrusive memories of the film during the subsequent 24-hour period (Day 0). The next day, they returned to the lab and were randomly assigned to one of four conditions: No-task control: participants in this condition (\\(n=18\\)) completed a 10-minute music filler task, rating excerpts of classical music for pleasantness. Tetris+Reactivation: participants in this condition (\\(n=18\\)) were shown a series of images from the scenes in the trauma film to reactivate the memories of the scenes. After this reactivation task, they completed the 10-minute music filler task, and then played the video game Tetris for 12 minutes. Tetris-Only: participants in this condition (\\(n=18\\)) performed the music filler task and then played Tetris for 12 minutes, but did not complete the reactivation task. Reactivation Only: participants in this condition (\\(n=18\\)) completed the reactivation and music filler task, but did not play Tetris. All participants then went home and were asked to record the number of intrusive memories they experienced over the next seven days (Day 1 to 7). After this week passed, participants returned to the lab and completed an Intrusion-Provocation Task, in which they were shown blurred images from the trauma film and asked to indicate whether each of these triggered an intrusive memory. The sample means and standard deviations of the number of intrusions in each condition are: \\(\\overline{Y}\\) \\(S_Y\\) Control 5.11 4.11 Tetris+Reactivation 1.89 1.70 Tetris-Only 3.89 2.81 Reactivation-Only 4.83 3.24 Boxplots for each condition are provided in Figure 7.1. Figure 7.1: Number of intrusive memories during days 1 to 7 for each condition of Experiment 2 of James et al. (2015). 7.2 Comparing two groups To start with a relatively straightforward example, let’s first focus only on the data from the Tetris+Reactivation and the Reactivation-Only conditions. We are interested in whether playing Tetris during reactivation reduces the number of memory intrusions on later days, in comparison to when the traumatic memory was reactivated only. To investigate this, we will extend the simple model of Chapter 3. In particular, we will assume that the number of intrusions is Normal-distributed with a mean that depends on the experimental condition participants were assigned to (Tetris+Reactivation or Reactivation-Only). We can write this model in two equivalent ways as: \\[Y_i \\sim \\mathbf{Normal}(\\mu_{\\text{con}},\\sigma)\\] and \\[\\begin{equation} Y_i = \\mu_\\text{con} + \\epsilon_i \\quad \\quad \\epsilon_i \\sim \\mathbf{Normal}(0,\\sigma) \\tag{7.1} \\end{equation}\\] where \\(\\mu_\\text{con}\\) is a placeholder for the mean of a particular condition. Specifying that mean explicitly, we can also write the model as \\[Y_{i} = \\begin{cases} \\mu_\\text{t+r} + \\epsilon_{i} \\hspace{2em} \\text{if condition = Tetris+Reactivation} \\\\ \\mu_\\text{react} + \\epsilon_{i} \\hspace{2em} \\text{if condition = Reactivation-Only} \\end{cases} \\quad \\quad \\epsilon_i \\sim \\mathbf{Normal}(0,\\sigma_\\epsilon)\\] Note that the model assumes that while the mean can differ between the conditions, deviations of the observations around the mean are assumed to have the same standard deviance \\(\\sigma_\\epsilon\\). An example of what the model might look like is given in Figure 7.2. Figure 7.2: The Normal density function for \\(\\mu = 0\\) and \\(\\sigma = 1\\) Can we estimate such a model, using the tools we have already learned about? Yes! It is actually quite straightforward to construct a linear model to represent the model with means depending on condition. Condition is a nominal variable, which we can’t simply include “as is” in a linear model. Linear models need metric predictors. But we can construct a new predictor \\(X\\), which has the value 0 for participants in the Tetris+Reactivation condition, and the value 1 for participants in the Reactivation-Only condition. This predictor \\(X\\), with values 0 and 1 referring to different groups, is commonly referred to as a dummy coding variable. With this dummy predictor, the model \\[Y_i = \\beta_0 + \\beta_1 \\times X_i + \\epsilon_i \\quad \\quad \\epsilon_i \\sim \\mathbf{Normal}(0,\\sigma)\\] is formally equivalent to the model of Equation (7.1). To see why, it is important to realize that linear regression concerns the conditional means of the dependent variable given the values of the predictor variable(s). If we give the model enough flexibility (and I will discuss more in detail what that means later), then the model will be able to represent those conditional means accurately. Let’s write out the structural part of the model (excluding the error term \\(\\epsilon_i\\)) for a participant from each condition. For everyone in the Tetris+Reactivation condition, \\(X_i = 0\\), so the model predictions are \\[\\begin{align} \\hat{Y}_i &amp;= \\beta_0 + \\beta_1 \\times 0 \\\\ &amp;= \\beta_0 \\end{align}\\] For everyone in the Reactivation-Only condition, \\(X_i = 1\\), so the model are \\[\\begin{align} \\hat{Y}_i &amp;= \\beta_0 + \\beta_1 \\times 1 \\\\ &amp;= \\beta_0 + \\beta_1 \\end{align}\\] For cases in the Tetris+Reactivation condition, the model predictions are a constant: \\(\\beta_0\\). If we “rename” \\(\\beta_0 = \\mu_\\text{t+r}\\), then we have the the model we wanted for this condition. For cases in the Reactivation-Only condition, the prediction consists of the sum of two constants, \\(\\beta_0 + \\beta_1\\). A sum of two constants is itself a constant. Moreover, because we have already “renamed” \\(\\beta_0 = \\mu_\\text{tetr}\\), we can also write the sum as \\(\\mu_\\text{react} = \\mu_\\text{t+r} + \\beta_1\\), which if we move \\(\\mu_\\text{t+r}\\) to the left-hand side, indicates that \\(\\beta_1 = \\mu_\\text{react} - \\mu_\\text{t+r}\\). In words, the slope of our predictor equals the difference between the mean of the the Reactivation-Only and the mean of Tetris+Reacivation condition. Remember that the linear model represents the conditional mean of the dependent variable for each set of values of the predictor variables. We have just constructed a predictor with a different value for cases in each condition, and we could write the resulting model predictions in terms of two parameters (the intercept and the slope) which we could relate to the means in each condition. In Chapter 3, we showed that the maximum likelihood estimate of the mean \\(\\mu\\) of a Normal-distributed variable is the sample mean \\(\\overline{Y}\\). In a model with multiple means, the maximum likelihood estimates are the respective sample means in each group (condition). In the model we just constructed, the parameter estimates are a function of the sample means in each group. More precisely, the estimate of the intercept equals the mean in the Tetris+Reacivation condition: \\[\\hat{\\beta}_0 = \\overline{Y}_\\text{t+r}\\] and the estimate of the slope equals the difference between the sample mean of the the Reactivation-Only and the sample mean of Tetris+Reacivation condition: \\[\\hat{\\beta}_1 = \\overline{Y}_\\text{react} - \\overline{Y}_\\text{t+r}\\] The average number of intrusive memories in the Tetris+Reacivation condition was \\(\\overline{Y}_\\text{t+r} = 1.89\\), and the corresponding mean in the Reacivation condition was \\(\\overline{Y}_\\text{react} = 4.83\\). Estimating the regression model provides the following estimates \\[\\texttt{intrusions}_i = 1.89 + 2.94 \\times \\texttt{dummy}_i + \\hat{\\epsilon}_i \\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, 2.66)\\] As you can see, the intercept equals \\(\\overline{Y}_\\text{t+r}\\), and the slope of \\(\\texttt{dummy}\\) equals \\(\\overline{Y}_\\text{react} - \\overline{Y}_\\text{t+r} = 4.83 - 1.89 = 2.94\\). The results of the hypothesis tests that the intercept and slope equal 0 are given in Table 7.1. The test result for the intercept indicates that the average number of intrusions in the Tetris+Reactivation condition is different than zero. More interestingly, the test result for the slope of \\(\\texttt{dummy}\\) indicates that the difference between the Tetris+Reactivation and Reactivation-Only condition is not equal to 0. In other words, there is a difference between the conditions in the average number of intrusions. Playing Tetris after memory reactivation seems to reduce the number of subsequent intrusions. Table 7.1: Linear model predicting number of intrusions by a dummy predictor. \\(\\hat{\\beta}\\) \\(\\text{SS}\\) \\(\\text{df}\\) \\(F\\) \\(P(\\geq \\lvert F \\rvert)\\) Intercept 1.89 64.2 1 9.09 0.005 \\(\\texttt{dummy}\\) 2.94 78.0 1 11.04 0.002 Error 240.3 34 A test comparing the means of two Normal-distributed variables is also known as an independent samples t-test. It involves an extension of the one-sample \\(t\\) test discussed in Chapter 3 and is based on the sampling distribution of the difference between two sample means. As the mathematical details of that test do not provide any new insights, we omit them here. A main thing to realise is that this test will provide identical results to the test of the slope of \\(\\texttt{dummy}\\). If you’d take the square-root of the \\(F\\) statistic for this test, you obtain the value of the \\(t\\) statistic of the independent samples t-test. 7.3 The ANOVA model The dummy coding procedure above can be generalized to the situation in which you want to compare more than two groups. In the Tetris study, there were four conditions. To allow a linear model to represent the means in all four conditions, you need to use more than one dummy-coding predictor. In fact, you would need 3 dummy-coding predictors. In such a model, the intercept represents the mean of one condition, which we can call the reference group. The slope of each predictor represents the difference between the mean of the remaining groups and the reference group. Whilst dummy coding is simple and provides interpretable parameters, there are alternative coding schemes that may provide more interesting tests. Before going into such alternative coding schemes, we will take a slight detour and consider the traditional model for the case of Normal-distributed variables in multiple groups with (potentially) different means, but the same standard deviation. This model is also called the oneway ANOVA model, and can be stated as follows: \\[\\begin{equation} Y_{j,i} = \\mu + \\tau_j + \\epsilon_{j,i} \\quad \\quad \\epsilon_{j,i} \\sim \\textbf{Normal}(0, \\sigma_\\epsilon) \\tag{7.2} \\end{equation}\\] Here, \\(Y_{j,i}\\) denotes the value of the dependent variable for case \\(i = 1,\\ldots, n_j\\) in group \\(j = 1, \\ldots, g\\). So \\(g\\) denotes the total number of groups (i.e. \\(g=4\\) in the Tetris study), and \\(n_j\\) the number of cases in group \\(j\\) (i.e. \\(n_j = 18\\) in the Tetris study). \\(\\epsilon_{j,i}\\) is the corresponding error term for participant \\(i\\) in group \\(j\\). The mean \\(\\mu\\) is the so-called “grand mean”, which is the overall mean of the dependent variable, the of any observation that could be produced by the Data Generating Process, regardless of group. The term \\(\\tau_j\\) represents the so-called “treatment effect” of group \\(j\\), defined as the difference between the mean of group \\(j\\) and the grand mean: \\[\\tau_j = \\mu_j - \\mu\\] The traditional goal of an ANOVA is to determine whether there is any treatment effect. That is, to test the null-hypothesis \\(H_0: \\tau_j = 0 \\text{ for all } j = 1, \\ldots, g\\). Note that the hypothesis states that \\(\\tau_j = 0\\) for all groups. We could have also stated this hypothesis as \\(H_0: \\tau_1 = \\tau_2 = \\ldots = \\tau_g = 0\\), if you find that clearer. As the hypothesis states that the difference between the group-specific mean \\(\\mu_j\\) and the grand mean \\(\\mu\\) is 0 for every group, the implication is that \\(\\mu_j = \\mu\\) for all groups (i.e. all groups have an identical mean \\(\\mu\\)). The test statistic for this null-hypothesis is the \\(F\\) statistic, calculated as a ratio of the (estimated) sample variance of the treatment effects and the (estimated) variance of residual error terms. As we will see, we can perform this hypothesis test in with the General Linear Model through an overall model test (comparing a “full” MODEL G to an intercept-only MODEL R). In addition, we can also test more specific hypotheses regarding differences between the group means and the overall mean, or specific hypotheses regarding differences between particular (combinations of) groups. In contrast to traditional ANOVA, the GLM approach also deals naturally with situations in which the groups have unequal sizes (i.e. \\(n_j\\) differs between the groups). This is not straightforward with the traditional ANOVA test. 7.4 Contrast coding As in the case of two groups, the approach to testing group differences in the GLM is to construct new predictor variables, which we might call contrast-coding predictors, which represent differences between groups. To illustrate the general concepts of contrast coding in a hopefully intuitive manner, let’s first consider a game in which you ask someone to pick a random number between 1 and 8 and your job is to determine the number they picked by asking questions which can be answered by “yes” or “no”. There are different ways in which you can play this game, and some of these are more efficient than others. For instance, you can ask “Is the number 1?”, “Is the number 2?”, “Is the number 3?”, etc. If they picked the number 1, then you would have needed just a single question, but if they picked the number 8, you would have needed a total of 7 questions. After having asked “Is the number 7?”, you would not need to ask whether the number was 8, because that would be the only remaining possibility. To uniquely determine any number between 1 and 8, your playing strategy will always consist of 7 questions. You would not need to ask all of these questions in any given play of the game, but over many plays with the same question strategy, you would ask a total of 7 unique questions. The strategy above is a bit like dummy coding. The final number 8 is the reference group, and each question such as “Is the number 1?” is implicitly the same as “Is the number 1 and not 8?”. A different way to play the game is by a strategy which guarantees you to always correctly “guess” the number in three questions. This strategy is depicted in Figure 7.3. In this strategy, each question halves the number of remaining options. If the first question “Is the number larger than 4?” is answered as “yes”, the number can only be 5, 6, 7, or 8. Subsequently asking “Is the number larger than 6?” would reduce the remaining options by half again. If the answer to this question was “yes”, then the number would have to be either 7 or 8. Asking “Is the number 7?” would then allow you to finish the game (if the answer is “yes”, the number is 7, if “no”, it would have to be 8). Just like the earlier strategy, this strategy consists of a total of 7 questions (and in any given game, you would ask 3 of these). Figure 7.3: Questions to guess a random number between 1 and 8 Constructing contrast codes can be seen as more complicated version of the game above. Suppose that instead of guessing a single number, the other person can assign numbers to 8 letters \\(A, B, \\ldots, H\\), and your job is to determine what all these numbers are. You are given one hint, and then you can ask questions only about what the differences are between the number(s) assigned to (combinations of) the letters. In the case of our analysis situation, the letters represent the groups, and the numbers the averages of those groups. This game is less fun to play than the earlier version, but its more interesting from a data analysis perspective. Let’s get back to the Tetris study. There were four conditions: the no-task Control condition, Tetris+Reactivation, Tetris-Only, and Reactivation-Only. Remember, your job is not to determine which condition someone was in. This can be answered with a total of two yes-no questions (“Did the condition involve playing Tetris?” and “Did the condition involve memory reactivation?”). Your job is now to determine the average number of intrusions in each condition by asking questions about differences between the averages. To start, I will give you a hint, which is that the average over all conditions (i.e. the grand mean) is \\(\\hat{\\mu} = 3.93\\). One set of questions you could ask is the following: What is the difference between the mean of the Control condition and the grand mean? What is the difference between the mean of the Tetris+Reactivation condition and the grand mean? What is the difference between the mean of the Tetris-Only condition and the grand mean? The answers to these questions are \\(\\hat{\\mu}_\\text{contr} - \\hat{\\mu} = 5.11 - 3.93 = 1.18\\). \\(\\hat{\\mu}_\\text{t+r} - \\hat{\\mu} = 1.89 - 3.93 = -2.04\\). \\(\\hat{\\mu}_\\text{tetr} - \\hat{\\mu} = 3.89 - 3.93 = -0.0417\\). Because I have told you the value of \\(\\hat{\\mu}\\) already, you can simply add the value of this “hint” to the answer of each question to determine the mean of a condition. You do not need to ask a fourth question (“What is the difference between the mean of the reactivation-Only condition and the grand mean?”), because with the information provided, you would be able to determine this. Firstly, I should point out that \\[\\begin{align} \\hat{\\mu} &amp;= \\frac{\\hat{\\mu}_\\text{contr} + \\hat{\\mu}_\\text{t+r} + \\hat{\\mu}_\\text{tetr} + \\hat{\\mu}_\\text{react}}{4} \\\\ &amp;= \\frac{\\overline{Y}_\\text{contr} + \\overline{Y}_\\text{t+r} + \\overline{Y}_\\text{tetr} + \\overline{Y}_\\text{react}}{4} \\end{align}\\] i.e., the estimate of the grand mean is a “mean of means”. From this, it directly follows that \\[\\hat{\\mu}_\\text{react} = 4 \\times \\mu - \\hat{\\mu}_\\text{contr} - \\hat{\\mu}_\\text{t+r} - \\hat{\\mu}_\\text{tetr}\\] and the treatment effect can also be determined as \\[\\hat{\\mu}_\\text{react} - \\hat{\\mu} = -1 \\times\\left( (\\hat{\\mu}_\\text{contr} - \\hat{\\mu}) + (\\hat{\\mu}_\\text{t+r} - \\hat{\\mu}) + (\\hat{\\mu}_\\text{tetr} - \\hat{\\mu}) \\right)\\] i.e. as minus one times the sum of the treatment effect of the other conditions. It is precisely for dependencies like these that when constructing contrast codes for a nominal variable representing group membership, you need \\(g-1\\) (the number of groups minus 1) contrast codes. 7.4.1 Effect coding The questions about deviations between group means and the grand mean (the treatment effects) correspond to the following three contrast codes \\(c_j\\): \\(c_1\\) \\(c_2\\) \\(c_3\\) Control 1 0 0 Tetris+Reactivation 0 1 0 Tetris-Only 0 0 1 Reactivation-Only -1 -1 -1 Just like for the dummy coding example discussed above, the idea is to construct a new predictor for each of these contrast codes. The contrast codes have values for each condition. The corresponding contrast-coding predictor has a value for each case \\(i\\) in the data, where we give all cases in a condition the corresponding value of \\(c_j\\) for that condition. For example, the first predictor \\(X_1\\), which corresponds to the first contrast code \\(c_1\\), would have the value \\(X_{1,i} = 1\\) if case \\(i\\) is in the Control condition, the value \\(X_{1,i} = 0\\) if case \\(i\\) is in the Tetris+Reactivation or Tetris-only condition, and the value \\(X_{1,i} = -1\\) if case \\(i\\) is in the Reactivation-Only condition. Similarly, the second predictor \\(X_2\\), which corresponds to the second contrast code \\(c_2\\), would have the value \\(X_{2,i} = 0\\) if case \\(i\\) is in the Control condition, the value \\(X_{2,i} = 1\\) if case \\(i\\) is in the Tetris+Reactivation condition, the value \\(X_{2,i} = 0\\) if case \\(i\\) is in the Tetris-only condition, and the value \\(X_{2,i} = -1\\) if case \\(i\\) is in the Reactivation-Only condition. Having defined three contrast-coding predictors, \\(X_1\\), \\(X_2\\), and \\(X_3\\) in this manner, we can then estimate the linear (regression) model \\[Y_i = \\beta_0 + \\beta_1 \\times X_{1,i} + \\beta_2 \\times X_{2,i} + \\beta_3 \\times X_{3,i} + \\epsilon_i \\quad \\quad \\epsilon_i \\sim \\mathbf{Normal}(0,\\sigma_\\epsilon)\\] This model is treated as any other regression model. So after re-coding a nominal variable “condition” with four levels (Control, Tetris+Reactivation, Tetris-Only, and Reactivation-Only) with three contrast codes \\(c_1\\), \\(c_2\\), and \\(c_3\\), each with a corresponding predictor \\(X_1\\), \\(X_2\\), and \\(X_3\\), we end up with a linear model that is effectively like any other multiple regression model. Estimating the model gives: \\[\\texttt{intrusions}_i = 3.93 + 1.18 \\times \\texttt{}X_1\\texttt{}_i - 2.04 \\times \\texttt{}X_2\\texttt{}_i - 0.0417 \\times \\texttt{}X_3\\texttt{}_i + \\hat{\\epsilon}_i \\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, 3.18)\\] Figure 7.4 shows how the parameters of the model are related to the average number of intrusions in each condition. Figure 7.4: Average number of instrusions in the four conditions in the Tetris study and how they are related to the parameters of the effect-coding model. The intercept \\(\\hat{\beta}_0 = \\hat{\\mu}\\) is the grand mean (dotted line). The slopes reflect treatment effects, which are deviations from the average intrusions in a condition and the grand mean. The averages of the first three conditions are equal to \\(\\beta_0 + \\beta_j\\), the sum of the intercept and the slope of the effect-coding predictor \\(X_j\\) representing the treatment effect of that condition. The mean of the last condition is the intercept minus the sum of the slopes of the effect-coding predictors. Test results for the parameters of the model are given in Table 7.2. As can be seen there, the test of the intercept is significant. In this model, the intercept represents the “grand mean” \\(\\mu\\) (the average of the means in each condition). The test indicates that the true value of the grand mean is unlikely to be 0. In addition, the slope of \\(X_2\\) is significant. This slope is equal to the treatment effect of the Tetris+Reactivation condition (i.e. \\(\\hat{\\beta}_2 = \\hat{\\mu}_{t+r} - \\hat{\\mu}\\)). The test is a test of the null-hypothesis \\(H_0: \\beta_2 = 0\\), and this test involves a comparison of the models \\[\\begin{align} \\text{MODEL G}: &amp;&amp; Y_i &amp;= \\beta_0 + \\beta_1 \\times X_{1,i} + \\beta_2 \\times X_{2,i} + \\beta_3 \\times X_{3,i} + \\epsilon_i \\\\ \\text{MODEL R}: &amp;&amp; Y_i &amp;= \\beta_0 + \\beta_1 \\times X_{1,i} + \\beta_3 \\times X_{3,i} + \\epsilon_i \\end{align}\\] The comparison indicates that fixing the slope \\(\\beta_2 = 0\\) in MODEL R results is a substantial increase in the Sum of Squared Error of MODEL R compared to MODEL G. As such, there is evidence that the true value of this slope does not equal 0, and with that, that the true treatment effect of the Tetris+Reactivation condition does not equal 0, i.e. that \\(\\mu_{t+r} - \\mu \\neq 0\\). Furthermore, the estimate of the treatment effect is \\(\\hat{\\beta}_2 = \\hat{\\mu}_{t+r} - \\hat{\\mu} = \\overline{Y}_{t+r} - \\frac{\\overline{Y}_\\text{contr} + \\overline{Y}_\\text{t+r} + \\overline{Y}_\\text{tetr} + \\overline{Y}_\\text{react}}{4} = -2.042\\), which indicates that the number of intrusions in this condition is lower than the grand mean. As such, we would conclude that playing Tetris after memory reactivation reduces the subsequent memory intrusions. The tests of the slopes of \\(X_1\\) and \\(X_3\\) are not significant. We therefore do not have sufficient evidence that the treatment effect of the Control condition, or of the Tetris-only condition, are different to 0. Table 7.2: Linear model predicting number of intrusions by three effect-coded predictors. \\(\\hat{\\beta}\\) \\(\\text{SS}\\) \\(\\text{df}\\) \\(F\\) \\(P(\\geq \\lvert F \\rvert)\\) Intercept 3.931 1112.347 1 110.289 0.000 Condition 114.819 3 3.795 0.014 \\(\\quad X_1\\) 1.181 33.449 1 3.316 0.073 \\(\\quad X_2\\) -2.042 100.042 1 9.919 0.002 \\(\\quad X_3\\) -0.042 0.042 1 0.004 0.949 Error 685.833 68 Table 7.2 also includes a row labelled “Condition”. This is a test of the hypothesis that all of the slopes of the contrast-coding predictors for Condition are equal to 0: \\[H_0\\!: \\beta_1 = \\beta_2 = \\beta_3 = 0\\] This hypothesis test is based on comparing the following two models: \\[\\begin{align} \\text{MODEL G}: &amp;&amp; Y_i &amp;= \\beta_0 + \\beta_1 \\times X_{1,i} + \\beta_2 \\times X_{2,i} + \\beta_3 \\times X_{3,i} + \\epsilon_i \\\\ \\text{MODEL R}: &amp;&amp; Y_i &amp;= \\beta_0 + \\epsilon_i \\end{align}\\] i.e., it is a “whole model test”. The result of this so-called omnibus test (a simultaneous test of multiple parameters) is significant, indicating that it is unlikely that the true values of the slopes are all equal to 0. In MODEL R above, there is only a single parameter \\(\\beta_0\\). As such, this model predicts that all conditions have the same mean \\(\\mu\\). This test is thus a test of the null-hypothesis that all treatment effects for Condition are equal to 0: \\[H_0\\!: (\\mu_\\text{contr} - \\mu) = (\\mu_\\text{t+r} - \\mu) = (\\mu_\\text{tetr} - \\mu) = (\\mu_\\text{react} - \\mu) = 0\\] which is also equivalent to the test that the means of all conditions are equal to eachother: \\[H_0\\!: \\mu_\\text{contr} = \\mu_\\text{t+r} = \\mu_\\text{tetr} = \\mu_\\text{react} \\] The result of the omnibus test indicates that there is at least one treatment effect which is different from 0. Omnibus tests for all slopes reflecting treatment effects are what is traditionally focused on in an ANOVA. But these omnibus tests are not always that informative. We’d generally like to know more than “there is at least one treatment that is likely to have an effect”. It seems inherently more interesting to know which conditions are associated with a treatment effect. This is where the tests of the individual slopes come in handy. Assessing the effect of the three effect-coding predictors, we can conclude that only the treatment effect of the Tetris+Reactivation condition is significant. As such, we only have sufficient evidence that a combination of memory reactivation and playing Tetris changes the number of subsequent memory intrusions. At this point, I’d like to make some important remarks. Firstly, the absence of sufficient evidence that any of the other treatment effects differs from 0 should not be taken as direct evidence that the true treatment effects equal 0. A non-significant result indicates a lack of evidence against the null-hypothesis, but not an abundance of evidence for the null hypothesis. You can think of this as follows: that a suspect in a murder trial is not able to provide evidence that she is innocent is in itself not sufficient evidence that she is guilty. You might also think of black swans. While the empirical statement “All swans are white” is impossible to prove conclusively without checking the colour of all swans that have and will ever grace this world, finding a single black swan disproves that statement immediately (Popper, 1959). Although significance testing does not provide conclusive evidence for or against the null-hypothesis1, the analogy can be described as follows: a significant test is like spotting a swan that is “off-white” enough for you to decide it is not exactly white. But not having spotted such a swan could either mean that such a swan does not exist (the null hypothesis is true), or that you have not searched hard enough (the null hypothesis is false, but your test lacked power). Secondly, the effect-coding predictors only reflect three out of a total of four treatment effects. The treatment effect of the reactivation-Only condition follows directly as minus the sum of these three treatment effects. As such, it is redundant. But we don’t have a hypothesis test for this redundant treatment effect. If we had chosen a slightly different coding scheme, where we would have estimated the treatment effects of the Control, Tetris-Only, and Reactivation-Only condition, such that the treatment effect of the Tetris+Reactivation condition is redundant, then none of the significance tests of the slopes of this different model would have been significant. The omnibus test would be give exactly the same results, thus indicating that at least one treatment effect does not equal 0, but we couldn’t have easily spotted which treatment effect(s) these were. Whilst the estimates and slopes of contrast-coding predictors are often more specific and informative than an omnibus test, because we can only use \\(g-1\\) of such predictors (one less than the number of groups), we can’t test for all treatment effects in a single model, nor test all hypotheses that we might be interested in. We will come back to this when we discuss “multiple testing” approaches. For now, a main thing to realise is that a significant omnibus test indicates that at least two groups differ in their means. If none of tests of the slopes of the contrast-coding predictors in a linear model are significant, but you have obtained a significant omnibus test, that indicates that none of these predictors encoded that specific difference. Whilst some authors bemoan the use of omnibus tests, they have a role to play in the inference process, for instance in spotting whether you have missed a potentially important effect. Other authors put too much emphasis on omnibus tests, for instance requiring a significant omnibus tests before you might consider tests of the individual slopes that comprise this omnibus test. It is perfectly possible for an omnibus test to be non-significant, whilst a slope for one (or more) contrast-coding predictors is significant. For instance, if you’d conduct an experiment with 10 conditions, and only on has an actual treatment effect, the omnibus test might be non-significant because it effectively assigns that one treatment effect to nine parameters (slopes for nine contrast-coding predictors). Let’s take an extreme example, where only \\(X_1\\) (the first contrast-coding predictor) reduces the SSE with \\(\\text{SSR}(X_1) &gt; 0\\), whilst the other predictors provide no reduction in the SSE whatsoever (i.e. \\(\\text{SSR}(X_j) = 0\\) for \\(j=2, \\ldots, 9\\)). Then the \\(F\\) statistic of the omnibus test might be \\[\\begin{align} F &amp;= \\frac{\\frac{\\text{SSE}(R) - \\text{SSE}(G)}{\\text{npar}(G) - \\text{npar}(R)}}{\\frac{\\text{SSE}(G)}{n-\\text{npar}(G)}} \\\\ &amp;= \\frac{\\text{SSR}(X_1)/9}{\\text{SSE}(G)/(n-10)} \\end{align}\\] whilst the \\(F\\) statistic for the slope of \\(X_1\\) would be \\[\\begin{align} F &amp;= \\frac{\\frac{\\text{SSE}(R) - \\text{SSE}(G)}{\\text{npar}(G) - \\text{npar}(R)}}{\\frac{\\text{SSE}(G)}{n-\\text{npar}(G)}} \\\\ &amp;= \\frac{\\text{SSR}(X_1)/1}{\\text{SSE}(G)/(n-10)} \\end{align}\\] which, with a higher value and smaller \\(\\text{df}_1\\), would be more likely to be significant. Thirdly, when you look at the value of \\(\\text{SSR}(\\text{Condition}) = 115\\) in Table 7.2, you can see that it is smaller than the sum of the SSR terms corresponding to the three predictors. This indicates that there is redundancy between the predictors \\(X_1\\), \\(X_2\\), and \\(X_3\\) (i.e., some form of multicollinearity). Although Venn diagrams such as Figure ?? imply that the SSR of the full model would be larger than the sum of the unique SSR terms attributable to each predictor, the opposite can also be true! This situation is commonly referred to as suppression and is rather difficult to explain here without a lengthy and detailed detour. For now, we will therefore leave it to the interested reader to consult other sources on this, and simply not that if the sum of the SSR terms of the predictors does not equal the “whole model” SSR, this indicates redundancy between the predictors. 7.4.2 Orthogonal contrast codes Whilst redundancy is not necessarily a problem, it is in some sense preferable if all the predictors in a linear model are independent, as we can then neatly distribute the total SSR to each predictor. When the number of cases in each group (\\(n_j\\)) is equal for all groups, there are contrast coding schemes that ensure that the resulting contrast-coding predictors are non-redundant (i.e., independent). Such coding schemes are called orthogonal contrast codes. There are some benefits to employing orthogonal contrast codes, although these benefits are sometimes overstated. Firstly, using orthogonal contrast codes ensures that the model predictions will exactly equal to (sample) averages in the conditions – as long as no additional predictors are included in the model; we will discuss such additional predictors in the context of Analysis of Covariance (ANCOVA). Whilst redundant coding schemes such as dummy coding or effect coding (amongst others) also ensure this, when you start defining your own contrast coding schemes, it might be difficult to check whether this is the case, and then resorting to orthogonal contrast codes may provide useful guidance. Secondly, as already mentioned, using independent predictors will ensure that the whole model SSR is completely distributed over the SSR terms for the individual predictors. This makes it somewhat less likely that you will miss a difference between the groups in the tests of individual parameters that would be identified in the whole model test. Thirdly, using orthogonal contrast codes provides a general formula to state the estimate of the slopes of contrast-coding predictors in terms of the averages of the groups. If you use a set of orthogonal contrast codes, then the estimated slope of each predictor \\(X_j\\) corresponding to contrast code \\(c_j\\) can be expressed as the following function of the values of \\(c_{j,k}\\) contrast code \\(c_j\\) for group \\(k\\) and the sample means \\(\\overline{Y}_k\\) of the dependent variable in group \\(k\\) as: \\[\\begin{equation} \\hat{\\beta}_j = \\frac{\\sum_{k=1}^{g} c_{j,k} \\overline{Y}_k}{\\sum_{k=1} c_{j,k}^2} \\tag{7.3} \\end{equation}\\] i.e. as the sum of the sample means \\(\\overline{Y}_k\\) multiplied by the group-wise values of contrast code \\(c_j\\), divided by the sum of \\(c_{j,k}^2\\), the squared group-wise values of contrast \\(c_j\\). Note that this formula does not hold for the effect-coding scheme discussed previously. For instance, if you’d fill in the values of \\(c_1\\), you’d get \\[\\begin{align} \\hat{\\beta}_1 &amp;= \\frac{1 \\times \\overline{Y}_\\text{contr} + 0 \\times \\overline{Y}_\\text{t+r} + 0 \\times \\overline{Y}_\\text{tetr} - 1 \\times \\overline{Y}_\\text{react}}{(1)^2 + (0)^2 + (0)^2 + (-1)^2} \\\\ &amp;= \\frac{\\overline{Y}_\\text{contr} - \\overline{Y}_\\text{react}}{2} \\end{align}\\] which is obviously not the same as the treatment effect the estimated slope actually reflects: \\[\\hat{\\beta}_1 = \\overline{Y}_\\text{contr} - \\frac{\\overline{Y}_\\text{contr} + \\overline{Y}_\\text{t+r} + \\overline{Y}_\\text{tetr} + \\overline{Y}_\\text{react}}{4}\\] Nevertheless, the slopes of the effect-coding scheme gave us interpretable parameters, hence using non-orthogonal contrast codes does not prohibit interpretable parameter estimates. With that said, if you can define comparisons you would like to make in terms of a set of orthogonal contrast codes, that would be preferable. If you can’t, there are ways to deal with that too, and you shouldn’t worry too much. There are two rules you can follow to create orthogonal contrast codes: For all contrast codes \\(c_j\\), \\(j = 1, \\ldots, g-1\\), \\(\\sum_{k=1}^g c_{j,k} = 0\\). The sum of the values of each contrast code \\(j\\) is zero. For all pairs of contrast codes \\(c_j\\) and \\(c_l\\), \\(j \\neq l\\), \\(\\sum_{k=1}^g c_{j,k} \\times c_{l,k} = 0\\). The sum of the cross-products of each pair of contrast codes \\(j\\) and \\(l\\) is equal to zero. In the statements above, \\(c_{j,k}\\) refers to the value of a contrast code \\(c_j\\) for group \\(k\\), and similarly \\(c_{l,k}\\) refers to the value of a contrast code \\(c_l\\) for group \\(k\\), whilst \\(g\\) refers to the total number of groups. Only the second rule is strictly necessary, but the first one is a useful one to follow as well, as –like in the effect coding scheme– it ensures that the intercept represents the grand mean. If we check the two rules for the effect coding scheme, we can see that the first requirement is adhered to: \\[\\begin{align} \\sum_{k=1}^g c_{1,k} &amp;= 1 + 0 + 0 + (-1) = 0 \\\\ \\sum_{k=1}^g c_{2,k} &amp;= 0 + 1 + 0 + (-1) = 0 \\\\ \\sum_{k=1}^g c_{3,k} &amp;= 0 + 0 + 1 + (-1) = 0 \\end{align}\\] However, the second is not: \\[\\begin{align} \\sum_{k=1}^g c_{1,k} \\times c_{2,k} &amp;= 1 \\times 0 + 0 \\times 1 + 0 \\times 0 + (-1) \\times (-1) = 1 \\\\ \\sum_{k=1}^g c_{1,k} \\times c_{3,k} &amp;= 1 \\times 0 + 0 \\times 0 + 0 \\times 1 + (-1) \\times (-1) = 1 \\\\ \\sum_{k=1}^g c_{2,k} \\times c_{3,k} &amp;= 0 \\times 0 + 0 \\times 1 + 0 \\times 1 + (-1) \\times (-1) = 1 \\end{align}\\] One set of questions you could ask, which will result in orthogonal contrast codes, is the following: What is the difference between doing nothing (the Control condition) and doing something (the three remaining conditions)? What is the difference between playing Tetris (the Tetris-Only condition) and a procedure involving memory reactivation (the Tetris+Reactivation and reactivation-Only condition)? What is the difference between memory reactivation without playing Tetris (the Reactivation-only condition) and the playing Tetris after reactivation (the Tetris+Reactivation condition)? To define out contrast codes, we need to define values for the contrast codes such that the slopes of the predictors based on them provide the answers to these questions. The first question refers to the difference between the Control condition and the three other conditions. As the answer to the question should be a single deviation, it makes sense to compare the mean of the Control condition to the average of the three remaining conditions combined. In this combination, we would not distinguish between those three conditions, and hence we should give each of those three conditions the same value. In addition, remember that a slope reflects an in- or decrease in the dependent variable for a one-unit increase in the predictor. It then makes sense to let the difference between the value we assign to the Control condition and the values we assign to the three other conditions equal 1. Finally, we would like to use values such that their sum over the groups equals 0. Combining these three ideas, to answer the first question, we could define a contrast code \\(c_1\\) with value \\(\\tfrac{3}{4}\\) for the control condition, and the value \\(-\\tfrac{1}{4}\\) for the Tetris+Reactivation, Tetris-Only, and Reactivation-Only condition. In particular, note that these values sum to 0, and that the distance between \\(\\tfrac{3}{4}\\) and \\(-\\tfrac{1}{4}\\) equals 1. As such, a one-unit increase for this contrast code is equal to going from one of the three experimental conditions to the Control condition. For the second question, we would like to compare the Tetris-Only condition to the Tetris+Reactivation and Reactivation-Only condition combined. In this comparison, we ignore the Control condition. By giving this condition a value of 0 in the contrast code, it will not be entered in the comparison. Additionally using similar reasoning as before, the suggested second contrast code \\(c_2\\) has values \\(0, -\\tfrac{1}{3}, \\tfrac{2}{3}, -\\tfrac{1}{3}\\) for the Control, Tetris+Reactivation, Tetris-Only, and Reactivation-Only conditions respectively. Again, the difference between \\(\\tfrac{2}{3}\\) and \\(-\\tfrac{1}{3}\\) equals 1, so that a one-unit increase in this contrast code is equal to going from one of the reactivation conditions to the Tetris-Only condition. For the third question, finally, we would like to compare the Reactivation-Only condition to the Tetris+Reactivation condition, ignoring the other two conditions. The suggested contrast code \\(c_3\\) then has values \\(0, -\\tfrac{1}{2}, 0, \\tfrac{1}{2}\\) for the four conditions respectively. Here, a one-unit increase is equal to going from the Tetris+Reactivation condition to the Reactivation-Only condition. The values of the three contrast codes \\(c_1\\), \\(c_2\\), and \\(c_3\\) (for questions 1 to 3 respectively) for each of the four conditions, are summarized given in the table below: Table 7.3: A set of orthogonal contrast codes. \\(c_1\\) \\(c_2\\) \\(c_3\\) Control \\(\\tfrac{3}{4}\\) 0 0 Tetris+Reactivation \\(-\\tfrac{1}{4}\\) \\(-\\tfrac{1}{3}\\) \\(-\\tfrac{1}{2}\\) Tetris-Only \\(-\\tfrac{1}{4}\\) \\(\\tfrac{2}{3}\\) 0 Reactivation-Only \\(-\\tfrac{1}{4}\\) \\(-\\tfrac{1}{3}\\) \\(\\tfrac{1}{2}\\) Let’s check whether these three contrast codes are orthogonal. The first requirements (values sum to 0) holds: \\[\\begin{align} \\sum_{k=1}^g c_{1,k} &amp;= \\tfrac{3}{4} - \\tfrac{1}{4} - \\tfrac{1}{4} - \\tfrac{1}{4} = 0 \\\\ \\sum_{k=1}^g c_{2,k} &amp;= 0 - \\tfrac{1}{3} + \\tfrac{2}{3} - \\tfrac{1}{3} = 0 \\\\ \\sum_{k=1}^g c_{3,k} &amp;= 0 - \\tfrac{1}{2} + 0 + \\tfrac{1}{2} = 0 \\end{align}\\] And indeed the second requirement (pairwise products sum to 0) also holds: \\[\\begin{align} \\sum_{k=1}^g c_{1,k} \\times c_{2,k} &amp;= \\tfrac{3}{4} \\times 0 + (- \\tfrac{1}{4}) \\times (- \\tfrac{1}{3}) + (-\\tfrac{1}{4}) \\times \\tfrac{2}{3} + (- \\tfrac{1}{4}) \\times (- \\tfrac{1}{3}) \\\\ &amp;= \\tfrac{1}{12} - \\tfrac{2}{12} + \\tfrac{1}{12} = 0 \\\\ \\sum_{k=1}^g c_{1,k} \\times c_{3,k} &amp;= \\tfrac{3}{4} \\times 0 + (- \\tfrac{1}{4}) \\times (- \\tfrac{1}{2}) + (-\\tfrac{1}{4}) \\times 0 + (- \\tfrac{1}{4}) \\times \\tfrac{1}{2} \\\\ &amp;= \\tfrac{1}{8} - \\tfrac{1}{8} = 0 \\\\ \\sum_{k=1}^g c_{2,k} \\times c_{3,k} &amp;= 0 \\times 0 + (-\\tfrac{1}{3}) \\times (- \\tfrac{1}{2}) + \\tfrac{2}{3} \\times 0 + (-\\tfrac{1}{3}) \\times \\tfrac{1}{2} \\\\ &amp;= \\tfrac{1}{6} - \\tfrac{1}{6} = 0 \\end{align}\\] Because the contrast codes \\(c_1\\), \\(c_2\\), and \\(c_3\\) are orthogonal, we can use Equation (7.3) to determine the estimates of the slopes. The estimated slope of the contrast-coding predictor \\(X_1\\) which takes its values from \\(c_1\\) will be: \\[ \\begin{align} \\hat{\\beta}_1 &amp;= \\frac{ \\tfrac{3}{4} \\times \\overline{Y}_\\text{contr} - \\tfrac{1}{4} \\times \\overline{Y}_\\text{t+r} - \\tfrac{1}{4} \\times \\overline{Y}_\\text{tetr} - \\frac{1}{4} \\times \\overline{Y}_\\text{react} }{ (\\frac{3}{4})^2 + (-\\frac{1}{4})^2 + (-\\frac{1}{4})^2 + (-\\frac{1}{4})^2 } \\\\ &amp;= \\frac{ \\tfrac{3}{4} \\times \\overline{Y}_\\text{contr} - \\tfrac{1}{4} \\times \\overline{Y}_\\text{t+r} - \\tfrac{1}{4} \\times \\overline{Y}_\\text{tetr} - \\frac{1}{4} \\times \\overline{Y}_\\text{react} }{ \\tfrac{3^2}{4^2} + \\tfrac{1^2}{4^2} + \\tfrac{1^2}{4^2} + \\tfrac{1^2}{4^2} } \\end{align} \\] The denominator evaluates to \\(\\tfrac{12}{16} = \\tfrac{3}{4}\\), and dividing both the numerator and denominator by \\(\\tfrac{3}{4}\\) gives2 \\[\\hat{\\beta}_1 = \\overline{Y}_\\text{contr} - \\frac{\\overline{Y}_\\text{t+r} + \\overline{Y}_\\text{tetr} + \\overline{Y}_\\text{react}}{3}\\] i.e. the slope of the first predictor is equal to the difference between the average intrusions in the Control condition, and the average of the average number of intrusions in the three other conditions. When we fill in the actual sample averages, we get: \\[ \\begin{align} \\hat{\\beta}_1 &amp;= \\frac{ \\tfrac{3}{4} \\times 5.11 - \\tfrac{1}{4} \\times 1.89 - \\tfrac{1}{4} \\times 3.89 - \\frac{1}{4} \\times 4.83 }{ \\tfrac{3}{4}} \\\\ &amp;= \\frac{1.181}{0.75} = 1.574 \\end{align} \\] The estimated slope of the contrast-coding predictor \\(X_2\\) which takes its values from \\(c_2\\) will be: \\[ \\begin{align} \\hat{\\beta}_2 &amp;= \\frac{ 0 \\times \\overline{Y}_\\text{contr} - \\tfrac{1}{3} \\times \\overline{Y}_\\text{t+r} + \\tfrac{2}{3} \\times \\overline{Y}_\\text{tetr} - \\frac{1}{3} \\times \\overline{Y}_\\text{react} }{ (0)^2 + (-\\frac{1}{3})^2 + (\\frac{2}{3})^2 + (-\\frac{1}{3})^2 } \\\\ &amp;= \\frac{ \\tfrac{2}{3} \\times \\overline{Y}_\\text{tetr} - \\tfrac{1}{3} \\times \\overline{Y}_\\text{t+r} - \\frac{1}{3} \\times \\overline{Y}_\\text{react} }{ \\tfrac{6}{9}} \\\\ &amp;= \\overline{Y}_\\text{tetr} - \\frac{\\overline{Y}_\\text{t+r} + \\overline{Y}_\\text{react}}{2} \\end{align} \\] The slope of the second predictor is thus equal to the difference between the average number of intrusions in the Tetris-Only condition, and the average of the average number of intrusions in the Tetris+Reactivation condition and the Reactivation-Only condition. Filling in the actual averages then gives. \\[ \\begin{align} \\hat{\\beta}_2 &amp;= \\frac{ 0 \\times 5.11 - \\tfrac{1}{3} \\times 1.89 + \\tfrac{2}{3} \\times 3.89 - \\frac{1}{3} \\times 4.83 }{ \\tfrac{2}{3} } \\\\ &amp;= \\frac{0.352}{0.667} = 0.528 \\end{align} \\] Finally, the estimated slope of the contrast-coding predictor \\(X_3\\) which takes its values from \\(c_3\\) will be: \\[ \\begin{align} \\hat{\\beta}_3 &amp;= \\frac{ 0 \\times \\overline{Y}_\\text{contr} - \\tfrac{1}{2} \\times \\overline{Y}_\\text{t+r} + 0 \\times \\overline{Y}_\\text{tetr} + \\frac{1}{2} \\times \\overline{Y}_\\text{react} }{ (0)^2 + (-\\frac{1}{2})^2 + (0)^2 + (\\frac{1}{2})^2 } \\\\ &amp;= \\frac{\\frac{1}{2} \\times \\overline{Y}_\\text{react} - \\tfrac{1}{2} \\times \\overline{Y}_\\text{t+r} }{ \\tfrac{2}{4} } \\\\ &amp;= \\overline{Y}_\\text{react} - \\overline{Y}_\\text{t+r} \\end{align} \\] i.e. the difference between the average intrusions in the Reactivation-Only condition and the average intrusions in Tetris+Reactivation condition. Filling in the actual averages gives: \\[ \\begin{align} \\hat{\\beta}_3 &amp;= \\frac{ 0 \\times 5.11 - \\tfrac{1}{2} \\times 1.89 + 0 \\times 3.89 + \\frac{1}{2} \\times 4.83 }{\\tfrac{1}{2}} \\\\ &amp;= \\frac{1.472}{0.5} = 2.944 \\end{align} \\] While it is important to understand what each slope reflects in terms of differences between the group means, you would not normally use these equations to actually estimate the slopes. It is much easier to use statistical software for that. This gives us the following estimated model: \\[\\texttt{intrusions}_i = 3.93 + 1.57 \\times \\texttt{}X_1\\texttt{}_i + 0.528 \\times \\texttt{}X_2\\texttt{}_i + 2.94 \\times \\texttt{}X_3\\texttt{}_i + \\hat{\\epsilon}_i \\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, 3.18)\\] As you can see, the parameter estimates are identical to those worked out from the sample means. Figure 7.5 shows how the parameters of the model are related to the average number of intrusions in each condition. Figure 7.5: Average number of instrusions in the four conditions in the Tetris study and how they are related to the parameters of the orthogonal contrast-coding model. The intercept \\(\\hat{\beta}_0 = \\hat{\\mu}\\) is the grand mean (dotted line). The slopes reflect deviations from the average intrusions in a condition and combinations of other conditions. Hypothesis tests for all the parameters, as well as the omnibus test for Condition, are provided in Table 7.4. Again, we find a significant overall effect for Condition, which indicates that at least one of the group means differs from another one. The tests for the first two contrasts are not significant. As such, we have no evidence that the Control condition differs from the other three conditions combined, or that the Tetris-Only condition differs from the Tetris+Reactivation or Reactivation-Only condition combined. The test of the third contrast is significant, however, indicating a difference between the Tetris+Reactivation and Reactivation-Only condition. The slope of this comparison is positive, indicating that, as expected, there are more memory intrusions in the Reactivation-Only condition compared to the Tetris+Reactivation condition. Table 7.4: Linear model predicting number of intrusions by three contrast-coded predictors. \\(\\hat{\\beta}\\) \\(\\text{SS}\\) \\(\\text{df}\\) \\(F\\) \\(P(\\geq \\lvert F \\rvert)\\) Intercept 3.931 1112.35 1 110.289 0.000 Condition 114.82 3 3.795 0.014 \\(\\quad X_1\\) 1.574 33.45 1 3.316 0.073 \\(\\quad X_2\\) 0.528 3.34 1 0.331 0.567 \\(\\quad X_3\\) 2.944 78.03 1 7.736 0.007 Error 685.83 68 Comparing the results in Table 7.4 to those in Table 7.2, there are a few things to note. Firstly, the result of the omnibus test is exactly the same, whether you use effect coding or orthogonal contrast coding. Because both ways of coding in the end make the same prediction \\(\\hat{Y}_i\\) (namely that \\(\\hat{Y}_i\\) equals the group mean of the condition that case \\(i\\) belongs to), the model as a whole has the same SSE for both coding schemes. And as the omnibus test involves the same MODEL R (an intercept-only model), the SSR term of the omnibus Condition effect is the same for effect-coding and orthogonal coding. Indeed, for any form of contrast coding which results in model predictions that equal the group means, the omnibus test gives exactly the same results! The model with orthogonal contrast coding, however, separates the whole model SSR term neatly into into different SSR terms for the contrast-coding predictors. If you add up the SSR terms for the three predictors (in the column labelled SS), you get exactly the SSR term given for Condition. This shows that when you use orthogonal contrast coding, the variance explained by the model as a whole is separated exactly into three independent parts reflecting the unique part of the variance explained due to each predictor. 7.4.3 Defining your own (orthogonal) contrasts Initially, coming up with orthogonal contrast codes (or for that matter, any coding scheme which allows the model predictions to equal all the group means exactly) will not be easy, especially for nominal variables with more than 3 levels (i.e. more than 3 groups). With practice, you should become better at this, though. Ideally, the contrast codes reflect at least some of the theoretically important research questions you want to ask (e.g., relevant comparisons of the effects of experimental manipulations). In an experimental design with \\(g\\) groups, you need to define \\(g-1\\) contrast codes which correspond to such questions. When designing your own contrast codes, you should start with the most important question you want to ask, For instance, if you want to compare the conditions with memory reactivation to those without memory reactivation, because you expect more memory intrusions after memory reactivation than without this reactivation, your initial contrast could be \\(c_1 = (-\\tfrac{1}{2}, \\tfrac{1}{2}, -\\tfrac{1}{2}, \\tfrac{1}{2})\\). Note that I’m giving the Control and Tetris-Only condition a negative value of \\(-\\tfrac{1}{2}\\) and the Tetris+Reactivation and Reactivation-Only condition a positive value of \\(\\tfrac{1}{2}\\) because the resulting slope would be positive if the expectation of more memory intrusions after reactivation holds true. For expectations which involve not only a difference, but also a direction of that difference, I find it handy to assign values which are in line with those expectations. Negative slopes in the model then indicate that my expectations did not hold true. Having defined this initial contrast, we have two more to go. Perhaps I’m also interested in assessing the effect of Tetris within the conditions that involve memory reactivation, because I expect playing Tetris to reduce the number of memory intrusions. This is a comparison between the Reactivation-Only and Tetris+Reactivation conditions. Because we gave these conditions the same value in the first contrast \\(c_1\\) (i.e. we did not differentiate between them in the first comparison), this new comparison will be independent of (orthogonal to) the first comparison. We can use the same contrast code for this as the one used earlier, so \\(c_2 = (0,-\\tfrac{1}{2}, 0, \\tfrac{1}{2})\\). It is easy to check that this contrast is indeed orthogonal to the first one \\[\\sum_{k=1}^g c_{1,k} \\times c_{2,k} = -\\tfrac{1}{2} \\times 0 + \\tfrac{1}{2} \\times (-\\tfrac{1}{2}) + (-\\tfrac{1}{2}) \\times 0 + \\tfrac{1}{2} \\times \\tfrac{1}{2} = 0\\] We now have one contrast left to specify. Perhaps we don’t really have any more important questions to ask, so this contrast might be arbitrary from a theoretical viewpoint. However, to have a model which is able to fit the group means exactly, we would need a contrast code that differentiates between the Control condition and the Tetris-Only condition, as these conditions received the same value on \\(c_1\\) (both \\(-\\tfrac{1}{2}\\)) and \\(c_2\\) (both 0). If our last contrast code would not differentiate between these two conditions, then there would be no way in which the model can make different predictions for those two conditions. The model then involves a quite strong assumption that means of the Control and Tetris-Only condition are exactly identical. I don’t see a reason why such equivalence between the conditions would necessarily hold. So the final contrast code will be \\(c_3 = (\\tfrac{1}{2}, 0, -\\tfrac{1}{2}, 0)\\). Note that if I had used different values for the Reactivation-Only and Tetris+Reactivation conditions, for instance a contrast code of \\((\\tfrac{1}{2}, -\\tfrac{1}{2}, -\\tfrac{1}{2}, \\tfrac{1}{2})\\), then the contrast code would correlate with \\(c_2\\) (or with \\(c_1\\) if I had used different values). So the final contrast code needs to give the same value (0) to the Reactivation-Only and Tetris+Reactivation conditions to give us a set of orthogonal contrast codes. So the final set of orthogonal contrast codes is \\(c_1\\) \\(c_2\\) \\(c_3\\) Control \\(-\\tfrac{1}{2}\\) 0 \\(\\tfrac{1}{2}\\) Tetris+Reactivation \\(\\tfrac{1}{2}\\) \\(-\\tfrac{1}{2}\\) 0 Tetris-Only \\(-\\tfrac{1}{2}\\) 0 \\(-\\tfrac{1}{2}\\) Reactivation-Only \\(\\tfrac{1}{2}\\) \\(\\tfrac{1}{2}\\) 0 I will leave checking the orthogonality, and estimating and testing the parameters of the resulting model, as an exercise to the reader. 7.5 Default orthogonal coding schemes While orthogonal contrasts provide benefits in terms of straightforward interpretation of the parameters, and, in the case of equally sized groups, independent predictors and and a neat division of the whole model \\(\\text{SSR}\\) to the \\(\\text{SSR}\\) terms for the predictors, orthogonality is not a strict requirement. For instance, effect coding and dummy coding also provide interpretable parameters, although the predictors are not independent. The most important thing is that the contrast codes can answer questions about the Data Generating Process which are of interest to the researcher. In the absence of any comparisons of interest, you shouldn’t really be conducting hypothesis tests in the first place. That said, there might be times where you would like to resort to a “default” way of orthogonal contrast coding. Such default coding schemes can also provide inspiration for defining your own contrast codes. Two default orthogonal contrast codes are Helmert coding, and polynomial contrast coding. Helmert coding involves a set of contrast codes in which each code compares a group mean to the average of all group means that come before it. You can find these contrast, for situations with a group size of \\(g=1, 2, \\ldots, 6\\), in Table 7.5. Using Helmert contrasts will always provide a set of orthogonal contrasts. And if you reorder the conditions in Table 7.3 as Tetris+Reactivation, Reactivation-Only, tetris-Only, Control, you should recognize that this was actually an example of Helmert coding for \\(g=4\\). Table 7.5: Helmert contrast codes for situations with two to six groups. group \\(c_1\\) \\(c_2\\) \\(c_3\\) \\(c_4\\) \\(c_5\\) 1 -0.5 2 0.5 1 -0.5 -0.333 2 0.5 -0.333 3 0.0 0.667 1 -0.5 -0.333 -0.25 2 0.5 -0.333 -0.25 3 0.0 0.667 -0.25 4 0.0 0.000 0.75 1 -0.5 -0.333 -0.25 -0.2 2 0.5 -0.333 -0.25 -0.2 3 0.0 0.667 -0.25 -0.2 4 0.0 0.000 0.75 -0.2 5 0.0 0.000 0.00 0.8 1 -0.5 -0.333 -0.25 -0.2 -0.167 2 0.5 -0.333 -0.25 -0.2 -0.167 3 0.0 0.667 -0.25 -0.2 -0.167 4 0.0 0.000 0.75 -0.2 -0.167 5 0.0 0.000 0.00 0.8 -0.167 6 0.0 0.000 0.00 0.0 0.833 Polynomial contrast codes tend to be used when the groups can be ordered in a meaningful way. For instance, a study might involve different age groups, e.g. “18-30 year old”, “31-40 year old”, and “41-50 year old”. Using polynomial contrast coding, you can then determine a linear trend over these age groups with a linear contrast \\(c_1 = (-1,0,1)\\), as well as whether the average of the middle age group is lower (or higher) than would be expected from the linear contrast through a quadratic contrast \\(c_2 = (1,-2,1)\\). Examples of polynomial contrast codes for situations with between 2 and 6 groups are provided in Table 7.6. Table 7.6: Polynomial contrast codes for situations with two to six groups. group \\(c_1\\) \\(c_2\\) \\(c_3\\) \\(c_4\\) \\(c_5\\) 1 -1 2 1 1 -1 1 2 0 -2 3 1 1 1 -3 1 -1 2 -1 -1 3 3 1 -1 -3 4 3 1 1 1 -2 2 -1 1 2 -1 -1 2 -4 3 0 -2 0 6 4 1 -1 -2 -4 5 2 2 1 1 1 -5 5 -5 1 -1 2 -3 -1 7 -3 5 3 -1 -4 4 2 -10 4 1 -4 -4 2 10 5 3 -1 -7 -3 -5 6 5 5 5 1 1 7.6 Multiple testing and post-hoc tests Come back later for this. References "]]
