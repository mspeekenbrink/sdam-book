[
["index.html", "Statistics, data analysis, and modelling Preface Acknowledgements Notation", " Statistics, data analysis, and modelling Maarten Speekenbrink 2020-09-15 Preface This book concerns statistics, and more specifically data analysis and modeling. It aims to be a self-contained source, requiring little background in mathematics or statistics. That said, it is not necessarily an “easy” book. It contains some mathematical notation and ideas. I do my best to properly explain how to a read and interpret these in both an intuitive and more formal way. I don’t do this to torture you. Neither you with little mathematical background, who might have to read some explanations twice or more, nor you with more mathematical background, who might find my intuitive explanations tedious, superfluous, and utterly pointless, not you in the middle, who like me, who will agree with both (but hopefully not simultaneously). What I hope to do is to bring you some sense of the beauty of a formal system that stems from its simplicity and exactness, as well as some of the excitement of grasping it. I’m under no illusion that this is an easy task, for you or for me. But I hope you will try, like I did writing this book. We will focus on building models of data in order to evaluate claims about how that data came to be what it is. Statistics is often taught with a “cookbook” approach. The “ingredients” are characteristics of the data (e.g. a variable is metric) and the goal of the analysis (e.g. compare the mean of two groups) and the cookbook provides the recipe (e.g., a two-sample t-test). While straightforward to teach, the limitations of this approach become quickly apparent when you need to analyse data from a less “standard” design. The cookbook can also become something of a straightjacket, when you are forced to change the design of a study in order to allow the subsequent analysis to be covered in the cookbook. Another limitation of the approach is that the different analyses and tests are often treated in isolation, which makes it more difficult to see the often strong connections between the models and statistical principles underlying them. By contrast, the modeling approach adopted here is much more flexible. The aim is to give you the confidence to, when given the ingredients, to design your own recipe. Unlike some other books, we don’t discuss using software such as R or JASP, or SPSS, here. Instead, there are companion books which discuss how to do analyses with specific software (R or JASP). Separating theory and practice in this way has as main benefit that each can be focused on fully. An additional benefit is that it is also easier to add companions for other software. A downside is that you might need to cross-reference between two books. However, by keeping the structure of the companions mostly in line with the structure of this book, this cross-referencing should be straightforward enough. Acknowledgements This book is inspired by lots of other statistics books I have read over the years. Some books deserve special mention. “Data analysis: A model comparison approach” (Judd, McClelland, and Ryan 2011) is a book I have used for many years as required reading for the MSc level statistics course. It is extremely clear in its coverage of the General Linear Model, and concistent in its use of the model comparison approach we adopt here as well. “Doing Bayesian data analysis” (Kruschke 2014) gave me the idea tp cover the basics of statistical modeling with a simple binomial example. Notation In a book with mathematics, notation is important. Mathematics can be viewed as a very precise language. For some of you, this may be a very foreign language. But languages can be learned. Like words, mathematical symbols represent important concepts and relations. The aim is to be consistent, so that each symbol has a clearly defined meaning. That isn’t always easy, or possible. Another aim is to stay close to prevailing conventions, so that it will be easier to understand papers and other books. This is also not always easy, or possible, because in different areas of statistics, the same symbol may have very different meanings. In any case, we will try to be very clear when introducing notation. You will come across symbols from the Greek alphabet. Generally, these symbols are used to reflect parameters of statistical models, and sometimes to reflect other fundamental aspects of statistical analyses. So it will be useful to familiarize yourself with the Greek alphabet. In the following table, each Greek letter is shown, with the corresponding name in English (useful if you want to read the symbols aloud), as well as the common parameter or concept the symbol refers to: symbol name usage \\(\\alpha\\) alpha significance level \\(\\beta\\) beta parameter (slope or intercept) in the General Linear Model \\(\\gamma\\) gamma \\(\\delta\\) delta \\(\\epsilon\\) epsilon error \\(\\zeta\\) zeta \\(\\eta\\) eta \\(\\theta\\) theta a parameter in general \\(\\iota\\) iota \\(\\kappa\\) kappa \\(\\lambda\\) lambda \\(\\mu\\) mu mean \\(\\nu\\) nu \\(\\xi\\) xi \\(\\omicron\\) omicron \\(\\pi\\) pi \\(\\rho\\) rho correlation \\(\\sigma\\) sigma standard deviation \\(\\tau\\) tau \\(\\upsilon\\) upsilon \\(\\phi\\) phi \\(\\chi\\) chi Chi-squared distribution \\(\\psi\\) psi \\(\\omega\\) omega References "],
["intro.html", "Chapter 1 Introduction 1.1 Paul the octopus 1.2 Experiments and observations 1.3 Data 1.4 Exploring and describing data 1.5 Analysis and modelling 1.6 Summary", " Chapter 1 Introduction In this chapter we will introduce some fundamental concepts, such as experiments, data, analysis, and modelling. We will also introduce you to Paul the octopus. 1.1 Paul the octopus We know that all octopus have nine brains so we know he has exceptional powers. Oliver Walenciak, Paul’s keeper source: Irish central Paul the Octopus (26 January 2008 – 26 October 2010) was born in the Sea Life Centre in Weymouth, England, and subsequently moved to the aquarium chain’s centre in Oberhausen, Germany. It was there he found world-wide fame as a caphalopod oracle. Figure 1.1: Paul the octpopus predicts Spain to win the final of the 2010 FIFA World Cup. Source: The Guardian Paul began his career during the 2008 UEFA Euro football tournament. In the lead-up to Germany’s international football matches, Paul was presented with two clear plastic boxes, each containing food (a mussel or an oyster). Each container was marked with the flag of a team, one the flag of Germany, and the other the flag of Germany’s opponent. The box which Paul opened first (and ate its contents) was deemed to be the predicted winner of the match. Paul predicted Germany to win all of their games, a correct prediction in 4 out of 6 cases. He failed to predict their defeats by Croatia in the group stage, and by Spain in the championship’s final. Table 1.1: Paul’s predictions for the UEFA Euro 2008 Match Prediction Result Outcome Germany-Poland Germany 2-0 correct Croatia-Germany Germany 2-1 incorrect Austria-Germany Germany 0-1 correct Portugal-Germany Germany 2-3 correct Germany-Turkey Germany 3-2 correct Germany-Spain Germany 0-1 incorrect Two years later, during the 2010 FIFA World football tournament, Paul obtained celebrity status and his divinations were broadcast live on German TV. This time, Paul made correct predictions for all matches in which Germany played, as well as the final between Spain and the Netherlands. Table 1.2: Paul’s predictions for the FIFA World Cup 2010 Match Prediction Result Outcome Germany-Australia Germany 4-0 correct Germany-Serbia Serbia 0-1 correct Ghana-Germany Germany 0-1 correct Germany-England Germany 4-1 correct Argentina-Germany Germany 0-4 correct Spain-Germany Spain 1-0 correct Uruguay-Germany Germany 2-3 correct Netherlands-Spain Spain 0-1 correct Paul’s record of 8/8 correct predictions in the 2010 world cup is quite amazing. The common octopus (Octopus vulgaris) is indeed quite an intelligent species. But was Paul really psychic? Could he really foresee the future? 1.2 Experiments and observations We can view Paul’s divinations as the result of an experiment to test his psychic abilities. Paul’s predictions were derived under more or less controlled conditions: The keeper didn’t put Paul’s favourite food in the box representing the team he’d hope to win, and didn’t always place the box representing Germany on one side of the tank. Had he done one or more of these things, he might have biased the results. A paradigmatic example of an experiment is a randomized-control trial, as common in studies assessing the effectiveness of medication or other therapeutic interventions. In such experiments, a sample of participants is taken from a population of interest, and each participant is randomly assigned to either an experimental condition or control condition. For example, participants in the experimental condition might be given an experimental drug, while participants in the control condition receive a placebo. Ideally, this is done in a “double-blind” setting, where neither the participant nor the person delivering the treatment or otherwise interacting with the participant knows which condition the participant is assigned to. Randomization is key here: By letting chance determine who gets which treatment, characteristics of the participants (e.g. age, sex, severity of symptoms, etc) will not be tied to the treatment they get. As such, it will be reasonable to assume that the experimental and control condition differ only in their treatment, such that any differences in outcome can be attributed to the treatment, and not to other differences between the groups of participants. Data obtained under less stringent conditions is often referred to as observational data. For instance, rather than random assignment, we could let the doctor choose which treatment to give to which patient. The doctor might choose to give the experimental drug to the more severe cases, and the placebo to the less severe cases. Severity is now confounded with the treatment, and any difference in outcome success observed between the experimental and control condition might be due to the treatment, or to the initial differences in severity. Or even to a third variable such as age, which is itself related to severity. In this case, we wouldn’t be able to tell, and hence the conclusions we can draw about the effects of the treatment and much less straightforward. We are not completely helpless though. Using statistics, we can attempt to control for pre-existing differences. As we will see in due course, we can attempt to build a model which includes both treatment and disease severity as predictors of outcome. In doing so, we can attempt to estimate the unique effect of treatment on outcome. By considering only those differences in outcome that cannot be attributed to severity, we can still say something meaningful about treatment effects. 1.3 Data Data are a collection of measured or otherwise observed or known values of variables. A variable is In a statistical model, a dependent variable is a variable of interest, a variable which we aim to predict, explain, or describe. An independent variable is a variable which we use to help predict, explain, or describe the dependent variable. For instance, in the medical example above, the dependent variable would be the severity of symptoms, while independent variables would be the treatment (medication or placebo), and potentially other factors such as patients’ age, sex, etc. Whether a variable is a dependent or independent variable may differ from model to model. As such, it is an aspect of the analysis, and not so much of the design of a study. Paul has provided us with two sets of data. The first contains his predictions for the 2008 UEFA Cup, and the second his predictions for the 2010 FIFA Cup. The dependent variable in both sets is the accuracy of Paul’s predictions. Paul’s “predictions” were of course not directly predictions who was going to win each game. Paul made a choice which of two containers to open first. But we’re not really interested in whether Paul opened the left or right container in his tank, or whether Paul chose the container with the Spanish flag or the container with the Dutch flag. What we are interested in is whether the first container to be opened contained the flag of the team that would win the match played that day. If it did, Paul made a correct prediction, if not, Paul made an incorrect prediction. The data then contains information about what we deem relevant to our purposes. In a sense, looking just at the accuracy of Paul’s predictions, we have already abstracted quite a bit from the experiment. We ignore whether Paul went left or right first, how long it took him to open the first container, … These are all aspects of Paul’s behaviour which we could have focused on, and hence could be variables in our data. These variables have different characteristics. For instance, accuracy (correct or incorrect) and direction (left or right container) both have only two possible values, while the duration until opening the first container has many possible values. Variables with a finite (limited) number of values are called discrete, while variables with an infinite (unlimited) number of values are called continuous. 1.3.1 Measurement scales A nominal scale allows identifying values as “identical” or “different”, but nothing more. Examples of variables on a nominal scale are pizza toppings, eye colour, etc. While we can assign numbers to the values (e.g. brown = 1, blue = 2, green = 3), these are arbitrary and can be changed without much consequence (e.g. brown = 21, blue = 7, green = 1). An ordinal scale allows values to be identified as “smaller”, “identical”, or “larger”, but nothing more. Variables on an ordinal scale can be meaningfully ordered or ranked. This means there is some constraint to numbers we can assign to values. An interval scale allows items to be assigned more meaningful numbers. On an interval scale, the differences between numbers can themselves be meaningfully ordered. Temperature is a common example. The difference between 16 and 14 degrees Celcius is smaller than the difference between 19 and 16 degrees Celcius. But we can not say that 30 degrees Celcius is twice as warm as 15 degrees Celcius. This is because an interval scale doesn’t have a fixed 0 point. For degrees Celcius, 0 degrees is the point at which water freezes. It is not the point at which there is no temperature. A ratio scale has everything an interval scale has, with the addition of an absolute 0 point. Weight is a common example. Not only can we say that the difference between 3 and 2 kilograms is smaller than the difference between 10 and 5 kilograms, we can also say that 10 kilograms is twice as heavy as 5 kilograms, and that an item with a weight of 0 kilograms has no weight whatsoever. Sometimes it is tricky to determine what the measurement scale is. For instance, are “correct” and “incorrect” ordered (correct is better than incorrect) or nominal? Most of the time, a courser distinction is enough, namely whether data is categorical (nominal or ordinal) or metric (interval or ratio). 1.3.2 The Data Generating Process In addition to considering the type or measurement scale of the data, you will generally also need to consider the process that in the end led to the data at hand. This process can be called the Data Generating Process. As the section on experimental vs observational data indicated, the way the data were obtained can affect and limit the inferences that can be made. It is also important to consider to which group or population our data speaks. This is generally related to how we have selected the subjects of a study. For instance, in the case of Paul, we have data from a single octopus, and we are mostly interested in assessing whether he was psychic. We are not interested in Paul’s psychic abilities for just those games for which he made predictions though. If Paul indeed had psychic abilities (but let’s restrict that to the context of football games), we would expect him to make accurate predictions for other games as well. So here, we would like to generalize beyond the data, from the games Paul actually made predictions for to all games in the UEFA 2008 and FIFA 2010 cups, or even to all international football matches. We then need to consider how representative the data is for the more general group we want to make claims about. As Paul mainly made predictions for games in which one of the teams was Germany, the sample is not a good representation of all the games played in both cups. If it is in one way or the other easier to determine the winner in games involving Germany than it is to determine the winner in other games, then the test of Paul’s abilities that was conducted is different than the test that would be conducted if we included other games. This, then, limits the claims we can make from the test. If we were interested in whether octopuses in general have psychic abilities, then using a single octopus would not be a great choice. Paul may be unique in many ways, so again he might not be representative of all octopuses. As in experiments, where random assignment to conditions can avoid confounds, random sampling can be used to make it more likely that data collected is representative. If we are interested in Paul’s predictions for all international matches, it is practically undoable to get him to make predictions for all of these. But by randomly sampling a set of international matches, we will not be able to let this set depend on certain characteristics (e.g. Germany playing) which might bias the results. Determining the representativeness of available data for a larger group of possible observations is usually not something that can be done with absolute certainty. And unless we have access to more data, it is generally also not something that statistical analysis can help with. Considerations of representativeness usually rest on (ideally sound) reasoning. The Data Generating Process (or DGP for short) is a concept we will encounter repeatedly throughout this book. The DGP contains everything of importance when considering the variability of the data in the context of other data that could have been collected. This includes how subjects were sampled (e.g. if we randomly sample octopuses, then other possible data sets would contain data from other octopuses), how subjects are assigned to conditions (if any), but most importantly also how subjects generate responses (or otherwise observable events), and any possible source of randomness in this generation. For instance, during a particular prediction, Paul might initially go for the box on the left, but then a ray of sunlight hits the box on the left, leading Paul to open that box first. Such small events can be considered a source of random noise in the experiment, that lead to variability in Paul’s behaviour, and ultimately to variability in the correctness of his predictions. As a concept, the purpose of the DGP is to describe all such sources of variability as they impact the variability of a particular variable of interest. The DGP then describes not only how the data set that we have access to was actually generated, but also how all possible other data sets would have been generated. This makes the DGP a rather abstract concept. Nevertheless, we see it is something real, something “out there in the world”, in contrast to statistical models, which inhabit the world of mathematics. For statistics, the most important aspect of the DGP is that it provides a distribution over the values of a variable if the DGP was “let loose” and generated all possible data sets (of which the actual data set we have access to is only one). For instance, this would be the frequency of correct and incorrect predictions that Paul makes in every possible version of the experiment (so every possible match, every possible assignment of teams to the left or right box, and perhaps every possible small disturbance in light conditions, etc) This distribution is sometimes also referred to as the population distribution. If this all sounds a little too abstract at the moment, don’t worry. We will discuss the DGP in more detail later. 1.4 Exploring and describing data Once you have collected data, you may feel the urge to immediately apply some statistical model or test to it. But really, a first step should be to explore the data. Statistical tests are designed to confirm or reject hypotheses, and part of confirmatory data analysis. They are a very important part of scientific research, but they always rely on assumptions about the data generating process. Before conducting these analyses, it is crucial to assess whether these assumptions are reasonable, and this is where exploratory data analysis can help. Tukey (1977), a major proponent of exploratory data analysis, also points out that exploratory data analysis can also suggest new hypotheses about the causes of observed phenomena, and with that provide a basis for further experimentation and data collection. As the distribution of Paul’s predictions is not so interesting to look into exploration of data, we will focus on other data. We will stay in the context of the FIFA 2010 cup, and consider team statistics such as the number of goals scored and the number of games played. Table 1.3: Team statistics in the FIFA 2010 World cup Team Matches played Goals for Goals against Yellow cards Direct red cards 31 Algeria 3 0 2 8 0 4 Argentina 5 10 6 7 0 19 Australia 3 3 6 7 2 5 Brazil 5 9 4 9 1 26 Cameroon 3 2 5 5 0 21 Chile 4 3 5 15 0 12 Côte d’Ivoire 3 4 3 5 0 20 Denmark 3 3 6 6 0 22 England 4 3 5 6 0 29 France 3 1 4 6 1 1 Germany 7 16 5 13 0 11 Ghana 5 5 4 11 0 27 Greece 3 2 5 5 0 32 Honduras 3 0 3 7 0 13 Italy 3 4 5 5 0 14 Japan 4 4 2 7 0 30 Korea DPR 3 1 12 2 0 8 Korea Republic 4 6 8 6 0 15 Mexico 4 4 5 9 0 2 Netherlands 7 12 6 24 0 24 New Zealand 3 2 2 6 0 17 Nigeria 3 3 5 5 1 23 Paraguay 5 3 2 9 0 7 Portugal 4 7 1 8 1 25 Serbia 3 2 3 10 0 10 Slovakia 4 5 7 11 0 16 Slovenia 3 3 3 9 0 18 South Africa 3 3 5 4 1 6 Spain 7 8 2 8 0 28 Switzerland 3 1 1 8 1 3 Uruguay 7 11 8 11 1 9 USA 4 5 5 9 0 1.4.1 Summary statistics Summary statistics are calculated values that aim to summarize key aspects of the distribution of observed values of a variable. Two main aspects of such distribution are the where the values are generally located within the range of possible values, and the variation in the values. Measures of the location of the values aim to reflect the central value, in some form or another, of a distribution. Measures of spread aim to reflect the level of variability around the central value. 1.4.1.1 Measures of location There are three common measures of location: the mean, median, and mode. Let’s consider each in turn. The mean is the average of a set of values, computed by adding all the values up and then dividing this sum by the total number of values. More formally, the mean of a variable \\(Y\\) is usually denoted by drawing a straight line over the variable (i.e. as \\(\\overline{Y}\\)), and the formula for its computation is \\[\\begin{equation} \\overline{Y} = \\frac{\\sum_{i=1}^N Y_i}{N} \\tag{1.1} \\end{equation}\\] Perhaps this is the first time you have seen the “summation operator” \\(\\Sigma\\), so let’s go over how to work with this. Basically, the summation operator states that you should sum up every value of a variable that is placed to the right of it. Below the summation operator (or on the lower-right side) is an iterator \\(i\\), which gives more information about which values to sum. The iterator goes from the value it is given below or on the bottom-right side (here, the iterator is assigned the value 1), to the value on the top (or top-right) of the summation operator (in this case \\(N\\)). So here, the operator states that we should sum up all values \\(Y_i\\), where \\(i\\) goes from 1 to N, i.e. all values \\(Y_1, Y_2, \\ldots, Y_N\\): \\[\\sum_{i=1}^N Y_i = Y_1 + Y_2 + Y_3 + \\ldots + Y_N\\] If we compute the average goals a team scored, we get \\[\\overline{Y} = \\frac{0 + 10 + 3 + 9 + 2 + \\ldots + 11 + 5}{32} = \\frac{145}{32} = 4.531\\] The median is defined as the value in the set of values such that 50% of the values are lower than it. It is easy to compute if the number of observations (\\(N\\)) is odd. We simply order the values in increasing or decreasing magnitude. The median is then simply the middle value in this ordered set. If \\(N\\) is an even number, the median cannot be uniquely determined. But it is common to take the value that is halfway between. The goals scored by each team, ordered in increasing magnitude are: 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, 16 There are \\(N=32\\) values, so the first 16 values in the ordered sequence above are the lowest 50% of the values, and the second 16 values are the highest 50% values. If we had one value less, i.e. if \\(N=31\\), then the 16th value would be the median. In our case, the median is a value between the 16th and 17th value. If you go along the sequence and stop at the 16th value, you will see that it equals 3. The next value is the 17th value, which also equals 3. So in this case, the median must be 3, as this is the only value between 3 and 3. If the 17th value happened to be 4, then we would have determined the median as the value halfway between 3 and 4, i.e. as 3.5. The mode is the most frequently occurring value. This measure is really only useful for relatively large data sets with a limited number of possible values. For a continuous variable, there is generally no value that occurs more than once in a data set, and hence all values could be considered a mode. If we look at the total goals scored, we see that the mode is 3, as this is the most frequent value (there are 8 teams who scored 3 points, while there are 4 teams who scored 2 points, and 4 teams who scored 4 points). 1.4.1.2 Measures of spread There are three common measures of spread: the inter-quartile range, the variance, and the standard deviation. The inter-quartile range is the distance between the 25% and the 75% percentile. A percentile is a value such that a specified proportion of the values are lower than it. So the 25% percentile is the lowest value in the data such that 25% are even lower. If you go back to the definition of the median, you may notice that the median is actually the 50% percentile. The 25%, 50%, 75% and 100% percentiles are also respectively called the 1st, 2nd, 3rd, and 4th quartile, hence the name inter-quartile range. To compute percentiles, it is handy to order the values in increasing magnitude (just like we did for computing the median). \\[\\text{rank}(P) = \\text{ceiling}\\left(\\frac{P}{100} \\times N\\right)\\] The ceiling function stands for rounding up to the nearest integer, so e.g. \\(\\text{ceiling}(2.1) = 3\\). To compute the inter-quartile range for total points scored, we compute the needed ordinal ranks as \\[\\text{rank}(25) = \\text{ceiling}\\left(\\frac{25}{100} \\times 32 \\right) = 8\\] which means we can take the 8th value in the ordered list as the 25% percentile, which equals 2, and \\[\\text{rank}(75) = \\text{ceiling}\\left(\\frac{75}{100} \\times 32 \\right) = 24\\] which means we can take the 24th value in the ordered list, which equals 5. The inter-quartile range is then \\[\\text{IQR} = [2, 5]\\] The variance is the average squared deviation of values from the mean. One reason for using squared deviations rather than the average deviation itself is that the average deviation \\[\\frac{\\sum_{i=1}^N(Y_i - \\overline{Y})}{N}\\] is always 0. An alternative is of course to then calculate the average of the absolute deviations. While this is a sensible measure of the spread of values, there are technical reasons to prefer the average squared deviation (i.e. the variance), which we won’t go into now. Let’s just plough on with how to calculate the variance. The variance of a collection of observed values, which is also referred to as the sample variance, is usually denoted as \\(S^2_Y\\), and defined as: \\[\\begin{equation} S^2_{Y} = \\frac{\\sum_{i=1}^N (Y_i - \\overline{Y})^2}{N} \\tag{1.2} \\end{equation}\\] So you simply subtract from each value the average value (the sample mean), and then raise each deviation to the power of 2. Then you add those squared deviations up, and divide the sum by the total number of observations. For the total points scored, the variance is calculated as: \\[\\begin{align} S^2_Y &amp;= \\frac{(0 - 4.531)^2 + (10 - 4.531)^2 + (3 - 4.531)^2 + \\ldots + (11 - 4.531)^2 + (5 - 4.531)^2}{32} \\\\ &amp;= \\frac{(-4.531)^2 + (5.469)^2 + (-1.531)^2 + \\ldots + (6.469)^2 + (0.469)^2}{32} \\\\ &amp;= \\frac{20.532 + 29.907 + 2.345 + \\ldots + 41.845 + 0.22}{32} \\\\ &amp;= \\frac{423.969}{32} \\\\ &amp;= 13.249 \\end{align}\\] A downside of the variance is that its value is difficult to interpret. By squaring each deviation, the value of the variance is on a different scale of magnitude than the original values. The standard deviation aims to provide a more interpretable measure of spread, by transforming the variance back to the scale of the original values. The standard deviation of a collection of observed values is usually denoted as \\(S_Y\\), is simply the square-root of the variance, so \\[S_Y = \\sqrt{S^2_Y}\\] or written out in more detail as \\[\\begin{equation} S_{Y} = \\sqrt{\\frac{\\sum_{i=1}^N (Y_i - \\overline{Y})^2}{N}} \\tag{1.3} \\end{equation}\\] For the total points scored, the standard deviation is \\[S^2_Y = \\sqrt{13.249} = 3.64\\] 1.4.2 Visual exploration Exploring a data set is generally done visually, using various plots such as histograms, boxplots, scatterplots, etc. A good plot can immediately provide a lot more information than summary measures. So before proceeding with statistical analysis: plot your data! Visualizing data in the most effective and pleasant way is an art form in itself. But here are some standard plots that are helpful and relatively easy to produce: Figure 1.2: A histogram and boxplot of the total goals scored by each team A histogram (left plot in Figure 1.2) counts the number of occurrences of values, or ranges of values in the case of continuous variables are variables with a lot of possible values. For the latter, it divides the range of possible values into equally sizes “bins” (equally sized ranges of values). The count of observations in each bin is then plotted as the height of a bar, and the width of the bar reflects the range (in the case of discrete values, the range is defined by the midpoints between these values. A histogram is useful to depict the distribution of values of a variable. The size of the bins is important though, as it determines how smooth or rough the histogram looks, and often requires some tweaking. If you make the bins too wide, it may be hard to spot the intricacies in a distribution. If you make the bins too narrow, on the other hand, each bin will contain only a small number of values and you won’t summarize the distribution in a visually meaningful way. In the extreme case, each bin will either contain just one observation, or none. A boxplot (right plot in Figure 1.2), also called a box and whiskers plot, depicts the inter-quartile range as a box, in which a line is placed to depict the median. Two lines extend from either side of the box. These are the “whiskers” and they depict the range of values in the data excluding any potential outliers. More precisely, the endpoints of the whiskers are the smallest and largest value in the data which are no more than \\(1.5 \\times \\text{IQR}\\) away from the 25% and 75% quartiles, respectively. If there are values in the data which extend beyond these endpoints, they are considered “outliers”, and plotted as points outside of the whiskers. In the example boxplot, you can see that there are three such outliers. A scatterplot is useful to see the relation between the values of two variables. For instance, it is likely that the total points scored by each team will depend on the number of games they played. We can visually inspect this by plotting each team as a point with the value of the x-axis representing the value of one variable, and the value on the y-axis as the value of the other variable. This is done in Figure 1.3, from which it seems quite clear that those teams who played more games generally also scored more points. Figure 1.3: A scatter plot of total goals scored by total matches played As all the plots above can tell us something meaningful about the data, and they often highlight different aspects of the data, it can be useful to combine them. One form of such a combination plot has been called a raincloud plot (Allen et al. 2019). A raincloud plot combines a boxplot with a scatterplot of the observed values and a type of continuous histogram called a nonparametric density plot. In my version of such plots (1.4), a boxplot is placed in the middle, with an additional point representing the sample mean. On the left are the observed values, where the points are “jittered” (randomly displaced on the x-axis) to try to avoid overlapping points. On the right of the boxplot is the nonparametric density plot. This part aims to depict the distribution in a smooth, continuous way. Figure 1.4: A raincloud plot of the average goals scored per match 1.5 Analysis and modelling Analysing data can mean lots of things. According to WikiPedia “Data analysis is a process of inspecting, cleansing, transforming and modelling data with the goal of discovering useful information, informing conclusions and supporting decision-making.” That is a reasonably accurate, but also quite lengthy and boring definition. An alternative to this I quite like is that data analysis is identifying a signal within noise. Data is noisy for many reasons. For one thing, the data that we have access to is generally limited, in the sense that we could have obtained other and/or more data. Other sources of noise include measurement error. Such sources of noise imply that the data set we have only gives a limited, incomplete, and corrupted view on the process that generated it (the Data Generating Process). We are not interested solely in describing the data set we have, we generally want to infer something about the Data Generating Process. We don’t just want to state that Paul made 8 out of 8 accurate predictions in the 2010 FIFA World Cup, we want to use this data to assess whether Paul had psychic abilities. Whether, had Paul’s keeper allowed Paul to make predictions for other matches, he would have also been correct. Such inferences go beyond the data that we have. Perhaps Paul was just lucky in those 8 predictions, and his 100% hit rate is not a good reflection of his hit rate in general. Statistical modelling can be viewed as describing the Data Generating Process in the language of mathematics. At its core, a statistical model defines the distribution of possible values of variables. In an ideal world, this model distribution is identical to the true distribution that follows from the Data Generating Process. In general, though, there will be some mismatch between the model distribution and the DGP distribution. Models are simplifications and idealizations. As statistician George Box famously noted “all models are wrong, but some are useful” (Box and Draper 1987). What do we mean by this? Consider the famous London tube map (??). This map is “wrong” because as any map, it is a simplification of reality. It does not provide a completely accurate representation of the world. But what is special about the tube map is that, unlike standard maps, it does not even attempt to reflect the geographical location of the different underground (or “tube”) lines and stations. Its initial designer, Harry Beck, realised that realised that because the lines run mostly underground, the exact physical locations of the stations were largely irrelevant to the traveller wanting to know how to get from one station to another. What matters to them is the relation between the stations, and which lines connect them. Hence, he designed the map as a schematic diagram which clearly shows the lines connecting stations. If you used the map to determine the walking distance between stations, you’d be very disappointed. But to determine your underground journey, the map is very useful. Figure 1.5: London tube map. Source: TfL Statistical models are a bit like the London tube map: their usefulness will depend on your goals. In order to derive properties of various statistical analyses and tests, we have to assume that a model is true (in the sense that it truly reflects the DGP distribution). While this assumption is inherently uncertain, the derived properties may hold, at least approximately, as long is the model distribution is an “accurate enough” representation of the DGP distribution. While in mathematics, statements are either true or false, in applied statistics, we have to deal with statements that are only approximately true. 1.6 Summary The objective of statistical data analysis is to go beyond a given data set, and make claims about aspects of the process that generated the data (the Data Generating Process). To make inferences beyond the limited data we have collected, we need statistical models which aim to describe the distribution of all possible data that we could have collected. Data can be of different types. The measurement scale of a variable determines the way in which numbers assigned to values of the variable can be interpreted and manipulated. Aspects of the data collection, such as whether data was collected in an experiment with random assignment, or whether the data was observed in a more naturalistic setting, are important aspects of the Data Generating Process, and determine to what extent we can rule out confounds that may bias our conclusions. Paul was one amazing octopus. References "],
["statistical-modeling.html", "Chapter 2 Statistical modeling 2.1 Coin flipping: Defining a statistical model 2.2 Probability 2.3 Flipping a biased coin: An alternative model 2.4 Estimation 2.5 Null-hypothesis significance testing 2.6 Confidence intervals 2.7 Summary 2.8 Epilogue", " Chapter 2 Statistical modeling Was Paul truly an oracle? Or was Paul (or his keeper more likely) just really lucky? How do we know? And what really are the chances of predicting the winner of 8 matches correct? These are questions we will focus on now. In this chapter, we will start defining statistical models and using them to make inferences about the Data Generating Process. We will cover some basic probability theory, and use this to define two alternative models of Paul’s predictions. One in which he is randomly guessing, and one in which his predictions can be more (or less) accurate than if he were randomly guessing, while we are unsure about how accurate Paul really is in that case. We will discuss how to estimate the unknown accuracy of Paul’s predictions in this model. After this. we will try to answer the question: was Paul (somewhat) psychic? Our answer will be based on whether we can refute the hypothesis that he was randomly guessing. This is generally done with a procedure called a null-hypothesis significance test. We will also consider another way in which to look at this, in the form of so-called confidence intervals. 2.1 Coin flipping: Defining a statistical model What does it mean for Paul to be an oracle? Sometimes it is easier to answer this question by considering the opposite: When would Paul not be an oracle? If Paul had no way to tell the future, he would be “merely guessing”. Purely random guessing between two options can be implemented by flipping a coin. Suppose Paul made his choices by “flipping a mental coin”, and choosing the box on the left when the outcome was “heads”, and the box on the right when the outcome was “tails”. Great, we now have a model of the Data Generating Process! Coin flipping can be seen as a physical model. It is something that we can do in the real world, as many times as we want. Because we understand the physical properties of coin flips reasonably well – it is really difficult to predict the outcome of a coin flip – it works well as a model of random guessing. But flipping coins over and over is rather tedious. If we have a mathematical description, a statistical model, of flipping coins, we can spare ourselves this trouble. To do so, we first need to know a bit more about probability. 2.2 Probability Let’s simulate Paul’s decisions for the FIFA world cup. Grab a coin and flip it 8 times. What are the outcomes? When I did this, the outcomes were: tails, tails, tails, heads, tails, heads, heads, tails Remember, Paul would go left upon heads, and right upon tails. Following these rules, Paul’s decisions are then: right, right, right, left, right, left, left, right To know whether these guesses are correct or incorrect, we need to know which team was placed in which box. This information is provided in Table 1.2: the first country under Match is the box on the left, and the second the box on the right. Using this, our simulation then provides the following: incorrect, correct, correct, correct, correct, correct, incorrect, correct So in our simulation, Paul made 6 correct predictions out of 8. Not quite as good as the real Paul, but still a reasonable performance. Notice how we went through quite a few steps. We defined a model of non-psychic-and-guessing Paul as making decisions by flipping a coin, we then simulated Paul’s decisions by flipping a coin, then transformed these coin flips into a variable containing the decisions to open the left or right box, and finally transformed this variable into another variable containing the accuracy of the predictions. The first transformation is easy. We could have simply put stickers on the coin relabeling “heads” as “left” and “tails” as “right”. The second transformation is not as obvious as it depends on a second variable (whether the winning team was represented by the left or right box). In this case, we could have saved ourselves a bit of work by immediately relabeling our coin flips as “correct” or “incorrect”. To see why, we need to know a bit more about how to calculate probabilities. Let’s start at the beginning. For one coin flip, there are two possible outcomes: the coin lands on heads or on tails. The outcome can’t be neither (we are assuming the coin is thin enough to not land on its side), nor two heads, or something else. The coin must land on either heads or tails, so there are two outcomes. We call the set of all possible outcomes the outcome space, and we’ll use a nice curly \\(\\mathcal{S}\\) to denote it: \\[\\mathcal{S} = \\{\\text{heads},\\text{tails}\\}\\] Definition 2.1 (outcome space) The outcome space is the set of all possible events. Our coin flip is a random variable that takes one of the values in the outcome space. It is traditional to denote random variables with capital Roman letters, and the values with small Roman letters. When we want to make generic statements that apply to different random variables (such as a coin flip, the roll of a die, the outcome of a prediction), we tend to use this notation, and \\(P(Y=y)\\) then means ‘The probability that random variable \\(Y\\) has value \\(y\\)’. Other times, it may be easier to use more descriptive names. In this book, when we use a name for a variable, we will use a computer font, such as coin_flip, for the random variable. So \\(P(\\texttt{coin_flip} = \\text{heads})\\) means ‘The probability that random variable coin_flip has the value “heads”’. 2.2.1 What is probability? I have just introduced the word “probability” without telling you what it means. You probably have some intuition yourself, for instance that probability is the chance of something happening. You might be surprised to know that even though statisticians can all perform probability calculations comfortably, there is quite some disagreement on what probability means. According to the traditional, Frequentist view, probability means long-run relative frequency. For instance, the probability of a coin flip landing on heads is defined as the proportion of times a coin lands heads when I flip a coin for a very, very large number of times. Table 2.1 shows the outcome and relative frequency calculations for 15 coin flips. Table 2.1: Relative frequency of heads for 1 to 15 flips. The frequency is the number of heads thus far, and the relative frequency the frequency divided by the number of flips. flip outcome frequency heads relative frequency heads 1 tails 0 0.000 2 tails 0 0.000 3 heads 1 0.333 4 heads 2 0.500 5 heads 3 0.600 6 tails 3 0.500 7 tails 3 0.429 8 tails 3 0.375 9 tails 3 0.333 10 tails 3 0.300 11 tails 3 0.273 12 tails 3 0.250 13 heads 4 0.308 14 heads 5 0.357 15 heads 6 0.400 Figure 2.1 shows the relative frequency (i.e. the proportion) of heads after each coin flip, when flipping a coin for 2000 times. Figure 2.1: Relative frequency of heads when flipping a coin for 2000 times As you can see, the relative frequency fluctuates quite a bit, but becomes more stable with more flips. But even after 5000 flips, it is not equal to 0.5. Flipping a coin can be viewed as an experiment, that we can repeat. Figure 2.2 shows the results of flipping a coin for 5000 times when repeating the experiment for 5 times. As you can see, the results of our experiments (the relative frequencies) are different each time. The differences are quite marked for a small number of flips, but become more alike after flipping the coin more times. This illustrates something known as The Law of Large Numbers, to which we will come back soon. For now, let’s focus on a different matter. If probability is a relative frequency, then which one is it? Each repetition of the experiment, as well as each number of throws, would give us a different answer! Figure 2.2: Relative frequency of heads when flipping a coin 5 times for 5000 times To give a single answer, a “long-run relative frequency” must be the relative frequency when flipping the coin for an infinite number of times. That may seem disappointing. A nice aspect of the Frequentist View is that probabilities are something of the real world, but surely, we can’t flip a coin for an infinite number of times in the real world. That is true, but to understand what probability is, we don’t necessarily have to perform such infinite experiments. When we have to estimate the probability from a given (non-infinite) number of coin flips, we do have to worry about these issues. This, indeed, is what statistics is about: dealing with sample variability to estimate and infer unknown things. But for defining what a probability is, we can use mathematics to show that the long run relative frequency converges to a single number as the number of coin flips gets closer and closer to infinity. Which, in other words, means there is a single long-run relative frequency, and hence the probability is mathematically well-defined. There is an alternative to the Frequentist View, called the Bayesian view or Subjective view, according to which probability means a rational degree of belief. This, in a sense, takes probability out of the real world, and into our minds. By doing so, it allows statements of probabilities for single events, such as what is the probability that it will rain tomorrow. In the Frequentist View, it either rains tomorrow or not. We can ask what the probability is that it rains on the 25th of July, by considering the long-run relative frequency of rain on every 25th of July in the past, present, and future. But there is no long-run relative frequency for the 25th of July 2020. 2.2.1.1 The rules of probability Although the interpretation of probability is debated, the mathematical rules of calculating probabilities are generally agreed upon. Suppose we have an outcome space \\(\\mathcal{S} = \\{E_1, E_2, E_3, \\ldots\\}\\). That is, the outcome space consists of (abstract) events \\(E_1\\), \\(E_2\\), \\(E_3\\), etc. To refer to any of these, we can use the notation \\(E_i\\), where \\(i\\) can equal 1, 2, 3, etc. \\(0 \\leq P(E_i) \\leq 1\\). The probability of any event \\(E_i\\) is greater or equal to 0 and smaller than or equal to 1. This rule requires little further explanation. Probabilities are chosen to lie on a scale between 0 and 1. \\(P(E_1 \\text{ or } E_2 \\text{ or } E_3 \\text{ or } \\ldots) = P(\\mathcal{S}) = 1\\). In words, this means that the probability of at least one event in the outcome space occurring is 1. If \\(E_i\\) and \\(E_j\\) are mutually exclusive events (if one of them occurs, the other cannot occur), then \\(P(E_i \\text{ or } E_j) = P(E_i) + P(E_j)\\). This is also called the sum rule. \\(P(\\neg E_i) = 1 - P(E_i)\\). Here, \\(\\neg\\) means “not”, so the probability of “not \\(E_i\\)” is 1 minus the probability of \\(E_i\\) occurring. This is also called the complement rule. For any events \\(E_i\\) and \\(E_j\\), \\(P(E_i \\text{ or } E_j) = P(E_i) + P(E_j) - P(E_i \\text{ and } E_j)\\). This rule holds whether the events are mutually exclusive or not, and can be called the general sum rule. Now, all these rules seem a little cumbersome when we are dealing with a simple coin flip. So let’s consider a slightly more complex situation: betting on the outcome a roll of a six-sided die. In this betting game, like in roulette, you are allowed to bet on many things, such as the exact number, but also whether the number is odd or even, whether the number is greater than 3, whether the number is greater smaller than 1, etc. All these things are events in the outcome space: \\[\\mathcal{S} = \\{1, 2, 3, 4, 5, 6, \\text{even}, \\text{odd}, &gt;3, &lt; 1, \\geq 4, 1 \\text{ or } 3, \\ldots \\}\\] The first six events in this outcome space (the numbers 1 to 6) are called elementary events: they are mutually exclusive (a single roll cannot result in both a 1 and a 3), and one of them must occur. So we know that \\(P(1) + P(2) + P(3) + P(4) + P(5) + P(6) = 1\\). But wait, there are more events in the outcome space, and \\(P(\\mathcal{S}) = 1\\), and these six events are only a small part of the outcome space, so does that imply that they all have probability 0? No! That is because not all events in the outcome space are mutually exclusive. For instance, if the outcome is 5, then the outcome is also odd, and it is also \\(&gt;3\\), \\(\\geq 4\\), etc. As you might have noticed, the other events are actually themselves sets of the elementary events. For instance, \\(\\text{odd} = \\{1, 3, 5\\}\\), \\(\\text{even} = \\{2, 4, 6\\}\\), and \\(&gt; 3 = \\{4, 5, 6 \\}\\). Figure 2.3 depicts the relations between these events, as well as the elementary events. Figure 2.3: A diagram depicting the relations between 9 events in the outcome space of the die betting game We just really need one more rule and then we’re done. This rule is to compute the probability of a conjunction of events (the probability of both events occurring). To be able to state this rule, we first need to consider the concept of conditional probability. In words, such a probability refers to the probability of one event given that another event occurred. For instance, we can consider the probability of the event “greater than 3” given that the event “odd” occurred (i.e., the probability that the outcome was greater than 3 given that the outcome was an odd number). If we know that the outcome was odd (i.e. 1, 3, or 5), we can rule out all occurrences where the outcome was even (i.e. 2, 4, or 6). To now consider the probability that the outcome was greater than 3, we only need the probability of an outcome greater than 3 within the set of odd numbers. If we are sure that the outcome was odd, we can set \\(P(\\text{odd}|\\text{odd}) = 1\\). There are three outcomes, each with equal probability, so \\(P(1|\\text{odd}) = P(3|\\text{odd}) = P(5|\\text{odd}) = \\frac{1}{3}\\). In the set of odd outcomes, there is only one number greater than 3, namely 5. That means that the conditional probability \\(P(&gt;3|\\text{odd}) = P(5|\\text{odd}) = \\frac{1}{3}\\). Conversely, we can also work out the conditional probability \\(P(\\leq 3| = P(5|\\text{odd})) = P(1|\\text{odd}) + P(3|\\text{odd}) = \\frac{2}{3}\\). In a sense, a conditional probability is just looking at the relative occurrence of one event \\(E_i\\) (e.g., the event “\\(&gt;3\\)”) within the set of outcomes defined by another event (e.g., the event “odd”). \\[P(E_i|E_j) = \\frac{P(E_i \\text{ and } E_j) }{P(E_j)}\\] \\(P(E_i \\text{ and } E_j) = P(E_i) \\times P(E_j | E_i)\\). Multiplication rule For two independent events \\(E_i\\) and \\(E_j\\), \\(P(E_i \\text{ and } E_j) = P(E_i) \\times P(E_j)\\) Kolmogorov, a rather brilliant mathematician, showed that you only need rules 1, 2 and 3; all the other rules follow from the first three. In mathematical terms, rules 1, 2, and 3 are axioms, statements which we assume are true without being able to prove them, while the remaining rules are theorems, statements which follow from the axioms and can be proven to be true. 2.2.1.2 A model of flipping an unbiased coin Now that we know more about probability, let’s get back to our model for Paul. With a balanced coin (and a not too cunning flipper), the probability of the coin landing heads should equal the probability of the coin landing tails. In mathematical notation, we can state this as \\[P(\\text{heads}) = P(\\text{tails})\\] Furthermore, as heads and tails the only two possible outcomes, we know that \\[P(\\text{heads} \\text{ or } \\text{tails}) = 1\\] Finally, because heads and tails are two mutually exclusive events, we then infer that \\[P(\\text{heads}) = P(\\text{tails}) = 0.5,\\] and given the way Paul bases his decisions on this coin flip, also that \\[P(\\text{paul left}) = P(\\text{paul right}) = 0.5.\\] Interestingly, if Paul made his decision for the left or right box by flipping a coin, it doesn’t really matter whether the team representing the left or right box was chosen randomly, or in some other way. Suppose that Paul’s keeper was very knowledgeable about football, or that he was the real psychic. Furthermore, let’s suppose that he thinks Paul might generally prefer the right box (as the light is particularly nice on that side of the tank, for instance). Then \\(P(\\text{winner right}) = 1\\) and \\(P(\\text{winner left}) = 0\\). Because Paul’s decisions are independent from which team was placed in which box (he’s merely flipping a coin), we know that \\(P(\\text{paul left} | \\text{winner left}) = P(\\text{paul left})\\), and similarly \\(P(\\text{paul right} | \\text{winner left}) = P(\\text{paul right})\\). There are two ways in which Paul can make a correct prediction: he opens the right box, and the winning team was in the right box, or he opens the left box and the winning team was in the left box. So \\[P(\\text{correct}) = P(\\text{paul left} \\text{ and } \\text{winner left}) + P(\\text{paul right} \\text{ and } \\text{winner right})\\] Using the multiplication rule, we can write this as \\[P(\\text{correct}) = P(\\text{paul left} | \\text{winner left}) \\times P(\\text{winner left}) + \\\\ P(\\text{paul right} | \\text{winner right}) \\times P(\\text{winner right})\\] And from the independence between Paul’s decisions and the assignment of teams to boxes, we can simplify this further as \\[P(\\text{correct}) = P(\\text{paul left}) \\times P(\\text{winner left}) + P(\\text{paul right}) \\times P(\\text{winner right})\\] which, filling in the appropriate values, becomes \\[P(\\text{correct}) = .5 \\times 0 + .5 \\times 1 = .5\\] Actually, you can fill in any valid probability for \\(P(\\text{winner left})\\) and \\(P(\\text{winner right})\\), and \\(P(\\text{correct})\\) will always be .5! Let’s say, in abstract terms, that \\(P(\\text{winning left}) = \\theta\\). From the complement rule, we know that \\(P(\\text{winning right}) = 1 - \\theta\\). Then \\[\\begin{align} P(\\text{correct}) &amp;= P(\\text{paul left}) \\times \\theta + P(\\text{paul right}) \\times (1-\\theta) \\\\ &amp;= .5 \\times \\theta + .5 \\times (1-\\theta) \\\\ &amp;= .5 \\times \\theta - .5 \\times \\theta + .5\\\\ &amp;= .5 \\end{align}\\] So according to our model, no matter what bias Paul’s keeper might have in placing winning teams in the left or right box, Paul’s accuracy for each prediction would be \\(P(\\text{correct}) = .5\\). 2.2.2 Distributions A probability distribution, as the name suggests, defines how probability is distributed over all possible values of a variable. You can think of “all probability” as a pie, and the probability distribution as slicing up the pie in possibly unequal pieces, and each possible value of a variable is given its own slice. Looking at the correctness of a single prediction, the pie is divided into two equal slices: half for \\(P(\\text{correct})\\), and the other half for \\(P(\\text{incorrect})\\). But Paul made more than one prediction (8 in the case of the FIFA 2010 World Cup), and what we’re really interested in is Paul’s accuracy in all his predictions: What is the probability that Paul made 8 out of 8 correct predictions if he was merely guessing? Using the rules of probability, we can work out this probability as well. According to our coin-flipping model, it does not matter which of Paul’s prediction we look at (e.g. whether the first, the second, the last, or any other one). In fact, we could generate the data in the reverse order, or start with the Germany-Australia match and then the Spain-Germany match. The probability that any prediction is correct is \\(P(\\text{correct}) = .5\\), and whether that prediction is correct is independent from whether any earlier (or later) prediction is correct. In fancy terms, this means that the observations \\(Y_i\\) are independent and identically distributed (IID). When considering the probability of a sequence of events (e.g. a consecutive run of 8 correct predictions), it can be useful to draw all possible outcomes in the form of a tree. 2.3 Flipping a biased coin: An alternative model Suppose that Paul did not make his decisions by flipping a mental coin. Perhaps Paul had extensive knowledge of all things football, or perhaps he could see into the future. In that case, we might expect Paul to have a higher rate of correct answers than predicted by our coin flipping model. But perhaps Paul, although an oracle, wasn’t so benevolent to his caretakers, or didn’t want to spoil the match for them. In that case, we might expect the rate of correct answers to be lower than predicted by the coin flipping model. In both case, we’d expect the probability of a correct answer to be different from .5: \\(P(\\text{correct}) \\neq .5\\). But whilst not .5, we don’t otherwise know the value. We call such an unknown value which determines a probability distribution a parameter. We will generally use symbols from the Greek alphabet to denote such parameters. Here, we will use \\(\\theta\\) (pronounced as “thee-ta”). Our alternative to the coin flipping model can now be stated as: \\[P(Y_i = \\text{correct}) = \\theta\\] Like a variable, a parameter such as \\(\\theta\\) can take different values. It belongs to a model, and its value can not be observed, only inferred. As our parameter reflects a probability, we know that it can never be lower than 0, and never larger than 1, so we know that: \\[0 \\leq \\theta \\leq 1\\] (“zero is less than or equal to \\(\\theta\\), which is less than or equal to 1”). Note that the possible values in this range includes \\(\\theta = 0.5\\). Our (unbiased) coin flipping model is actually one of the possible versions (a “special case”) of our new more general model. Some other ways of saying this are that the coin flipping model is contained within, or nested under, the more general new model. As we will see, nested models play an important role in statistical modeling. In particular, we will often compare how well a special-case or nested model describes the data compared to a more general model. In such model comparisons, we will usually refer to the special-case model as MODEL S, and the more general model as MODEL G. It is important to realise that these names (like variables) are containers and we can put any models in these, as long as the one we put into MODEL S is a special case of the one we put into MODEL G. 2.4 Estimation From data produced by the Data Generating Process (DGP), we can estimate the value of a parameter. Remember, when we collect data in real life, it is obtained from DGP. Our model, on the other hand, provides a mathematical abstraction of the DGP. The DGP is The Truth. It is how the real world generated the data. Our model, on the other hand, is almost surely an approximation to The Truth. Think about it. Can you summarize Paul’s choices by a mathematical formula with a distribution that takes a single parameter (\\(\\theta\\))? Probably not when you see Paul swimming in his tank, catching a glimmer of light shining from one of the boxes, than being distracted by the sound of a distant door opening, then going back and sensing a vibration in the water. How would all these serendipitous things not affect Paul’s decision at the time? Our model ignores all these factors, and for a reason. Imagine we had to take all sensory input of Paul into account. We would not only have to take into account the actual stimuli present at the time, but also how they affected Paul’s neuronal reactions to those. Even if we only take into account the neurons responding directly to external input, the number of possible neuronal excitation patterns quickly become enormous. When you think about it, you’d also want to take into account the activation state of all the neurons at the time just before (and possibly before that), which makes the number of possibilities even more enormous. And what about the chemical consistency of the water at the time? Learning a model of particular decision by a particular person at a particular time seems simply impossible without assuming that some of these things don’t really matter to the decision at hand. Estimation, in some sense, means an “educated guess”. It is a guess, because we cannot be completely sure that we have picked the true value. It is educated because we didn’t just pluck the value from thin air. We used information in the data to guide our guess. 2.4.1 Properties of good estimators An estimator is an algorithm which takes data as input and produces a parameter estimate as output. Evaluating the quality of the resulting estimates is generally done under the assumption that our model represents the Data Generating Process accurately, i.e. our model is a true representation of the DGP. If our model has unknown parameters, that means that there are values for those parameters such that our model produces data with exactly the same distribution as the DGP. We can call those parameter values the true parameter values. Now, even if we have access to such a true model, estimates of its parameters from a given data set are unlikely to be identical to the true parameters. The main reason is that any actual data set will not cover all of the values that are possible from the Data Generating Process, and/or not in their expected proportions. But the fact is that in practice we have to rely on such limited data sets; we never have access to all the data that could be produced by the Data Generating Process. So we will have to work with estimates that are different from the true parameters. One thing we can do is quantify how much our estimates are likely to deviate from the true parameters. This, in a nutshell, is what statistical estimation theory is all about. 2.4.1.1 Unbiasedness An unbiased estimator gives, on average, estimates which are equal to the true parameter. 2.4.1.2 Consistency A good estimator should become more precise as we have more data. The larger the number of observations from the Data Generating Process, the closer the resulting estimates should be to the true parameters. A consistent estimator is one which provides less variable estimates if we give it more data. 2.4.1.3 Efficiency An efficient estimator is an estimator which is both unbiased and the most consistent possible. Such an estimator is, in some sense, the Holy Grail of statistical estimation. 2.5 Null-hypothesis significance testing 2.5.1 Decisions and types of error 2.5.2 Significance and power 2.6 Confidence intervals 2.7 Summary 2.8 Epilogue "],
["a-model-with-a-mean-introduction-to-the-general-linear-model.html", "Chapter 3 A model with a mean: Introduction to the General Linear Model 3.1 Numeric judgment and anchoring 3.2 The Normal distribution 3.3 The Central Limit Theorem 3.4 Parameter estimation 3.5 Testing whether \\(\\mu\\) has an specific value 3.6 The General Linear Model", " Chapter 3 A model with a mean: Introduction to the General Linear Model In this chapter, we will start our journey into the General Linear Model with the most basic example within the GLM: a model concerning a single Normal distribution, where we are interested in statements about the mean of that distribution. We will look at the Normal distribution in detail, and cover estimation of its parameters, before focusing on testing hypotheses about the mean. We end the chapter with a look ahead towards the other chapters, by providing a brief overview of the General Linear Model. but first, let’s discuss the research example used throughout the chapter. 3.1 Numeric judgment and anchoring In 1906, Sir Francis Galton attended The West of England Fat Stock and Poultry Exhibition in Plymouth. One of the attractions at the fair was a contest to guess the weight of a “dressed” ox on display. The nearest guess to the actual weight of the ox would win a prize, and a total of 800 attendees participated for a small fee. Not only was Francis Galton a now-viled eugenicist (founder of the Galton Laboratory of National Eugenics at UCL), he was also a statistician, and bought the used tickets off the stall holder. Back home he analysed the 787 usable tickets and published his findings in Galton (1907). What surprised Galton was that while the individual guesses were often far from the true weight of the ox, the average guess of the group as a whole was highly accurate. Such findings are now often referred to as the “Wisdom of the crowds”. Although the wisdom of the crowds has been demonstrated in various domains, from decades of research, we also know that human judgment is subject to a long list of biases. One such bias is called “anchoring”, and it pertains to the influence of prior exposure to a more or less arbitrary numerical value on the subsequent numerical judgment. For instance, if I ask you “to estimate the height of”How tall do you think Mount Everest is?\", but first state that Mount Everest is taller than 2000 feet, you are likely to give a smaller number than if I first state that Mount Everest is shorter than 45,500 feet. In this example, we will focus on data obtained from a large scale replication study, covering the above described anchoring manipulation, as well as many other experiments (Klein et al. 2014). This “Many Labs” study contained data collected by a large number of labs from different countries, and here we will use data collected by Konrad Bocian and Natalia Frankowska from the University of Social Sciences and Humanities Campus in Sopot, Poland. In particular, we will focus on the judged height of Mount Everest (in meters) after a low anchor. From the wisdow of the crowds idea, we might expect the average judgment to be identical (or at least very close) to the true height of Mount Everest, which is 8848 meters above sea level. On the other hand, if the low anchor biased the judgments, we might expect the average judgment to deviate from the true height. 3.1.1 Exploring the data Before diving into statistical modeling, as discussed, it is always good to explore the data. A graphical overview in the form of a histogram and combined violin and boxplot, is given in Figure 3.1. The minimum was judged height was 650 meters (so quite close to the anchor), and the maximum was 12,459 meters. The median judgment was 8000 and the mean 6312.193. Figure 3.1: Histogram and boxplot of participants’ judgments 3.2 The Normal distribution The Normal distribution has the well-known bell-shaped curve depicted in Figure 3.2. Figure 3.2: The Normal density function for \\(\\mu = 0\\) and \\(\\sigma = 1\\) The curve is given by the following function: \\[\\begin{equation} p(y) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} \\tag{3.1} \\end{equation}\\] If that looks complicated, don’t worry. First, let me point out that \\(\\pi\\) refers to the mathematical constant pi, i.e. \\(\\pi= 3.141593\\ldots\\), and \\(e\\) refers to the mathematical constant also known as Euler’s number, i.e. \\(e = 2.718282\\ldots\\). They are known constants and not parameters. The Normal distribution does have two parameters: \\(\\mu\\) (“mu”), the mean, and \\(\\sigma\\) (“sigma”), also called the standard deviation. Confusingly, the formula above is not actually the formula for the Normal probability distribution. Rather, it is the formula of the Normal density function. The Normal distribution applies to continuous variables. Technically, you can’t assign a probability to a particular value of a continuous variable, you can only assign probabilities to ranges of values. This is because a continuous variable has an infinite number of values (even if we restrict ourselves to a range). If we gave each unique value a probability of anything larger than 0, the total probability would not be 1, but infinite. If you add up an infinite amount of values, no matter how tiny they are, at some point you would reach the sum of 1, and then you’d have to keep on adding more. No matter how large the sum becomes, you would have to keep on going. Adding up an infinite number of values is not light work… \\[P(a \\leq Y \\leq b) = \\int_a^b \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} d y\\] If you are unfamiliar with calculus and have never seen an integration sign (\\(\\int\\)), don’t worry, there won’t be much more of this. It is a mathematical way of referring to the “area under the curve” between point \\(a\\) and \\(b\\). A proper probability density function respects the rule of total probability, such that \\(P(-\\infty \\leq Y \\leq \\infty) = 1\\). Figure 3.3 shows the probability of three of such ranges, each symmetrical around the mean. The probability of a value falling in the range between \\(\\mu - 3 \\times \\sigma\\) and \\(\\mu + 3 \\times \\sigma\\) is very close to 1. Hence, values more than three standard deviations from the mean are very unlikely. Figure 3.3: Normal distribution and the probability of a value falling in one of three overlapping ranges. Note that the ranges are overlapping, so that the range between \\(\\mu - 3 \\sigma\\) and \\(\\mu + 3 \\sigma\\) also covers the range between \\(\\mu - 2 \\sigma\\) and \\(\\mu + 2 \\sigma\\), but that this isn’t shown in the colours. 3.3 The Central Limit Theorem A main reason that the Normal distribution is used so often (and perhaps called “Normal”, rather than “Abnormal”), is due to a mathematical fact known as the Central Limit Theorem: Definition 3.1 (Central Limit Theorem) The distribution of the sum of \\(N\\) independent variables approaches the Normal distribution as the number of variables approaches infinity (\\(N \\rightarrow \\infty\\)). This is quite an amazing theorem. Although you might wonder why you should care about sums of an infinite number of random variables, there are two things to note. Firstly, the rate at which the distribution “approaches” the normal distribution can be relatively quick. Secondly, the sample mean can be viewed as the (normalized) sum of \\(N\\) variables: \\[\\overline{Y} = \\frac{1}{N} \\sum_{i=1}^N Y_i\\] So the central limit theorem implies that as long as the number of samples is large enough, the sample mean will follow a Normal distribution. 3.3.1 The Central Limit Theorem in action To see the central limit theorem in action, let’s pick an arbitrary probability distribution over 5 values, as shown in Figure 3.4. This will be the true distribution (i.e. the Data Generating Process) from which we can sample values. Figure 3.4: An aribitrary probability distribution over 5 values, clearly not a Normal distribution! When we repeatedly sample 5 values and calculate the mean, we can look at the distribution of these sampled means. This is shown in Figure 3.5. Figure 3.6 shows the same when we repeatedly sample 20 values and calculate the mean. Figure 3.5: The distribution of the mean of 5 samples (from the distribution in Figure XX Figure 3.6: The distribution of the mean of 20 samples from the distribution in Figure XX So according to the Central Limit Theorem, if we focus on the mean of a sufficiently large number of independent observations, we can reasonably assume that the sampling distribution of the mean will follow a Normal distribution, even if the distribution from which the actual values were drawn is far from Normal. We might also imagine that if a variable is the outcome of a process in which lots of independent sources of noise are added, that the resulting variable will follow a Normal distribution. [todo: add example] 3.4 Parameter estimation The maximum likelihood estimate of \\(\\mu\\) is the sample mean: \\[\\begin{equation} \\hat{\\mu} = \\overline{Y} = \\frac{\\sum_{i=1}^N Y_i}{N} \\tag{3.2} \\end{equation}\\] The maximum likelihood estimate of the variance is the sample variance \\(\\hat{\\sigma}^2_\\text{ML} = \\frac{\\sum_{i=1}^N (Y_i - \\overline{Y})}{N}\\). However, this estimator of the variance is biased. It is therefore common to estimate the variance with the unbiased estimator: \\[\\begin{equation} \\hat{\\sigma}^2 = S^2 = \\frac{\\sum_{i=1}^N (Y_i - \\overline{Y})}{N-1} \\tag{3.3} \\end{equation}\\] The estimator above is for \\(\\sigma^2\\) (sigma squared, i.e. sigma raised to the power of 2). If we want an estimator of \\(\\sigma\\), we can simply take the square-root to get the following estimator of \\(\\sigma\\): \\[\\hat{\\sigma} = S = \\sqrt{S^2}\\] 3.4.1 Sampling distribution of the estimated mean Remember that the estimator of the mean is an algorithm that provides estimates from data. Different data sets will give different estimates, even when these are generated from the same Data Generating Process. Our model of the Data Generating Process is the Normal distribution, which has two parameters: \\(\\mu\\) and \\(\\sigma\\). Let’s pick some values for these parameters, so that we can use our model to simulate data. For instance, suppose the judgments are on average equal to the true value, so \\(\\mu = 8848\\). Individual judgments are quite variable, however, so let’s take \\(\\sigma = 2000\\). Figure~@ref{fig:histogram-sampling-mean-simple} shows the distribution of the estimated mean of 10,000 data sets each consisting of \\(N=109\\) observations (just like in the anchoring data). Also shown (as a dotted line) is the Normal distribution of the model we used to generate the data. Clearly, the estimated means are much less variable than the simulated judgments. Figure 3.7: Estimated means for 10000 simulated data sets of N = 109, drawn from a Normal distribution with mean 8848 and standard deviation 2000. The dotted line is represents the Normal distribution from which the data sets were generated, and the solid line the theoretical distribution of the estimated means. 3.5 Testing whether \\(\\mu\\) has an specific value 3.5.1 The classical way \\[\\begin{equation} t = \\frac{\\hat{\\mu} - \\underline{\\mu}}{\\hat{\\sigma}/\\sqrt{N}} = \\frac{\\overline{Y} - \\underset{\\sim}{\\mu}}{S/\\sqrt{N}} \\end{equation}\\] This is also called a one-sample t-test. 3.5.1.1 Two-tailed and one-tailed tests 3.5.2 The model comparison way http://www.stats.ox.ac.uk/~dlunn/b8_02/b8pdf_8.pdf http://www.stat.ucla.edu/~hqxu/stat105/pdf/ch09.pdf https://www.math.arizona.edu/~jwatkins/v-anova.pdf There is an equivalent way to perform the hypothesis test that \\(\\mu = \\underline{\\mu}\\), by comparing two versions of a statistical model, one in which we assume we know the value of \\(\\mu\\) to be \\(\\underline{\\mu}\\) (Model R), and one in which we don’t (Model G). As we did for Paul the Octopus, we then look at the likelihood ratio to see whether Model R is tenable. Note that in both models, the standard deviation is considered unknown and will have to be estimated. 3.6 The General Linear Model \\[Y_i = \\beta_0 + \\beta_1 X_{1i} + \\ldots + \\beta_K X_{Ki} + \\epsilon_i \\quad \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\] References "],
["linear-regression.html", "Chapter 4 Linear regression 4.1 The model 4.2 Estimation 4.3 Hypothesis testing 4.4 Confidence intervals 4.5 Assumptions and outliers 4.6 Summary", " Chapter 4 Linear regression Donald Trump is perhaps the most divisive president in American history. Figure 4.1: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens Figure 4.2: Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens To assess the relation between Trump votes and hate groups, we will use a linear regression model. As the name suggests, this model involves a line, a straight one in fact. This straight line represents the predicted value of the dependent variable \\(Y\\) (i.e. the percentage of Trump votes) for each value of the predictor (or independent) variable \\(X\\) (i.e. the number of hate groups). The model allows the predictions to be wrong by incorporating an error term, 4.1 The model The bivariate regression model can be defined as follows: \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 X_{i} + \\epsilon_i \\quad \\quad \\quad \\epsilon_i \\sim \\mathcal{N}(0,\\sigma) \\tag{4.1} \\end{equation}\\] The intercept \\(\\beta_0\\) is The slope \\(\\beta_1\\) is The error or residual \\(\\epsilon_i\\) Figure 4.3: The simple regression model. Left: The intercept is the predicted value of Y when X=0 and is point at which the regression line crosses the y-axis. The slope determines the steepness of the regression line and represents the increase (or decrease) in the predicted Y-value for every 1-unit increase in X. Right: the error terms or residuals are the vertical distances of each observed Y-value from the regression line Figure 4.4: The errors in a regression model are assumed to follow a Normal distribution around the regression line. 4.2 Estimation The maximum likelihood estimates of the model parameters are \\[\\begin{equation} \\hat{\\beta}_0 = \\overline{Y} - \\hat{\\beta}_1 \\overline{X} , \\tag{4.2} \\end{equation}\\] for the intercept, and \\[\\begin{equation} \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^N (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sum_{i=1}^N (X_i - \\overline{X})^2} . \\tag{4.3} \\end{equation}\\] for the slope. Note that to estimate the intercept, we need the estimate of the slope. Let’s focus on the estimate of the slope first. The top part of the division (the numerator) contains a sum of deviations of the predictor values (\\(X_i\\)) from its average (\\(\\overline{X}\\)) multiplied by deviations of the values \\(Y_i\\) of the dependent variable from its average (\\(\\overline{Y}\\)). Let’s consider these multiplied deviations a little further. Each deviation is positive (larger than 0) when the value is higher than the average, and negative (smaller than 0), when the value is lower than the average. So the multiplied deviations are positive whenever both values are larger than their average, and whenever both values are below their average (a negative value multiplied by another negative value is positive). If we were to divide the sum of the multiplied deviations by \\(N\\) (the number of observations), we’d get the average of these multiplied deviations. This average is also called the covariance between \\(X\\) and \\(Y\\): \\[\\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^N (X_i - \\overline{X})(Y_i - \\overline{Y})}{N}\\] Note that, as an estimator, this provides biased estimates of the true covariance. An unbiased estimator of the true covariance is obtained by dividing by \\(N-1\\) instead of \\(N\\) (just as for the variance). Going back to our example, the covariance between trump votes and hate groups would be positive whenever states with higher-than-average Trump votes are generally also states with higher-than-average hate groups, and whenever states with lower-than-average Trump votes are generally also states with lower-than-average hate groups. The covariance would be negative, on the other hand, whenever states with higher-than-average Trump votes are generally states with lower-than-average hate groups, and whenever states with lower-than-average Trump votes are generally states with higher-than-average hate groups. A positive or negative covariance is indicative of a relation between \\(X\\) and \\(Y\\). Indeed, the well-known Pearson correlation coefficient is a standardized covariance, where the standardization scales the correlation to always be between -1 and 1 and involves dividing the covariance by the product of the standard deviations of both variables: \\[\\begin{align} r_{X,Y} &amp;= \\frac{\\text{Cov}(X,Y)}{S_X \\times S_Y} \\\\ &amp;= \\frac{\\frac{\\sum_{i=1}^N (X_i - \\overline{X})(Y_i - \\overline{Y})}{N}}{\\sqrt{\\frac{\\sum_{i=1}^N (X_i - \\overline{X})^2}{N}} \\times \\sqrt{\\frac{\\sum_{i=1}^N (Y_i - \\overline{Y})^2}{N}}} \\end{align}\\] Going back to the estimate of the slope (Equation (4.3)), we can view this as a different way of standardizing the covariance. Looking at the bottom part of the division (the denominator), we can see it consists of the sum of squared deviations of the predictor values from its mean. If we were to divide this sum by the number of observations, we’d get the variance of \\(X\\). As dividing both the top part (numerator) and bottom part (denominator) in a division by the same value does not affect the outcome of the division (i.e. \\(\\frac{a}{b} = \\frac{a/c}{b/c}\\)), we can choose to divide both by \\(N\\) so the numerator becomes the covariance and the denominator the variance (we could also divide both by \\(N-1\\) so they become unbiased estimators of the covariance and variance). So an alternative way of computing the estimate of the slope is \\[\\hat{\\beta}_1 = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\\] Note that the variance of \\(X\\) equals the product of the standard deviation of \\(X\\) and itself, as \\(S^2_X = S_X \\times S_X\\). So the slope estimate looks quite a bit like the correlation coefficient, where instead of the standard deviation of \\(Y\\), we use the standard deviation of \\(X\\) twice. With a little algebraic manipulation, we can also state the slope estimate in terms of the correlation as \\[\\hat{\\beta}_1 = \\frac{S_Y}{S_X} r_{X,Y}\\] The reason for going into these alternative formulations is not to pain or confuse you with lots of equations. What these alternative formulations show is that the slope tells us something about the relation between \\(X\\) and \\(Y\\), just like the covariance and correlation do. If the sample correlation is 0, then so is the estimated slope. It is important to realise that we have been discussing the estimate of the slope, not the true value itself. But the same relations hold for the true values. If we denote the true correlation as \\(\\rho_{X,Y}\\), then the true value of the slope can be defined as \\[\\beta_1 = \\frac{\\sigma_Y}{\\sigma_X} \\rho_{X,Y}\\] The true value of the slope is 0 when the true correlation between \\(X\\) and \\(Y\\) equals \\(\\rho_{X,Y} = 0\\). It would also be 0 if the true standard deviation of \\(Y\\) equals \\(\\sigma_Y = 0\\), but this implies that \\(Y\\) is a constant and that is not a very interesting situation. That was perhaps a little tortuous, and we haven’t even discussed the estimate of the intercept! Remember that the intercept represents the predicted value of \\(Y\\) at the point where \\(X=0\\). It is the average value of \\(Y\\) for all those cases where \\(X=0\\). This is often not so interesting, although in our example, we might be interested in what the average percentage of votes for Trump would be in places where there are no hate groups. Equation (4.2) shows that we can estimate this value by adjusting the average of \\(Y\\) by subtracting \\(\\hat{\\beta}_1 \\times \\overline{X}\\) from it. How come? [TODO] So, what are the estimates of the model predicting Trump votes by hate groups? \\[\\texttt{trump_votes}_i = 42.897 + 2.3 \\times \\texttt{hate_groups} + e_i\\] Figure 4.5: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens with the estimated regression line. 4.3 Hypothesis testing As before, there are two main ways in which too look at testing whether the parameters of the simple regression model are different from a priori values. The first is to consider how variable the parameter estimates are under the assumption that the true parameter is identical to the a priori value. The second way is to compare two models, one in which the parameter is fixed to the a priori value, and one where it is freely estimated. Both of these ways will provide us with the same outcome. Personally, I find the model comparison way easier to think about. 4.3.1 Sampling distribution of estimates Remember, an estimate (whether of the mean or of another parameter) is a noisy reflection of the true value of the parameter, and the noise comes from having access only to limited data and not all the data that the data generating process can produce. Suppose in reality there is no relation between hate groups and Trump votes, so that the true slope is \\(\\beta_1 = 0\\). In that case, the model becomes \\[\\begin{align} Y_i &amp;= \\beta_0 + 0 \\times X_i + \\epsilon \\\\ &amp;= \\beta_0 + \\epsilon_i \\end{align}\\] which is identical to the simple model of the previous chapter. Then, the true value of the intercept would be \\(\\beta_0 = \\mu\\). If we’d know the true value of the standard deviation \\(\\sigma\\), we’d have a fully specified model (a Normal distribution) which we can use to generate as many alternative data sets as we’d like. By generating these data sets, and estimating the slope of our model for each, we can get an overview of the variability of the estimates when the true slope equals \\(\\beta_1 = 0\\). An unbiased estimator ensures that on average, these estimates equal the true value. The main thing of interest is then the variability around this value. A consistent estimator ensures that this variability becomes smaller with the larger data sets. But to understand how well we can estimate the parameter for the present data, we would simulate data sets with the same number of observations (so \\(N=50\\)). Unfortunately, we don’t know the true value of \\(\\sigma\\). Our data provides an estimate of \\(\\sigma\\), but we know this estimate is noisy itself. Thinking first in the same way about the sampling distribution of \\(\\hat{\\sigma}\\), we could first sample a values of \\(\\sigma\\), and then use each of these to generate a data set for which to estimate \\(\\beta_1\\). Doing this many (many many!) times would give us a good overview of the variability of the estimates. As before, we don’t have to actually simulate the data sets. If the null model is true, then we can derive that the sampling distribution of the estimates follows a t-distribution. Thus, we should look for the \\(t\\) value our data provides, and evaluate this value within the context of the sampling distribution derived under the model where \\(\\beta_1 = 0\\). For both paraneters (intercept and slope), the same logic applies. In general then, for parameters \\(\\beta_j\\) (where \\(j = 0\\) or 1), the \\(t\\)-value of our data is computed as \\[\\begin{equation} t_{N-2} = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)} \\quad \\quad \\quad t_{N-1} \\sim \\mathcal{T}(N-2) \\end{equation}\\] where \\(\\text{SE}(\\hat{\\beta}_j)\\) is the standard error of the estimate, which you should remember is the standard deviation of the sampling distribution of the estimates. I won’t bore you with how to compute this standard error; statistical software does a good job at this. One thing to realise though is that the computation assumes that the data always have the same values for the predictor. Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 42.9 2.409 17.80 0.000 hate_groups_per_million 2.3 0.672 3.43 0.001 4.3.2 Model comparison Df Sum Sq Mean Sq F value Pr(&gt;F) hate_groups_per_million 1 981 980.8 11.7 0.001 Residuals 48 4011 83.6 NA NA 4.4 Confidence intervals The formula to compute confidence intervals for the two parameters can be written as: \\[\\hat{\\beta}_j \\pm t_{1-\\alpha/2,N-2} \\times \\text{SE}(\\hat{\\beta}_j)\\] where \\(t_{1-\\alpha/2,N-2}\\) is the right-critical value in a t-distribution with \\(N-2\\) degrees of freedom and a significance level of \\(\\alpha\\). Using \\(\\alpha=.05\\) gives us the conventional 95%-confidence interval. 4.5 Assumptions and outliers Figure 4.6: Predicted vs residual plot and a QQ plot of the residuals 4.6 Summary "],
["multiple-regression.html", "Chapter 5 Multiple regression 5.1 The model 5.2 Estimation 5.3 Inference 5.4 Multicollinearity 5.5 Polynomial regression 5.6 Summary", " Chapter 5 Multiple regression 5.1 The model 5.2 Estimation 5.3 Inference 5.4 Multicollinearity 5.5 Polynomial regression 5.6 Summary "],
["moderation-and-interactions-glm-3.html", "Chapter 6 Moderation and interactions (GLM 3) 6.1 Moderated regression", " Chapter 6 Moderation and interactions (GLM 3) 6.1 Moderated regression "],
["a-model-of-means-glm-4-anova.html", "Chapter 7 A model of means (GLM 4: ANOVA) 7.1 Two-sample t-test 7.2 Contrast coding 7.3 Oneway ANOVA 7.4 Factorial ANOVA", " Chapter 7 A model of means (GLM 4: ANOVA) 7.1 Two-sample t-test 7.2 Contrast coding 7.3 Oneway ANOVA 7.4 Factorial ANOVA "],
["mixing-categorical-and-metric-predictors-glm-5-ancova.html", "Chapter 8 Mixing categorical and metric predictors (GLM 5: ANCOVA)", " Chapter 8 Mixing categorical and metric predictors (GLM 5: ANCOVA) "],
["repeated-measures-glm-6-repeated-measures-anova.html", "Chapter 9 Repeated-measures (GLM 6: Repeated-measures ANOVA)", " Chapter 9 Repeated-measures (GLM 6: Repeated-measures ANOVA) "],
["linear-mixed-effects-models.html", "Chapter 10 Linear mixed-effects models", " Chapter 10 Linear mixed-effects models "],
["references.html", "References", " References "]
]
