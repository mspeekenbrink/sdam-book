[["index.html", "Statistics: data analysis and modelling Preface Acknowledgements Notation", " Statistics: data analysis and modelling Maarten Speekenbrink 2020-10-19 Preface This book concerns statistics, and more specifically data analysis and modeling. It aims to be a self-contained source, requiring little background in mathematics or statistics. That said, it is not necessarily an “easy” book. It contains some mathematical notation and ideas. I do my best to properly explain how to a read and interpret these in both an intuitive and more formal way. I don’t do this to torture you. Neither you with little mathematical background, who might have to read some explanations twice or more, nor you with more mathematical background, who might find my intuitive explanations tedious, superfluous, and utterly pointless, nor you in the middle, who, like me, will agree with both (but hopefully not simultaneously). What I hope to do is to bring you some sense of the beauty of a formal system that stems from its simplicity and exactness, as well as some of the excitement of grasping it. I’m under no illusion that this is an easy task, for you or for me. But I hope you will try, like I did writing this book. We will focus on building models of data in order to evaluate claims about how that data came to be what it is. Statistics is often taught with a “cookbook” approach. The “ingredients” are characteristics of the data (e.g. a variable is metric) and the goal of the analysis (e.g. compare the mean of two groups) and the cookbook provides the recipe (e.g., a two-sample t-test). While straightforward to teach, the limitations of this approach become quickly apparent when you need to analyse data from a less “standard” design. The cookbook can also become something of a straightjacket, when you are forced to change the design of a study in order to allow the subsequent analysis to be covered in the cookbook. Another limitation of the approach is that the different analyses and tests are often treated in isolation, which makes it more difficult to see the often strong connections between the models and statistical principles underlying them. By contrast, the modelling approach adopted here is much more flexible. The aim is to give you the confidence to, when given the ingredients, design your own recipe. Unlike some other books, we don’t discuss using software such as R, JASP, or SPSS. Instead, there are companion books which discuss how to do analyses with specific software (R or JASP). Separating theory and practice in this way has as main benefit that each can be focused on fully. An additional benefit is that it is also easier to add companions for other software. A downside is that you might need to cross-reference between two books. However, by keeping the structure of the companions mostly in line with the structure of this book, this cross-referencing should be straightforward enough. Acknowledgements This book is inspired by lots of other statistics books I have read over the years. Some books deserve special mention. “Data analysis: A model comparison approach” (Judd, McClelland, &amp; Ryan, 2011) is a book I have used for many years as required reading for the MSc level statistics course. It is extremely clear in its coverage of the General Linear Model, and consistent in its use of the model comparison approach we adopt here as well. “Doing Bayesian data analysis” (Kruschke, 2014) gave me the idea to cover all the basics of statistical modelling with a simple binomial example. Notation In a book with mathematics, notation is important. Mathematics can be viewed as a very precise language. For some of you, this may be a very foreign language. But languages can be learned. Like words, mathematical symbols represent important concepts and relations. The aim is to be consistent, so that each symbol has a clearly defined meaning. That isn’t always easy, or possible. Another aim is to stay close to prevailing conventions, so that it will be easier to understand papers and other books. This is also not always easy, or possible, because in different areas of statistics, the same symbol may have very different meanings. In any case, we will try to be very clear when introducing notation. You will come across symbols from the Greek alphabet. Generally, these symbols are used to reflect parameters of statistical models, and sometimes to reflect other fundamental aspects of statistical analyses. So it will be useful to familiarize yourself with the Greek alphabet. In the following table, each Greek letter is shown, with the corresponding name in English (useful if you want to read the symbols aloud), as well as the common parameter or concept the symbol refers to: symbol name usage \\(\\alpha\\) alpha significance level \\(\\beta\\) beta parameter (slope or intercept) in the General Linear Model \\(\\gamma\\) gamma \\(\\delta\\) delta \\(\\epsilon\\) epsilon error \\(\\zeta\\) zeta \\(\\eta\\) eta \\(\\theta\\) theta a parameter in general \\(\\iota\\) iota \\(\\kappa\\) kappa \\(\\lambda\\) lambda \\(\\mu\\) mu mean \\(\\nu\\) nu \\(\\xi\\) xi \\(\\omicron\\) omicron \\(\\pi\\) pi \\(\\rho\\) rho correlation \\(\\sigma\\) sigma standard deviation \\(\\tau\\) tau \\(\\upsilon\\) upsilon \\(\\phi\\) phi \\(\\chi\\) chi Chi-squared distribution \\(\\psi\\) psi \\(\\omega\\) omega References "],["intro.html", "Chapter 1 Introduction 1.1 Paul the octopus 1.2 Experiments and observations 1.3 Data 1.4 Exploring and describing data 1.5 Analysis and modelling 1.6 Summary", " Chapter 1 Introduction In this chapter we will introduce some fundamental concepts, such as experiments, data, analysis, and modelling. We will also introduce you to Paul the octopus. 1.1 Paul the octopus We know that all octopus have nine brains so we know he has exceptional powers. Oliver Walenciak, Paul’s keeper Source: Irish central Paul the Octopus (26 January 2008 – 26 October 2010) was born in the Sea Life Centre in Weymouth, England, and subsequently moved to the aquarium chain’s centre in Oberhausen, Germany. It was there he found world-wide fame as a cephalopod oracle. Figure 1.1: Paul the octopus predicts Spain to win the final of the 2010 FIFA World Cup. Source: The Guardian Paul began his career during the UEFA Euro 2008 football tournament. In the lead-up to Germany’s international football matches, Paul was presented with two clear plastic boxes, each containing food (a mussel or an oyster). Each container was marked with the flag of a team, one the flag of Germany, and the other the flag of Germany’s opponent. The box which Paul opened first (and ate its contents) was deemed to be the predicted winner of the match. Paul predicted Germany to win all of their games, a correct prediction in 4 out of 6 cases. He failed to predict their defeats by Croatia in the group stage, and by Spain in the championship’s final. Table 1.1: Paul’s predictions for the UEFA Euro 2008 Match Prediction Result Outcome Germany-Poland Germany 2-0 correct Croatia-Germany Germany 2-1 incorrect Austria-Germany Germany 0-1 correct Portugal-Germany Germany 2-3 correct Germany-Turkey Germany 3-2 correct Germany-Spain Germany 0-1 incorrect Two years later, during the 2010 FIFA World Cup football tournament, Paul obtained celebrity status and his divinations were broadcast live on German TV. This time, Paul made correct predictions for all matches in which Germany played, as well as the final between Spain and the Netherlands. Table 1.2: Paul’s predictions for the 2010 FIFA World Cup Match Prediction Result Outcome Germany-Australia Germany 4-0 correct Germany-Serbia Serbia 0-1 correct Ghana-Germany Germany 0-1 correct Germany-England Germany 4-1 correct Argentina-Germany Germany 0-4 correct Spain-Germany Spain 1-0 correct Uruguay-Germany Germany 2-3 correct Netherlands-Spain Spain 0-1 correct Paul’s record of 8/8 correct predictions in the 2010 world cup is quite amazing. The common octopus (Octopus vulgaris) is indeed quite an intelligent species. But was Paul really psychic? Could he really foresee the future? 1.2 Experiments and observations We can view Paul’s divinations as the result of an experiment to test his psychic abilities. Paul’s predictions were derived under more or less controlled conditions: The keeper didn’t put Paul’s favourite food in the box representing the team he’d hope to win, and didn’t always place the box representing Germany on one side of the tank. Had he done one or more of these things, he might have biased the results. A paradigmatic example of an experiment is a randomized-control trial, as common in studies assessing the effectiveness of medication or other therapeutic interventions. In such experiments, a sample of participants is taken from a population of interest, and each participant is randomly assigned to either an experimental condition or control condition. For example, participants in the experimental condition might be given a new drug, while participants in the control condition receive a placebo. Ideally, this is done in a “double-blind” setting, where neither the participant nor the person delivering the treatment or otherwise interacting with the participant knows which condition the participant is assigned to. Randomization is key here: By letting chance determine who gets which treatment, characteristics of the participants (e.g. age, sex, severity of symptoms, etc.) will not be tied to the treatment they get. As such, it will be reasonable to assume that the experimental and control condition differ only in their treatment, such that any differences in outcome can be attributed to the treatment, and not to other differences between the groups of participants. Data obtained under less stringent conditions is often referred to as observational data. For instance, rather than random assignment, we could let the doctor choose which treatment to give to which patient. The doctor might choose to give the new drug to the more severe cases, and the placebo to the less severe cases. Initial severity of disease is now confounded with the treatment, and any difference in outcome success observed between the experimental and control condition might be due to the treatment, or to the initial differences in severity. Or even to a third variable such as age, if it is itself related to both initial severity of disease as well as the effect of the drug. We wouldn’t be able to tell, and hence the conclusions we can draw about the effects of the treatment are much less straightforward. We are not completely helpless though. Using statistics, we can attempt to control for pre-existing differences in age and initial disease severity. As we will see in due course, we can attempt to build a model which includes both treatment and initial disease severity as predictors of outcome. In doing so, we can attempt to estimate the unique effect of treatment on outcome. By considering only those differences in outcome that cannot be attributed to initial severity or age, we can still say something meaningful about treatment effects. Nevertheless, there could always be other confounding factors that we haven’t though of. As such, conclusions about causality (whether it is truly the difference in medication that caused a difference in outcome), are not as straightforward in observational studies. 1.3 Data Data are a collection of measured or otherwise observed or known values of variables. A variable is, as the name implies, something that can take a variety of values. In the medical example, it might be the severity of the symptoms after 3 weeks of treatment, while in Paul’s case, it might be the correctness of his predictions. In a statistical model, a dependent variable is a variable of interest, a variable which we aim to predict, explain, or describe. An independent variable is a variable which we use to help predict, explain, or describe the dependent variable. For instance, in the medical example above, the dependent variable would be the severity of symptoms, while independent variables would be the treatment (medication or placebo), and potentially other factors such as patients’ age, sex, etc. Whether a variable is a dependent or independent variable may differ from model to model. As such, it is an aspect of the analysis, and not so much of the design of a study. Paul has provided us with two sets of data. The first contains his predictions for the UEFA Euro 2008 cup, and the second his predictions for the 2010 FIFA World Cup. The dependent variable in both sets is the accuracy of Paul’s predictions. Paul’s “predictions” were of course not directly predictions who was going to win each game. Paul made a choice which of two containers to open first. But we’re not really interested in whether Paul opened the left or right container in his tank, or whether Paul chose the container with the Spanish flag or the container with the Dutch flag. What we are interested in is whether the first container to be opened contained the flag of the team that would win the match played that day. If it did, Paul made a correct prediction, if not, Paul made an incorrect prediction. The data then contains information about what we deem relevant to our purposes. In a sense, looking just at the accuracy of Paul’s predictions, we have already abstracted quite a bit from the experiment. We ignore whether Paul went left or right first, how long it took him to open the first container, … These are all aspects of Paul’s behaviour which we could have focused on, and hence could be variables in our data. These variables have different characteristics. For instance, accuracy (correct or incorrect) and direction (left or right container) both have only two possible values, while the duration until opening the first container has many possible values. Variables with a finite (limited) number of values are called discrete, while variables with an infinite (unlimited) number of values are called continuous.1 In practice, all data is discrete. Even if we were to measure the time it took Paul to open the left or right box first, when we have to store the data, or otherwise make it available for analysis, we simply wouldn’t be able to express such a number in finite time or an otherwise finite manner which can be represented in a computer. It would take an infinite amount of time to read a continuous number aloud, and an infinite amount of memory to store this on a computer. 1.3.1 Measurement scales In addition to whether data is discrete or continuous, it is useful to distinguish between different scales of measurement. The measurement scale of a variable determines how we can assign numbers to its values, and how meaningful computations with those numeric values are. For instance, if you determine whether people are male or female, you can assign numbers to this variable “biological sex”, such as \\(\\text{female} = 2\\) and \\(\\text{male} = 1\\), but it doesn’t make that much sense to treat these numbers in the usual way. For instance, that we assigned a higher value to females does not imply that they have more of “biological sex”. And if we calculate an average of these numbers for a group of people, it might be 1.67. But a statement like “the average biological sex in this group was 1.67” is pretty meaningless. There are four scales of measurement: A nominal scale allows identifying values as “identical” or “different”, but nothing more. Examples of variables on a nominal scale are pizza toppings, eye colour, etc. While we can assign numbers to the values (e.g., brown = 1, blue = 2, green = 3), these are arbitrary and can be changed without much consequence (e.g. brown = 21, blue = 7, green = 1). An ordinal scale allows values to be identified as “smaller”, “identical”, or “larger”, but nothing more. For instance, first, second, or third to cross the finish line in a race. Variables on an ordinal scale can be meaningfully ordered or ranked. This means there is some constraint to numbers we can assign to values. An interval scale allows items to be assigned more meaningful numbers. On an interval scale, the differences between numbers can themselves be meaningfully ordered. Temperature is a common example. The difference between 16 and 14 degrees Celsius is smaller than the difference between 19 and 16 degrees Celsius. But we can not say that 30 degrees Celsius is twice as warm as 15 degrees Celsius. This is because an interval scale doesn’t have a fixed 0 point. For degrees Celsius, 0 degrees is the point at which water freezes. It is not the point at which there is no temperature. A ratio scale has all characteristics an interval scale has, with the addition of an absolute 0 point. Weight is a common example. Weight cannot be smaller than 0, and 0 indicates no weight at all. Not only can we say that the difference between 3 and 2 kilograms is smaller than the difference between 10 and 5 kilograms, we can also say that 10 kilograms is twice as heavy as 5 kilograms. Sometimes it is tricky to determine what the measurement scale is. For instance, are “correct” and “incorrect” ordered (correct is better than incorrect) or nominal? Most of the time, a courser distinction is enough, namely whether data is categorical (nominal or ordinal) or metric (interval or ratio). 1.3.2 The Data Generating Process In addition to considering the type or measurement scale of the data, you will generally also need to consider the process that in the end led to the data at hand. This process can be called the Data Generating Process. As the section on experimental vs observational data indicated, the way the data were obtained can affect and limit the inferences that can be made. It is also important to consider to which group or population our data speaks. This is generally related to how we have selected the subjects of a study. For instance, in the case of Paul, we have data from a single octopus, and we are mostly interested in assessing whether he was psychic. We are not interested in Paul’s psychic abilities for just those games for which he made predictions though. If Paul indeed had psychic abilities (but let’s restrict that to the context of football games), we would expect him to make accurate predictions for other games as well. So here we would like to generalize beyond the data. We would like to generalize from the games for which Paul actually made predictions to all games in the UEFA Euro 2008 and 2010 FIFA World cups, or even to all international football matches. We then need to consider how representative the data is for the more general group we want to make claims about. As Paul mainly made predictions for games in which one of the teams was Germany, the sample is not a good representation of all the games played in both cups. If it is in one way or the other easier to determine the winner in games involving Germany than it is to determine the winner in other games, then the test of Paul’s abilities that was conducted is different than the test that would be conducted if we included other games. This, then, limits the claims we can make from the test. If we were interested in whether octopuses in general have psychic abilities, then using a single octopus would not be a great choice. Paul may be unique in many ways, so again he might not be representative of all octopuses. As in experiments, where random assignment to conditions can avoid confounds, random sampling can be used to make it more likely that data collected is representative. If we are interested in Paul’s predictions for all international matches, it is practically undoable to get him to make predictions for all of these. But by randomly sampling a set of international matches, we will not be able to let this set depend on certain characteristics (e.g. Germany playing) which might bias the results. Determining the representativeness of available data for a larger group of possible observations is usually not something that can be done with absolute certainty. And unless we have access to more data, it is generally also not something that statistical analysis can help with. Considerations of representativeness usually rest on (ideally sound) reasoning. The Data Generating Process (or DGP for short) is a concept we will encounter repeatedly throughout this book. The DGP contains everything of importance when considering the variability of the data in the context of other data that could have been collected. This includes how subjects were sampled (e.g., if we randomly sample octopuses then other possible data sets would contain data from other octopuses), how subjects are assigned to conditions (if any), but most importantly also how subjects generate responses (or otherwise observable events), and any possible source of randomness in this generation. For instance, during a particular prediction, Paul might initially go for the box on the left, but then a ray of sunlight hits the box on the right, leading Paul to open that box first. Such small events can be considered a source of random noise in the experiment, that lead to variability in Paul’s behaviour, and ultimately to variability in the correctness of his predictions. As a concept, the purpose of the DGP is to describe all such sources of variability as they impact the variability of a particular variable of interest. The DGP then describes not only how the data set that we have access to was actually generated, but also how all possible other data sets would have been generated. This makes the DGP a rather abstract concept. Nevertheless, we see it is something real, something “out there in the world”, in contrast to statistical models, which inhabit the world of mathematics. For statistics, the most important aspect of the DGP is that it provides a distribution over the values of a variable if the DGP was “let loose” and generated all possible data sets (of which the actual data set we have access to is only one). For instance, this would be the frequency of correct and incorrect predictions that Paul makes in every possible version of the experiment (so every possible match, every possible assignment of teams to the left or right box, and perhaps every possible small disturbance in light conditions, etc.) This distribution is sometimes also referred to as the population distribution. If this all sounds a little too abstract at the moment, don’t worry. We will discuss the DGP in more detail later. 1.4 Exploring and describing data Once you have collected data, you may feel the urge to immediately apply some statistical model or test to it. But really, a first step should be to explore the data. Statistical tests are designed to confirm or reject hypotheses, and part of confirmatory data analysis. They are a very important part of scientific research, but they always rely on assumptions about the data generating process. Before conducting these analyses, it is crucial to assess whether these assumptions are reasonable, and this is where exploratory data analysis can help. Tukey (1977), a major proponent of exploratory data analysis, points out that exploratory data analysis can also suggest new hypotheses about the causes of observed phenomena, and with that provide a basis for further experimentation and data collection. As the distribution of Paul’s predictions is not so interesting to look into exploration of data, we will focus on other data. We will stay in the context of the 2010 FIFA World Cup, and consider team statistics such as the number of goals scored and the number of games played. Table 1.3: Team statistics in the 2010 FIFA World cup Team Matches played Goals for Goals against Yellow cards Direct red cards 31 Algeria 3 0 2 8 0 4 Argentina 5 10 6 7 0 19 Australia 3 3 6 7 2 5 Brazil 5 9 4 9 1 26 Cameroon 3 2 5 5 0 21 Chile 4 3 5 15 0 12 Côte d’Ivoire 3 4 3 5 0 20 Denmark 3 3 6 6 0 22 England 4 3 5 6 0 29 France 3 1 4 6 1 1 Germany 7 16 5 13 0 11 Ghana 5 5 4 11 0 27 Greece 3 2 5 5 0 32 Honduras 3 0 3 7 0 13 Italy 3 4 5 5 0 14 Japan 4 4 2 7 0 30 Korea DPR 3 1 12 2 0 8 Korea Republic 4 6 8 6 0 15 Mexico 4 4 5 9 0 2 Netherlands 7 12 6 24 0 24 New Zealand 3 2 2 6 0 17 Nigeria 3 3 5 5 1 23 Paraguay 5 3 2 9 0 7 Portugal 4 7 1 8 1 25 Serbia 3 2 3 10 0 10 Slovakia 4 5 7 11 0 16 Slovenia 3 3 3 9 0 18 South Africa 3 3 5 4 1 6 Spain 7 8 2 8 0 28 Switzerland 3 1 1 8 1 3 Uruguay 7 11 8 11 1 9 USA 4 5 5 9 0 1.4.1 Summary statistics Summary statistics are calculated values that aim to summarize key aspects of the distribution of observed values of a variable. Two main aspects of such distributions are (a) where the values are generally located within the range of possible values, and (b) the variation in the values. Measures of the location of the values aim to reflect the central value, in some form or another, of a distribution. Measures of spread aim to reflect the level of variability around the central value. 1.4.1.1 Measures of location There are three common measures of location: the mean, median, and mode. Let’s consider each in turn. The mean is the average of a set of values, computed by adding all the values up and then dividing this sum by the total number of values. More formally, the mean of a variable \\(Y\\) is usually denoted by drawing a straight line over the variable (i.e. as \\(\\overline{Y}\\)), and the formula for its computation is \\[\\begin{equation} \\overline{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} \\tag{1.1} \\end{equation}\\] Perhaps this is the first time you have seen the “summation operator” \\(\\Sigma\\), so let’s go over how to work with this. Basically, the summation operator states that you should sum up every value of a variable that is placed to the right of it. Below the summation operator (or on the lower-right side) is an iterator \\(i\\), which gives more information about which values to sum. The iterator goes from the value it is given below or on the bottom-right side (here, the iterator is assigned the value 1), to the value on the top (or top-right) of the summation operator (in this case \\(n\\)). So here, the operator states that we should sum up all values \\(Y_i\\), where \\(i\\) goes from 1 to \\(n\\), i.e. all values \\(Y_1, Y_2, \\ldots, Y_n\\): \\[\\sum_{i=1}^n Y_i = Y_1 + Y_2 + Y_3 + \\ldots + Y_n\\] For the FIFA 2010 World Cup, we have \\(n=32\\) teams, so to compute the mean number of goals scored, we would sum from \\(Y_1\\) (the number of goals scored by the first team) to \\(Y_{32}\\) (the number of goals scored by the final team), and get \\[\\overline{Y} = \\frac{0 + 10 + 3 + 9 + 2 + \\ldots + 11 + 5}{32} = \\frac{145}{32} = 4.531\\] The median is defined as the value in the set of values such that 50% of the values are lower than it. It is easy to compute if the number of observations (\\(n\\)) is odd. We simply order the values in increasing or decreasing magnitude. The median is then simply the middle value in this ordered set. If \\(n\\) is an even number, the median cannot be uniquely determined. But it is common to take the value that is halfway between. The goals scored by each team, ordered in increasing magnitude are: 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, 16 There are \\(n=32\\) values, so the first 16 values in the ordered sequence above are the lowest 50% of the values, and the second 16 values are the highest 50% values. If we had one value less, i.e. if \\(n=31\\), then the 16th value would be the median. In our case, the median is a value between the 16th and 17th value. If you go along the sequence and stop at the 16th value, you will see that it equals 3. The next value is the 17th value, which also equals 3. So in this case, the median must be 3, as this is the only value between 3 and 3. If the 17th value happened to be 4, then we would have determined the median as the value halfway between 3 and 4, i.e. as 3.5. The mode is the most frequently occurring value. This measure is really only useful for relatively large data sets with a limited number of possible values. For a continuous variable, there is generally no value that occurs more than once in a data set, and hence all values could be considered a mode. If we look at the total goals scored, we see that the mode is 3, as this is the most frequent value (there are 8 teams who scored 3 points, while there are 4 teams who scored 2 points, and 4 teams who scored 4 points). 1.4.1.2 Measures of spread There are three common measures of spread: the inter-quartile range, the variance, and the standard deviation. The inter-quartile range is the distance between the 25th and the 75th percentile. A percentile is a value such that a specified proportion of the values are lower than it. So the 25th percentile is the lowest value in the data such that 25% are even lower. If you go back to the definition of the median, you may notice that the median is actually the 50th percentile. The 25th, 50th, 75th and 100th percentiles are also respectively called the 1st, 2nd, 3rd, and 4th quartile, hence the name inter-quartile range. To compute percentiles, it is handy to order the values in increasing magnitude (just like we did for computing the median). A sample percentile can then be computed by determining the appropriate rank (the place of a value in the ordering) that corresponds to the required percentile: \\[\\text{rank}(\\text{percentile}) = \\text{ceiling}\\left(\\frac{\\text{percentile}}{100} \\times n\\right)\\] Here, \\(\\text{percentile}\\) is the required percentile (e.g. 25, or 75). The ceiling function stands for rounding up to the nearest integer, so e.g. \\(\\text{ceiling}(2.1) = 3\\). To compute the inter-quartile range for total goals scored, we compute the needed ordinal ranks as \\[\\text{rank}(25) = \\text{ceiling}\\left(\\frac{25}{100} \\times 32 \\right) = 8\\] which means we can take the 8th value in the ordered list as the 25th percentile, which equals 2, and \\[\\text{rank}(75) = \\text{ceiling}\\left(\\frac{75}{100} \\times 32 \\right) = 24\\] which means we can take the 24th value in the ordered list, which equals 5. The inter-quartile range is then \\[\\text{IQR} = [2, 5]\\] The variance is the average squared deviation of values from the mean. One reason for using squared deviations rather than the average deviation itself is that the average deviation \\[\\frac{\\sum_{i=1}^n (Y_i - \\overline{Y})}{n}\\] is always 0. An alternative is of course to then calculate the average of the absolute deviations. While this is a sensible measure of the spread of values, there are technical reasons to prefer the average squared deviation (i.e. the variance), which we won’t go into now. Let’s just plough on with how to calculate the variance. The variance of a collection of observed values, which is also referred to as the sample variance, is usually denoted as \\(S^2_Y\\), and defined as: \\[\\begin{equation} S^2_{Y} = \\frac{\\sum_{i=1}^n (Y_i - \\overline{Y})^2}{n} \\tag{1.2} \\end{equation}\\] So you simply subtract from each value the average value (the sample mean), and then raise each deviation to the power of 2. Then you add those squared deviations up, and divide the sum by the total number of observations. For the total goals scored, the variance is calculated as: \\[\\begin{align} S^2_Y &amp;= \\frac{(0 - 4.531)^2 + (10 - 4.531)^2 + (3 - 4.531)^2 + \\ldots + (11 - 4.531)^2 + (5 - 4.531)^2}{32} \\\\ &amp;= \\frac{(-4.531)^2 + (5.469)^2 + (-1.531)^2 + \\ldots + (6.469)^2 + (0.469)^2}{32} \\\\ &amp;= \\frac{20.532 + 29.907 + 2.345 + \\ldots + 41.845 + 0.22}{32} \\\\ &amp;= \\frac{423.969}{32} \\\\ &amp;= 13.249 \\end{align}\\] A downside of the variance is that its value is difficult to interpret. By squaring each deviation, the value of the variance is on a different scale of magnitude than the original values. The standard deviation aims to provide a more interpretable measure of spread, by transforming the variance back to the scale of the original values. The standard deviation of a collection of observed values, usually denoted as \\(S_Y\\), is simply the square-root of the variance, so \\[S_Y = \\sqrt{S^2_Y}\\] or written out in more detail as \\[\\begin{equation} S_{Y} = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\overline{Y})^2}{n}} \\tag{1.3} \\end{equation}\\] For the total goals scored, the standard deviation is \\[S_Y = \\sqrt{13.249} = 3.64\\] 1.4.2 Visual exploration Exploring a data set is generally done visually, using various plots such as histograms, boxplots, scatterplots, etc. A good plot can immediately provide a lot more information than summary measures. So before proceeding with statistical analysis: plot your data! Visualizing data in the most effective and pleasant way is an art form in itself. But here are some standard plots that are helpful and relatively easy to produce: Figure 1.2: A histogram and boxplot of the total goals scored by each team A histogram (left plot in Figure 1.2) counts the number of occurrences of values, or ranges of values in the case of continuous variables with a lot of possible values. For the latter, it divides the range of possible values into “bins” (equally sized ranges of values). The count of observations in each bin is then plotted as the height of a bar, and the width of the bar reflects the range (in the case of discrete values, the range is defined by the midpoints between these values. A histogram is useful to depict the distribution of values of a variable. The size of the bins is important though, as it determines how smooth or rough the histogram looks, and often requires some tweaking. If you make the bins too wide, it may be hard to spot the intricacies in a distribution. If you make the bins too narrow, on the other hand, each bin will contain only a small number of values and you won’t summarize the distribution in a visually meaningful way. In the extreme case, each bin will either contain just one observation, or none. A boxplot (right plot in Figure 1.2), also called a box and whiskers plot, depicts the inter-quartile range as a box, in which a line is placed to depict the median. Two lines extend from either side of the box. These are the “whiskers” and they depict the range of values in the data excluding any potential outliers. More precisely, the endpoints of the whiskers are the smallest and largest value in the data which are no more than \\(1.5 \\times \\text{IQR}\\) away from the 25% and 75% quartiles, respectively. If there are values in the data which extend beyond these endpoints, they are considered “outliers”, and plotted as points outside of the whiskers. In the example boxplot, you can see that there are three such outliers. A scatterplot is useful to see the relation between the values of two variables. For instance, it is likely that the total goals scored by each team will depend on the number of games they played. We can visually inspect this by plotting each team as a point with the value of the x-axis representing the value of one variable, and the value on the y-axis as the value of the other variable. This is done in Figure 1.3, from which it seems quite clear that those teams who played more games generally also scored more points. Figure 1.3: A scatter plot of total goals scored by total matches played As all the plots above can tell us something meaningful about the data, and they often highlight different aspects of the data, it can be useful to combine them. One form of such a combination plot has been called a raincloud plot (Allen, Poggiali, Whitaker, Marshall, &amp; Kievit, 2019). A raincloud plot combines a boxplot with a scatterplot of the observed values and a type of continuous histogram called a nonparametric density plot. In my version of such plots (1.4), a boxplot is placed in the middle, with an additional point representing the sample mean. On the left are the observed values, where the points are “jittered” (randomly displaced on the x-axis) to try to avoid overlapping points. On the right of the boxplot is the nonparametric density plot. This part aims to depict the distribution in a smooth, continuous way. Figure 1.4: A raincloud plot of the average goals scored per match 1.5 Analysis and modelling Analysing data can mean lots of things. According to WikiPedia “Data analysis is a process of inspecting, cleansing, transforming and modelling data with the goal of discovering useful information, informing conclusions and supporting decision-making.” That is a reasonably accurate, but also a quite lengthy and boring definition. An alternative to this I quite like is that data analysis is identifying a signal within noise. Data is noisy for many reasons. For one thing, the data that we have access to is generally limited, in the sense that we could have obtained other and/or more data. Other sources of noise include measurement error. Such sources of noise imply that the data set we have only gives a limited, incomplete, and corrupted view on the process that generated it (the Data Generating Process). We are not interested solely in describing the data set we have, we generally want to infer something about the Data Generating Process. We don’t just want to state that Paul made 8 out of 8 accurate predictions in the 2010 FIFA World Cup, we want to use this data to assess whether Paul had psychic abilities. Whether, had Paul’s keeper allowed Paul to make predictions for other matches, he would have also been correct. Such inferences go beyond the data that we have. Perhaps Paul was just lucky in those 8 predictions, and his 100% hit rate is not a good reflection of his hit rate in general. Statistical modelling can be viewed as describing the Data Generating Process in the language of mathematics. At its core, a statistical model defines the distribution of possible values of variables. In an ideal world, this model distribution is identical to the true distribution that follows from the Data Generating Process. In general, though, there will be some mismatch between the model distribution and the DGP distribution. Models are simplifications and idealizations. As statistician George Box famously noted “all models are wrong, but some are useful” (Box &amp; Draper, 1987). What do we mean by this? Consider the famous London tube map (Figure 1.5). This map is “wrong” because as any map, it is a simplification of reality. It does not provide a completely accurate representation of the world. But what is special about the tube map is that, unlike standard maps, it does not even attempt to reflect the geographical location of the different underground (or “tube”) lines and stations. Its initial designer, Harry Beck, realised that because the lines run mostly underground, the exact physical locations of the stations were largely irrelevant to the traveller wanting to know how to get from one station to another. What matters to them is the relation between the stations, and which lines connect them. Hence, he designed the map as a schematic diagram which clearly shows the lines connecting stations. If you used the map to determine the walking distance between stations, you’d be very disappointed. But to determine your underground journey, the map is very useful. Figure 1.5: London tube map. Source: TfL Statistical models are a bit like the London tube map: their usefulness will depend on your goals. In order to derive properties of various statistical analyses and tests, we have to assume that a model is true (in the sense that it truly reflects the DGP distribution). While this assumption is inherently uncertain, the derived properties may hold, at least approximately, as long is the model distribution is an “accurate enough” representation of the DGP distribution. While in mathematics, statements are either true or false, in applied statistics, we have to deal with statements that are only approximately true. 1.6 Summary The objective of statistical data analysis is to go beyond a given data set, and make claims about aspects of the process that generated the data (the Data Generating Process). To make inferences beyond the limited data we have collected, we need statistical models which aim to describe the distribution of all possible data that we could have collected. Data can be of different types. The measurement scale of a variable determines the way in which numbers assigned to values of the variable can be interpreted and manipulated. Aspects of the data collection, such as whether data was collected in an experiment with random assignment, or whether the data was observed in a more naturalistic setting, are important aspects of the Data Generating Process, and determine to what extent we can rule out confounds that may bias our conclusions. Paul was one amazing octopus. References "],["statistical-modelling.html", "Chapter 2 Statistical modelling 2.1 Coin flipping: Defining a statistical model 2.2 Probability 2.3 Flipping a biased coin: An alternative model 2.4 Estimation 2.5 Comparing models: Null-hypothesis significance testing 2.6 Hypothesis testing directly with the Binomial distribution 2.7 Summary 2.8 Epilogue", " Chapter 2 Statistical modelling Was Paul truly psychic, an oracle of some kind? Or was Paul – or his keeper, as what does fame really mean to an octopus? – just really lucky? How do we know? And what really are the chances of predicting the winner of 8 matches correct? These are questions we will focus on now. In this chapter, we will start defining statistical models and using them to make inferences about the Data Generating Process. We will cover some basic probability theory, and use this to define two alternative models of Paul’s predictions. One in which he is randomly guessing, and one in which his predictions can be more (or less) accurate than if he were randomly guessing, while we are unsure about how accurate Paul really is in that case. We will discuss how to estimate the unknown accuracy of Paul’s predictions in this model. After this, we will try to answer the question: was Paul (somewhat) psychic? Our answer will be based on whether we can refute the hypothesis that he was randomly guessing. This is generally done with a procedure called a null-hypothesis significance test. 2.1 Coin flipping: Defining a statistical model What does it mean for Paul to be psychic, to be an oracle? Sometimes it is easier to answer this question by considering the opposite: When would Paul not be an oracle? If Paul had no way to tell the future, he would be “merely guessing”. Purely random guessing between two options can be implemented by flipping a coin. Suppose Paul made his choices by “flipping a mental coin”, and choosing the box on the left when the outcome was “heads”, and the box on the right when the outcome was “tails”. Great, we now have a model of the Data Generating Process! Coin flipping can be seen as a physical model. It is something that we can do in the real world, as many times as we want. Because we understand the physical properties of coin flips reasonably well – it is really difficult to predict the outcome of a coin flip – it works well as a model of random guessing. But flipping coins over and over is rather tedious. If we have a mathematical description, a statistical model, of flipping coins, we can spare ourselves this trouble. To do so, we first need to know a bit more about probability. 2.2 Probability Let’s simulate Paul’s decisions for the 2010 FIFA World cup. Grab a coin and flip it 8 times. What are the outcomes? When I did this, the outcomes were: tails, tails, tails, heads, tails, heads, heads, tails Remember, Paul would go left upon heads, and right upon tails. Following these rules, Paul’s decisions are then: right, right, right, left, right, left, left, right To know whether these guesses are correct or incorrect, we need to know which team was placed in which box. This information is provided in Table 1.2: the first country under Match is the box on the left, and the second the box on the right. Using this, our simulation then provides the following: incorrect, correct, correct, correct, correct, correct, incorrect, correct So in our simulation, Paul made 6 correct predictions out of 8. Not quite as good as the real Paul, but still a reasonable performance. Notice how we went through quite a few steps. We defined a model of non-psychic-and-guessing Paul as making decisions by flipping a coin, we then simulated Paul’s decisions by flipping a coin, then transformed these coin flips into a variable containing the decisions to open the left or right box, and finally transformed this variable into another variable containing the accuracy of the predictions. The first transformation is easy. We could have simply put stickers on the coin relabelling “heads” as “left” and “tails” as “right”. The second transformation is not as obvious as it depends on a second variable (whether the winning team was represented by the left or right box). In this case, we could have saved ourselves a bit of work by immediately relabelling our coin flips as “correct” or “incorrect”. To see why, we need to know a bit more about how to calculate probabilities. Let’s start at the beginning. For one coin flip, there are two possible outcomes: the coin lands on heads or on tails. The outcome can’t be neither (we are assuming the coin is thin enough to not land on its side), nor two heads, or something else. The coin must land on either heads or tails, so there are two outcomes. We call the set of all possible outcomes the outcome space, and we’ll use a nice curly \\(\\mathcal{S}\\) to denote it: \\[\\mathcal{S} = \\{\\text{heads},\\text{tails}\\}\\] Our coin flip is a random variable that takes one of the values in the outcome space. It is traditional to denote random variables with capital Roman letters, and the values with small Roman letters. When we want to make generic statements that apply to different random variables (such as a coin flip, the roll of a die, the outcome of a prediction), we tend to use this notation, and \\(P(Y=y)\\) then means ‘The probability that random variable \\(Y\\) has value \\(y\\)’. Other times, it may be easier to use more descriptive names. In this book, when we use a name for a variable, we will use a computer font, such as coin_flip, for the random variable. So \\(P(\\texttt{coin_flip} = \\text{heads})\\) means ‘The probability that random variable coin_flip has the value “heads”’. 2.2.1 What is probability? I have just introduced the word “probability” without telling you what it means. You probably have some intuition yourself, for instance that probability is the chance of something happening. You might be surprised to know that even though statisticians can all perform probability calculations comfortably, there is quite some disagreement on what probability means. According to the traditional, Frequentist view, probability means long-run relative frequency. For instance, the probability of a coin flip landing on heads is defined as the proportion of times a coin lands heads when I flip a coin for a very, very large number of times. Table 2.1 shows the outcome and relative frequency calculations for 15 coin flips. Table 2.1: Relative frequency of heads for 1 to 15 flips. The cumulative frequency is the number of heads thus far, and the relative frequency the cumulative frequency divided by the number of flips. flip outcome cumulative frequency heads relative frequency heads 1 tails 0 0.000 2 tails 0 0.000 3 heads 1 0.333 4 heads 2 0.500 5 heads 3 0.600 6 tails 3 0.500 7 tails 3 0.429 8 tails 3 0.375 9 tails 3 0.333 10 tails 3 0.300 11 tails 3 0.273 12 tails 3 0.250 13 heads 4 0.308 14 heads 5 0.357 15 heads 6 0.400 Figure 2.1 shows the relative frequency (i.e. the proportion) of heads after each coin flip, when flipping a coin for 2000 times. As you can see, the relative frequency fluctuates quite a bit, but becomes more stable with more flips. But even after 2000 flips, it is not equal to 0.5. Figure 2.1: Relative frequency of heads when flipping a coin for 2000 times Flipping a coin can be viewed as an experiment, that we can repeat. Figure 2.2 shows the results of flipping a coin for 5000 times when repeating the experiment 5 times. As you can see, the results of our experiments (the relative frequencies) are different each time. The differences are quite marked for a small number of flips, but become more alike after flipping the coin more times. This illustrates something known as The Law of Large Numbers, to which we will come back soon. For now, let’s focus on a different matter. If probability is a relative frequency, then which one is it? Each repetition of the experiment, as well as each number of throws, would give us a different answer! Figure 2.2: Relative frequency of heads when flipping a coin 5 times for 5000 times To give a single answer, a “long-run relative frequency” must be the relative frequency when flipping the coin for an infinite number of times. That may seem disappointing. A nice aspect of the Frequentist View is that probabilities are something of the real world, but surely, we can’t flip a coin for an infinite number of times in the real world. That is true, but to understand what probability is, we don’t necessarily have to perform such infinite experiments. When we have to estimate the probability from a given (non-infinite) number of coin flips, we do have to worry about these issues. This, indeed, is what statistics is about: dealing with sample variability to estimate and infer unknown things. But for defining what a probability is, we can use mathematics to show that the long run relative frequency converges to a single number as the number of coin flips gets closer and closer to infinity. Which, in other words, means there is a single long-run relative frequency, and hence the probability is mathematically well-defined. There is an alternative to the Frequentist View, called the Bayesian view or Subjective view, according to which probability means a rational degree of belief. This, in a sense, takes probability out of the real world, and into our minds. By doing so, it allows statements of probabilities for single events, such as what is the probability that it will rain tomorrow. In the Frequentist View, it either rains tomorrow or not. We can ask what the probability is that it rains on the 25th of July in general, by considering the long-run relative frequency of rain on every 25th of July in past, present, and future years. But there is no long-run relative frequency for the 25th of July in 2020. 2.2.1.1 The rules of probability Although the interpretation of probability is debated, the mathematical rules of calculating probabilities are generally agreed upon. Suppose we have an outcome space \\(\\mathcal{S} = \\{E_1, E_2, E_3, \\ldots\\}\\). That is, the outcome space consists of (abstract) events \\(E_1\\), \\(E_2\\), \\(E_3\\), etc. To refer to any of these, we can use the notation \\(E_i\\), where \\(i\\) can equal 1, 2, 3, etc. The rules of probability are then as follows: \\(0 \\leq P(E_i) \\leq 1\\). The probability of any event \\(E_i\\) is greater or equal to 0 and smaller than or equal to 1. This rule requires little further explanation. Probabilities are chosen to lie on a scale between 0 and 1. \\(P(E_1 \\text{ or } E_2 \\text{ or } E_3 \\text{ or } \\ldots) = P(\\mathcal{S}) = 1\\). In words, this means that the probability of at least one event in the outcome space occurring is 1. If \\(E_i\\) and \\(E_j\\) are mutually exclusive events (if one of them occurs, the other cannot occur), then \\(P(E_i \\text{ or } E_j) = P(E_i) + P(E_j)\\). This is also called the sum rule. \\(P(\\neg E_i) = 1 - P(E_i)\\). Here, \\(\\neg\\) means “not”, so the probability of “not \\(E_i\\)” is 1 minus the probability of \\(E_i\\) occurring. This is also called the complement rule. For any events \\(E_i\\) and \\(E_j\\), \\(P(E_i \\text{ or } E_j) = P(E_i) + P(E_j) - P(E_i \\text{ and } E_j)\\). This rule holds whether the events are mutually exclusive or not, and can be called the general sum rule. Now, all these rules seem overkill when we are dealing with a simple coin flip. So let’s consider a slightly more complex situation: betting on the outcome of a roll of a six-sided die. In this betting game, like in roulette, you are allowed to bet on many things, such as the exact number, but also whether the number is odd or even, whether the number is greater than 3, whether the number is greater than 1, etc. All these things are events in the outcome space: \\[\\mathcal{S} = \\{1, 2, 3, 4, 5, 6, \\text{even}, \\text{odd}, &gt;3, &lt; 1, \\geq 4, 1 \\text{ or } 3, \\ldots \\}\\] The first six events in this outcome space (the numbers 1 to 6) are called elementary events: they are mutually exclusive (a single roll cannot result in both a 1 and a 3), and one of them must occur. So we know that \\(P(1) + P(2) + P(3) + P(4) + P(5) + P(6) = 1\\). But wait, there are more events in the outcome space, and \\(P(\\mathcal{S}) = 1\\), and these six events are only a small part of the outcome space, so does that imply that they all have probability 0? No! That is because not all events in the outcome space are mutually exclusive. For instance, if the outcome is 5, then the outcome is also odd, and it is also \\(&gt;3\\), \\(\\geq 4\\), etc. As you might have noticed, the other events are actually themselves sets of the elementary events. For instance, \\(\\text{odd} = \\{1, 3, 5\\}\\), \\(\\text{even} = \\{2, 4, 6\\}\\), and \\(&gt; 3 = \\{4, 5, 6 \\}\\). Figure 2.3 depicts the relations between these events, as well as the elementary events. Figure 2.3: A diagram depicting the relations between 9 events in the outcome space of the die betting game We just really need two more rules and then we’re done. These rules are to compute the probability of a conjunction of events (the probability of both events occurring). To be able to state these rules, we first need to consider the concept of conditional probability. In words, such a probability refers to the probability of one event given that another event occurred. For instance, we can consider the probability of the event “greater than 3” given that the event “odd” occurred (i.e., the probability that the outcome was greater than 3 given that the outcome was an odd number). If we know that the outcome was odd (i.e. 1, 3, or 5), we can rule out all occurrences where the outcome was even (i.e. 2, 4, or 6). To now consider the probability that the outcome was greater than 3, we only need the probability of an outcome greater than 3 within the set of odd numbers. If we are sure that the outcome was odd, we can set \\(P(\\text{odd}) = 1\\). There are three outcomes, each with equal probability, so \\(P(1|\\text{odd}) = P(3|\\text{odd}) = P(5|\\text{odd}) = \\frac{1}{3}\\). In the set of odd outcomes, there is only one number greater than 3, namely 5. That means that the conditional probability \\(P(&gt;3|\\text{odd}) = P(5|\\text{odd}) = \\frac{1}{3}\\). Conversely, we can also work out the conditional probability \\(P(\\leq 3|\\text{odd}) = P(1|\\text{odd}) + P(3|\\text{odd}) = \\frac{2}{3}\\). In a sense, a conditional probability is just looking at the relative occurrence of one event \\(E_i\\) (e.g., the event “\\(&gt;3\\)”) within the set of outcomes defined by another event (e.g., the event “odd”). \\[P(E_i|E_j) = \\frac{P(E_i \\text{ and } E_j) }{P(E_j)}\\] \\(P(E_i \\text{ and } E_j) = P(E_i) \\times P(E_j | E_i)\\). Multiplication rule For two independent events \\(E_i\\) and \\(E_j\\), \\(P(E_i \\text{ and } E_j) = P(E_i) \\times P(E_j)\\) Kolmogorov, a rather brilliant mathematician, showed that you only need rules 1, 2 and 3; all the other rules follow from the first three. In mathematical terms, rules 1, 2, and 3 are axioms, statements which we assume are true without being able to prove them, while the remaining rules are theorems, statements which follow from the axioms and can be proven to be true. 2.2.1.2 A model of flipping an unbiased coin Now that we know more about probability, let’s get back to our model for Paul. With a balanced coin (and a not too cunning flipper), the probability of the coin landing heads should equal the probability of the coin landing tails. In mathematical notation, we can state this as \\[P(\\text{heads}) = P(\\text{tails})\\] Furthermore, as heads and tails are the only two possible outcomes, we know that \\[P(\\text{heads} \\text{ or } \\text{tails}) = 1\\] Finally, because heads and tails are two mutually exclusive and equally likely events, we then infer that \\[P(\\text{heads}) = P(\\text{tails}) = 0.5,\\] and given the way Paul bases his decisions on this coin flip, also that \\[P(\\text{paul left}) = P(\\text{paul right}) = 0.5.\\] Interestingly, if Paul made his decision for the left or right box by flipping a coin, it doesn’t really matter whether the team representing the left or right box was chosen randomly, or in some other way. Suppose that Paul’s keeper was very knowledgeable about football, or that he was the real psychic. Furthermore, let’s suppose that he thinks Paul might generally prefer the right box (as the light is particularly nice on that side of the tank, for instance). Then \\(P(\\text{winner right}) = 1\\) and \\(P(\\text{winner left}) = 0\\). Because Paul’s decisions are independent from which team was placed in which box (he’s merely flipping a coin), we know that \\(P(\\text{paul left} | \\text{winner left}) = P(\\text{paul left})\\), and similarly \\(P(\\text{paul right} | \\text{winner left}) = P(\\text{paul right})\\). There are two ways in which Paul can make a correct prediction: he opens the right box, and the winning team was in the right box, or he opens the left box and the winning team was in the left box. So \\[P(\\text{correct}) = P(\\text{paul left} \\text{ and } \\text{winner left}) + P(\\text{paul right} \\text{ and } \\text{winner right})\\] Using the multiplication rule, we can write this as \\[P(\\text{correct}) = P(\\text{paul left} | \\text{winner left}) \\times P(\\text{winner left}) + \\\\ P(\\text{paul right} | \\text{winner right}) \\times P(\\text{winner right})\\] And from the independence between Paul’s decisions and the assignment of teams to boxes, we can simplify this further as \\[P(\\text{correct}) = P(\\text{paul left}) \\times P(\\text{winner left}) + P(\\text{paul right}) \\times P(\\text{winner right})\\] which, filling in the appropriate values, becomes \\[P(\\text{correct}) = .5 \\times 0 + .5 \\times 1 = .5\\] Actually, you can fill in any valid probability for \\(P(\\text{winner left})\\) and \\(P(\\text{winner right})\\), and \\(P(\\text{correct})\\) will always be .5! Let’s say, in abstract terms, that \\(P(\\text{winning left}) = w\\). From the complement rule, we know that \\(P(\\text{winning right}) = 1 - w\\). Then \\[\\begin{align} P(\\text{correct}) &amp;= P(\\text{paul left}) \\times w + P(\\text{paul right}) \\times (1-w) \\\\ &amp;= .5 \\times w + .5 \\times (1-w) \\\\ &amp;= .5 \\times w - .5 \\times w + .5\\\\ &amp;= .5 \\end{align}\\] So according to our model, no matter what bias Paul’s keeper might have in placing winning teams in the left or right box, Paul’s accuracy for each prediction would be \\(P(\\text{correct}) = .5\\). 2.2.2 Distributions A probability distribution, as the name suggests, defines how probability is distributed over all possible values of a variable. You can think of “all probability” as a pie, and the probability distribution as slicing up the pie in (possibly unequal) pieces. Each possible value of a variable is given its own slice, and the size of the slice relative to the overall pie is the probability. Looking at the correctness of a single prediction, the pie is divided into two equal slices: half for \\(P(\\text{correct})\\), and the other half for \\(P(\\text{incorrect})\\). But Paul made more than one prediction (8 in the case of the 2010 FIFA World cup), and what we’re really interested in is Paul’s accuracy in all his predictions: What is the probability that Paul made 8 out of 8 correct predictions if he was merely guessing? Using the rules of probability, we can work out this probability as well. According to our coin-flipping model, it does not matter which of Paul’s prediction we look at (e.g. whether the first, the second, the last, or any other one). In fact, we could generate the data in the reverse order, or start with the Germany-Australia match and then the Spain-Germany match. Let’s write Paul’s predictions as a variable \\(\\texttt{pred}_i\\), where the index \\(i\\) can be a value between 1 and 8, corresponding to the first, second, etc., prediction he made. The probability that any prediction is correct is \\(P(\\texttt{pred}_i = \\text{correct}) = .5\\), and whether that prediction is correct is independent from whether any earlier (or later) prediction is correct. In fancy terms, this means that the observations \\(Y_i\\) are independent and identically distributed (IID). When considering the probability of a sequence of events (e.g. a consecutive run of 8 correct predictions), it can be useful to draw all possible outcomes in the form of a tree. In Figure 2.4, we show such a tree for the first three of Paul’s predictions according to our model in which he was randomly guessing. Starting at the “root node” on the left, the first guess can be either correct or incorrect, and these two outcomes are represented as separate branches. If the first guess was correct, we take the “correct” branch. The next guess can be either correct or incorrect, which splits this branch into two further branches, and so forth. Figure 2.4: Outcome tree for three random guesses. For three coin flips, there are 8 unique sequences of outcomes (the tree ends in 8 branches). Because the probability of “heads” and “tails” are identical, and each coin flip is independent of the others, each unique sequence such as “correct, correct, incorrect” has exactly the same probability, namely \\(0.5 \\times 0.5 \\times 0.5 = (0.5)^3 = 0.125\\). However, if we are not interested in the probability of a unique sequence of outcomes, but rather the total number of correct guesses, the corresponding probabilities are not equal. For example, there is only one sequence in which the number of correct guesses is 3, but there are three sequences in which the number of correct guesses equals 2. Each of these sequences has the sample probability, so we can just add these up \\[P(\\text{2 correct and 1 incorrect}) = 3 \\times 0.125 = 0.375\\] Similarly, the probability \\(P(\\text{1 correct and 2 incorrect}) = 0.375\\), as there are three sequences in which the number of correct guesses equals 1. Note that we have used a distribution over the outcomes of a single variable (a single prediction) to effectively construct three variables (\\(\\texttt{pred}_1\\), \\(\\texttt{pred}_2\\), and \\(\\texttt{pred}_3\\)) reflecting Paul’s first, second, and third prediction, respectively. We can take the outcomes of each of these three variables and turn this into a new variable reflecting the outcome of all three predictions. This new variable has 8 possible outcomes (unique sequences), each with an equal probability of occurring. Finally, we can then use this sequence variable to construct yet a new variable, the one we are really interested in, reflecting the number of correct predictions. This variable (in the case of three predictions), has four unique outcomes (0, 1, 2, or 3 correct) with unequal probabilities. Perhaps all these variables make your head spin. But a main thing to realize is that from a quite simple model of the Data Generating Process (the outcome of each prediction is an independent coin flip), we can start to say something about how likely Paul’s performance of 8 out of 8 correct predictions really was. To work out the probability distribution of the number of correct predictions out of 8, we could expand the tree of 2.4, but the tree would become very large. There is a simpler way to count the number of sequences with a particular number of correct predictions in them. The number of sequences of \\(n\\) predictions where \\(k\\) predictions are correct can be computed as \\[\\frac{n!}{k!(n-k)!} ,\\] where the exclamation mark refers to the so-called factorial function, where a positive integer is multiplied by all positive integers which are smaller than it. So \\[n! = n \\times (n-1) \\times (n-2) \\times (n-3) \\times \\ldots \\times 2 \\times 1\\] Furthermore, there is the convention that \\[0! = 1\\] So we can write \\[P(k \\text { correct and } n-k \\text{ incorrect}) = \\frac{n!}{k! \\times (n-k)!} (0.5)^n\\] For example, we can work out the probability of \\(k=8\\) correct and \\(n-k=0\\) incorrect as \\[\\begin{align} P(8 \\text { correct and } 0 \\text{ incorrect}) &amp;= \\frac{8 \\times 7 \\times ... \\times 2 \\times 1}{(8 \\times 7 \\times ... \\times 2 \\times 1) \\times 1} (0.5)^8 \\\\ &amp;= \\frac{1}{1} (0.5)^8 = 0.004 \\end{align} \\] As another example, the probability of \\(k=3\\) correct and \\(n-k=5\\) incorrect is \\[\\begin{align} P(8 \\text { correct and } 0 \\text{ incorrect}) &amp;= \\frac{8 \\times 7 \\times ... \\times 2 \\times 1}{(3 \\times 2 \\times 1) \\times (5 \\times 4 \\times 3 \\times 2 \\times 1)} (0.5)^8 \\\\ &amp;= \\frac{40320}{6 \\times 120} \\times (0.5)^8 = 56 (0.5)^8 = 0.219 \\end{align} \\] We can follow the same procedure to compute the probability of all other outcomes. The resulting distribution is depicted in Figure 2.5. Figure 2.5: probability distribution over the number of correct predictions out of 8, assuming random guessing. 2.3 Flipping a biased coin: An alternative model Suppose that Paul did not make his decisions by flipping a mental coin. Perhaps Paul had extensive knowledge of all things football, or perhaps he could see into the future. In that case, we might expect Paul to have a higher rate of correct answers than predicted by our coin flipping model. But perhaps Paul, although an oracle, wasn’t so benevolent to his caretakers, or didn’t want to spoil the match for them. In that case, we might expect the rate of correct answers to be lower than predicted by the coin flipping model. In both cases, we’d expect the probability of a correct answer to be different from .5: \\(P(\\text{correct}) \\neq .5\\). But whilst not .5, we don’t otherwise know the value. We call such an unknown value which determines a probability distribution a parameter. We will generally use symbols from the Greek alphabet to denote such parameters. Here, we will use \\(\\theta\\) (pronounced as “thee-ta”). Our alternative to the coin flipping model can now be stated as: \\[P(\\texttt{pred}_i = \\text{correct}) = \\theta\\] Like a variable, a parameter such as \\(\\theta\\) can take different values. It belongs to a model, and its value can not be observed, only inferred. As our parameter reflects a probability, we know that it can never be lower than 0, and never larger than 1. So we know that: \\[0 \\leq \\theta \\leq 1 .\\] Note that the possible values in this range includes \\(\\theta = 0.5\\). Our (unbiased) coin flipping model is actually one of the possible versions (a “special case”) of our new, more general model. Some other ways of saying this are that the coin flipping model is contained within, or nested under, the more general new model. As we will see, nested models play an important role in statistical modelling. In particular, we will often compare how well a special-case or nested model describes the data compared to a more general model. In such model comparisons, we will usually refer to the special-case model as MODEL S, and the more general model as MODEL G. It is important to realise that these names (like variables) are containers and we can put any models in these, as long as the one we put into MODEL S is a special case of the one we put into MODEL G. To work out the probability of the number of correct predictions according to this model, we can draw a similar tree as in Figure 2.4, but instead of the probability of each branch being 0.5, we’d use \\(\\theta\\) for any branch that leads to correct, and \\(1-\\theta\\) for any branch leading to incorrect. That means that the probability of a particular sequence, such as correct, incorrect, correct, is not \\((0.5)^{3}\\), but rather \\(\\theta \\times (1-\\theta) \\times \\theta = \\theta^2 \\times (1-\\theta)\\). Apart from this, the logic of constructing the probability distribution of \\(k\\) out of \\(n\\) correct predictions is the same, and can be computed as \\[\\begin{equation} P(k \\text { correct and } n-k \\text{ incorrect}) = \\frac{n!}{k! \\times (n-k)!} (\\theta)^k \\times (1-\\theta)^{(n-k)} \\tag{2.1} \\end{equation}\\] This distribution is quite common in statistics, and is called the Binomial distribution. Note that if we fill in \\(\\theta = 0.5\\), then \\((1-\\theta) = 0.5\\) and the resulting distribution is exactly the one we worked out for the unbiased coin-flipping model earlier. So our new, more general model can indeed exactly produce the distribution of the simpler model, and this is what we mean when we say that the simpler model is nested under the more general model. To work out the probability that Paul made 8 out of 8 correct predictions according this new model, we need to know the value of the parameter \\(\\theta\\). But what should this be? There are infinite possible values that might be reasonable (e.g., \\(\\theta = .9\\), or \\(\\theta = .947873\\)). Luckily, we don’t need to pluck one of these values out of thin air: we can use the data we have to estimate the value of this parameter. 2.4 Estimation Estimation, in some sense, means an “educated guess”. It is a guess, because we cannot be completely sure that we have picked the true value. It is educated because we don’t just pluck the value from thin air: we use information in the data to guide our guess. There are different ways in which we can use the information in the data to estimate parameters, such as the method of moments, minimum mean square estimation, and maximum likelihood estimation. Here, we will focus on the latter. 2.4.1 Maximum likelihood estimation The general idea of maximum likelihood estimation can be summarized as follows: Find that value of a parameter \\(\\theta\\) such that the probability assigned to the data at hand by the model based on \\(\\theta\\) is highest. Let’s think this through. If we pick any particular value for \\(\\theta\\), we can use the Binomial distribution (Equation (2.1)) to work out the probability that Paul made 8 out of 8 predictions. For instance, if we pick \\(\\theta = .4\\), then \\[\\begin{align} P(\\text{8 correct and 0 incorrect}) &amp;= \\frac{8!}{8! \\times 0!} (0.4)^8 \\times (0.6)^0 \\\\ &amp;= 1 \\times (0.4)^8 \\times 1 = 0.000655 \\end{align}\\] If we pick \\(\\theta = .95\\), on the other hand, we’d get \\[P(\\text{8 correct and 0 incorrect}) = 1 \\times (0.95)^8 \\times 1 = 0.663\\] which is obviously higher. If we plot \\(P(\\text{8 correct} | \\theta)\\), the probability of 8 correct given or conditional upon a particular value of \\(\\theta\\), for all possible values of \\(\\theta\\), we get the curve shown in Figure 2.6. Figure 2.6: Probability of Paul’s performance in the 2010 FIFA World Cup for all possible values of \\(\\theta\\) in the biased coin-flipping model Looking at this plot, is it obvious that the probability of Paul’s observed performance is highest in the model with \\(\\theta = 1\\), thus, our maximum likelihood estimate becomes \\(\\hat{\\theta} = 1\\). As can be shown mathematically, the maximum likelihood estimate of \\(\\theta\\) for our model and any data set with \\(k\\) correct out of \\(n\\) is always \\[\\begin{equation} \\hat{\\theta} = \\frac{k}{n} , \\tag{2.2} \\end{equation}\\] i.e. equal to the proportion of correct. Estimating a probability as a proportion seems intuitively reasonable. But how good is it really? 2.4.2 Properties of good estimators An estimator like specified in Equation (2.2) is an algorithm which takes data as input and produces a parameter estimate as output. Evaluating the quality of the resulting estimates is generally done under the assumption that our model represents the Data Generating Process accurately, i.e. our model is a true representation of the DGP. If our model has unknown parameters, that means that there are values for those parameters such that our model produces data with exactly the same distribution as the DGP. We can call those parameter values the true parameter values. Now, even if we have access to such a true model, estimates of its parameters from a given data set are unlikely to be identical to the true parameters. The main reason is that any actual data set will not cover all of the values that are possible from the Data Generating Process, and/or not in their expected proportions. But the fact is that in practice we have to rely on such limited data sets; we never have access to all the data that could be produced by the Data Generating Process. So we will have to work with estimates that are different from the true parameters. One thing we can do is quantify how much our estimates are likely to deviate from the true parameters. This, in a nutshell, is what statistical estimation theory is all about. Let’s focus on the estimator of \\(\\theta\\) in Equation (2.2) a bit more. For any fixed sample size \\(n\\) and a particular true value of \\(\\theta\\), the variability in the estimates will be completely determined by the variability in \\(k\\), the number correct. The variability in \\(k\\) directly follows from the Binomial distribution itself. If you look at the distribution depicted in Figure 2.5, which is the distribution of \\(k\\) for \\(\\theta = .5\\), you can see there are 9 possible values for \\(k\\), and hence also 9 possible values of \\(\\frac{k}{n}\\), the estimator of \\(\\theta\\). Assuming the true parameter equals \\(\\theta = .5\\), each time we generate a new data set of 8 predictions, it is possible that the estimate of \\(\\theta\\) will equal any value \\(\\left\\{\\frac{0}{8}, \\frac{1}{8}, \\frac{2}{8}, \\ldots, \\frac{7}{8}, \\frac{8}{8}\\right\\}\\), because for each new data set, the value of \\(k\\) can be any value between 0 and 8. Exactly like the values of \\(k\\), not each of these estimates will be equally likely. For most of the new data sets, the estimate will equal \\(\\hat{\\theta} = \\frac{4}{8}\\), the true value of \\(\\theta\\). If the true \\(\\theta = 1\\), then there would be no variability in \\(k\\). It is simply impossible to make a wrong prediction, and hence the estimate would always equal the true value. In this case, how variable the estimates are depends on the true value of \\(\\theta\\). There is no variability whenever \\(\\theta = 0\\) or \\(\\theta = 1\\), and the variability increases the closer \\(\\theta\\) gets to .5. Because in this case, the variability in the estimates depends on the true value of \\(\\theta\\), which is unknown and the reason we have to estimate this parameter in the first place, we cannot easily say how far off we might expect our estimate to be from the true parameter. Nevertheless, we can still assess how good our estimator is by considering some important properties good estimators in general: unbiasedness, consistency, and efficiency. An unbiased estimator gives, on average, estimates which are equal to the true parameter. A good estimator should become more precise as we have more data. The larger the number of observations from the Data Generating Process, the closer the resulting estimates should be to the true parameters. A consistent estimator is one which provides less variable estimates if we give it more data. An efficient estimator is an estimator which provides the least possible variable estimates, meaning it is the most consistent possible. Variability in estimates is unavoidable, as data sets are always limited samples from the Data Generating Process. It can be proven mathematically that there is level of variance of the estimates that no unbiased estimator can go below, or no biased estimator with a given level of bias can go below. An estimator for which the variance in the estimates is equal to this lower bound is called efficient. Such an estimator is, in some sense, the Holy Grail of statistical estimation, especially an unbiased one. In general, maximum likelihood estimators are consistent and (asymptotically) efficient. They are not always unbiased though, although the bias (the difference between the mean estimate and the true value) decreases when the number of observations increases. Given these desirable properties, maximum likelihood estimators are often the estimators of choice. 2.5 Comparing models: Null-hypothesis significance testing So, at this point, we have three models of Paul’s accuracy in predictions. The biased coin-flipping model with an unknown parameter \\(\\theta\\). The unbiased coin-flipping, or random guessing, model (which is a special case of the first one, with \\(\\theta = 0.5\\)). And an estimated version of the biased coin-flipping model, with \\(\\hat{\\theta} = 1\\) (which is also a special case). Now what? What we would like to know is whether Paul was randomly guessing, or whether he had some psychic abilities. In other words, whether the unbiased coin-flipping model or the biased coin-flipping model provides a better description of the Data Generating Process. While it is reasonable to define a model as providing a better description of the DGP when the distribution it proposes is a better match to the true DGP distribution, this definition is not usable in practice, as we simply don’t know the true DGP distribution. We have to work with the data at hand, and there are many possible ways in which we can use the data to evaluate whether one statistical model provides a better description of the DGP. One principled way is to compare the probability assigned to the observed data by each model. This is usually done by computing what is called the likelihood ratio. Suppose we have two versions of the general biased-coin flipping model, one where we assume that the parameter \\(\\theta\\) has a particular value \\(\\underline{\\theta}\\), and one where we assume \\(\\theta\\) takes a different specified value \\(\\underline{\\theta}&#39;\\). Note that by placing a line underneath a parameter, we denote that it is a particular value of interest. In this case, we are interested in a single variable, namely the number of correct predictions Paul made, which we will denote by \\(Y\\), our general symbol for the dependent variable. We can then write the likelihood ratio as: \\[\\frac{P(Y = k | n, \\theta = \\underline{\\theta})}{P(Y = k | n, \\theta = \\underline{\\theta}&#39;)}\\] At the top (the numerator), is the probability of \\(k\\) out of \\(n\\) correct when the probability of a single prediction correct equals \\(\\underline{\\theta}\\). The bottom part (the denominator), is the same probability if the probability of a single correct prediction equals another value \\(\\underline{\\theta}&#39;\\). Suppose \\(\\underline{\\theta} = .5\\) (as in the random guessing model), and \\(\\underline{\\theta}&#39; = 1\\), as in the estimated biased coin-tossing model. The likelihood ratio is then \\[\\frac{P(Y = 8 | n = 8, \\theta = 0.5)}{P(Y = 8 | n = 8, \\theta = 1)} = \\frac{0.00391}{1} = 0.00391\\] What does this mean? Well, if both models assigned the same probability to the observed data, the likelihood ratio would equal one. If the probability according to the model of the numerator is higher, the value of the likelihood ratio would be greater than 1. And if the probability according to the model of the denominator is higher, the value of the likelihood ratio would be smaller than 1. So, as is clear, the model with the estimated value of \\(\\theta\\) assigns a higher probability to the observed data. But that is no wonder! We estimated \\(\\theta\\) by maximum likelihood, meaning that no other value of \\(\\theta\\) could assign a higher probability to the data! If it did, our estimate would simply not be the maximum likelihood estimate. So, if the model in the denominator is the one estimated by maximum likelihood, we know the likelihood ratio could never be larger than 1. So that’s a little annoying. What seemed like a reasonable way to compare two models describe a particular data set, now seems pretty useless. We cannot use the general biased coin-flipping model to compute the likelihood ratio, because without choosing a particular value for \\(\\theta\\), the model can not be used to compute the probability of the data. And we can not just use the estimated value of \\(\\theta\\), because then that model would always win the “likelihood competition”. But computing likelihood ratio’s is not, in fact, completely useless. We just need to take into account the fact that the estimated model will always win. What then matters is: by how much? The null-hypothesis significance test provides an answer to “by how much should the estimated model be better to refute an alternative model?”. The logic is roughly as follows: Suppose the restricted model (MODEL R, e.g. the one where \\(\\theta = .5\\)) is the true model. Then we can use this model to simulate lots of data sets. For each data set, we can estimate \\(\\theta\\) to give us an estimated version of the general model (i.e. MODEL G, the one with \\(\\theta\\) unknown). Then we can use both models to compute a value of the likelihood ratio. For each data set, the likelihood ratio may take a different value. What we end up with is thus a distribution of values of the likelihood ratio, produced by MODEL R. We will know that this likelihood ratio is always below 1, but by inspecting this distribution, we will roughly know what the usual values are, in the case where MODEL R is true. We can then try to find a range of values which are relatively unusual, given that MODEL R is true. If the value of the likelihood ratio we found for the real data is within this range, we might reason that this value is “unusual enough” to reject MODEL R, and accept that MODEL G provides a better description. So let’s do this. I used MODEL R (the one with \\(\\theta = .5\\)) to simulate 100,000 data sets in which Paul made 8 predictions. For each of these, I estimated MODEL G, and then computed the likelihood ratio for that dataset, comparing MODEL R to the estimated MODEL G: \\[\\frac{P(Y = k | n = 8, \\theta = 0.5)}{P(Y = k | n = 8, \\theta = \\frac{k}{8})}\\] Note that \\(k\\) is the main thing that varies from simulated dataset to simulated dataset, and that for \\(\\theta\\) in the model of the denominator, I have filled in the maximum likelihood estimate \\(\\frac{k}{8}\\), which also varies over the simulations. I then determined the proportion (out of all 100,000) simulations of each value of the likelihood ratio. You can see the result in Figure 2.7. Figure 2.7: Likelihood ratio values for 100,000 simulated data sets from MODEL R with \\(\\theta = .5\\) A first thing to notice is that there are only 5 unique values of the likelihood ratio in all 100,000 simulations. This is not a strange coincidence. While there are 9 possible values of \\(k\\) for datasets with \\(n=8\\), some of these will provide exactly the same likelihood ratio. For example, the likelihood ratio is identical for \\(k=3\\) and \\(k=5\\): \\[\\frac{P(Y = 3 | n = 8, \\theta = 0.5)}{P(Y = 3 | n = 8, \\theta = \\frac{3}{8})} = \\frac{P(Y = 5 | n = 8, \\theta = 0.5)}{P(Y = 5 | n = 8, \\theta = \\frac{5}{8})} = 0.777\\] Similarly, the value of the likelihood ratio is identical for \\(k=2\\) and \\(k=6\\), \\(k=1\\) and \\(k=7\\), and \\(k=0\\) and \\(k=8\\). With that out of the way, let’s look at how often each of these 5 values occurred. As MODEL R was the true model (we used it as the Data Generating Process for our simulations), you might expect that a likelihood ratio of 1 would be the most frequent value, as that would imply that MODEL R and (estimated) MODEL G provide an equally good description of a data set. But while this value occurs relatively frequently (27.45%), the most common value is actually 2.158 (43.694%). Only the value of 0.00391 is rather unlikely (0.735%). In all our simulations, less than 1% had a value of the likelihood ratio as low as the one we found for the real data. Thus, if MODEL R is true, we would not expect an estimated MODEL G as well very often. But is that reasonable grounds to conclude MODEL G provides a better description than the random-guessing MODEL R? Before we go into this, let me point out that I used the idea of simulating data sets mainly for educational purposes. Although simulation is useful, it is imprecise. Even if we simulate 100,000 data sets, this is still a limited number, and hence any results will differ (even if only slightly) when we simulate 100,000 other data sets. In this case, it is actually quite straightforward to avoid simulation completely. Using the Binomial distribution, it is – with a little thinking – quite easy to work out the distribution of the likelihood ratio comparing MODEL R to estimated versions of MODEL G. Recall that for \\(n=8\\), there are 9 possible values of \\(k\\). If MODEL R is true, we know the probability of each of these values from the Binomial distribution. Moreover, each of these values will lead to a different estimate of \\(\\theta\\) in MODEL G, which in turn, paired with the corresponding value of \\(k\\), each provide a value of the likelihood ratio. Let’s collect all these things in a table: Table 2.2: Possible outcomes and their probabilities according to MODEL R, and estimates of \\(\\theta\\), probabilities of outcomes according to estimated MODEL G, and likelihood ratio values. \\(k\\) \\(P(k \\mid \\theta = .5)\\) \\(\\hat{\\theta}\\) \\(P(k \\mid \\theta = \\hat{\\theta})\\) likelihood ratio 0 0.00391 0.000 1.000 0.00391 1 0.03125 0.125 0.393 0.07958 2 0.10938 0.250 0.311 0.35117 3 0.21875 0.375 0.282 0.77672 4 0.27344 0.500 0.273 1.00000 5 0.21875 0.625 0.282 0.77672 6 0.10938 0.750 0.311 0.35117 7 0.03125 0.875 0.393 0.07958 8 0.00391 1.000 1.000 0.00391 The column \\(P(k|\\theta = .5)\\) contains the probability of each possible value of \\(k\\) according to MODEL R. As this is for the moment the true model, it is both the actual probability of \\(k\\), and the “model assigned” value which goes into the likelihood ratio. In the table, we see that \\(k=0\\) and \\(k=8\\) both give the same value of the likelihood ratio. We know the probability of each value of \\(k\\) is \\(P(k|\\theta = .5)\\) (because MODEL R generated the data), and therefore we can work out that the probability of obtaining this value of the likelihood is simply the sum of the probabilities of \\(k=0\\) and \\(k=8\\): \\[\\begin{align} P(\\text{likelihood ratio} &amp;= P(k=0) + P(k=8) \\\\ &amp;= 0.00391) = 0.00391 + 0.00391 = 0.00781 \\end{align}\\] Similarly, for the other possible values of the likelihood, we can work out the probabilities as: \\[\\begin{align} P(\\text{likelihood ratio} = 0.07958) &amp;= 0.03125 + 0.03125 = 0.0625 \\\\ P(\\text{likelihood ratio} = 0.35117) &amp;= 0.10938 + 0.10938 = 0.21875 \\\\ P(\\text{likelihood ratio} = 0.77672) &amp;= 0.21875 + 0.21875 = 0.4375 \\\\ P(\\text{likelihood ratio} = 1) &amp;= 0.27344 \\end{align}\\] This is similar to the example of throwing dice before. There, we had elementary events (each number that can be thrown), but we also had other events, such as “greater than 3”, which we could compute the probability of by using the probability of the elementary events. Here, the number of correct guesses \\(k\\) are the elementary events, and values of the likelihood ratio can be considered compound events. You can view a plot of the resulting distribution of likelihood ratio values in Figure 2.8. Figure 2.8: The true distibution of likelihood ratio’s comparing MODEL R with estimated MODEL G, when the true model is MODEL R. When you compare this plot to Figure 2.7, you see there are indeed very alike. So we don’t need to simulate data to determine what values of the likelihood ratio we might see – and how often – if MODEL R is true. MODEL R allows us to easily compute the probability of each possible number of correct guesses, \\(k\\). This number, in turn, determines everything else of interest: the estimates of \\(\\theta\\) for MODEL G, the probability of the data according to (the estimated) MODEL G, and the likelihood ratio between MODEL R and (the estimated) MODEL G. When possible, it is better to work out the distribution like we did here, instead of relying on simulation. Simulation is inherently noisy, even with 100,000 samples. That’s all well and good, you might object, but you have done all this by assuming that MODEL R is true. But we don’t know whether MODEL R is true, we don’t know that Paul was randomly guessing. In fact, the question whether he was actually psychic (and not randomly guessing) is what brought us to this point in the first place! That is indeed very true. The issue is that there are many ways in which MODEL R can be false and MODEL G true. In fact, there are an infinite number of values for \\(\\theta\\) which aren’t equal to 0.5. Each of these implies a different distribution of the likelihood ratio where we compare MODEL R (with \\(\\theta = .5\\)) to the estimated MODEL G (with \\(\\hat{\\theta} = \\frac{k}{n}\\)). Some examples are given in Figure 2.9. If we assume that MODEL R is false, then any of these distributions would be the true distribution of the likelihood ratio. But without knowing the exact value of \\(\\theta\\), we simply don’t know which one. Figure 2.9: The true distibution of likelihood ratio’s comparing MODEL R with estimated MODEL G, when the true model is MODEL G with different values of \\(\\theta\\). 2.5.1 Decisions and types of error Let’s relocate ourselves. We started by defining a model in which Paul was randomly guessing (MODEL R). We then defined another model in which Paul could be (somewhat) psychic, but as we don’t know how psychic and whether Paul would want to make correct predictions or incorrect predictions. In this model, Paul’s probability of making a correct prediction could be anything. Because it is so general, MODEL G can effectively predict anything. We can use the data to estimate MODEL G. But doing that, it makes MODEL G a bit like an annoying friend, who always says “I knew that was going to happen” after it happened, but never before. Because MODEL G is so fickle, our procedure focuses almost exclusively on MODEL R, the version of MODEL G (remember, MODEL R is a special case of MODEL G) which is much more strident, and does tell us how likely things are to happen before they happen. In the null hypothesis significance test, the hypothesis that MODEL R is true is called the null hypothesis, or \\(H_0\\), in the conventional notation: \\[H_0: \\text{MODEL R is true}\\] This hypothesis itself may be either true or false. If \\(H_0\\) is true, then MODEL R is true. If \\(H_0\\) is false, then the statement “MODEL R is true” is false, so MODEL R is false. These are the two possibilities that we are faced with, but we don’t know which of these is actually the case.2 Based on data, we can decide to accept \\(H_0\\) as a true statement, or reject \\(H_0\\). But these decisions can be correct, or incorrect. Combining the actual state of the world with our decisions, we have the following four possibilities: accept \\(H_0\\) reject \\(H_0\\) \\(H_0\\) is true correct error (Type 1) \\(H_0\\) is false error (Type 2) correct If \\(H_0\\) is actually true, and we accept \\(H_0\\), we can rejoice, as we made a correct decision. Similarly, if \\(H_0\\) is actually not true, and we reject \\(H_0\\), we can also rejoice, as this is another correct decision. If, on the other hand, \\(H_0\\) is true, but we reject \\(H_0\\), we made a wrong decision. This is called a Type 1 error. A type 2 error refers to making the other wrong decision, namely if we accept \\(H_0\\), but \\(H_0\\) is in fact false. Null hypothesis significance testing is a procedure to make a decision based on data such that we can limit the chances of making one of these errors, namely the Type 1 error. 2.5.2 Significance and power In null hypothesis significance testing, we focus on the probability of making a Type 1 error, which is usually denoted as \\(\\alpha\\): \\[\\begin{equation} \\alpha = P(\\text{reject } H_0| H_0 \\text{ is true}) \\tag{2.3} \\end{equation}\\] It is also referred to as the significance level. If \\(H_0\\) is true, then we can define a decision procedure to either accept or reject \\(H_0\\), such that we know the probability of a Type 1 error, and moreover, we can set this probability to a desired value. Let’s consider Figure 2.8 again. This is the theoretical distribution of the likelihood ratio values when MODEL R is true, i.e., when \\(H_0\\) is true. Large values of the likelihood ratio indicate that MODEL G does not provide a much better fit to the data than MODEL R, so it makes sense that such large values should not persuade us to reject MODEL R. Values close to 0, on the other hand, indicate that MODEL G does provide a substantially better fit to the data than MODEL R. Hence, it makes sense to decide to reject \\(H_0\\) if the likelihood ratio is “close enough” to 0. Clearly, such low values can occur even if \\(H_0\\) is true (as Figure 2.8 attests). But they are unlikely to do so. If \\(H_0\\) is false, such low values become more likely (see Figure 2.9). The idea is now to pick a cutoff value for the likelihood ratio such that if we find a value equal to or smaller than it, we reject the null hypothesis. We choose this cutoff value, called the critical value, by considering the probability of the values, assuming MODEL R is true (as this is the only model that can provide us with a proper distribution). More precisely, we choose the critical value such that the probability of making a Type 1 error does not exceed a chosen value, called the significance level, which is usually set to \\(\\alpha = .05\\). That means that, if \\(H_0\\) is true, we are allowing at most 5% of our tests to result in a Type 1 error. So how do we pick this critical value? Let’s again consider the theoretical distribution of the likelihood ratio we worked out and depicted in Figure 2.8. As it is difficult to see the precise probabilities in this figure, let’s first create another table like the one we did before, but now focussing specifically on the values of the likelihood ratio. Table 2.3 doesn’t really contain any more information than Table 2.2. We have simply added up some of the numbers in that table. A new concept though is that of a cumulative probability, by which we mean the probability of a given value or any lower value. Here, we focus on the cumulative probability for the likelihood ratio, \\(P(\\text{likelihood ratio} \\leq \\text{value})\\). Such cumulative probabilities are interesting here, because we want to find a cutoff value, and then limit the probability of getting any values lower than it to the significance level \\(\\alpha\\). Table 2.3: Possible values of the likelihood ratio, their probability and cumulative probability, assuming MODEL R is true. value \\(P(\\text{likelihood ratio} = \\text{value})\\) \\(P(\\text{likelihood ratio} \\leq \\text{value})\\) 0.00391 0.00781 0.00781 0.07958 0.06250 0.07031 0.35117 0.21875 0.28906 0.77672 0.43750 0.72656 1.00000 0.27344 1.00000 Let’s look at the cumulative probabilities (simply computed by adding, on each row of the table, the value of \\(P(\\text{likelihood ratio} = \\text{value})\\) on that row, and all rows above it). How should we place our critical value to ensure that \\(\\alpha \\leq .05\\)? If we picked the value 0.07958, and rejected the null hypothesis if we obtained a value equal to or lower than that, then the probability of a Type I error would be 0.07031. Thus, if we don’t want that probability to be higher than .05, then we can’t pick that value. If, on the other hand, we pick the value 0.00391, then the probability of a Type I error would be 0.007813, which is lower than \\(\\alpha = .05\\). As we should not go higher than our desired value of \\(\\alpha\\), that would be the appropriate critical value.3 Indeed, we could place the critical value somewhere between 0.003906 and 0.07958, as I have done in Figure 2.10 to make the plot easier to read. As, with \\(n=8\\), there are no possible values of the likelihood ratio between 0.003906 and 0.07958 anyway, the precise placement of the critical value does not really matter. Figure 2.10: Critical values (dotted lines) and the true distibution of likelihood ratios comparing MODEL R with estimated MODEL G, when the true model is MODEL R (left) and the true model is one particular version of MODEL G (right). The plot on the left of Figure 2.10 shows the situation considered by the null hypothesis significance test, namely the situation in which MODEL R is true. If MODEL R is false, then \\(\\theta\\) can, in principle, take any value \\(\\theta \\neq 0.5\\). If \\(\\theta\\) does not equal 0.5, the probability \\(P(\\text{likelihood ratio} \\leq \\text{critical value})\\) will be higher than when \\(\\theta = 0.5\\). As an example, the right side of Figure 2.10 shows that \\[P(\\text{likelihood ratio} \\leq \\text{critical value} | \\theta = 0.9) = 0.43\\] This probability of obtaining a value equal to or lower than the critical value, given that \\(H_0\\) is false, is also called the power of a test: \\[\\begin{equation} \\text{power} = P(\\text{test value} \\leq \\text{critical value} | H_0 \\text{ is false} ) \\tag{2.4} \\end{equation}\\] Note that I’m using “test value” rather than likelihood ratio, because there will be times when we use a different test statistic than the likelihood ratio, and I want the definition of power here to cover those situations too. Furthermore, the complementary probability, \\(P(\\text{value} &gt; \\text{critical value} | H_0 \\text{ is false})\\), is the probability of a Type 2 error. Now, while power is a very important concept in hypothesis testing, it is also elusive. If you look at Figure 2.9, you can see that the power is different for different values of \\(\\theta\\). Power is an important concept because usually, it is less interesting if the null hypothesis is true than if it is false. For instance, it is less interesting if Paul was merely guessing than if he was psychic. So usually, we are more interested in rejecting the null hypothesis than in accepting it. Disappointingly, in this case, the power is pretty low even when \\(\\theta = .9\\), and it is even lower for values of \\(\\theta\\) closer to 0.5. We can increase the power by increasing \\(\\alpha\\). You can visualize this in Figure 2.10 as moving the critical value to the right, for instance to the right of the second possible value of the likelihood ratio. If \\(\\theta = .9\\), that would increase the power to 0.813. But conversely, the probability of a Type 1 error would now be 0.07. Generally speaking, we don’t want the probability of a Type 1 error so high. A better way of increasing the power of a test is to collect more data; e.g., by letting Paul make more predictions than 8. The tricky thing about the power of a test is that it depends on the unknown parameter \\(\\theta\\), and can never really be known. We can calculate the power of a test for various values of \\(\\theta\\), but in a sense, the choice of such values is arbitrary if we don’t know the true value of \\(\\theta\\). Of course, using our data, we could estimate \\(\\theta\\), and then calculate what some have called the “observed power” of a test. In this example, \\(\\hat{\\theta} = \\frac{8}{8} = 1\\), and the “observed power” is 1. But, as we know, the estimate of \\(\\theta\\) is not identical to the true value of \\(\\theta\\), and hence the estimated power is unlikely to be equal to the true power, unless we have a lot of data, but then the power would be reasonably large in the first place. As such, “observed power” is rather useless. Perhaps the most useful way (at least in my opinion) to consider power is to determine the value of \\(\\theta\\) that would provide a reasonable level of power. Conventionally, a power of \\(P(\\text{test value} \\leq \\text{critical value} | H_0 \\text{ is false} ) = 0.8\\) is considered reasonable. By plotting the power for different values of \\(\\theta\\), as done in Figure 2.11, we can then determine the values of \\(\\theta\\) which would obtain this level of power. Such a plot is also called a power-curve. As you can see, values of \\(\\theta\\) close to 0 and 1 provide a high power. To reach a power of at least 0.8, it would need to be the case that \\(\\theta \\leq 0.028\\) or \\(\\theta \\geq 0.972\\). Thus, Paul would have to be rather good at giving correct predictions (or very good at giving incorrect predictions), to make it reasonably likely that we would reject the null hypothesis. If you consider \\(\\theta \\leq 0.028\\) or \\(\\theta \\geq 0.972\\) too extreme to be likely, then it would be a good idea to get more data in order to make the required value of \\(\\theta\\) more reasonable. Figure 2.11: A power-curve, plotting the power of the test for different values of \\(\\theta\\). The broken line indicates a power of 0.8, and the dotted lines indicate the values of \\(\\theta\\) which obtain this power. 2.5.3 Testing whether Paul was guessing Finally then, let’s actually test the null hypothesis that Paul was guessing. When just considering his performance in the 2010 FIFA World cup, Paul made \\(n=8\\) predictions and \\(k=8\\) of these were correct. This gives a likelihood ratio of MODEL R (\\(\\theta = .5\\)) over estimated MODEL G (\\(\\hat{\\theta} = 1\\)) of 0.00391. Using a significance level of \\(\\alpha = .05\\), the critical value is set to 0.00391, and as the likelihood ratio is equal to or smaller than this value, we reject \\(H_0: \\theta = .5\\), and conclude that it is rather unlikely that Paul was merely guessing. In fact, \\(P(\\text{likelihood ratio} \\leq 0.00391 | \\theta = .5) = 0.00781\\). This latter probability, the probability of a test value equal to more extreme than the test value calculated for the data, is generally also called the p-value. If the p-value is smaller than the required significance level, the test result is called significant, and we reject \\(H_0\\). What “more extreme” means can vary, depending on how the test is performed. Now, in the preceding discussion, we have completely ignored Paul’s performance in the UEFA Euro 2008 cup. If we can assume that Paul’s psychic abilities did not change between the UEFA Euro 2008 and the 2010 FIFA World cup, it makes sense to pool both data sets to get a more reliable and more powerful test. As the logic is entirely the same as before, we will go through this quickly. Pooling both datasets, we now have \\(n=14\\) predictions, of which \\(k= 12\\) were correct. This provides a likelihood ratio of \\[\\text{likelihood ratio} = \\frac{P(k=12|n=14, \\theta = .5)}{P(k=12|n=14, \\hat{\\theta} = \\frac{12}{14})} = 0.019\\] For \\(\\alpha = .05\\), the critical value is 0.019, so again, we can reject \\(H_0: \\theta = .5\\). In fact, the probability of obtaining a likelihood ratio of 0.019 or smaller if \\(H_0\\) is true, the p-value, is .0129. 2.6 Hypothesis testing directly with the Binomial distribution In the preceding, we have focused on the likelihood ratio as the test statistic for our hypothesis tests. The likelihood ratio is a very general measure to compare how well two models describe the data. And it clearly shows how hypothesis testing can be seen as a form of model comparison. That said, the likelihood ratio can also be a bit cumbersome to work with, and deriving the distribution of the likelihood ratio under the null hypothesis is not always straightforward. As we will see later, we can often use a different measure in place of the likelihood ratio, which will provide testing procedures that are equivalent to it. As we saw when we worked out the theoretical distribution of the likelihood ratio (Table 2.2), there is a direct relation between the number of correct guesses and the value of the likelihood ratio. In a way, you can think of the likelihood ratio as a transformation of the number of correct guesses into a new measure (the likelihood ratio). For example, \\(k=8\\) out of \\(n=8\\) correct guesses becomes a likelihood ratio of 0.00391, and \\(7\\) out of \\(8\\) correct guesses becomes a likelihood ratio of 0.07958. Similarly, \\(k=0\\) out of \\(n=8\\) correct guesses becomes a likelihood ratio of 0.00391, and \\(1\\) out of \\(8\\) correct guesses becomes a likelihood ratio of 0.07958. So different values of \\(k\\) may provide the same likelihood ratio value (in mathematical terms, this is also called a many-to-one mapping). This is similar to outcomes 1, 3, and 5 of a die roll all mapping onto “odd number”. Because both the value and the probability of likelihood ratio values depend solely on the number of correct guesses, the number of correct guesses contains all the information about the likelihood ratio. Because of this, we can define our hypothesis tests directly in terms of correct guesses, without losing any information. For instance, when we considered the pooled data from the UEFA Euro 2008 cup and the 2010 FIFA World cup, we determined that any likelihood ratio equal to or smaller than 0.00756 would lead to a rejection of the null hypothesis. There are three possible values of the likelihood ratio in this critical region (values between 0 and 0.00756), and these are associated with \\(k=0,1,2,12,13,\\text{ or } 14\\). Thus, we can equivalently define the critical region in terms of the number of correct guesses: \\(k \\leq 2\\) and \\(k \\geq 12\\). Figure 2.12 depicts the distribution of \\(k\\) under \\(H_0\\) and the critical region for \\(\\alpha=.05\\). Figure 2.12: Distribution of the number of correct predictions (\\(k\\)) out of \\(n=14\\) under MODEL R (\\(\\theta = .5\\)), and the critical regions for \\(\\alpha = .05\\). Note that both this new way of testing \\(H_0\\), as well as the likelihood ratio test we performed earlier, base their decisions on Paul’s number of correct guesses (i.e. \\(Y = 12\\)). This new way checks directly whether the value of \\(Y\\) is in the critical region, while the likelihood ratio test first transforms \\(Y\\) into a likelihood ratio, and then checks whether this value is in the critical region. Both ways of testing provide exactly the same decisions. But testing \\(Y\\) directly is somewhat easier, as we don’t have to compute anything else. In addition, using \\(Y\\) directly also makes it more straightforward to perform a one-sided hypothesis test. Up to now, we have compared a model with \\(\\theta = .5\\) to a model that merely assumed \\(0 \\leq \\theta \\leq 1\\). The latter model can not only account for Paul making very accurate predictions, but also for Paul making very inaccurate predictions. But perhaps it is more reasonable to expect that if Paul was indeed psychic, that he would make correct predictions, rather than deceiving his keeper by making inaccurate predictions. In other words, if Paul was psychic, we would expect the probability of him making a correct prediction to be larger than if he were just guessing, so \\(\\theta &gt; .5\\). We can still test this model against the null hypothesis of random guessing, but now small numbers of correct guesses should not move us to reject the null hypothesis, as such small numbers would be even less likely if a model with \\(\\theta &gt; .5\\) were true. If we’d use the right critical region of Figure 2.12, the actual significance level would be only \\(\\alpha = 0.006\\) (the probability \\(P(k \\geq 12 | \\theta = .5)\\)). If we’d like a significance level of \\(\\alpha = .05\\), we can actually move the critical value to 11, as \\(P(k \\geq 11 | \\theta = .5) = 0.029\\), which is still below the desired \\(\\alpha = .05\\), but it is as close as we can get here. The resulting right-sided test of the hypothesis \\(H_0: \\theta = .5\\) versus the alternative that \\(\\theta &gt; .5\\) is depicted in Figure 2.13. The result of this test, with \\(Y = 12\\), is again to reject \\(H_0\\). Figure 2.13: Distribution of the number of correct predictions (\\(k\\)) out of \\(n=14\\) under MODEL R (\\(\\theta = .5\\)), and the critical region for a one-sided test and \\(\\alpha = .05\\). It is possible to formulate this one-sided test in terms of a likelihood ratio test as well, but that wouldn’t necessarily make things clearer, and I think this chapter is long enough already. 2.7 Summary Statistical models define a probability distribution for the Data Generating Process. To test a particular hypothesis about an aspect of the DGP, such as that the probability of a correct prediction by Paul equals .5, we can define two alternative models, one which assumes the DGP has that characteristic, and one which doesn’t. When it is difficult to decide upon a reasonable alternative value for the hypothesised characteristic of the DGP, the model can leave this value unspecified as a parameter. Generally, apart from the value of the parameter(s), the models are identical in other respects. In that case, the model where we assume we know the value of the parameter(s) is a special case of the more general model where we leave the parameter(s) unspecified. The specified model (e.g., one in which the probability of a correct prediction is \\(\\theta = .5\\)) is then said to be nested within the more general model (e.g., the one in which \\(\\theta\\) can take any value between 0 and 1). We refer to the more general model as MODEL G (the “general” model), and the specified model as MODEL R (the “restricted” model, in which we restrict the value of a parameter to have a specified value). Unknown parameters of statistical models can be estimated from the data. Maximum likelihood estimation is a widely used and useful method to estimate parameters of statistical models. It estimates parameters by finding the value which maximises the probability of the observed data according to the model. Maximum likelihood estimators are consistent and efficient, although they can be biased. When comparing statistical models, we want to know how good each is in describing the data. A useful and general measure for this is the likelihood ratio. However, when comparing an estimated model to a model where we restrict the value of the parameter(s) to take a particular value, we know the likelihood ratio will never show the estimated model is worse than the restricted model, as we estimated the model to maximise the likelihood. The idea of null hypothesis significance testing is to consider how the parameter estimates and the likelihood ratio values are distributed if the null hypothesis (the restricted model) is true. The procedure relies on a critical value such that the null hypothesis is rejected if the likelihood ratio is equal to or more extreme than the critical value. the critical value is chosen such that the probability of rejecting the null hypothesis when it is true (a Type 1 error) does not exceed a particular value, called the significance level (\\(\\alpha\\)). The power of a test, the probability of rejecting the null hypothesis when it is false, is also an important concept, but in practice, it is difficult to determine, as it requires knowing the true value of the parameter(s) of the general model. 2.8 Epilogue Was Paul really psychic? Quite likely not. Paul’s accuracy is quite unlikely if he were randomly guessing, and we have rejected this null hypothesis. That doesn’t mean that the null hypothesis is definitely not true; the null hypothesis significance test merely limits the probability of a wrong rejection to a maximum of e.g. \\(\\alpha = .05\\). In statistics, we can never really be sure of anything. But we shouldn’t use this argument to allow us to conclude whatever we want, because then it becomes rather pointless to test a hypothesis in the first place. So we should really conclude that Paul’s performance was higher than expected if he were guessing. But there are various ways to explain his performance which don’t involve him being psychic. For instance, Germany was a team in a rather disproportionate number of the matches for which Paul provided predictions, and in many of these, he opened the box with the German flag on it first. Perhaps Paul preferred the German flag over most other flags? Germany was also a strong team in both the UEFA Euro 2008 and the 2010 FIFA World cup. These factors could easily combine to allow Paul a large number of correct predictions. You can probably think of other confounds to explain his performance as well. Paul was not the only animal making predictions about sports matches. Mani the parakeet, Nelly the elephant, Flopsy the kangaroo, Madame Shiva the guinea pig, Big Head the sea turtle, Shaheen the camel, and Achilles the cat, are some other ones with more or less success. And there are probably many more who no one has heard about. If you think of all these animals making predictions as repetitions of a larger experiment (e.g. testing whether animals in general are psychic), it does not seem surprising that in some of these, an animal will make a large number of correct predictions. But then again, Paul was not just any animal… He was an octopus…with nine brains! Perhaps it is a little confusing to first talk about models being true, and then about hypotheses stating that these models are true. As it is conventional to talk about the null hypothesis (\\(H_0\\)) in the context of significance testing, it is important to know what this concept implies though, hence that’s why I’m using it here also.↩︎ Of course, another way to ensure that we never go above \\(\\alpha\\) is to set the critical value to 0. But that would mean we never reject \\(H_0\\), which makes the test pointless.↩︎ "],["a-model-with-a-mean-one-sample-t-test.html", "Chapter 3 A model with a mean (one sample t-test) 3.1 Numeric judgement and anchoring 3.2 A statistical model of judgements 3.3 Parameter estimation 3.4 Testing whether \\(\\mu\\) has an specific value 3.5 Confidence intervals 3.6 Assumptions 3.7 The Central Limit Theorem", " Chapter 3 A model with a mean (one sample t-test) In this chapter, we will start our journey into the General Linear Model with the most basic example within the GLM: a model concerning a single Normal distribution, where we are interested in statements about the mean of that distribution. The resulting test is also known as the one-sample t-test. We will look at the Normal distribution in detail, and cover estimation of its parameters, before focusing on testing hypotheses about the mean. We will also introduce confidence intervals as an alternative, but mostly equivalent means to make inferences about parameters. We end the chapter by looking at the assumptions of the model, and what happens to the testing procedure if those assumptions are false. 3.1 Numeric judgement and anchoring In 1906, Sir Francis Galton attended The West of England Fat Stock and Poultry Exhibition in Plymouth. One of the attractions at the fair was a contest to guess the weight of a “dressed” ox on display. The nearest guess to the actual weight of the ox would win a prize, and a total of 800 attendees participated for a small fee. Not only was Francis Galton a eugenicist4 – and rightly condemned for this in more modern times – he was also a statistician, and bought the used tickets off the stall holder. Back home he analysed the 787 usable tickets and later published his findings (Galton, 1907). What surprised Galton was that while the individual guesses were often far from the true weight of the ox, the average guess of the group as a whole was highly accurate. Such findings are now often referred to as the “Wisdom of the crowds”. Although the wisdom of the crowds has been demonstrated in various domains, from decades of research, we also know that human judgement is subject to a long list of biases. One such bias is called “anchoring”, and it pertains to the influence of prior exposure to a more or less arbitrary numerical value on the subsequent numerical judgement. For instance, if I ask you f“How tall do you think Mount Everest is?”, but first state that Mount Everest is taller than 2000 feet, you are likely to give a smaller number than if I first state that Mount Everest is shorter than 45,500 feet. In this example, we will focus on data obtained from a large scale replication study, covering the above described anchoring manipulation, as well as many other experiments (Klein et al., 2014). This “Many Labs” study contained data collected by a large number of labs from different countries, and here we will use data collected by Konrad Bocian and Natalia Frankowska from the University of Social Sciences and Humanities Campus in Sopot, Poland. In particular, we will focus on the judged height of Mount Everest (in meters, as their participants were Polish) after a low anchor (610 meters5). From the wisdom of the crowds idea, we might expect the average judgement to be identical (or at least very close) to the true height of Mount Everest, which is 8848 meters above sea level. On the other hand, if the low anchor biased the judgements, we might expect the average judgement to deviate from the true height. 3.1.1 Exploring the data Before diving into statistical modelling, as discussed, it is always good to explore the data. There were judgements by \\(n=109\\) people in this dataset. A graphical overview in the form of a histogram and raincloud plot is given in Figure 3.1. The minimum was judged height was 650 meters (so quite close to the anchor), and the maximum was 12,459 meters. The median judgement was 8000 and the mean 6312.193. The sample variance of the judgements was 9,742,645 and the sample standard deviation 3121.321. Figure 3.1: Histogram and boxplot of participants’ judgments Looking at Figure 3.1, you can see a peak in the distribution around the true height of Mount Everest (8848 meters), but the judgements are also quite varying, with a large number of judgements substantially below the actual height. This may be due to some people knowing the correct answer, whilst others don’t and have to guess. Those guessing would likely be more influenced by the low anchor. The question is whether, on average, the group’s judgements are equal to the actual height of Mount Everest. 3.2 A statistical model of judgements Clearly, not everyone gave a correct answer, and there is substantial variation in people’s judgements. The goal of a statistical model is to account for this variation. In this chapter, we will consider one of the simplest statistical models for metric data such as the judgements, namely the Normal distribution. In this model, we will assume that the variation in judgements is entirely random. We may also assume that, on average, the judgements are equal to the actual height of Mount Everest, but some people will overestimate, and some people underestimate the height in their judgements. Moreover, we might assume that the probability of an underestimation is equal to the probability of an overestimation, and that very large over- or underestimations are less likely than smaller over- or underestimations. The Normal distribution encapsulates such assumptions. 3.2.1 The Normal distribution The Normal distribution is the well-known bell-shaped curve depicted in Figure 3.2. Figure 3.2: The Normal density function for \\(\\mu = 0\\) and \\(\\sigma = 1\\) The curve is given by the following function: \\[\\begin{equation} p(y) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} \\tag{3.1} \\end{equation}\\] If that looks complicated, don’t worry. First, let me point out that \\(\\pi\\) refers to the mathematical constant pi, i.e. \\(\\pi= 3.141593\\ldots\\), and \\(e\\) refers to the mathematical constant also known as Euler’s number, i.e. \\(e = 2.718282\\ldots\\). They are known constants and not parameters. The Normal distribution does have two parameters: \\(\\mu\\) (“mu”), the mean, and \\(\\sigma\\) (“sigma”), also called the standard deviation. Confusingly, the formula above is not actually the formula for the Normal probability distribution. Rather, it is the formula of the Normal density function. The Normal distribution applies to continuous variables. Technically, you can’t assign a probability to a particular value of a continuous variable, you can only assign probabilities to ranges of values. This is because a continuous variable has an infinite number of values (even if we restrict ourselves to a range). If we gave each unique value a probability of anything larger than 0, the total probability would not be 1, but infinite. If you add up an infinite amount of values, no matter how tiny they are, at some point you would reach a sum of 1, and then you’d have to keep on adding more. No matter how large the sum becomes, you would have to keep on going. Adding up an infinite number of values is not light work… While we can’t define probability of any particular value, the probability that an observation is within a particular range is well-defined. So, rather than asking “What is the probability that the height is exactly 8567.46384634748763…?”, we can ask “What is the probability that the height is between 8567 and 8568?”, or “What is the probability that the height is between 8567.575 and 8567.576?”. These probabilities are defined by the “area under the curve” within that range. Using calculus, they can be computed as: \\[P(a \\leq Y \\leq b) = \\int_a^b \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} d y\\] If you are unfamiliar with calculus and have never seen an integration sign (\\(\\int\\)), don’t worry, you don’t have to solve equations like this, and there won’t be many more like this. You can just see it as a mathematical way of referring to the area under the curve between point \\(a\\) and \\(b\\). A proper probability density function respects the rule of total probability. The Normal distribution is defined over all real numbers (i.e. all possible numbers between minus infinity and infinity). Thus, the rule of total probability rquires that \\(P(-\\infty \\leq Y \\leq \\infty) = 1\\), and this is true for the Normal distribution. Figure 3.3 shows the probability of three ranges, each symmetrical around the mean. The probability of a value falling in the range between \\(\\mu - 3 \\times \\sigma\\) and \\(\\mu + 3 \\times \\sigma\\) is very close to 1. Hence, values more than three standard deviations from the mean are extremely unlikely. Figure 3.3: Normal distribution and the probability of a value falling in one of three overlapping ranges. Note that the ranges are overlapping, so that the range between \\(\\mu - 3 \\sigma\\) and \\(\\mu + 3 \\sigma\\) also covers the range between \\(\\mu - 2 \\sigma\\) and \\(\\mu + 2 \\sigma\\), but that this isn’t shown in the colours. Because the Normal distribution is symmetric, with a single peak in the middle at \\(\\mu\\), the parameter \\(\\mu\\) not only equals the mean, but also the median (50% of all values are lower than \\(\\mu\\), and 50% are higher), and the mode. The Normal distribution is rather popular in statistical modelling. One important reason we’ll mention later (the Central Limit Theorem). Here, we’ll focus on two properties that make the Normal distribution quite convenient to work with. Before we continue, let’s introduce some new notation. As before, we will use \\(Y\\) to denote the dependent variable (i.e. people’s judgements of the height of Mount Everest), and \\(Y_i\\) to denote the \\(i\\)-th value of the dependent variable (i.e. the judgement by participant \\(i\\)), where \\(i=1,\\ldots,n\\). We will state the assumption that \\(Y\\) follows a Normal distribution as \\[Y_i \\sim \\mathbf{Normal}(\\mu,\\sigma)\\] You can read this as “\\(Y_i\\) is sampled from a Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)”. So the tilde (\\(\\sim\\)) stands for “sampled from”. When we denote distributions, such as the Normal distribution, we will use a bold font for the name of the distribution, and between the parentheses that follow we will indicate the parameters of the distribution. 3.2.2 Two useful properties of the Normal distribution 1. A linear transformation of a Normal-distributed variable is also Normal-distributed. More formally, if \\[Y \\sim \\mathbf{Normal}(\\mu_y, \\sigma_y)\\] and \\[Y&#39; = a + b \\times Y\\] i.e. a new variable \\(Y&#39;\\) is constructed by multiplying \\(Y\\) by \\(b\\) and then adding a constant \\(a\\), the distribution of this new variable \\(Y&#39;\\) is \\[Y&#39; \\sim \\mathbf{Normal}\\left(a + b \\times \\mu_y, | b | \\times \\sigma_y\\right)\\] i.e. a Normal distribution with mean equal to the linear transformation of the mean of \\(Y\\), and standard deviation equal to the standard deviation of \\(Y\\) multiplied by the absolute value of \\(b\\). For example, suppose a fast-food chain is known for its “mega-megalicious” burger. The weight of each handmade patty is Normal-distributed with a mean of 1.2 pounds, and a standard deviation of 0.16 pounds. We can work out the distribution of the weight in kilograms by first noting that 1 kilogram = 2.2046 pounds and conversely, that one pound is \\(1/2.2046 = 0.4536\\) kilograms. Weight in kilograms is thus a linear transformation of weight in pounds: \\[\\text{kg} = 0 + 0.4536 \\times \\text{lbs}\\] with \\(a=0\\) and \\(b=0.4536\\). The distribution of the weight in kilograms is then a Normal distribution with mean \\(0 + 0.4536 \\times 1.2 = 0.5443\\) kilograms, and standard deviation \\(|0.4536| \\times .16 = 0.0725\\) kilograms. As another example, we can standardize a Normal variable by subtracting the mean and then dividing by the standard deviation. The resulting variable is conventionally denoted as \\[\\begin{equation} Z = \\frac{Y-\\mu}{\\sigma} \\tag{3.2} \\end{equation}\\] and the transformation of \\(Y\\) to \\(Z\\) is also called the Z-transformation. We can view \\(Z\\) as a linear transformation of \\(Y\\) with \\(a=-\\frac{\\mu_y}{\\sigma_y}\\) and \\(b = \\frac{1}{\\sigma_y}\\). So \\(Z\\) has a mean of \\(\\mu_z = -\\frac{\\mu_y}{\\sigma_y} + \\frac{1}{\\sigma_y} \\times \\mu_y= 0\\) and a standard deviation of \\(\\sigma_z = |\\frac{1}{\\sigma_y}|\\sigma_y = 1\\). A Normal-distribution with a mean of 0 and a standard deviation of 1 is also called a standard Normal distribution. 2. The sum of two Normal-distributed variables is also Normal-distributed. More formally, if \\[Y_1 \\sim \\mathbf{Normal}(\\mu_1,\\sigma_1)\\] and \\[Y_2 \\sim \\mathbf{Normal}(\\mu_2,\\sigma_2)\\] and \\(Y_s\\) is the sum of \\(Y_1\\) and \\(Y_2\\), i.e. \\[Y_s = Y_1 + Y_2\\], then \\(Y_s\\) is distributed as \\[Y_s \\sim \\mathbf{Normal}\\left(\\mu_1 + \\mu_2, \\sqrt{\\sigma_1^2 + \\sigma^2_2}\\right)\\] i.e. a Normal distribution with mean equal to the sum of the mean of \\(Y_1\\) and the mean of \\(Y_2\\), and standard deviation equal to the square-root of the sum of the variance of \\(Y_1\\) and the variance of \\(Y_2\\). As we will see, these properties will come in handy later. For instance, when working out the sampling distribution of the mean. 3.2.3 Back to anchoring If people’s judgements are on average correct, this implies that \\(\\mu = 8848\\). Alternatively, the average judgement might be biased, for instance because of the low anchor. If this is so, what would the average judgement be? It is difficult to make a clear prediction about this. If we can’t really come up with anything sensible, we may as well assume that this parameter can have any value, as we did for Paul’s probability of a correct prediction if he were (somewhat) psychic. We can then formulate our two alternative models for people’s judgements as: MODEL R: \\(\\quad Y_i \\sim \\mathbf{Normal}(8848, \\sigma)\\) MODEL G: \\(\\quad Y_i \\sim \\mathbf{Normal}(\\mu, \\sigma)\\) MODEL R is the restricted model, a special case of the more general MODEL G. The restriction is on \\(\\mu\\). In MODEL G, the mean can take any value, so \\(-\\infty \\leq \\mu \\leq \\infty\\) (remember that \\(\\infty\\) stands for infinity). MODEL R picks a specific value \\(\\underline{\\mu}\\) from this infinite range, namely \\(\\mu = \\overline{\\mu} = 8848\\). In both models, the standard deviation is not specified, so is considered unknown. Standard deviations, like variances, can never be negative, so at least we know that \\(\\sigma \\geq 0\\), but this is all that we will specify in advance for that parameter. 3.3 Parameter estimation MODEL G has two parameters: the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). The maximum likelihood estimate of \\(\\mu\\) is the sample mean: \\[\\begin{equation} \\hat{\\mu} = \\overline{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} \\tag{3.3} \\end{equation}\\] The maximum likelihood estimate of the variance is the sample variance (see Equation (1.2)), i.e. \\(\\hat{\\sigma}^2_\\text{ML} = S^2\\). However, this estimator of the variance is biased. It is therefore common to estimate the variance with the unbiased estimator: \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (Y_i - \\overline{Y})^2}{n-1} \\tag{3.4} \\end{equation}\\] The difference is to divide the sum of squared deviations by \\(n-1\\), rather than by \\(n\\). Intuitively, you can think of the reason for this as follows: When we use the sample mean \\(\\overline{Y}\\) rather than the true mean \\(\\mu\\) in computing the variance, we don’t take into account that the sample mean is a noisy estimate of the true mean \\(\\mu\\). Compared to \\(\\overline{Y}\\), the true mean could be somewhat higher or lower, and therefore the sample variance is likely to be an underestimate of the true variance. In other words, it is biased. By dividing by \\(n-1\\) instead of \\(n\\), the resulting estimate is a little higher. It so happens this completely removes the bias. When \\(n\\) is large (i.e. there are a large number of observations), there will be little difference between the unbiased estimate and the sample variance, but for small \\(n\\) the difference will be more marked. Note that the estimator above is for the variance \\(\\sigma^2\\) (sigma squared, i.e. sigma raised to the power of 2), not for teh standard deviation \\(\\sigma\\). If we want an estimator of the standard deviation, we can simply take the square-root to get the following estimator of \\(\\sigma\\): \\[\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}\\] If the mean is known to equal a particular value, \\(\\mu = \\underline{\\mu}\\), as in MODEL R, then we should use that value \\(\\underline{\\mu}\\) instead of \\(\\overline{Y}\\) to estimate the variance and standard deviation. The unbiased estimate of the standard deviation for MODEL R is \\[\\hat{\\sigma}_R = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\underline{\\mu})^2}{n}}\\] Perhaps confusingly, we can now divide by \\(n\\) instead of by \\(n-1\\) to get an unbiased estimate. The reason for this is that when we know the true mean, there is no estimation error like we had for \\(\\overline{Y}\\), and hence no additional source of variability which would bias the estimate. 3.3.1 Sampling distribution of the estimated mean Remember that the estimator of the mean is an algorithm that provides estimates from data. Different datasets will give different estimates, even though all these datasets are generated from the same Data Generating Process. Our model of the Data Generating Process is the Normal distribution, which has two parameters: \\(\\mu\\) and \\(\\sigma\\). Let’s pick some values for these parameters, so that we can use our model to simulate data. For instance, suppose the judgements are on average equal to the true value, so \\(\\mu = 8848\\). Individual judgements are quite variable, however, so let’s take \\(\\sigma = 2000\\). Figure 3.4 shows the distribution of the estimated mean of 10,000 data sets each consisting of \\(n=109\\) observations (the same number of observations as our anchoring data). Also shown (as a dotted line) is the Normal distribution of the model we used to generate the data. Clearly, the estimated means are much less variable than the simulated judgements themselves. Figure 3.4: Estimated means for 10000 simulated data sets of \\(n = 109\\), drawn from a Normal distribution with mean 8848 and standard deviation 2000. The dotted line represents the Normal distribution from which the data sets were generated, and the solid line the theoretical distribution of the estimated means. Simulating data and then looking at the resulting distribution of estimates is straightforward, but also noisy. Luckily, it is quite easy to derive the true distribution of the estimated means (i.e. the sample means). Using the two properties of the Normal distribution, we know that the sum of two Normal-distributed variables also follows a Normal distribution. This is easily generalized to the sum of any number of Normal-distributed variables. For instance, we can construct the distribution of the sum of three variables by adding the sum of two Normal-distributed variables (which we know is a Normal-distributed variable) to a third Normal-distributed variable: \\(Y_s = (Y_1 + Y_2) + Y_3\\). Hence, the sum of three Normal-distributed variables is also Normal-distributed. We can then take the sum of three variables, and add a fourth one, and this sum will also be Normal-distributed: \\(Y_s = ((Y_1 + Y_2) + Y_3) + Y_4\\), and so will the sum of five, six, and any number of Normal-distributed variables. This is a nice example of a recursive function. In short, if each \\(Y_i \\sim \\mathbf{Normal}(\\mu,\\sigma)\\), then \\[\\sum_{i=1}^n Y_i \\sim \\mathbf{Normal}(n \\mu, \\sqrt{n}\\sigma)\\] Moreover, as we discussed earlier, we can view the mean as a linear transformation of this sum with \\(a = 0\\) and \\(b=\\frac{1}{n}\\), from which we can derive that \\[\\overline{Y} \\sim \\mathbf{Normal}\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\] Hence, the sampling distribution of the estimated means is a Normal distribution with a mean equal to \\(\\mu\\). The estimator is thus unbiased. The standard deviation of the sampling distribution of the estimated means is equal to \\(\\frac{\\sigma}{\\sqrt{n}}\\), i.e. the standard deviation of the dependent variable divided by \\(\\sqrt{n}\\). The standard deviation of the sampling distribution of estimates is also called the standard error of the estimates. Dividing by \\(\\sqrt{n}\\) implies that the standard deviation of the sample means is smaller than the standard deviation of the dependent variable. And, as \\(n\\) increases, it becomes smaller and smaller. The estimator is therefore also consistent. And, as it turns out, the estimator is also efficient. To simulate judgements, we had to pick an arbitrary value of \\(\\sigma\\). But how can we simulate the data when \\(\\sigma\\) is unknown? We could of course use the unbiased estimate \\(\\hat{\\sigma}\\). For MODEL R and the present data, that would be \\(\\hat{\\sigma} = 4021.562\\), which is obviously substantially larger than the value of 2000 we used before. If we’d use this value to simulate data sets and look at the distribution of the sample mean, we’d get a similar plot to the one of Figure 3.4, but with a larger standard deviation. Suppose that MODEL R is true, that the Data Generating Process indeed results in a Normal distribution of people’s judgements with a mean \\(\\mu = 8848\\) and a standard deviation \\(\\sigma\\). If we knew the value of \\(\\sigma\\), then we’d know everything there is to know about the distribution of the DGP. And knowing this, we’d know everything there is to know about the distribution of the sample means. But we don’t know \\(\\sigma\\). While it makes sense to use an estimate of \\(\\sigma\\), this estimate will be noisy. We shouldn’t just pretend that our estimate \\(\\hat{\\sigma}\\) is identical to the true \\(\\sigma\\). The problem is that different values of \\(\\sigma\\) lead to different sampling distributions of the mean. Key to working out the sampling distribution of the mean when \\(\\sigma\\) is unknown is to also take into account the sampling distribution of the estimates of \\(\\sigma\\). Roughly, the idea is that, for a given data set, we can work out how likely different values of \\(\\sigma\\) are, and we can then derive the average of all the Normal distributions that follow from each possible value of \\(\\sigma\\). The resulting distribution is not a Normal distribution. It was derived by William Sealy Gosset (1876–1937) in 1904. Gosset worked as Head Experimental Brewer for Guinness in Dublin, and the company had a rule forbidding their chemists to publish their findings (Zabell, 2008). Gosset was able to convince his boss that his mathematical work was of no practical use to competing brewers, and was allowed to publish them in Student (1908), but under a pseudonym to avoid his colleagues getting similar ideas. “Student” was the pseudonym chosen by the managing director of Guinness, and hence the distribution is now known as Student’s t-distribution. Figure 3.5: (Understandardized) Student t-distribution which is the true sampling distribution of the mean under MODEL R with unknown \\(\\sigma\\) (solid line) and the Normal distribution that would be the sampling distribution if it were known that \\(\\sigma = \\hat{\\sigma}\\) (broken line). Note that these curves are drawn for the case of \\(n=10\\) observations, to make the differences between the distributions more marked. For \\(n=109\\), the t-distribution is almost identical to the Normal distribution. You can see a comparison of the t-distribution and the Normal distribution in Figure 3.5. The main thing to notice is that the t-distribution is also bell-shaped and symmetric, but it is wider (has fatter tails) than the Normal distribution. The difference between the t-distribution and Normal distribution depends on the value of \\(n-1\\). When \\(n&gt;30\\), the difference is, for most practical purposes, negligible. 3.4 Testing whether \\(\\mu\\) has an specific value Our main interest here is to assess whether it is reasonable to assume that, in the Data Generating Process, the average judgement equals the true height of Mount Everest. More bluntly, we simply want to know whether \\(\\mu = 8848\\). 3.4.1 The classical way Now we know what happens to the estimate \\(\\hat{\\mu}\\) of MODEL G when MODEL R is in fact true, we can use a similar logic as before to decide when \\(\\hat{\\mu}\\) is different enough from \\(\\underline{\\mu}\\) to reject MODEL R. To work this out, it is convenient to standardize the sample mean by subtracting the (supposed) true average, and dividing by the standard error of the sample mean: \\[\\begin{equation} t = \\frac{\\hat{\\mu} - \\underline{\\mu}}{\\hat{\\sigma}/\\sqrt{n}} = \\frac{\\overline{Y} - \\underline{\\mu}}{S/\\sqrt{n}} \\tag{3.5} \\end{equation}\\] We call the resulting standardized value the \\(t\\) statistic. It is much like the \\(Z\\)-transformation, but now we’re using an estimated \\(\\hat{\\sigma}\\) rather than a known value. The standardized \\(t\\) statistic follows a \\(t\\) distribution with \\(n-1\\) degrees of freedom, which reflects the amount of data we used to estimate \\(\\sigma\\). What we showed in Figure 3.5 was an “unstandardized” t-distribution, which also has a location and scale parameter. By standardizing \\(t\\), we don’t need to worry about these. Generally, when someone mentions the t-distribution, they refer to the standardized version, which has a single parameter, the degrees of freedom (commonly denoted with the Greek symbol \\(\\nu\\), pronounced as “nu”; I will just use “degrees of freedom”, or \\(df\\) though). Back in the times of Gosset, this was particularly important, as there was no access to modern computers to quickly work out the probabilities for different versions of unstandardized t-distributions. Statisticians had to rely on tables for a standard version of a distribution, which were painstakingly computed by hand. In fact, the term “computer” originally referred to a person performing mathematical calculations. Complex, long, and often rather tedious calculations were performed by teams of such computers. I won’t bore you with the mathematical details of the t-distribution. As I already mentioned, it looks quite like a Normal distribution. When the degrees of freedom get larger, the distributions looks more and more like the standard Normal distribution. Because we know that if MODEL R is true, the \\(t\\) statistic follows a t-distribution with \\(n-1\\) degrees of freedom, we know all there is to know about the sampling distribution of \\(t\\). If \\(t\\) is far away from 0, that means that the estimate \\(\\hat{\\mu}\\) of MODEL G is far away from the assumed value \\(\\underline{\\mu}\\) of MODEL R, and hence we have reason to reject MODEL R. Following the logic of the null-hypothesis significance test, we will limit the number of wrong rejections of MODEL R by choosing critical values such that \\[P(t \\leq \\text{lower critical value} | \\text{MODEL R}) + P(t \\geq \\text{upper critical value} | \\text{MODEL R}) = \\alpha\\] The resulting critical regions are depicted in Figure 3.6. Figure 3.6: Critical regions for rejecting \\(H_0\\): \\(\\mu = 8848\\) with \\(\\alpha = .05\\) So how about the judgements in the anchoring experiment? Is it reasonable to assume that \\(\\mu = 8848\\)? Let’s perform the test and see. For MODEL G, our estimated mean is \\(\\hat{\\mu} = 6312.193\\), and the unbiased estimate of the standard deviation is \\(\\hat{\\sigma} = 3135.738\\). The \\(t\\) statistic is then \\[t = \\frac{6312.193 - 8848}{3135.738/\\sqrt109} = -8.443\\] For a significance level of \\(\\alpha = .05\\), we can use the t-distribution with 108 degrees of freedom to work out the lower and upper critical value as -1.982 and 1.982. Clearly, the value of the \\(t\\)-statistic that we computed is far below the lower critical value, which means we reject the null hypothesis. The associated \\(p\\)-value with this two-sided test (we would reject \\(H_0\\) if we had very large negative, as well as very large positive values of the \\(t\\) statistic), is \\(P(t \\leq -8.443| \\text{df} = 108) + P(t \\geq 8.443| \\text{df} = 108) = P(|t| \\geq 8.443| \\text{df} = 108) &lt; .001\\). So, in conclusion, the \\(t\\)-statistic is in the critical region for our chosen significance level of \\(\\alpha = .05\\), which leads us to reject the null hypothesis. Equivalently, the \\(p\\)-value, the probability of obtaining a \\(t\\) statistic equal to or more extreme than the one we found, is smaller than our chosen significance level \\(\\alpha=.05\\), so we reject the null hypothesis. In our anchoring data, the “wisdom of crowds” doesn’t seem to hold, because the average judgement does not equal the true height of Mount Everest. 3.4.2 The model comparison way There is an equivalent way to perform the hypothesis test that \\(\\mu = \\underline{\\mu}\\), by directly comparing two versions of our statistical model, one in which we assume we know the value of \\(\\mu\\) to be \\(\\underline{\\mu}\\) (MODEL R), and one in which we don’t (MODEL G). As we did for Paul the Octopus, we then look at the likelihood ratio to see whether MODEL R is tenable. Note that in both models, the standard deviation is considered unknown and will have to be estimated. The procedure is essentially the same as before. We find a way to calculate the likelihood ratio, and work out the distribution of the likelihood ratio values we would get for repetitions of an experiment assuming MODEL R is true. Using this distribution, we can then work out a critical value, such that the probability of obtaining a likelihood ratio value equal to or smaller than it is equal to a significance level \\(\\alpha\\), i.e. , \\(P(\\text{likelihood ratio } \\leq \\text{critical value} | \\text{MODEL R}) = \\alpha\\). The derivation of the likelihood ratio is a little more complicated than before though. Feel free to glance over the following, as the derivation is not that important. The paragraphs marked with a vertical line in the left margin are more advanced and can be skipped without too much consequence. When all observations \\(Y_i\\), \\(i=1, \\ldots, n\\) are independent draws from a Normal distribution, then we can use the product rule to work out the joint “probability” (density value really) of all observations. That is, we use Equation (3.1) to compute \\(p(Y_i)\\), the density value of each observation, and then multiply these together: \\[ \\begin{align} p(Y_1, Y_2, \\ldots, Y_n) &amp;= p(Y_1) \\times p(Y_2) \\times \\ldots \\times p(Y_n) \\\\ &amp;= \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(Y_i-\\mu)^2}{2\\sigma^2}} \\\\ &amp;= \\left( \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\right)^n e^{ \\frac{n}{2\\sigma^2} \\sum_{i=1}^n (Y_i - \\mu)^2} \\end{align} \\] The \\(\\prod\\) sign on the second line is similar to the summation sign, but tells you to multiply all the values that follow it, so e.g. \\[\\prod_{i=1}^3 Y_i = Y_1 \\times Y_2 \\times Y_3\\] On the third line, we make use of the fact that \\(e^a \\times e^b = e^{a + b}\\). We can write the likelihood ratio of MODEL R over MODEL G as \\[ \\begin{align} L\\!R(R,G) &amp;= \\frac{\\left( \\frac{1}{\\sqrt{2 \\pi} \\hat{\\sigma}_{ml,R}} \\right)^n e^{ \\frac{n}{2\\hat{\\sigma}^2_{ml,R}} \\sum_{i=1}^n (Y_i - \\mu)^2}}{\\left( \\frac{1}{\\sqrt{2 \\pi} \\hat{\\sigma}_{ml,G}} \\right)^n e^{ \\frac{n}{2\\hat{\\sigma}^2_{ml,G}} \\sum_{i=1}^n (Y_i - \\mu)^2}} \\\\ &amp;= \\frac{ \\left(\\sqrt{\\frac{2\\pi}{n} \\sum_{i=1}^n(Y_i - \\underline{\\mu})^2}\\right)^{-n} e^{\\frac{\\sum_{i=1}^n(Y_i - \\underline{\\mu})^2}{2 \\sum_{i=1}^n(Y_i - \\underline{\\mu})^2/n}}}{\\left(\\sqrt{\\frac{2\\pi}{n} \\sum_{i=1}^n(Y_i - \\overline{Y})^2}\\right)^{-n} e^{ \\frac{\\sum_{i=1}^n (Y_i - \\overline{Y})^2}{2 \\sum_{i=1}^n (Y_i - \\overline{Y})^2/n}} } \\\\ &amp;= \\left(\\frac{\\sum_{i=1}^n(Y_i - \\underline{\\mu})^2}{\\sum_{i=1}^n(Y_i - \\overline{Y})^2}\\right)^{-n/2} \\end{align} \\] What is interesting to point out is that, on the last line of the equation above, we see that the two main ingredients of the likelihood ratio are sums of squared deviations. Moreover, as the deviations are between observations and the mean (either assumed mean or sample mean), these sums of squared deviations are very closely related to variances. As we will see later, “sums of squares” are important quantities when comparing General Linear Models. While the likelihood ratio is reasonably straightforward to compute, it is not easy to derive the sampling distribution of the likelihood ratio. Previously, we could work out the distribution of the likelihood ratio when MODEL R is true relatively easily from the Binomial distribution of the outcomes. We can do something similar here, which effectively means transforming the likelihood ratio into a statistic we do know the distribution of, or conversely, transforming a statistic we know the distribution of into the likelihood ratio. To do this, first we can rewrite \\[\\sum_{i=1}^n(Y_i - \\underline{\\mu})^2 = \\sum_{i=1}^n(Y_i - \\overline{Y} + \\overline{Y} - \\underline{\\mu})^2 = n(\\overline{Y} - \\underline{\\mu})^2 + \\sum_{i=1}^n(Y_i - \\overline{Y})^2\\] and filling this in the numerator of the likelihood ratio gives \\[L\\!R(R,G) = \\left(1 + \\frac{n(\\overline{Y} - \\underline{\\mu})^2}{\\sum_{i=1}^n(Y_i - \\overline{Y})^2}\\right)^{-n/2}\\] For a given experiment or study, we can treat the number of observations \\(n\\) as a given fixed number. What we are really interested in is the part of the likelihood ratio that depends on the data. If we take the square root of this part, we get \\[\\frac{|\\overline{Y} - \\underline{\\mu}|}{\\sqrt{\\sum_{i=1}^n(Y_i - \\overline{Y})^2}} = \\sqrt{n (n-1)} \\times | t |\\] with \\(t\\) defined as in Equation (3.5). When the absolute value of the \\(t\\) statistic becomes larger, the likelihood ratio value becomes smaller (less supportive of MODEL R). We can thus view the likelihood ratio as a (rather complicated) transformation of the absolute value of the \\(t\\) statistic. The distribution of the likelihood ratio is then also a (rather complicated) transformation of the distribution of the \\(t\\)-statistic. Just like the coin tossing model, where the likelihood ratio was effectively a transformation of the statistic \\(k\\) (the number of correct guesses), and the distribution of the likelihood ratio a transformation of the distribution of \\(k\\), i.e. the Binomial distribution. What all this means is that the one-sample t-test, as worked out before, is equivalent to a likelihood ratio test comparing MODEL R to MODEL G. Whether you perform the test in the “classic” way by computing the \\(t\\) statistic and assessing the magnitude of this in the \\(t\\) distribution, or whether you compute a likelihood ratio and assess the magnitude of this in the distribution of the likelihood ratio under MODEL R, you will get exactly the same outcome. 3.5 Confidence intervals Confidence intervals are interval estimates. That is, rather than providing a single point estimate of a parameter, they specify a range of values for a parameter. That range is chosen in a precise, but somewhat counter-intuitive and difficult to understand manner. Ideally, what you might want to obtain is a range such that you can be certain that, with a specified probability, the true parameter is within this range. Unfortunately, this is not what confidence intervals do. Like much of Frequentist statistics, confidence intervals concern “the long run”, they are based on the idea of an infinite number of possible (simulated) datasets from a true model. Confidence intervals are the result of an algorithm which is constructed in such a way that the ranges it produces will, for a specified proportion of all those possible datasets, contain the true value of the parameter. While difficult to interpret properly, the confidence interval for the mean of Normal interval with unknown standard deviation is straightforward to compute: \\[\\begin{equation} \\hat{\\mu} \\pm t_{n-1; 1- \\frac{\\alpha}{2}} \\frac{\\hat{\\sigma}}{\\sqrt{n}} \\tag{3.6} \\end{equation}\\] where \\(t_{n-1; 1- \\frac{\\alpha}{2}}\\) stands for the upper critical value of a test with significance level \\(\\alpha\\). In other words, we take our estimate of the mean (i.e. \\(\\hat{\\mu} = \\overline{Y}\\)), and add or subtract from that a multiple of the estimated standard error of the mean (which is \\(\\frac{\\hat{\\sigma}}{\\sqrt{n}}\\); remember that the standard error of a statistic is the standard deviation of the sampling distribution of that statistic). The multiple is derived from the t-distribution. As explained earlier, for the Normal distribution, we know that 95% of the observations lie in the range between \\(\\mu - 1.96 \\times \\sigma\\) and \\(\\mu + 1.96 \\times \\sigma\\). Similarly, for the Normal distribution, 95% of the sample means lie in the range between \\(\\mu - 1.96 \\times \\frac{\\sigma}{\\sqrt{n}}\\) and \\(\\mu + 1.96 \\times \\frac{\\sigma}{\\sqrt{n}}\\). The value \\(t_{n-1; \\frac{\\alpha}{2}}\\) is effectively a replacement for 1.96, which takes into account that we have estimated \\(\\sigma\\). Figure 3.7: 95% confidence intervals for 100 simulated datasets with \\(\\mu=8848\\) and \\(\\sigma=2000\\). Most of the intervals contain the true value \\(\\mu\\), but some do not. If we would simulate an infinite number of datasets, then exactly 95% of the intervals would contain \\(\\mu\\), and exactly 5% would not. Earlier on, we simulated datasets of size \\(n=109\\) from a Normal distribution with mean \\(\\mu = 8848\\) and \\(\\sigma = 2000\\). In Figure 3.7 we show the resulting 95% confidence intervals (now estimating \\(\\sigma\\) from the data) for 100 of such simulated datasets. As you can see, the location, and the width of each confidence interval is different. Moreover, you can see that while most of the confidence intervals contain the true value \\(\\mu = 8848\\), some don’t. Indeed, 6 out of 100 confidence intervals do not include the true value of \\(\\mu\\), which is pretty close to 5%. For the judgements in the anchoring experiment, the estimated mean was \\(\\hat{\\mu} = 6312.193\\), the unbiased estimate of the standard deviation was \\(\\hat{\\sigma} = 3135.738\\), and the upper critical value was \\(t_{108, .975} = 1.982\\). Thus, the confidence interval is \\[ \\begin{align} \\text{95% confidence interval} &amp;= \\hat{\\mu} - t_{n-1; 1 - \\frac{\\alpha}{2}} \\times \\frac{\\hat{\\sigma}}{\\sqrt{n}} \\leq \\mu \\leq \\hat{\\mu} + t_{n-1; 1- \\frac{\\alpha}{2}} \\times \\frac{\\hat{\\sigma}}{\\sqrt{n}} \\\\ &amp;= 6312.193 - 1.982 \\times \\frac{3135.738}{\\sqrt{109}} \\leq \\mu \\leq 6312.193 + 1.982 \\times \\frac{3135.738}{\\sqrt{109}} \\\\ &amp;= 5716.848 \\leq \\mu \\leq 6907.537 \\end{align} \\] Confidence intervals can be used to perform null hypothesis significance tests. In fact, a confidence interval contains all values of \\(\\underline{\\mu}\\) for which MODEL R would not be rejected with the current data. In other words, if the confidence interval includes the value of \\(\\underline{\\mu}\\) of interest (i.e. \\(\\underline{\\mu} = 8848\\) in our example), that means that the null hypothesis would not be rejected. In addition, we can also determine a whole range of other values of \\(\\underline{\\mu}\\) which, if purported as a null hypothesis, would also not be rejected. In that sense, a confidence interval provides more information than a simple “reject” or “not reject” decision of a null hypothesis significance test. If a confidence interval is very wide, that implies the significance test has low power. Of course, what “very wide” is, depends on the scale of the data, and a subjective evaluation by the researcher. There have been calls to abandon hypothesis tests in favour of estimation and reporting confidence intervals (e.g. Cumming, 2014). Given the tricky interpretation of confidence intervals (Morey, Hoekstra, Rouder, Lee, &amp; Wagenmakers, 2016), and their rather close ties to significance tests, such ideas seem to promise more than they can deliver. There is no need to abandon either significance tests or confidence intervals, provided you know how to interpret them. 3.6 Assumptions When we started modelling the data, we assumed each judgement was effectively an independent random draw from a Normal distribution. In MODEL R, we assumed that the mean of that Normal distribution was \\(\\mu = 8848\\), while in MODEL G, we left both the mean and the standard deviation unknown. Both models thus assume the judgements follow a Normal distribution. Looking at Figure 3.1, you might wonder whether that’s a reasonable assumption. The histogram doesn’t look exactly Normal. With real and limited data, histograms can be deceiving though. A Q-Q (quantile-quantile) plot aims to provide a better means to visually inspect whether data (approximately) follows an assumed distribution. Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Percentiles are an example of quantiles, which divide a probability distribution into ranges each containing .01 of the total probability (i.e. 1%). By ordering the values of a variable in a data set in increasing order of magnitude, you can also obtain “empirical quantiles”. Suppose you have 5 observations ordered in increasing magnitude: 1, 4, 4.5, 9, 20 For each value, you can determine the proportion of values in the data which are equal or smaller than it. For instance, the proportion of observations equal to or smaller than 1 is \\(1/5 = 0.2\\), the proportion of observations equal to or smaller than 4 is \\(2/5 = 0.4\\), the proportion of observations equal to or smaller than 4.5 is \\(3/5 = 0.6\\), etc. For each of these proportions, we can also work out the corresponding value of the variable for the probability of a value equal to smaller than it equals this proportion. For instance, for a Normal distribution with mean \\(\\mu=7\\) and standard deviation \\(\\sigma = 7\\), the quantile such that \\(P(\\text{value } \\leq \\text{ quantile}) = .2\\) is 1.109, and the quantile such that \\(P(\\text{value } \\leq \\text{ quantile}) = .4\\) is 5.227. In a Q-Q plot, the values of the observed data are plotted against these theoretical quantiles in a scatter-plot. If the data follows the assumed distribution, the empirical quantiles should generally match the theoretical quantiles, and hence the points in the plot should roughly lie on a straight line. Figure 3.8 shows such a Q-Q plot for the judgements. As you can see, rather than on a straight-line, the points seem to lie on an s-shaped curve, which indicates that the assumption of a Normal distribution might not be a good match to the data. Figure 3.8: A Quantile-quantile plot for the judgments of the height of Mount Everest There are also two common tests for the null hypothesis that the distribution is Normal: the Kolmogorov-Smirnov, and the Shapiro-Wilk test. The former is a generic test, which can be used to check for other distributions besides the Normal one as well. The second one only focuses on the Normal distribution, and because it was specifically designed for this distribution, it is more powerful. I’m not going to describe how these tests actually work. You can look that up elsewhere if you like. I will tell you that both tests reject the null-hypothesis of a Normal distribution. For both tests, the \\(p\\)-value (the probability of obtaining a test result equal to or more extreme, given that the null hypothesis of a Normal distribution is true), is \\(p&lt;.001\\). I should also caution you in relying too much on such tests. If you have large datasets, the tests are rather powerful, and may therefore reject the null hypothesis of a Normal distribution when there is just a small deviation from Normality in the data. Such small deviations might be rather inconsequential, so you should use your judgement and also rely on visual inspection of histograms and Q-Q plots. For the current data, both the plots and the tests indicate quite some deviation from a Normal distribution… Perhaps there are really two groups of participants: those who know the height of the Mount Everest, and those who don’t. The first group will provide a highly accurate answer (allowing for small deviations, as their knowledge may not be exact), while the second group will guess, providing much more variable judgements which are more likely to be influenced by the anchor. If the judgements of each of these groups can be represented by a Normal distribution, but with different means and variances, then the distribution as a whole might look a bit like the one depicted in Figure 3.9. This is also called a mixture distribution and clearly, it is not a Normal distribution. How bad would it be if this were there case? Figure 3.9: A bimodal distribution, arising when there are two groups of participants: those with good knowledge who provide answers close to the true height of Mount Everest, and those with poor knowledge, providing much more variable judgements. The distribution for each group is depicted by dotted lines, and the distribution over both groups as a solid line. This is where things get a bit messy. One thing to realise is that we are mainly interested in the mean judgement, and whether this equals 8848 meters. We needed to make an assumption about the distribution of the judgements to (1) determine an appropriate estimator of the mean, and (2) derive a sampling distribution of the estimated means. In making inferences about the true mean of the judgements, we only really need to focus on this sampling distribution. And as it happens, an important result in statistics tells us that, no matter what distribution the data itself follows, if we use enough observations, the sampling distribution of the mean will be very close to Normal. This result is called the Central Limit Theorem. 3.7 The Central Limit Theorem A main reason that the Normal distribution is used so often (and perhaps called “Normal”, rather than “Abnormal”), is due to a mathematical fact known as the Central Limit Theorem: Definition 3.1 (Central Limit Theorem) The distribution of the sum of \\(n\\) independent variables approaches the Normal distribution as the number of variables approaches infinity (\\(n \\rightarrow \\infty\\)). This is quite an amazing theorem. Although you might wonder why you should care about sums of an infinite number of random variables, there are two things to note. Firstly, the rate at which the distribution “approaches” the normal distribution can be rather fast. Secondly, as we showed when we discussed two useful properties of the Normal distribution, the sample mean can be viewed as a normalized sum of \\(n\\) variables: \\(\\overline{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i\\). So the Central Limit Theorem applies also to the sample mean, and implies that as long as the size of the samples is large enough, the sample mean will follow a Normal distribution. 3.7.1 The Central Limit Theorem in action To see the central limit theorem in action, let’s pick an arbitrary probability distribution over 5 values, as shown in Figure 3.10. This will be the true distribution (i.e. the Data Generating Process) from which we can draw of simulate values. Figure 3.10: An aribitrary probability distribution over 5 values, clearly not a Normal distribution! When we repeatedly draw 5 values and calculate the mean over these 5 observations, we can look at the distribution of these sampled means. This is shown in Figure 3.11. You can see that even when we draw just 5 values from a decidedly non-Normal distribution, the sample distribution of the resulting average already has the characteristic bell-shaped curve. Figure 3.11: The distribution of the mean of 5 observations by simulating 200 data sets (from the distribution in Figure 3.10) From the Central Limit Theorem, we can conclude that if we focus on the mean of a sufficiently large number of independent observations, we can reasonably assume that the sampling distribution of the mean will be a Normal distribution, even when the distribution from which the actual values were drawn is far from Normal. We can also use this to argue that the \\(t\\)-statistic is a valid means to test our hypothesis even when the data is not Normal-distributed. Indeed, simulations show that hypothesis tests with the \\(t\\)-statistic are valid for sufficiently large data sets even when the distribution of the observations is far from Normal (e.g. Lumley, Diehr, Emerson, &amp; Chen, 2002). Finally, the Central Limit Theorem may also be applied to justify the assumption of a Normal distribution, at least for certain variables. For instance, in (cognitive) psychology, we might imagine that processes such as perception and judgement rely on processing and transforming information, and that various processing and transformation steps are subject to many independent sources of noise (e.g., things we might call visual, neural, and memory noise). If these sources of noise combine additively, we could argue that the resulting percept or judgement will follow a Normal distribution. References "],["simple-linear-regression.html", "Chapter 4 Simple linear regression 4.1 Trump, votes, and hate groups 4.2 The model 4.3 Estimation 4.4 Hypothesis testing", " Chapter 4 Simple linear regression In this chapter, we continue our journey into the General Linear Model by extending the very simple model of the previous chapter to include a predictor. The resulting model assumes there is a linear relation between the dependent variable and the predictor, and is also known as a simple linear regression model. We will look at the parameters of the model in detail, and discuss their estimation, as well as testing whether their values are equal to a priori ones of interest. 4.1 Trump, votes, and hate groups Donald Trump is perhaps the most divisive president in American history. The 2016 US elections were mired in controversy. Some reports indicate the number of white nationalist hate groups have gone up by 55% in the Trump era. One study found a strong relation between Trump support and anti-immigrant sentiment, racism, and sexism (Schaffner, Macwilliams, &amp; Nteta, 2018). Paul Ricci collated data about the number of hate groups in the different US states and votes for Trump. A scatterplot of these two variables is provided in Figure 4.1. You can see that states with relatively little hate groups appear to also have relatively little votes for Trump, while states with a relatively large number of hate groups have relatively many votes for Trump. Figure 4.1: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens To assess the relation between Trump votes and the number of hate groups, we will use a linear regression model. As the name suggests, this model involves a line, a straight one in fact. This straight line represents the mean value of the dependent variable \\(Y\\) (i.e. the percentage of Trump votes) for cases which have a particular value for the predictor (or independent) variable \\(X\\) (i.e. the number of hate groups). The model concerns e.g. the mean percentage of Trump votes in all states with 5.55 hate groups per 1 million citizens. The model allows for variation in the actual percentages of Trump votes in all states with 5.55 hate groups per 1 million citizens around the mean. In particular, it assumes that these actual percentages are Normal-distributed. 4.2 The model In the previous chapter, we used a simple statistical model: \\[Y_i \\sim \\mathbf{Normal}(\\mu,\\sigma)\\] This model assumes that each observation is independently drawn from a Normal distribution, with a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\). We can state this model in an equivalent way as: \\[\\begin{equation} Y_i = \\mu + \\epsilon_i \\quad \\quad \\quad \\epsilon_i \\sim \\mathbf{Normal}(0,\\sigma) \\tag{4.1} \\end{equation}\\] In this two-part formulation, the formula on the left side decomposes observation in a structural part (here the mean, \\(\\mu\\)), and a random part that we usually call the error (\\(\\epsilon_i\\)). We don’t know much about this error. We just assume that is is drawn from a Normal distribution with a mean of 0 and standard deviation \\(\\sigma_\\epsilon\\). This is stated in the formula on the right.6 A model like this doesn’t allow for very precise predictions. As the error terms is assumed completely random, it is unpredictable. So if you were asked to predict the value of \\(Y\\), all you could really do is to use \\(\\mu\\) as you prediction. If, however, the dependent variable (e.g. votes for Trump) is related to an independent or predictor variable (e.g. number of hate groups), then we would like to be able to use the predictor to adjust our prediction. Linear regression allows us to to this in a straightforward way. The bivariate regression model is depicted in Figure 4.2. More formally, the model can be defined as follows: \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 \\times X_{i} + \\epsilon_i \\quad \\quad \\quad \\epsilon_i \\sim \\mathbf{Normal}(0,\\sigma_\\epsilon) \\tag{4.2} \\end{equation}\\] Here, \\(Y_i\\) is the value of the \\(i\\)-th observation of the dependent variable (with \\(i = 1, \\ldots, n\\)) and \\(X_i\\) the value of the \\(i\\)-th observation of the predictor variable. These are the observable parts of the data. The model contains three important parameters: The intercept \\(\\beta_0\\), which is the mean of \\(Y\\) when \\(X=0\\). The slope \\(\\beta_1\\), which reflects the increase or decrease in the mean of \\(Y\\) for every 1-unit increase in the predictor \\(X\\). By a 1-unit increase, we simply mean that the value of \\(X\\) goes up by 1, e.g. from \\(X=2\\) to \\(X=3\\), or from \\(X=12.63\\) to \\(X=13.63\\). The standard deviation \\(\\sigma_\\epsilon\\) of the error error or residual terms \\(\\epsilon_i\\). The errors are assumed to be drawn independently from the same Normal distribution, with a mean of 0 and standard deviation \\(\\sigma_\\epsilon\\). Figure 4.2: The simple regression model. A: The intercept is mean of \\(Y\\) when \\(X=0\\) and is point at which the regression line crosses the y-axis. The slope determines the steepness of the regression line and represents the increase (or decrease) in the mean of \\(Y\\) for every 1-unit increase in the predictor \\(X\\). B: the error terms or residuals are the vertical distances of each observed Y-value from the regression line. C: The errors in a regression model are assumed to follow a Normal distribution around the regression line. Remember, unlike variables, parameters are not observable. While we can infer their value from the data, by estimation and/or performing hypothesis tests. But we can never be completely sure that such inferences are correct. Let’s consider the main model formula again: \\[Y_i = \\beta_0 + \\beta_1 \\times X_{i} + \\epsilon_i\\] This formula involves both observable variables (\\(Y\\) and \\(X\\)) and unobservable parameters (\\(\\beta_0\\), \\(\\beta_1\\), and \\(\\epsilon_i\\)). Indeed, the errors \\(\\epsilon_i\\) are really parameters as well, because they cannot be directly observed. There are many of these (one for each observation \\(i = 1, \\ldots, n\\)), and they are not of primary concern. The other part, \\(\\beta_0 + \\beta_1 \\times X_i\\), determines the regression line, representing the mean value of \\(Y\\) for each possible value of \\(X\\). These mean values are conditional means, they are the mean of \\(Y\\), conditional upon a particular value of the predictor \\(X\\). We can use \\(\\mu_{Y|X_{i}}\\) to denote this conditional mean, so \\[\\begin{equation} \\mu_{Y|X_{i}} = \\beta_0 + \\beta_1 \\times X_{i} \\tag{4.3} \\end{equation}\\] Now, plugging the conditional mean into Equation (4.2) we get \\[Y_i = \\mu_{Y|X_{i}} + \\epsilon_i \\quad \\quad \\quad \\epsilon_i \\sim \\mathbf{Normal}(0,\\sigma_\\epsilon)\\] which, in many respects, is very similar to the simple model of Equation (4.1). The key difference is the use of a conditional mean \\(\\mu_{Y|X_{i}}\\) instead of a constant mean \\(\\mu\\). So we can view our new model as an extension of the simple one of the previous chapter, where we now use a straight line to represent the relation between the mean and a predictor variable. 4.3 Estimation The maximum likelihood estimates of the model parameters are \\[\\begin{equation} \\hat{\\beta}_0 = \\overline{Y} - \\hat{\\beta}_1 \\overline{X} \\tag{4.4} \\end{equation}\\] for the intercept, and \\[\\begin{equation} \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sum_{i=1}^n (X_i - \\overline{X})^2} \\tag{4.5} \\end{equation}\\] for the slope. Note that to estimate the intercept, we first need the estimate of the slope. So let’s focus on this one first. The top part of the division (the numerator) contains a sum of deviations of the predictor values (\\(X_i\\)) from its average (\\(\\overline{X}\\)) multiplied by deviations of the values \\(Y_i\\) of the dependent variable from its average (\\(\\overline{Y}\\)). Let’s consider these multiplied deviations a little further. Each deviation is positive (larger than 0) when the value is higher than the average, and negative (smaller than 0), when the value is lower than the average. So the multiplied deviations are positive whenever both values are larger than their average, and whenever both values are below their average (a negative value multiplied by another negative value is positive). If we were to divide the sum of the multiplied deviations by \\(n\\) (the number of observations), we’d get the average of these multiplied deviations. This average is also called the covariance between \\(X\\) and \\(Y\\): \\[\\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{n}\\] Note that, as an estimator, this provides biased estimates of the true covariance. An unbiased estimator of the true covariance is obtained by dividing by \\(n-1\\) instead of \\(n\\) (just as for the variance). Going back to our example, the covariance between trump votes and hate groups would be positive whenever states with higher-than-average Trump votes are generally also states with higher-than-average hate groups, and whenever states with lower-than-average Trump votes are generally also states with lower-than-average hate groups. The covariance would be negative, on the other hand, whenever states with higher-than-average Trump votes are generally states with lower-than-average hate groups, and whenever states with lower-than-average Trump votes are generally states with higher-than-average hate groups. A positive or negative covariance is indicative of a relation between \\(X\\) and \\(Y\\). Indeed, the well-known Pearson correlation coefficient is a standardized covariance, where the standardization scales the correlation to always be between -1 and 1 and involves dividing the covariance by the product of the standard deviations of both variables: \\[\\begin{align} r_{X,Y} &amp;= \\frac{\\text{Cov}(X,Y)}{S_X \\times S_Y} \\\\ &amp;= \\frac{\\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{n}}{\\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n}} \\times \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\overline{Y})^2}{n}}} \\end{align}\\] Going back to the estimate of the slope (Equation (4.5)), we can view this as a different way of standardizing the covariance. Looking at the bottom part of the division (the denominator), we can see it consists of the sum of squared deviations of the predictor values from its mean. If we were to divide this sum by the number of observations, we’d get the variance of \\(X\\). As dividing both the top part (numerator) and bottom part (denominator) in a division by the same value does not affect the outcome of the division (i.e. \\(\\frac{a}{b} = \\frac{a/c}{b/c}\\)), we can choose to divide both by \\(n\\) so the numerator becomes the covariance and the denominator the variance (we could also divide both by \\(n-1\\) so they become unbiased estimators of the covariance and variance). So an alternative way of computing the estimate of the slope is \\[\\hat{\\beta}_1 = \\frac{\\text{Cov}(X,Y)}{S^2_X}\\] Note that the variance of \\(X\\) equals the product of the standard deviation of \\(X\\) and itself, as \\(S^2_X = S_X \\times S_X\\). So the slope estimate looks quite a bit like the correlation coefficient, where instead of the standard deviation of \\(Y\\), we use the standard deviation of \\(X\\) twice. With a little algebraic manipulation, we can also state the slope estimate in terms of the correlation as \\[\\hat{\\beta}_1 = \\frac{S_Y}{S_X} r_{X,Y}\\] The reason for going into these alternative formulations is not to pain or confuse you with lots of equations. What these alternative formulations show is that the slope tells us something about the relation between \\(X\\) and \\(Y\\), just like the covariance and correlation do. If the sample correlation is 0, then so is the estimated slope. It is important to realise that we have been discussing the estimate of the slope, not the true value itself. But the same relations hold for the true values. If we denote the true correlation as \\(\\rho_{X,Y}\\), then the true value of the slope can be defined as \\[\\beta_1 = \\frac{\\sigma_Y}{\\sigma_X} \\rho_{X,Y}\\] The true value of the slope is 0 when the true correlation between \\(X\\) and \\(Y\\) equals \\(\\rho_{X,Y} = 0\\). It would also be 0 if the true standard deviation of \\(Y\\) equals \\(\\sigma_Y = 0\\), but this implies that \\(Y\\) is a constant and that is not a very interesting situation. That was perhaps a little tortuous, and we haven’t even discussed the estimate of the intercept! Remember that the intercept represents the mean value of \\(Y\\) for all cases where \\(X=0\\). This average is often not so interesting itself, although in our example, we might be interested in what the average percentage of votes for Trump would be in the absence of any hate groups. Equation (4.4) shows that we can estimate this value by adjusting the sample average \\(\\overline{Y}\\) by subtracting \\(\\hat{\\beta}_1 \\times \\overline{X}\\) from it. How come? Well, it can be shown that the regression line always passes through the point \\((\\overline{X},\\overline{Y})\\). The derivation for showing that this has to be the case is not that interesting, so you’ll just have to trust me, or look it up elsewhere. But this implies that \\[\\overline{Y} = \\beta_0 + \\beta_1 \\times \\overline{X}\\] and then we can simply subtract \\(\\beta_1 \\times \\overline{X}\\) from both sides to get \\(\\overline{Y} - \\beta_1 \\times \\overline{X} = \\beta_0\\). Finally, we can also estimate \\(\\sigma_\\epsilon\\), the standard deviation of the error. As usual for variances and standard deviations, the maximum likelihood estimate is biased, so we’ll focus on an unbiased estimator. Like before, an estimate of the variance is computed from a sum of squared deviations from an estimated mean. In this case, we need to use the estimated conditional means \\[\\hat{\\mu}_{Y|X_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times X_i\\] to compute the following estimate of the error variance: \\[\\begin{equation} \\hat{\\sigma}^2_\\epsilon = \\frac{\\sum_{i=1}^n (Y_i - \\hat{\\mu}_{Y|X_i})^2}{n-2} \\tag{4.6} \\end{equation}\\] Note that we are dividing by \\(n-2\\) here, rather than by \\(n-1\\) as we did when estimating the variance of \\(Y\\) in a model without predictors, The reason for this is that we are now using two noisy parameter estimates (i.e. \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)), rather than just one. As usual, to get the estimate of \\(\\hat{\\sigma}_\\epsilon\\), we can just take the square root of \\(\\hat{\\sigma}^2_\\epsilon\\). 4.3.1 Estimating the relation between Trump votes and hate groups So, what are the estimates of the model predicting Trump votes by hate groups? We can calculate the estimates relatively easily by first computing the following sample quantities of location, scale, and covariation \\(\\overline{\\mathtt{hate}}\\) \\(\\overline{\\mathtt{votes}}\\) \\(S^2_\\mathtt{hate}\\) \\(S^2_\\mathtt{votes}\\) \\(\\text{Cov}(\\mathtt{hate},\\mathtt{votes})\\) 3.03 49.9 3.71 99.8 8.53 The estimate of the slope is then \\[\\hat{\\beta}_1 = \\frac{8.527}{3.707} = 2.3\\] and the estimate of the intercept is \\[\\hat{\\beta}_0 = 49.862 - 2.3 \\times 3.028 = 42.897\\] Of course, we would not normally bother with calculating these estimates “hy hand”, but rather rely on statistical software such as R or JASP to calculate these for us. In any case, the estimated model becomes: \\[\\texttt{votes}_i = 42.897 + 2.3 \\times \\texttt{hate} + e_i\\] According to this model, the average percentage of Trump votes in states without any hate groups is 42.897. For every one additional hate group (per million citizens), the percentage of Trump votes increases by 2.3. You can view the resulting regression line (the conditional means of votes for Trump as predicted by the model) in Figure 4.3. Figure 4.3: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens with the estimated regression line. 4.4 Hypothesis testing The estimated model indicates a positive relation between the number of hate groups and votes for Donald Trump. But the slope is an estimated parameter. It might be that, in reality, there is no relation between hate groups and Trump votes. But because we have limited and noisy data, sometimes the estimated slope might be positive, and other times it might be negative. So the question is whether we have enough evidence to reject the null hypothesis that \\(\\beta_1 = 0\\). As before, there are two main ways in which too look at testing whether the parameters of the model are equal to a priori values. The first is to consider how variable the parameter estimates are under the assumption that the true parameter is identical to the a priori value. The second way is to compare two models, one in which the parameter is fixed to the a priori value, and one where it is freely estimated. Both of these ways will provide us with the same outcome. The model comparison approach is more flexible, however, as it also allows you to test multiple parameters simultaneously. We’ll start with discussing the first method, but it will be the last time until a while. After that, we’ll focus on model comparison throughout. 4.4.1 Sampling distribution of estimates Remember, an estimate (whether of the mean or another parameter) is a noisy reflection of the true value of that parameter. The noise comes from having access only to limited data, not all the data the data generating process can produce. Suppose in reality there is no relation between hate groups and Trump votes, so that the true slope is \\(\\beta_1 = 0\\). In that case, the model becomes \\[\\begin{align} Y_i &amp;= \\beta_0 + 0 \\times X_i + \\epsilon \\\\ &amp;= \\beta_0 + \\epsilon_i \\end{align}\\] which is identical to the simple model in Equation, renaming \\(\\beta_0 = \\mu\\) and \\(\\sigma_\\epsilon = \\sigma\\). Then, the true value of the intercept would be \\(\\beta_0 = \\mu\\). If we’d know the true value of the standard deviation \\(\\sigma_\\epsilon\\), we’d have a fully specified model (a Normal distribution) which we can use to generate as many alternative data sets as we’d like. By generating these data sets, and estimating the slope of our model for each, we can get an overview of the variability of the estimates when the true slope equals \\(\\beta_1 = 0\\). An unbiased estimator ensures that on average, these estimates equal the true value. The main thing of interest is then the variability around this value. A consistent estimator ensures that this variability becomes smaller with the larger data sets. But to understand how well we can estimate the parameter for the present data, we would simulate data sets with the same number of observations (so \\(n=50\\)). Unfortunately, we don’t know the true value of \\(\\sigma\\). Our data provides an estimate of \\(\\sigma\\), but we know this estimate is noisy itself. Thinking first in the same way about the sampling distribution of \\(\\hat{\\sigma}\\), we could first sample a values of \\(\\sigma\\), and then use each of these to generate a data set for which to estimate \\(\\beta_1\\). Doing this many (many many!) times would give us a good overview of the variability of the estimates. As before, we don’t have to actually simulate the data sets. If the null model is true, then we can derive that the sampling distribution of the estimates follows a t-distribution. Thus, we should look for the \\(t\\) value our data provides, and evaluate this value within the context of the sampling distribution derived under the model where \\(\\beta_1 = 0\\). For both parameters (intercept and slope), the same logic applies. In general then, for parameters \\(\\beta_j\\) (where \\(j = 0\\) or 1), the \\(t\\)-value of our data is computed as \\[\\begin{equation} t = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)} \\quad \\quad \\quad t \\sim \\mathbf{T}(n-2) \\end{equation}\\] where \\(\\mathbf{T}(n-1)\\) denotes a standard Student t-distribution with \\(n-2\\) degrees of freedom, and \\(\\text{SE}(\\hat{\\beta}_j)\\) is the standard error of the estimate, which you should remember is the standard deviation of the sampling distribution of the estimates. I won’t bore you with how to compute this standard error; statistical software does a good job at this. One thing to realise though is that the computed standard error is valid for datasets with exactly the same values of the predictor. In other words, it assumes the predictor values are fixed (a part of the Data Generating Process). If you’d collect a different dataset with different values for e.g. the number of hate groups, the standard error would also be different. R would for instance provide the following results for this regression model: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 42.9 2.409 17.80 0.000 hate_groups_per_million 2.3 0.672 3.43 0.001 The values listed for (intercept) concern \\(\\beta_0\\), and the values listed for hate_groups_per_million concern the slope \\(\\beta_1\\). You can see the estimates are identical to those computed earlier (phew, no mistakes there! :-). You can see that the standard errors for the two parameters are quite different in magnitude. This is not so surprising, as they reflect quite different things, the intercept being a particular value of the dependent variable, while the slope represents an increase in the dependent variable for an increase in the predictor. If you’d change the scale of the predictor (e.g., from hate groups per million to hate groups per 10,000 citizens), the slope would change, as well as the corresponding standard error. Changing the scale in this way would not affect the standard error of the intercept. If you divide each estimate by the standard error, you obtain the value of the \\(t\\)-statistic (just like we did earlier in Equation (3.5)). In a manner analogous to that depicted in Figure 3.6, you then determine critical values for the t-statistic (based on the degrees of freedom and the significance level \\(\\alpha\\)), and determine whether the value you computed for the parameters lies within the critical range. If it does, the test result is called significant, and you reject the null hypothesis. If not, then the result is called insignificant, and you don’t reject the null hypothesis. Now that these are easy to compute with statistical software, it is more common to check the \\(p\\)-values. Remember, the \\(p\\)-value is the probability of the computed \\(t\\)-statistic or a more extreme value, assuming the null hypothesis is true. If the \\(p\\)-value is smaller than the chosen significance level (e.g. \\(\\alpha = .05\\)), that means that the test is significant, and the null hypothesis rejected. In the results above, you can see that both p-values are below \\(\\alpha = .05\\). Hence, we can reject the null hypothesis that \\(\\beta_0 = 0\\) and the null hypothesis that \\(\\beta_1 = 0\\). In other words, there is good evidence that, in the absence of hate groups, the percentage of people voting for Trump is not equal to 0.7 Also, there is good evidence that there is a relation between the number of hate groups and Trump votes. Personally, I find the \\(t\\) statistic quite intuitive in the context of a one-sample t-test. Generalizing the concept to a standardized estimate (dividing the estimate by its standard error) is also reasonably intuitive. However, in a multi-parameter model such as here, the sampling distribution of a single parameter is dependent on the estimation of all the other parameters. For instance, the test of the null hypothesis \\(H_0: \\beta_1 = 0\\) (i.e., no effect of hate groups on Trump votes) is based on deriving the sampling distribution of \\(\\hat{\\beta}_1\\) in a model where \\(\\beta_1 = 0\\), but all the other parameters (i.e. \\(\\beta_0\\) and \\(\\sigma_\\epsilon\\)) cannot be assumed known). This uncertainty is dealt with similarly as before, resulting in a t-distribution, but now there are two sources of uncertainty (two parameters to estimate). Hence, the degrees of freedom are \\(n-2\\) here. 4.4.2 Model comparison The fact that, in a multi-parameter model, a test of one parameter is not conducted in isolation, but rather in the context of all the other parameters in the model, is more explicit in the model comparison approach. When we compare two models, we have to make clear what the parameters are in each: what are the unknown quantities which we will have to estimate, and what are quantities which we can assume a precise value for? As before, we will consider comparing nested models, in which a restricted MODEL R is a special case of a more general MODEL G. Sticking to simple linear regression models, the most general model we have is that of Equation (4.2). As for the simple model of the previous chapter, it turns out we can write the likelihood for a model as a function of a sum of squared deviations. We will not go through the derivation of this again, as it is rather similar and equally tedious. But, because we now have been calling such deviations errors, will start to refer to as a Sum of Squared Errors (SSE). Let’s call the model of Equation (4.2) MODEL G. Before we go on, I want to warn you that whatever we call MODEL G, and whatever we call MODEL R, can change from situation to situation. Basically, the identity of MODEL G and MODEL R are “local” to the particular model comparison. You can think of MODEL G as a parent, and MODEL R as a child. While the relation between them is similar, within an extended family, someone can be both a parent to one family member, and be the child of another. This is the form of flexibility that you will need when thinking about nested models. One model can be both more general than one model, and be more restricted than another. I will come back to this soon. First, let’s consider what the Sum of Squared Error of a model is. The easiest way to define these is in terms of the predictions of each model. Recall that a regression model has a structural and random part. The structural part defines the conditional mean of the dependent variable, while the random part concerns the random variation of the actual observations around the conditional means. As the random part is by definition unpredictable, there is not much we can do with that in terms of forming predictions. So we’re stuck with the structural part. Although not a universal principle, there are many situations in which it makes sense to predict outcomes by the (conditional) mean. If we do this, then we can predict the outcomes with MODEL G as \\[\\hat{Y}_{G,i} = \\hat{\\beta}_{G,0} + \\hat{\\beta}_{G,1} \\times X_i\\] where \\(\\hat{Y}\\) stands for a predicted value. We’re using the same “hat” for this as an estimated value, because it is really also a good estimate of what the value of \\(Y_i\\) might have been if it was another observation with the same value for \\(X\\). We are also assigning the subscript “G” to all the estimates, to distinguish them from those of a different model. Now let’s consider a MODEL R in which we assume that there is no relation between Trump votes and hate groups, so that \\(\\beta_1 = 0\\). The predictions for this model would be \\[\\hat{Y}_{R,i} = \\hat{\\beta}_{R,0} + 0 \\times X_i = \\hat{\\beta}_{R,0}\\] Now we have two models to make predictions, we can write the corresponding Sum of Squared Error of each as: \\[\\begin{equation} \\text{SSE}(M) = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_{M,i} \\right)^2 \\tag{4.7} \\end{equation}\\] where we can replace the general letter \\(M\\) (for Model) with either \\(G\\) of \\(R\\), to get \\(\\text{SSE}(G)\\) or \\(\\text{SSE}(R)\\), respectively. So the Sum of Squares is based on a difference between each observation and the model prediction for that observation. These are thus the prediction errors. If the prediction was equal to the true model (e.g. \\(\\beta_0 + \\beta_1 X_{i}\\)), then they would be the error terms \\(\\epsilon_i\\). But because we only have an estimated model, they are effectively estimates of the true errors. You can see these errors for the two models in Figure 4.4. Figure 4.4: Estimated regression lines for MODEL G (left) and MODEL R (right) with \\(\\beta_1 = 0\\) and the errors. When you compare the errors between the models, you can see that MODEL G does not provide a better prediction for each observation. Sometimes the distance from an observation to the regression line is larger for MODEL G than for MODEL R. However, MODEL G does seem to provide a better prediction for most observations. This is unsurprising, as MODEL G, by maximising the likelihood, should be at least as good as MODEL R. This means that it is always the case that \\[\\text{SSE}(R) \\geq \\text{SSE}(G)\\] The Sum of Squared Errors of MODEL G will never be higher than that of MODEL R. So we cannot just select the model with the lowest SSE, as this would mean we’d always select MODEL G. We need to find a way to determine whether the SSE of MODEL G is sufficiently lower than that of MODEL R to make us believe that MODEL G is indeed superior to MODEL R. In the previous chapter, we discussed that the \\(t\\) statistic can be used to perform a test which is equivalent to the likelihood ratio test, and that this was useful because the sampling distribution of the \\(t\\) statistic is known, while the sampling distribution of the likelihood ratio is difficult to determine. For the case of linear models, there is a similar argument that leads to a new statistic, which can be viewed as a generalization of the \\(t\\) statistic. This is the \\(F\\) statistic. If was given letter in honour of Sir Ronald A. Fisher, a rather brilliant and very influential statistician, who developed it in the 1920s. For comparing two linear models, where MODEL R is a special case of MODEL G, we can define the \\(F\\) statistic as: \\[\\begin{equation} F = \\frac{\\frac{\\text{SSE}(R) - \\text{SSE}(G)}{\\text{npar}(G) - \\text{npar}(R)}}{\\frac{\\text{SSE}(G)}{n-\\text{npar(G)}}} \\tag{4.8} \\end{equation}\\] Here, \\(\\text{npar}(G)\\) stands for the number of parameters which are estimated in MODEL G, and \\(\\text{npar}(R)\\) for the number of parameters which are estimated in MODEL R. In counting the number of estimated parameters, we are excluding the error variance \\(\\sigma^2_\\epsilon\\). Although this is really also a parameter of the models, it is generally not of direct interest. It is a so-called “nuisance parameter”, something that we need to take into account, but we’d rather forget about. In the present example, MODEL G then has two estimated parameters: \\(\\hat{\\beta}_{G,0}\\) and \\(\\hat{\\beta}_{G,1}\\), while MODEL R has just one: \\(\\hat{\\beta}_{R,0}\\). If we compute the SSE for each model (i.e. taking the vertical lines to the regression line in Figure 4.4 and squaring them), we get \\(\\text{SSE}(G) = 4011.398\\) and \\(\\text{SSE}(R) = 4992.178\\). \\[\\begin{align} F &amp;= \\frac{\\frac{4992.178 - 4011.398}{2 - 1}}{\\frac{4011.398}{50 - 2}} \\\\ &amp;= \\frac{980.779}{83.571} \\\\ &amp;= 11.736 \\end{align}\\] If MODEL R is true, the sampling distribution of the \\(F\\) statistic follows an \\(F\\) distribution. This distribution has two parameters, both degrees of freedom, which we’ll refer to as \\(\\text{df}_1\\) and \\(\\text{df}_2\\). The first one reflects the difference in the number of estimated parameters between the models \\[\\text{df}_1 = \\text{npar}(G) - \\text{npar}(R)\\] and the second one the number of observations minus the number of estimated parameters of the more general model \\[\\text{df}_2 = n - \\text{npar}(G)\\] Figure 4.5: The F distribution with \\(\\text{df}_1 = 1\\) and \\(\\text{df}_2 = 48\\), and the critical value for \\(\\alpha = .05\\) Note that the value of the \\(F\\) statistic can never be negative; the distribution is thus defined only over positive values of \\(F\\). High values of \\(F\\) indicate that the MODEL G has substantially less error than MODEL R. Another thing to note is that, whenever \\(\\text{df}_1 = 1\\), there is a direct relation between the \\(F\\) statistic and the \\(t\\) statistic: \\(F = t^2\\), or conversely, \\(\\sqrt{F} = t\\). It is easy to check that this is indeed the case here: \\(\\sqrt{11.736} = 3.426\\), which is the value of the \\(t\\) statistic we computed earlier. The critical value of the \\(F\\) statistic, with \\(\\alpha = .05\\) and \\(\\text{df}_1 = 2-1 = 1\\) and \\(\\text{df}_2 = 50 - 2 = 48\\), is 4.043, and any value of \\(F\\) above this critical value would result in a rejection of the null hypothesis. So, because the \\(F\\) value we computed was 11.736, which is larger than the critical value, we reject the null hypothesis \\(H_0: \\beta_1 = 0\\), and conclude that there is evidence of a relation between the number of hate groups and Trump votes. Instead of checking whether the \\(F\\) value is larger than a critical value, we can also compute the p-value, which is the probability of an \\(F\\) value equal to or larger than 11.736 in the distribution depicted in Figure , More formally, the p-value can be defined as \\(P(F \\geq 11.736 | \\text{df}_1 = 1, \\text{df}_2 = 48) = 0.00127\\). As this probability is smaller than the significance level, this again implies we reject the null hypothesis. A nice thing about the \\(F\\) statistic is that it is general, and can be used to compare any nested linear models. For instance, we can also compare MODEL G to a different MODEL R in which we assume the intercept equals \\(\\beta_0 = 0\\), whilst allowing the slope to take any value. This model forces the regression line to go through the (0,0) point. Estimating this MODEL R gives: \\[\\begin{align} Y_i &amp;= 0 + \\hat{\\beta}_{R,1} \\times X_i \\\\ &amp;= 12.389 \\end{align}\\] You can see the resulting regression line and error terms in Figure 4.6. Figure 4.6: Estimated regression lines for MODEL G (left) and MODEL R (right) with \\(\\beta_1 = 0\\) and the errors. This alternative MODEL R seems clearly inferior to MODEL G. The SSE of this model is \\(\\text{SSE}(R) = 3.05\\times 10^{4}\\). Computing the \\(F\\) statistic gives \\[ \\begin{align} F &amp;= \\frac{\\frac{3.05\\times 10^{4} - 4011.398}{2 - 1}}{\\frac{4011.398}{50 - 2}} \\\\ &amp;= \\frac{2.649\\times 10^{4}}{83.571} \\\\ &amp;= 316.981 \\end{align} \\] Because the test involves the same degrees of freedom, the critical value is the same as before. So we reject the null hypothesis again, which here is \\(H_0: \\beta_0 = 0\\). The p-value now is \\(P(F \\geq 316.981 | \\text{df}_1 = 1, \\text{df}_2 = 48) = &lt;.0001.\\) References "],["multiple-regression.html", "Chapter 5 Multiple regression 5.1 Trump, votes, and hate groups (again) 5.2 The multiple regression model 5.3 Estimation 5.4 Inference 5.5 Partitioning and explaining variance 5.6 Assumptions 5.7 Multicollinearity 5.8 Outliers", " Chapter 5 Multiple regression In this chapter, we extend the simple regression model to include multiple predictor variables. The slopes of the predictors in the resulting multiple regression model reflect the unique effect of each predictor on the dependent variable, after removing the effect of all other predictors from both the dependent and predictor variable. We will look at what that means in some detail. We then go on to discuss parameter estimation and testing. 5.1 Trump, votes, and hate groups (again) In the previous chapter, we found evidence for a relation between the number of hate groups and votes for Trump. This relation was found with an observational study, not an experimental one. Because the number of hate groups was not randomly assigned to states, there are possible confounding factors that could account for the result. The well-known phrase “correlation does not imply causation” should be taken seriously. For instance, it could be that hate groups are especially prevalent in places with relatively low levels of education. And it might be that those with a relatively low level of education are also more likely to vote for Trump. In other words, education level might be a common cause of both hate groups and Trump votes, whilst there is no direct relation between hate groups and Trump votes. The difference between a direct relation, and a spurious one through a common cause, is depicted in Figure 5.1. You can see more, often rather amusing examples of spurious relations on the spurious correlation website. Figure 5.1: A direct relation between hate groups and Trump votes vs a spurious relation through a common cause (education). To assess the relation between hate groups and Trump votes in the possible presence of common causes and other confounds, ideally, we would like to “remove” the effect of such confounds from both the “hate groups” predictor, and the “Trump votes” dependent variable. If, after statistically controlling for confounds in such a way, we still find evidence for a relation between hate groups and Trump votes, that would strengthen our belief that the relation is real, rather than spurious. Multiple regression is a way in which to determine unique effects of predictor variables on the dependent variable. Before we move on to define the multiple regression model, we will work through an indirect way to control for a third variable (e.g. a possible common cause), which will provide insight into what is meant by unique effects in a multiple regression model. 5.1.1 Controlling for education level We have indicated that, potentially, the level of education might be a common cause for both the prevalence of hate groups, and voter support for Trump. What if we could assess the relation between hate groups and Trump votes after removing the effect of education level from both? One reasonable way to do this, using the tools we already have, is by means of simple regression models. The idea is reasonably straightforward. In a regression model predicting Trump votes from education level, the error terms (residuals) of that model reflect that part of the variation in Trump votes which can not be predicted from education level. Similarly, in a regression model predicting hate groups from education level, the error terms (residuals) of that model reflect the variation in hate groups which can not be predicted from education level. So, if there is still a relation between the residual Trump vote, and the residual hate groups, then the relation can not be due to education level as a common cause. When we estimate a regression model predicting Trump votes from education level (defined as the percentage of citizens who obtained a Bachelors degree or higher), we obtain the following estimated model \\[\\texttt{Trump_votes}_i = 90.069 - 1.357 \\times \\texttt{education}_i + \\hat{\\epsilon}_{\\texttt{votes},i}\\] and for the hate groups we obtain \\[\\texttt{hate_groups}_i = 6.146 - 0.105 \\times \\texttt{education}_i + \\hat{\\epsilon}_{\\texttt{hate},i}\\] Note that I’m labelling the error terms for both models with a subscript (either \\(\\texttt{votes}\\) or \\(\\texttt{hate}\\)) to denote these are the residual terms for a model predicting Trump votes and hate groups from education levels respectively. Also, I’m adding the hat above them to indicate that these are not the “true errors” (from a model with the true values of the slope and intercept) but rather estimates of these resulting from using estimated parameters. The residuals, as well as the estimated regression lines, are depicted in in Figure 5.2. The slope of \\(\\texttt{education}_i\\) is significant in both models. In the model for Trump votes, \\(\\hat{\\beta}_\\texttt{education} = -1.357\\), \\(t(48) = -7.30\\), \\(p &lt; .001\\). In the model for hate groups, \\(\\hat{\\beta}_\\texttt{education} = -0.105\\), \\(t(48) = -2.11\\), \\(p = .040\\). This thus shows a reliable relation between education level and Trump votes, and between education level and hate groups, which is supportive of the idea of a common cause. However, significance is not a prerequisite to statistically control for a third variable. Figure 5.2: Predicting Trump votes and hate groups by linear regression models with education level as predictor. The residuals of each model are depicted as vertical lines from the observations to the regression line. The regression line in both models represents the variation in the dependent variable (Trump votes and hate groups) that is dependent on education level. In both models, the error term \\(\\epsilon\\) is assumed to be completely independent from the predictor. Hence, these are the aspects of Trump votes and hate groups that cannot be explained by education level. To assess whether there is a relation between these residual terms, which would indicate a relation between Trump votes and hate groups which is independent from education level, we can estimate another linear regression model \\[ \\begin{equation} \\hat{\\epsilon}_{\\texttt{votes},i} = 0 + 1.314 \\times \\hat{\\epsilon}_{\\texttt{hate},i} + \\hat{\\epsilon}_i \\tag{5.1} \\end{equation} \\] Note that the intercept is equal to 0. This is necessarily the case, because the residuals of estimated regression models always have a mean of 0. If you go back to the estimate of the intercept in Equation (4.4), you will see that if the means are both 0, the result has to be 0 as well: \\[\\begin{align} \\hat{\\beta}_0 &amp;= \\overline{Y} - \\hat{\\beta}_1 \\overline{X} \\\\ &amp;= 0 - \\hat{\\beta}_1 \\times 0 \\\\ &amp;= 0 \\end{align}\\] A scatterplot of the dependent variable (the residual Trump votes \\(\\hat{\\epsilon}_{\\texttt{votes},i}\\)) and the predictor (the residual hate groups \\(\\hat{\\epsilon}_{\\texttt{hate},i}\\)), together with the estimated regression line, is provided in Figure 5.3. Figure 5.3: Residual Trump votes and hate groups, after statistically removing the effect of education level from both, and the estimated regression line. We can see that there appears to be a positive relation between residual Trump votes and residual hate groups. If we perform a test of the null hypothesis \\(H_0: \\beta_1 = 0\\) in this model, we find that the result is significant, \\(t(48) = 2.60\\), \\(p = .012\\). This thus indicates that there is a relation between Trump votes and hate groups that can not be attributed to education level. While the procedure of removing the effect of education level from both Trump votes and hate groups is appropriate, there however is a slight issue with the hypothesis test we just performed. To remove the effect of education level, we first had to estimate two new models. These estimates are noisy, and this noise may affect the estimates of the third model in which we predicted the residual Trump votes from the residual hate groups. The hypothesis test does not take this additional source of noise into account. It turns out that we do not need to estimate separate models to remove the effect of education level. By including both education level and hate groups as predictors of Trump votes in a multiple regression model, the estimated slope of hate groups will be exactly equal to the one that we just computed from the residuals. Multiple regression models thus concern the unique effects of each predictor on the dependent variable, removing the effect of all other predictors from that relation. 5.2 The multiple regression model The multiple regression model is a straightforward extension of the simple linear regression model, including more than one predictor \\(X\\): \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 \\times X_{1,i} + \\beta_2 \\times X_{2,i} + \\ldots + \\beta_k \\times X_{m,i} + \\epsilon_i \\quad \\quad \\quad \\epsilon_i \\sim \\mathbf{Normal}(0,\\sigma_\\epsilon) \\tag{5.2} \\end{equation}\\] Note that we are using \\(m\\) to reflect the total number of predictors \\(X\\) in the model. As for the simple regression model, this model consists of a structural part which reflects the conditional mean of the dependent variable \\(Y\\), conditional upon all predictors: \\[\\beta_0 + \\beta_1 \\times X_{1,i} + \\beta_2 \\times X_{2,i} + \\ldots + \\beta_k \\times X_{m,i} = \\mu_{Y|X_1,\\ldots,X_m}\\] and a random part \\(\\epsilon_i\\). For a model with \\(m=2\\) predictors, such as \\[\\begin{equation} \\texttt{trump_votes}_i = \\beta_0 + \\beta_1 \\times \\texttt{hate_groups}_i + \\beta_2 \\times \\texttt{education}_i + \\epsilon_i \\quad \\quad \\epsilon_i \\sim \\mathbf{Normal}(0,\\sigma_\\epsilon) \\tag{5.3} \\end{equation}\\] the data can be represented in a three-dimensional space. In this space, the conditional means \\(\\mu_{Y | X_{1,i}, X_{2,i}}\\) can be represented as a regression plane. A visual representation is given below. Figure 5.4: Three-dimensional representation of a regression model predicting Trump votes from hate groups and education level. By clicking on the image and moving your mouse, you should be able to rotate the image to explore it further. You can also click on the ‘Play’ button for an animation of such a rotation. Keeping all predictors constant apart from one, the model becomes a simple linear regression model for the non-constant variable. If you pick a value of “% bachelors degree or higher”, you can think of this as slicing the regression plane at that point. The slice of the plane is a single straight line, which is then a simple regression model. For example, if we focus on states with an education level of 15.4% with a bachelors degree or higher, then the multiple regression model can be written as \\[\\begin{align} \\texttt{trump_votes}_i &amp;= \\beta_0 + \\beta_1 \\times \\texttt{hate_groups}_i + \\beta_2 \\times 15.4 + \\epsilon_i \\\\ &amp;= (\\beta_0 + \\beta_2 \\times 15.4) + \\beta_1 \\times \\texttt{hate_groups}_i + \\epsilon_i \\\\ &amp;= \\beta_0&#39; + \\beta_1 \\times \\texttt{hate_groups}_i + \\epsilon_i \\end{align} \\] which is a simple regression model with a new intercept \\(\\beta_0&#39; = \\beta_0 + \\beta_2 \\times 15.4\\) composed as the sum of the original intercept and 15.4 times the slope of \\(\\texttt{education_level}\\), and the same slope \\(\\beta_1\\) for \\(\\texttt{hate_groups}\\). You can see these simple regression lines, for three different values of education level, in Figure 5.5. Notice that the regression lines for different education levels are parallel, because they have the same slope. However, the regression lines are at different heights, because they have a different intercept. Figure 5.5: Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens, with regression lines for three different values of education level. The colour of each data point and regression line reflects the value of education level. 5.3 Estimation The estimate of the intercept is a direct generalization of that for a simple regression model: \\[\\begin{equation} \\hat{\\beta}_0 = \\overline{Y} - \\hat{\\beta}_1 \\times \\overline{X}_1 - \\hat{\\beta}_2 \\times \\overline{X}_2 - \\ldots - \\hat{\\beta}_m \\times \\overline{X}_m \\tag{5.4} \\end{equation}\\] Again, it is an adjustment of the mean of the dependent variable, subtracting means multiplied by slopes of each predictor variable. The estimates of the slopes are straightforward to compute using matrix algebra.8 However, we won’t cover matrix algebra here, so we will leave the estimation of the slopes to statistical software. The (unbiased) estimate of the error variance is also a direct generalization of the estimate for a simple regression model: \\[\\begin{equation} \\hat{\\sigma}^2_\\epsilon = \\frac{\\sum_{i=1}^n (Y_i - \\hat{\\mu}_{Y|X_1, \\ldots, X_m})^2}{n-\\text{npar}(M)} \\tag{5.5} \\end{equation}\\] Here, \\(\\text{npar}(M)\\) is the number of parameters in the model, excluding the error variance (i.e., it is the number of \\(\\beta_j\\) parameter, where \\(j=0,\\ldots,m\\). So \\(\\text{npar}(M) = m + 1\\). Estimating the model in which we predict Trump votes from hate groups and education level gives the following estimates: \\[\\texttt{trump_votes}_i = 81.99 + 1.314 \\times \\texttt{hate_groups}_i - 1.219 \\times \\texttt{education}_i + \\hat{\\epsilon}_i \\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, 6.643)\\] A main thing to note here is that the estimated slope of \\(\\texttt{hate_groups}\\) is exactly the same as in Equation (5.1) when first computing the residual Trump votes and hate groups! This is because the slopes in a multiple regression model are unique effects of a predictor on the dependent variable, after removing the effect of all other predictors from both that predictor and the dependent variable. So, equivalently, the slope of \\(\\texttt{education}\\) in the model above is identical to the slope we would obtain by first computing the residual Trump votes from \\[\\texttt{trump_votes}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\texttt{hate_groups}_i + \\hat{\\epsilon}_{\\text{vote},i}\\] and the residual education from \\[\\texttt{education}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\texttt{hate_groups}_i + \\hat{\\epsilon}_{\\text{education},i}\\] and then determining the slope of the residual \\(\\texttt{education}\\) in the model \\[\\hat{\\epsilon}_{\\text{vote},i} + \\beta_0 + \\beta_1 \\times \\hat{\\epsilon}_{\\text{education},i} + \\epsilon_i\\] 5.4 Inference Testing hypotheses regarding the parameters of multiple regression models is analogous to testing the parameters of simple regression models. So one approach us to look at the sampling distribution of the estimates under the null hypothesis and compute the appropriate \\(t\\) statistic. Testing, for each parameter the null hypothesis \\(H_0: \\beta_j = 0\\), where \\(j = 0,\\ldots,2\\), we obtain the results given in 5.1. Table 5.1: Null-hypothesis significance tests using the t statistic. \\(\\hat{\\beta}\\) \\(\\text{SE}(\\hat{\\beta})\\) \\(t\\) \\(P(\\geq \\lvert t \\rvert)\\) Intercept 81.99 6.155 13.32 0.000 Hate groups per million 1.31 0.510 2.58 0.013 % bachelors degree or higher -1.22 0.184 -6.62 0.000 Alternatively, we can compare models in which we either fix the parameter to the value assumed in the null hypothesis or not. As discussed in the previous chapter, this is a more general approach. When comparing two multiple regression models, we use the \\(F\\)-statistic defined in Equation (4.8): \\[F = \\frac{\\frac{\\text{SSE}(R) - \\text{SSE}(G)}{\\text{npar}(G) - \\text{npar}(R)}}{\\frac{\\text{SSE}(G)}{n-\\text{npar(G)}}}\\] For instance, to test the null hypothesis \\(\\beta_1 = 0\\) in the model defined in Equation (5.3), we compare this general model \\[\\text{MODEL G: } \\quad \\texttt{trump_votes}_i = \\beta_0 + \\beta_1 \\times \\texttt{hate_groups}_i + \\beta_2 \\times \\texttt{education}_i + \\epsilon_i\\] to a restricted model where we fix \\(\\beta_1 = 0\\): \\[\\begin{align} \\text{MODEL R: } \\quad \\texttt{trump_votes}_i &amp;= \\beta_0 + 0 \\times \\texttt{hate_groups}_i + \\beta_2 \\times \\texttt{education}_i + \\epsilon_i \\\\ &amp;= \\beta_0 + \\beta_1 \\times \\texttt{education}_i + \\epsilon_i \\end{align} \\] In other words, we would compare the multiple regression model to a simple regression model where we don’t include \\(\\texttt{hate_groups}\\), as the null hypothesis assumes that this predictor has no unique effect on the dependent variable. If the Sum of Squared Error of MODEL R is not substantially higher than that of MODEL G, then there would be no good evidence that \\(\\beta_1 \\neq 0\\). Remember that the Sum of Squared Error of MODEL R can never be lower than that of MODEL G, i.e. \\(\\text{SSE}(R) \\geq \\text{SSE}(G)\\). This is because estimating the parameters of MODEL G by maximum likelihood is equivalent to minimizing the Sum of Squared Error. If \\(\\hat{\\beta}_1 = 0\\) in MODEL G, that would mean that the SSE of both models is exactly the same. If \\(\\hat{\\beta}_1 \\neq 0\\) in MODEL G, then \\(\\text{SSE}(R) &gt; \\text{SSE}(G)\\), because otherwise the estimated MODEL G would not have maximised the likelihood (or equivalently minimised the SSE). We have already shown results for MODEL G. The Sum of Squared Error for that model is \\(\\text{SSE}(G) = 2074.202\\). Estimating MODEL R gives \\[\\texttt{trump_votes}_i = 90.07 - 1.357 \\times \\texttt{education}_i + \\hat{\\epsilon}_i \\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, 7.022)\\] with \\(\\text{SSE}(R) = 2366.875\\). The \\(F\\) statistic then becomes: \\[\\begin{align} F &amp;= \\frac{\\frac{2366.875 - 2074.202}{3 - 2}}{\\frac{2074.202}{50 - 3}} \\\\ &amp;= \\frac{292.673}{44.132} \\\\ &amp;= 6.632 \\end{align}\\] To determine whether this value is significant, we need to compare it to an \\(F\\)-distribution with \\(\\text{df}_1 = 3-2 = 1\\) and \\(\\text{df}_2 = 50 - 3 = 47\\) degrees of freedom. For this distribution and a significance level of \\(\\alpha = .05\\), the critical value is 4.047, which is lower than the computed \\(F\\) value, and hence the result is significant and we reject the null hypothesis. Similarly, to test the null-hypothesis \\(\\beta_2 = 0\\) in MODEL G as defined above, we compare this model to a new restricted MODEL R where we fix \\(\\beta_2 = 0\\): \\[ \\begin{align} \\text{MODEL R: } \\quad \\texttt{trump_votes}_i &amp;= \\beta_0 + \\beta_1 \\times \\texttt{hate_groups}_i + 0 \\times \\texttt{education}_i + \\epsilon_i \\\\ &amp;= \\beta_0 + \\beta_1 \\times \\texttt{hate_groups}_i + \\epsilon_i \\end{align} \\] This is the model we estimated in the previous chapter, where we determined the estimates as \\[\\texttt{trump_votes}_i = 42.9 + 2.3 \\times \\texttt{hate_groups}_i + \\hat{\\epsilon}_i \\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, 9.142)\\] with an associated \\(\\text{SSE}(R) = 4011.398\\). The \\(F\\) statistic is then computed as \\[\\begin{align} F &amp;= \\frac{\\frac{4011.398 - 2074.202}{3 - 2}}{\\frac{2074.202}{50 - 3}} \\\\ &amp;= \\frac{1937.197}{44.132} \\\\ &amp;= 43.896 \\end{align}\\] The critical value is the same as before, and hence we also reject this null hypothesis. To test whether the intercept \\(\\beta_0 = 0\\), we would compare MODEL G to yet another MODEL R: \\[ \\begin{align} \\text{MODEL R: } \\quad \\texttt{trump_votes}_i &amp;= 0 + \\beta_1 \\times \\texttt{hate_groups}_i + \\beta_2 \\times \\texttt{education}_i + \\epsilon_i \\\\ &amp;=\\beta_1 \\times \\texttt{hate_groups}_i + \\beta_1 \\times \\texttt{education}_i + \\epsilon_i \\end{align} \\] which is estimated as \\[\\texttt{trump_votes}_i = 4.776 \\times \\texttt{hate_groups}_i + 1.13 \\times \\texttt{education}_i + \\hat{\\epsilon}_i \\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, 14.36)\\] with \\(\\text{SSE}(R) = 24.084\\). The \\(F\\) statistic is computed in the same way as before: \\[\\begin{align} F &amp;= \\frac{\\frac{9904.891 - 2074.202}{3 - 2}}{\\frac{2074.202}{50 - 3}} \\\\ &amp;= \\frac{7830.689}{44.132} \\\\ &amp;= 177.438 \\end{align}\\] The results of all three tests are collected in Table 5.2. Table 5.2: Null-hypothesis significance tests by model comparisons and the F statistic. \\(\\hat{\\beta}\\) \\(\\text{SS}\\) \\(\\text{df}\\) \\(F\\) \\(P(\\geq \\lvert F \\rvert)\\) Intercept 81.99 7831 1 177.44 0.000 Hate groups per million 1.31 293 1 6.63 0.013 % bachelors degree or higher -1.22 1937 1 43.90 0.000 Error 2074 47 When performing such model comparisons, it is very important to remember that what we call MODEL G and MODEL R will depend on the null hypothesis tested. Generally, MODEL G will remain the same within the context of an analysis, but MODEL R will vary from test to test. There is one more comparison that we could perform, namely to compare MODEL G to a MODEL R without any predictors. This is a so-called omnibus test of the null hypothesis about two parameters simultaneously, namely \\(H_0: \\beta_1 = 0 \\text { and } \\beta_2 = 0\\). This results in another vesrion of the restricted MODEL R: \\[ \\begin{align} \\text{MODEL R: } \\quad \\texttt{trump_votes}_i &amp;= \\beta_0 + 0 \\times \\texttt{hate_groups}_i + 0 \\times \\texttt{education}_i + \\epsilon_i \\\\ &amp;= \\beta_0 + \\epsilon_i \\end{align} \\] This is the like the alternative model of the one-sample t-test we considered earlier. Here, the model is estimated as \\[\\texttt{trump_votes}_i = 49.86 + \\hat{\\epsilon}_i \\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, 10.09)\\] with an associated \\(\\text{SSE}(R) = 4992.178\\). The \\(F\\) statistic is then computed as \\[\\begin{align} F &amp;= \\frac{\\frac{4992.178 - 2074.202}{3 - 1}}{\\frac{2074.202}{50 - 3}} \\\\ &amp;= \\frac{1458.988}{44.132} \\\\ &amp;= 33.06 \\end{align}\\] The critical value is different this time, because the degrees of freedom are now \\(\\text{df}_1 = 3 - 1 = 2\\) and \\(\\text{df} = 50 - 3 = 47\\), and equals 3.195. Clearly, the computed \\(F\\) value is well above the critical value, hence we reject the null hypothesis (i.e. rerect MODEL R in favour of MODEL G). 5.5 Partitioning and explaining variance Let’s have another look at the formula for the \\(F\\) statistic: \\[F = \\frac{\\frac{\\text{SSE}(R) - \\text{SSE}(G)}{\\text{npar}(G) - \\text{npar}(R)}}{\\frac{\\text{SSE}(G)}{n-\\text{npar(G)}}}\\] The numerator (top part) consists of a difference between two SSE terms, divided by the difference in the number of estimated parameters. Because the comparison is between nested models, and MODEL G contains more estimated parameters than MODEL R, we can view this part as the average reduction of the SSE (per parameter) due to the additional parameters of MODEL G. The denominator (bottom part) consists of the SSE of MODEL G divided by \\(n-\\text{npar}(G)\\). This is an unbiased estimate of the error variance of MODEL G, and also referred to as the Mean Squared Error (MSE). The SSE of any model \\(M\\) is itself related to an error variance, as the SSE is the sum of squared deviations between the observations and the model predictions (\\(\\hat{Y} = \\mu_{Y|X_1,\\ldots,X_m}\\)): \\[\\text{SSE}(M) = \\sum_{i=1}^n (Y_i - \\hat{Y}_{M,i})^2\\] Dividing the SSE by \\(n - \\text{npar}(M)\\) would provide us with an unbiased estimate of the error variance of the model. Although this is only done in the denominator of the \\(F\\) statistic, it should be clear there is a close relation between SSE terms and variances. A useful perspective on unique effects in multiple regression models and GLMs in general is to consider them as partitioning the variance of the dependent variable in parts that are uniquely attributable to the predictors. If you consider the SSE of a very simple MODEL R \\[Y_i = \\beta_0 + \\epsilon_i\\] with estimate \\(\\hat{\\beta}_0 = \\overline{Y}\\), then for this model \\[\\text{SSE}(R) = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0)^2 = (Y_i - \\overline{Y})^2 = n S_Y^2\\] So the SSE of a regression model without predictors is equal to \\(n\\) times the sample variance of the dependent variable. When we add predictors to the model, we know the SSE can only decrease (or remain the same, but that is rather unlikely). The proportion of the SSE that is reduced by adding a predictor to the model can be seen as the proportion of the variance that is “explained” by the predictor. This proportional reduction in error is a useful measure of the strength of the relation between a predictor and the dependent variable. As usual, this strength will have a true value for the Data Generating Process, and a value we can compute for a given limited dataset. We’ll denote the true value as \\(\\eta^2\\). The sample estimate is: \\begin{equation} ^2 = \\tag{5.6} \\end[equation} Because \\(0 \\leq \\text{SSE}(G) \\leq \\text{SSE}(R)\\), the value of \\(\\hat{\\eta}^2\\) is between 0 and 1 (as a proportion should be). Figure 5.6 depicts such “proportions of variance explained” for a situation with two predictors (\\(X_1\\) and \\(X_2\\)) of a dependent variable \\(Y\\). Figure 5.6: Partitioning the variance in a multiple regression model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables. In this Figure, the variance of \\(Y\\) is represented by the circle labelled \\(Y\\), and the variance of the predictors is represented by the circles labelled \\(X_1\\) and \\(X_2\\) respectively. Overlap between the circles represents shared variance (e.g. covariance) between the variables. First, let’s consider a simple regression model where \\(X_1\\) is the sole predictor of \\(Y\\). The proportion of variance of \\(Y\\) that this model accounts for is the total shared area between \\(X_1\\) and \\(Y\\), which is the sum of the regions labelled as \\(B\\) and \\(D\\). Similarly, if we’d consider a simple regression model where \\(X_2\\) is the sole predictor of \\(Y\\), the proportion of variance of \\(Y\\) that this model accounts for is the total shared area between \\(X_2\\) and \\(Y\\), which is the sum of the regions labelled as \\(C\\) and \\(D\\). If we consider a multiple regression model with both \\(X_1\\) and \\(X_2\\) as predictors of \\(Y\\), the proportion of variance of \\(Y\\) that this model accounts for is the total shared area between \\(X_1\\), \\(X_2\\) and \\(Y\\), which is the sum of the regions labelled as \\(B\\), \\(C\\), and \\(D\\). The area labelled as \\(A\\) is variance of \\(Y\\) that cannot be accounted for by \\(X_1\\) or \\(X_2\\), and is the SSE (the random, unexplainable part) of a multiple regression model which includes both \\(X_1\\) and \\(X_2\\) as predictors. When MODEL R is an intercept-only model (i.e. a model without any predictors) and MODEL G a model with predictors, the estimate of the variance accounted for is often denoted by \\(R^2\\). In this case, \\(R^2\\) reflects the proportion of the variance of the dependent variable \\(Y\\) accounted for, or “explained”, by the model as a whole. The \\(\\eta^2\\) is generally reserved to denote the proportion of variance explained by a single predictor. Note that the area labelled as \\(D\\) is “explained” by both \\(X_1\\) and \\(X_2\\) when each is used as a sole predictor of \\(Y\\). In a multiple regression model however, the effects of predictors are unique effects, meaning that we consider only that part of the variance of \\(Y\\) which is solely explained by each predictor. For \\(X_1\\), this unique proportion of the variance is the area labelled as \\(B\\), and for \\(X_2\\) it is the area labelled as \\(C\\). The area labelled as \\(D\\) is not uniquely attributable to either \\(X_1\\) or \\(X_2\\). Although it is not purely random variability of \\(Y\\) – in principle, it could be explained by either \\(X_1\\) or \\(X_2\\) – because it cannot be uniquely attributed to either \\(X_1\\) or \\(X_2\\), it will not contribute to a hypothesis test of the effect of \\(X_1\\) or \\(X_2\\). We will discuss this more when we consider the practical problem of multicollinearity later on. 5.6 Assumptions The assumptions of the multiple regression model, and with that any of the versions of the General Linear Model we discuss, are all really about the errors, which are assumed to be independently and identically distributed: \\[\\epsilon_i \\sim \\mathbf{Normal}(0, \\sigma)\\] In words, this implies the following assumptions for a multiple regression and/or GLM: Normality: the errors \\(\\epsilon_i\\) are Normal-distributed Unbiasedness: the mean of \\(\\epsilon_i\\) is 0. This means that the conditional means are indeed a linear function of the predictors, and that the model predictions are unbiased. Homoscedasticity: the errors \\(\\epsilon_i\\) have a constant variance \\(\\sigma^2\\) and thus also a constant standard deviation \\(\\sigma\\). Independence: any error term \\(\\epsilon_i\\) is independent of any other \\(\\epsilon_j\\) (for all \\(i \\text{ and } j \\neq i\\)). Independence here means that \\(p(\\epsilon_j|\\epsilon_i) = p(\\epsilon_j)\\) for all \\(i \\text{ and } j \\neq i\\) It is important to realise that the assumption of Normal-distributed errors does not necessarily translate in assuming that the dependent variable \\(Y\\) is itself Normal-distributed. This will often to be the case. Not does the model require the predictors to be Normal-distributed. No assumption is made at all about the distribution of the predictors. In Section 3.6 we discussed some methods to assess the assumption of Normality (i.e. histograms, QQ-plots and statistical tests such as the Shapiro-Wilk test). The assumption of unbiasedness and homoscedasticity is generally assessed visually with a so-called predicted-ny-residual plot. This plot (see Figure 5.7) depicts the residuals as a function of the model predictions (i.e. the estimated conditional means). The unbiasedness assumption implies that the for each predicted value \\(\\hat{Y}\\) (i.e. each conditional mean), the residual or error terms are scattered around 0. For instance, it should not be the case that for relatively low and high values of the predictions, the errors are generally above 0, while for medium values of the predictions, the errors are generally below 0. Such a pattern would be indicative of biased predictions and a likely non-linear relation between predictors and the dependent variable. The homoscedasticity assumption implies that the spread of the residuals is equal for each predicted value \\(\\hat{Y}\\). Looking at the predicted vs residual plot in Figure 5.7, there is no clear indication that either assumption is violated. In addition, the QQ-plot indicates that the expected and sample quantiles do not differ radically, at least for the middle quantiles. Note that at the extremes (low and high quantiles), you will generally find more variable results, as quantiles in the tails of a distribution are less reliable. Figure 5.7: Predicted vs residual plot, and a QQ plot, for MODEL G predicting Trump votes from hate groups and education level. For comparison, Figure shows two examples of predicted vs residual plots where the assumption of unbiasedness and homoscedasticity do not hold. Figure 5.8: Predicted vs residual plots for two cases where the assumptions of the GLM do not hold. In the plot of the left, the assumption of unbiasedness is violated. There is clearly a nonlinearity in the data, which result in the underprediction (positive residuals) for low and high predicted values, and an overprediction (negative residuals) for medium predicted values. In the plot on the right, the homoscedasticity assumption is violated. The variability of the residuals clearly increases as a function of the predicted values. 5.7 Multicollinearity Come back later for this… Figure 5.9: Partitioning the variance in a multiple regression model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables. 5.8 Outliers When we analysed the data regarding Trump votes and hate groups up to now, we excluded the data from the District of Columbia (Washington D.C.), because, while it is not part of any US state, it is itself not a state either. If we had included the data from this district, the results would have been quite different. Figure 5.10 shows the Trump votes and hate groups with the Distric of Columbia included. You can see that the data point for the District of Columnbia is far removed from all other data points. The District of Columbia has both a very high number of hate groups (30.83 per million citizens), and a low percentage of votes for Trump (4.1%). This makes the data District of Columbia rather unusual when compared to the 50 states. The plot also shows the estimated regression line of a simple regression model (with hate groups as a single predictor) for all the data, and the data excluding the District of Columbia. After inclusion of the District of Columbia, the estimated relation goes from positive to negative! Figure 5.10: Percentage of votes for Trump in the 2016 elections for 50 US states and the District of Columbia, as a function of the number of hate groups per 1 million citizens. The solid line is the estimated regression line for all data, and the dotted line the estimated regression line excluding the District of Columbia. When we estimate a multiple regression model for all the data, we obtain the results given in Table 5.3. Instead of a significant positive relation between hate groups and Trump votes, we now find the slope is estimated close to 0, and not significant. Note that this estimated relation is not negative (as in Figure 5.10), which illustrates the fact that the unique relations determined in multiple regression models can differ substantially from the bivariate relations of simple regression models. Table 5.3: Null-hypothesis significance tests by model comparisons and the F statistic. \\(\\hat{\\beta}\\) \\(\\text{SS}\\) \\(\\text{df}\\) \\(F\\) \\(P(\\geq \\lvert F \\rvert)\\) Intercept 93.517 18525.97 1 367.803 0.000 Hate groups per million 0.054 2.29 1 0.046 0.832 % bachelors degree or higher -1.484 3912.90 1 77.684 0.000 Error 2417.72 48 The question now: how reasonable are these new results? When looking at all 50 states (excluding the District of Columbia), there appears to be rather good evidence for a positive relation between hate groups and Trump votes, whether assessing this relation in a simple regression model or in a multiple regression model where we control for the possible confounding effect of education level. That these results change dramatically after inclusion of a single additional data point indicates that this additional data point might be an outlier. An outlier can be rather imprecisely defined as an unusual data point that does not follow the general trend of the rest of the data. Clearly, the District of Columbia does not appear to follow the general trend in terms of Trump votes and hate groups. Figure 5.11: Percentage of votes for Trump in the 2016 elections for 50 US states and the District of Columbia, as a function of the percentage of citizens with a bachelors degree or higher. The solid line is the estimated regression line for all data, and the dotted line the estimated regression line excluding the District of Columbia. Figure 5.11 shows the relation between Trump votes and education level for all the data. While the District of Columbia is again far removed from the other data points, it doesn’t seem to “break” the negative relation between education level and Trump votes. Indeed, the estimated regression lines for a simple regression model predicting Trump votes from education level don’t differ all that much whether including the District of Columbia or not. So, sometimes “unusual” data points can have a large influence on model estimates, and sometimes not. There are several measures which aim to determine undue influence of observations on the model estimates, such as the leverage, Studentized residuals, Studentized deleted residuals, and Cook’s distance. A vector with the intercept and slopes can be computed by first constructing a so-called design matrix \\(\\mathbf{X}\\), where the first column contains only 1’s, and the remaining columns contain values for each predictor. The vector with estimates is then computed as \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}\\), where \\(\\mathbf{Y}\\) is the vector with the values of the dependent variable, and \\(^\\top\\) stands for the matrix transpose, and \\(\\mathbf{X}^{-1}\\) for the inverse of matrix \\(\\mathbf{X}\\).↩︎ "],["moderation-and-mediation.html", "Chapter 6 Moderation and mediation 6.1 Moderated regression 6.2 Mediation", " Chapter 6 Moderation and mediation 6.1 Moderated regression 6.2 Mediation "],["a-model-of-means-anova.html", "Chapter 7 A model of means (ANOVA) 7.1 Two-sample t-test 7.2 Contrast coding 7.3 Oneway ANOVA 7.4 Factorial ANOVA", " Chapter 7 A model of means (ANOVA) 7.1 Two-sample t-test 7.2 Contrast coding 7.3 Oneway ANOVA 7.4 Factorial ANOVA "],["mixing-categorical-and-metric-predictors-ancova.html", "Chapter 8 Mixing categorical and metric predictors (ANCOVA)", " Chapter 8 Mixing categorical and metric predictors (ANCOVA) "],["repeated-measures-anova.html", "Chapter 9 Repeated-measures ANOVA", " Chapter 9 Repeated-measures ANOVA "],["linear-mixed-effects-models.html", "Chapter 10 Linear mixed-effects models", " Chapter 10 Linear mixed-effects models "],["introduction-to-bayesian-hypothesis-testing.html", "Chapter 11 Introduction to Bayesian hypothesis testing", " Chapter 11 Introduction to Bayesian hypothesis testing "],["being-a-responsible-data-analyst.html", "Chapter 12 Being a responsible data analyst", " Chapter 12 Being a responsible data analyst "],["references.html", "References", " References "]]
