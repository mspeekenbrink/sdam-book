<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Repeated-measures ANOVA | Statistics: Data analysis and modelling</title>
  <meta name="description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Repeated-measures ANOVA | Statistics: Data analysis and modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  <meta name="github-repo" content="mspeekenbrink/sdam-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Repeated-measures ANOVA | Statistics: Data analysis and modelling" />
  
  <meta name="twitter:description" content="A book about statistics for data analysis, with a main focus on statistical modelling." />
  

<meta name="author" content="Maarten Speekenbrink" />


<meta name="date" content="2021-11-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mixing-categorical-and-metric-predictors-ancova.html"/>
<link rel="next" href="linear-mixed-effects-models.html"/>
<script src="book_assets/header-attrs-2.11/header-attrs.js"></script>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notation"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#paul-the-octopus"><i class="fa fa-check"></i><b>1.1</b> Paul the octopus</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#experiments-and-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments and observations</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#measurement-scales"><i class="fa fa-check"></i><b>1.3.1</b> Measurement scales</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#the-data-generating-process"><i class="fa fa-check"></i><b>1.3.2</b> The Data Generating Process</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#exploring-and-describing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and describing data</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#summary-statistics"><i class="fa fa-check"></i><b>1.4.1</b> Summary statistics</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#visual-exploration"><i class="fa fa-check"></i><b>1.4.2</b> Visual exploration</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#analysis-and-modelling"><i class="fa fa-check"></i><b>1.5</b> Analysis and modelling</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-modeling.html"><a href="ch-modeling.html"><i class="fa fa-check"></i><b>2</b> Statistical modelling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#coin-flipping-defining-a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> Coin flipping: Defining a statistical model</a></li>
<li class="chapter" data-level="2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#probability"><i class="fa fa-check"></i><b>2.2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-probability-definition"><i class="fa fa-check"></i><b>2.2.1</b> What is probability?</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-modeling.html"><a href="ch-modeling.html#distributions"><i class="fa fa-check"></i><b>2.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-binomial-model"><i class="fa fa-check"></i><b>2.3</b> Flipping a biased coin: An alternative model</a></li>
<li class="chapter" data-level="2.4" data-path="ch-modeling.html"><a href="ch-modeling.html#estimation"><i class="fa fa-check"></i><b>2.4</b> Estimation</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-modeling.html"><a href="ch-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-modeling.html"><a href="ch-modeling.html#properties-of-good-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Properties of good estimators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-modeling.html"><a href="ch-modeling.html#sec:02-likelihood-ratio"><i class="fa fa-check"></i><b>2.5</b> Comparing models: Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-modeling.html"><a href="ch-modeling.html#decisions-and-types-of-error"><i class="fa fa-check"></i><b>2.5.1</b> Decisions and types of error</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-modeling.html"><a href="ch-modeling.html#significance-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Significance and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-modeling.html"><a href="ch-modeling.html#testing-whether-paul-was-guessing"><i class="fa fa-check"></i><b>2.5.3</b> Testing whether Paul was guessing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-modeling.html"><a href="ch-modeling.html#hypothesis-testing-directly-with-the-binomial-distribution"><i class="fa fa-check"></i><b>2.6</b> Hypothesis testing directly with the Binomial distribution</a></li>
<li class="chapter" data-level="2.7" data-path="ch-modeling.html"><a href="ch-modeling.html#summary-1"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="ch-modeling.html"><a href="ch-modeling.html#epilogue"><i class="fa fa-check"></i><b>2.8</b> Epilogue</a></li>
<li class="chapter" data-level="2.9" data-path="ch-modeling.html"><a href="ch-modeling.html#exercises"><i class="fa fa-check"></i><b>2.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html"><i class="fa fa-check"></i><b>3</b> A model with a mean (one sample t-test)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#numeric-judgement-and-anchoring"><i class="fa fa-check"></i><b>3.1</b> Numeric judgement and anchoring</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#exploring-the-data"><i class="fa fa-check"></i><b>3.1.1</b> Exploring the data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#a-statistical-model-of-judgements"><i class="fa fa-check"></i><b>3.2</b> A statistical model of judgements</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-normal-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Normal distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#two-useful-properties-of-the-normal-distribution"><i class="fa fa-check"></i><b>3.2.2</b> Two useful properties of the Normal distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#back-to-anchoring"><i class="fa fa-check"></i><b>3.2.3</b> Back to anchoring</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sampling-distribution-of-the-estimated-mean"><i class="fa fa-check"></i><b>3.3.1</b> Sampling distribution of the estimated mean</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#testing-whether-mu-has-an-specific-value"><i class="fa fa-check"></i><b>3.4</b> Testing whether <span class="math inline">\(\mu\)</span> has an specific value</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-classical-way"><i class="fa fa-check"></i><b>3.4.1</b> The classical way</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-model-comparison-way"><i class="fa fa-check"></i><b>3.4.2</b> The model comparison way</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#ch3-confidence-interval"><i class="fa fa-check"></i><b>3.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="3.6" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#effect-size"><i class="fa fa-check"></i><b>3.6</b> Effect size</a></li>
<li class="chapter" data-level="3.7" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#sec:02-assumptions"><i class="fa fa-check"></i><b>3.7</b> Assumptions</a></li>
<li class="chapter" data-level="3.8" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.8</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>3.8.1</b> The Central Limit Theorem in action</a></li>
<li class="chapter" data-level="3.8.2" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#bootstrapping-a-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Bootstrapping a statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#in-practice"><i class="fa fa-check"></i><b>3.9</b> In practice</a></li>
<li class="chapter" data-level="3.10" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#summary-2"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
<li class="chapter" data-level="3.11" data-path="ch-simple-GLM.html"><a href="ch-simple-GLM.html#the-general-linear-model"><i class="fa fa-check"></i><b>3.11</b> The General Linear Model</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.1</b> Trump, votes, and hate groups</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>4.2</b> The model</a></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sec:04-estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#estimating-the-relation-between-trump-votes-and-hate-groups"><i class="fa fa-check"></i><b>4.3.1</b> Estimating the relation between Trump votes and hate groups</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.4</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#sampling-distribution-of-estimates"><i class="fa fa-check"></i><b>4.4.1</b> Sampling distribution of estimates</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Model comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>4.4.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#assumptions-and-outliers"><i class="fa fa-check"></i><b>4.5</b> Assumptions and outliers</a></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary-3"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary-4"><i class="fa fa-check"></i><b>4.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#trump-votes-and-hate-groups-again"><i class="fa fa-check"></i><b>5.1</b> Trump, votes, and hate groups (again)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#controlling-for-education-level"><i class="fa fa-check"></i><b>5.1.1</b> Controlling for education level</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#the-multiple-regression-model"><i class="fa fa-check"></i><b>5.2</b> The multiple regression model</a></li>
<li class="chapter" data-level="5.3" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-estimation"><i class="fa fa-check"></i><b>5.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-inference"><i class="fa fa-check"></i><b>5.4</b> Inference</a></li>
<li class="chapter" data-level="5.5" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#partitioning-and-explaining-variance"><i class="fa fa-check"></i><b>5.5</b> Partitioning and explaining variance</a></li>
<li class="chapter" data-level="5.6" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size and the importance of predictors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#r2-changes-and-the-coefficient-of-semi-partial-determination"><i class="fa fa-check"></i><b>5.6.1</b> <span class="math inline">\(R^2\)</span> changes and the coefficient of (semi-)partial determination</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#controlling-for-education-level-and-poverty"><i class="fa fa-check"></i><b>5.7</b> Controlling for education level and poverty</a></li>
<li class="chapter" data-level="5.8" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-assumptions"><i class="fa fa-check"></i><b>5.8</b> Assumptions</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#transforming-variables"><i class="fa fa-check"></i><b>5.8.1</b> Transforming variables</a></li>
<li class="chapter" data-level="5.8.2" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.8.2</b> Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#multicollinearity-redundancy-between-predictors"><i class="fa fa-check"></i><b>5.9</b> Multicollinearity: Redundancy between predictors</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#detecting-and-dealing-with-multicollinearity"><i class="fa fa-check"></i><b>5.9.1</b> Detecting and dealing with multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#sec:04b-outliers"><i class="fa fa-check"></i><b>5.10</b> Outliers</a></li>
<li class="chapter" data-level="5.11" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#in-practice-1"><i class="fa fa-check"></i><b>5.11</b> In practice</a></li>
<li class="chapter" data-level="5.12" data-path="ch-multiple-regression.html"><a href="ch-multiple-regression.html#summary-5"><i class="fa fa-check"></i><b>5.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html"><i class="fa fa-check"></i><b>6</b> Moderation and mediation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#moderation"><i class="fa fa-check"></i><b>6.1</b> Moderation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#physical-attractiveness-and-intelligence-in-speed-dating"><i class="fa fa-check"></i><b>6.1.1</b> Physical attractiveness and intelligence in speed dating</a></li>
<li class="chapter" data-level="6.1.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#conditional-slopes"><i class="fa fa-check"></i><b>6.1.2</b> Conditional slopes</a></li>
<li class="chapter" data-level="6.1.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#modeling-slopes-with-linear-models"><i class="fa fa-check"></i><b>6.1.3</b> Modeling slopes with linear models</a></li>
<li class="chapter" data-level="6.1.4" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#simple-slopes-and-centering"><i class="fa fa-check"></i><b>6.1.4</b> Simple slopes and centering</a></li>
<li class="chapter" data-level="6.1.5" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#sec:05-dont-forget-about-the-fun"><i class="fa fa-check"></i><b>6.1.5</b> Don’t forget about fun! A model with multiple interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#mediation"><i class="fa fa-check"></i><b>6.2</b> Mediation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#legacy-motives-and-pro-environmental-behaviours"><i class="fa fa-check"></i><b>6.2.1</b> Legacy motives and pro-environmental behaviours</a></li>
<li class="chapter" data-level="6.2.2" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#causal-steps"><i class="fa fa-check"></i><b>6.2.2</b> Causal steps</a></li>
<li class="chapter" data-level="6.2.3" data-path="moderation-and-mediation.html"><a href="moderation-and-mediation.html#estimating-the-mediated-effect"><i class="fa fa-check"></i><b>6.2.3</b> Estimating the mediated effect</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html"><i class="fa fa-check"></i><b>7</b> A model of means (ANOVA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#can-playing-tetris-reduce-intrusive-memories"><i class="fa fa-check"></i><b>7.1</b> Can playing Tetris reduce intrusive memories?</a></li>
<li class="chapter" data-level="7.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#sec:06-two-groups"><i class="fa fa-check"></i><b>7.2</b> Comparing two groups</a></li>
<li class="chapter" data-level="7.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#the-anova-model"><i class="fa fa-check"></i><b>7.3</b> The ANOVA model</a></li>
<li class="chapter" data-level="7.4" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#contrast-coding"><i class="fa fa-check"></i><b>7.4</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-coding"><i class="fa fa-check"></i><b>7.4.1</b> Effect coding</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#orthogonal-contrast-codes"><i class="fa fa-check"></i><b>7.4.2</b> Orthogonal contrast codes</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#defining-your-own-orthogonal-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Defining your own (orthogonal) contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#default-orthogonal-coding-schemes"><i class="fa fa-check"></i><b>7.5</b> Default orthogonal coding schemes</a></li>
<li class="chapter" data-level="7.6" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#effect-size-in-anova"><i class="fa fa-check"></i><b>7.6</b> Effect-size in ANOVA</a></li>
<li class="chapter" data-level="7.7" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#assumptions"><i class="fa fa-check"></i><b>7.7</b> Assumptions</a></li>
<li class="chapter" data-level="7.8" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#multiple-testing-and-post-hoc-tests"><i class="fa fa-check"></i><b>7.8</b> Multiple testing and post-hoc tests</a></li>
<li class="chapter" data-level="7.9" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#in-practice-2"><i class="fa fa-check"></i><b>7.9</b> In practice</a></li>
<li class="chapter" data-level="7.10" data-path="ch-ANOVA.html"><a href="ch-ANOVA.html#sec:06-assumptions"><i class="fa fa-check"></i><b>7.10</b> Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#experimenter-beliefs-and-social-priming"><i class="fa fa-check"></i><b>8.1</b> Experimenter beliefs and social priming</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-oneway-anova"><i class="fa fa-check"></i><b>8.1.1</b> A oneway ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#factorial-designs"><i class="fa fa-check"></i><b>8.2</b> Factorial designs</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#sec:06-main-effects-and-interactions"><i class="fa fa-check"></i><b>8.2.1</b> Main effects and interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#the-factorial-anova-model"><i class="fa fa-check"></i><b>8.3</b> The factorial ANOVA model</a></li>
<li class="chapter" data-level="8.4" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#a-threeway-factorial-anova"><i class="fa fa-check"></i><b>8.4</b> A threeway factorial ANOVA</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#interpreting-interactions"><i class="fa fa-check"></i><b>8.4.1</b> Interpreting interactions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#orthogonal-contrast-codes-and-unequal-sample-sizes"><i class="fa fa-check"></i><b>8.5</b> Orthogonal contrast codes and unequal sample sizes</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#comparison-schemes-and-ss-types"><i class="fa fa-check"></i><b>8.5.1</b> Comparison schemes and SS types</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ch-factorial-ANOVA.html"><a href="ch-factorial-ANOVA.html#in-practice-3"><i class="fa fa-check"></i><b>8.6</b> In practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html"><i class="fa fa-check"></i><b>9</b> Mixing categorical and metric predictors (ANCOVA)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#subjective-feelings-of-power-and-priming"><i class="fa fa-check"></i><b>9.1</b> Subjective feelings of power and priming</a></li>
<li class="chapter" data-level="9.2" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#acounting-for-pre-existing-differences"><i class="fa fa-check"></i><b>9.2</b> Acounting for pre-existing differences</a></li>
<li class="chapter" data-level="9.3" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#slopes-of-contrast-coded-predictors-in-ancova-models"><i class="fa fa-check"></i><b>9.3</b> Slopes of contrast-coded predictors in ANCOVA models</a></li>
<li class="chapter" data-level="9.4" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#homogeneity-of-slopes"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of slopes</a></li>
<li class="chapter" data-level="9.5" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#sec:ANCOVA-power"><i class="fa fa-check"></i><b>9.5</b> Power considerations in ANCOVA</a></li>
<li class="chapter" data-level="9.6" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#models-with-multiple-covariates"><i class="fa fa-check"></i><b>9.6</b> Models with multiple covariates</a></li>
<li class="chapter" data-level="9.7" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#mediation-with-categorical-independent-variables"><i class="fa fa-check"></i><b>9.7</b> Mediation with categorical independent variables</a></li>
<li class="chapter" data-level="9.8" data-path="mixing-categorical-and-metric-predictors-ancova.html"><a href="mixing-categorical-and-metric-predictors-ancova.html#ancova-vs-difference-scores"><i class="fa fa-check"></i><b>9.8</b> ANCOVA vs difference scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html"><i class="fa fa-check"></i><b>10</b> Repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#non-independence-in-linear-models"><i class="fa fa-check"></i><b>10.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="10.2" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#the-cheerleader-effect"><i class="fa fa-check"></i><b>10.2</b> The cheerleader effect</a></li>
<li class="chapter" data-level="10.3" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#a-oneway-repeated-measures-anova"><i class="fa fa-check"></i><b>10.3</b> A oneway repeated-measures ANOVA</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#including-effects-for-participants"><i class="fa fa-check"></i><b>10.3.1</b> Including effects for participants</a></li>
<li class="chapter" data-level="10.3.2" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#fixed-vs-random-effects"><i class="fa fa-check"></i><b>10.3.2</b> Fixed vs random effects</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#assumptions-1"><i class="fa fa-check"></i><b>10.4</b> Assumptions</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#sphericity"><i class="fa fa-check"></i><b>10.4.1</b> Sphericity</a></li>
<li class="chapter" data-level="10.4.2" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#correcting-for-non-sphericity"><i class="fa fa-check"></i><b>10.4.2</b> Correcting for non-sphericity</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#effect-size-1"><i class="fa fa-check"></i><b>10.5</b> Effect size</a></li>
<li class="chapter" data-level="10.6" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#mixed-anova-with-between-and-within-factors"><i class="fa fa-check"></i><b>10.6</b> Mixed ANOVA with between and within factors</a></li>
<li class="chapter" data-level="10.7" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#in-practice-4"><i class="fa fa-check"></i><b>10.7</b> In practice</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#defining-and-estimating-the-model"><i class="fa fa-check"></i><b>10.7.1</b> Defining and estimating the model</a></li>
<li class="chapter" data-level="10.7.2" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#assessing-the-assumptions"><i class="fa fa-check"></i><b>10.7.2</b> Assessing the assumptions</a></li>
<li class="chapter" data-level="10.7.3" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#reporting-the-results"><i class="fa fa-check"></i><b>10.7.3</b> Reporting the results</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#summary-6"><i class="fa fa-check"></i><b>10.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Linear mixed-effects models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#non-independence-in-linear-models-1"><i class="fa fa-check"></i><b>11.1</b> Non-independence in linear models</a></li>
<li class="chapter" data-level="11.2" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#random-intercept-models"><i class="fa fa-check"></i><b>11.2</b> Random intercept models</a></li>
<li class="chapter" data-level="11.3" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#parameter-estimation-1"><i class="fa fa-check"></i><b>11.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="11.4" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#parameter-inference"><i class="fa fa-check"></i><b>11.4</b> Parameter inference</a></li>
<li class="chapter" data-level="11.5" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#application-of-the-random-intercepts-model"><i class="fa fa-check"></i><b>11.5</b> Application of the random-intercepts model</a></li>
<li class="chapter" data-level="11.6" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#models-with-random-intercepts-and-slopes"><i class="fa fa-check"></i><b>11.6</b> Models with random intercepts and slopes</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#correlation-between-random-effects"><i class="fa fa-check"></i><b>11.6.1</b> Correlation between random effects</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#crossed-random-effects-dating-partners-in-the-speed-dating-experiment"><i class="fa fa-check"></i><b>11.7</b> Crossed random effects: dating partners in the speed dating experiment</a></li>
<li class="chapter" data-level="11.8" data-path="linear-mixed-effects-models.html"><a href="linear-mixed-effects-models.html#choosing-the-random-effects-structure"><i class="fa fa-check"></i><b>11.8</b> Choosing the random effects structure</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html"><i class="fa fa-check"></i><b>12</b> Introduction to Bayesian hypothesis testing</a>
<ul>
<li class="chapter" data-level="12.1" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#fundamentals-of-bayesian-inference"><i class="fa fa-check"></i><b>12.1</b> Fundamentals of Bayesian inference</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#probability-in-times-of-covid"><i class="fa fa-check"></i><b>12.1.1</b> Probability in times of Covid</a></li>
<li class="chapter" data-level="12.1.2" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#bayes-rule"><i class="fa fa-check"></i><b>12.1.2</b> Bayes’ rule</a></li>
<li class="chapter" data-level="12.1.3" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#we-missed-you-paul"><i class="fa fa-check"></i><b>12.1.3</b> We missed you Paul!</a></li>
<li class="chapter" data-level="12.1.4" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#the-marginal-likelihood-and-prior-predictive-distribution"><i class="fa fa-check"></i><b>12.1.4</b> The marginal likelihood and prior predictive distribution</a></li>
<li class="chapter" data-level="12.1.5" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#hypothesis-testing-relative-evidence-and-the-bayes-factor"><i class="fa fa-check"></i><b>12.1.5</b> Hypothesis testing, relative evidence, and the Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#a-bayesian-t-test"><i class="fa fa-check"></i><b>12.2</b> A Bayesian t-test</a></li>
<li class="chapter" data-level="12.3" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#bayes-factors-for-general-linear-models"><i class="fa fa-check"></i><b>12.3</b> Bayes factors for General Linear Models</a></li>
<li class="chapter" data-level="12.4" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#some-objections-to-null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>12.4</b> Some objections to null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#the-p-value-is-not-a-proper-measure-of-evidential-support"><i class="fa fa-check"></i><b>12.4.1</b> The <span class="math inline">\(p\)</span>-value is not a proper measure of evidential support</a></li>
<li class="chapter" data-level="12.4.2" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#the-p-value-depends-on-researcher-intentions"><i class="fa fa-check"></i><b>12.4.2</b> The <span class="math inline">\(p\)</span>-value depends on researcher intentions</a></li>
<li class="chapter" data-level="12.4.3" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#results-of-a-nhst-are-often-misinterpreted"><i class="fa fa-check"></i><b>12.4.3</b> Results of a NHST are often misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#to-bayes-or-not-to-bayes-a-pragmatic-view"><i class="fa fa-check"></i><b>12.5</b> To Bayes or not to Bayes? A pragmatic view</a></li>
<li class="chapter" data-level="12.6" data-path="introduction-to-bayesian-hypothesis-testing.html"><a href="introduction-to-bayesian-hypothesis-testing.html#summary-7"><i class="fa fa-check"></i><b>12.6</b> “Summary”</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html"><i class="fa fa-check"></i><b>13</b> Being a responsible data analyst</a>
<ul>
<li class="chapter" data-level="13.1" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#consider-analysis-before-data-collection"><i class="fa fa-check"></i><b>13.1</b> Consider analysis before data collection</a></li>
<li class="chapter" data-level="13.2" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#explore-the-data"><i class="fa fa-check"></i><b>13.2</b> Explore the data</a></li>
<li class="chapter" data-level="13.3" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#evaluate-the-assumptions-underlying-your-analyses"><i class="fa fa-check"></i><b>13.3</b> Evaluate the assumptions underlying your analyses</a></li>
<li class="chapter" data-level="13.4" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#distinguish-between-confirmatory-and-exploratory-analyses"><i class="fa fa-check"></i><b>13.4</b> Distinguish between confirmatory and exploratory analyses</a></li>
<li class="chapter" data-level="13.5" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#aim-for-openness-and-reproducibility"><i class="fa fa-check"></i><b>13.5</b> Aim for openness and reproducibility</a></li>
<li class="chapter" data-level="13.6" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#communicate-clearly-and-concisely"><i class="fa fa-check"></i><b>13.6</b> Communicate clearly and concisely</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#example-of-reporting-a-multiple-regression-analysis"><i class="fa fa-check"></i><b>13.6.1</b> Example of reporting a multiple regression analysis</a></li>
<li class="chapter" data-level="13.6.2" data-path="being-a-responsible-data-analyst.html"><a href="being-a-responsible-data-analyst.html#example-of-reporting-a-factorial-anova"><i class="fa fa-check"></i><b>13.6.2</b> Example of reporting a factorial ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics: Data analysis and modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="repeated-measures-anova" class="section level1" number="10">
<h1><span class="header-section-number">Chapter 10</span> Repeated-measures ANOVA</h1>
<p>In this chapter, we will discuss how to deal with the non-independence of errors that can result from repeated measures of the same individual or other forms of groupings in the data, within the context of ANOVA analyses. The resulting class of models is known as repeated-measures ANOVA.</p>
<div id="non-independence-in-linear-models" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Non-independence in linear models</h2>
<p>The General Linear Model we have considered thus far can be stated as follows:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)\]</span>
The assumptions of this model concern the residuals or errors <span class="math inline">\(\epsilon_i\)</span>. They are assumed to be <em>independent and identically distributed</em> (iid), following a Normal distribution with a mean of 0 and a standard deviation <span class="math inline">\(\sigma_\epsilon\)</span>.</p>
<p>You can expect violations of the <em>iid</em> assumption if data are collected from “units of observation” that are clustered in groups. A common example of this in psychology is when you repeatedly observe behaviour from the same person.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> For example, suppose participants in an experiment perform two tests to measure their working memory capacity. A person with a high working memory capacity would score high on both tests, whilst a person with a low working memory capacity would score low on both. That means that the scores in the two tests are correlated, and hence <em>not</em> independent. If the model does not adequately account for this, then the residuals (errors) would also not be independent.</p>
<!-- As another example of dependency between model residuals, let's go back to the simplest version of the GLM, the model with just an intercept that was the focus of Chapter \@ref(ch-simple-GLM):
\begin{equation}
Y_i = \beta_0 + \epsilon_i
(\#eq:simple-glm-ch08)
\end{equation}
Suppose the dependent variable $Y$ again reflects judgements of the height of Mount Everest, but that one of the labs in the ManyLabs study that collected the data, was based in Nepal. We might expect participants in Nepal to have a more accurate idea of the height of Mount Everest than participants in Poland (the country the participants were from in the subset of the data we considered in Chapter \@ref(ch-simple-GLM)). As a result, participants in Nepal would likely be less influenced by the low anchor than those with less knowledge of Mount Everest. If $\beta_0$ represents the mean judgement over all participants, then participants in Nepal would generally provide judgements which are higher than $\beta_0$ (i.e positive errors), whilst participants in Poland would generally provide judgements that are lower than $\beta_0$ (i.e. negative errors). If our model does not account for such grouping in the data, then the errors for cases from Nepal would be correlated, because all would tend to be positive. Similarly, the errors for cases from Poland would also be correlated, as all would tend to be negative. 

What this example points to is that the simple model does not account for the fact that participants from different countries might have a different average judgement. As such, the model is misspecified. Here, that leads to dependent errors. Unfortunately, compared to violations of other assumptions, such as the normality assumption or the assumption of variance homogeneity, standard statistical procedures are usually _not_ robust to violations of the independence assumption [@kenny_consequences_1986;@judd_treating_2012]. They often lead to considerably increased Type I errors (i.e., false positives) and more generally can produce overconfident results (e.g., too narrow standard errors).

In the example above, we could alleviate the problem of dependent errors by including an effect for country, using a contrast-coding predictor $X_1$ to reflect the difference between participants from Nepal ($X_1 = \tfrac{1}{2}$) and Poland ($X_1 = -\tfrac{1}{2}$) in an expanded model
$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i$$
This model would allow for a different mean judgement for cases from Nepal ($\beta_0 + \tfrac{1}{2} \times \beta_1$) and cases from Poland ($\beta_0 - \tfrac{1}{2} \times \beta_1$). Effectively, this is the strategy employed in repeated-measures ANOVA: include an additional grouping factor in the model.

<!-- The assumption that the errors are independent means that knowing the value of the error for one case $i$ in the data does not give you any information to determine the value of the error for another case $j$. More formally, statistical independence is defined in terms of conditional probabilities. Remember when we discussed the rules of probability (Section \@ref(sec:02-rules-of-probability))? In particular, we used independence to state rule 7, which is a specific case of the multiplication rule (rule 6). We can define the distribution of the error for case $i$ as a conditional probability $p(\epsilon_i | \epsilon_i, X_{1,i}, \ldots, X_{m,i})$, where we conditionalise on all the predictor values, as well as another error term $\epsilon_j$. The errors are independent when 
$$p(\epsilon_i | \epsilon_j, X_{1,i}, \ldots, X_{m,i}) = p(\epsilon_i |  X_{1,i}, \ldots, X_{m,i})$$
i.e. the distribution of $\epsilon_i$, conditional upon knowing the value of the predictors and the error $\epsilon_j$ of another case $j$, is the same as the distribution conditional upon just knowing the value of the predictors. This means that error $\epsilon_j$ provides no information about the error $\epsilon_i$.
-->
</div>
<div id="the-cheerleader-effect" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> The cheerleader effect</h2>
<p>The attractiveness of a person’s face is traditionally considered to be related to physical features, such as how symmetrical the features are, how close a face is to the “average” face over many people, and sexual dimorphism (whether a face looks exclusively male or female). However, there is also evidence that perceived facial attractiveness can vary due to factors outside of the face. In what has become known as the “cheerleader effect,” the same face is perceived to be more attractive when seen in a group, as compared to when it is seen alone. <span class="citation"><a href="#ref-walker2014hierarchical" role="doc-biblioref">Walker &amp; Vul</a> (<a href="#ref-walker2014hierarchical" role="doc-biblioref">2014</a>)</span> proposed that the cheerleader effect arises due to people encoding faces in a hierarchical manner. When presented with a group of faces, people encode the display by first calculating an average face for the group, and then encoding individual faces through deviations from the group average. As average faces tend to be perceived as more attractive than individual faces, this then results in each face in the group being perceived as more attractive, because the average part of the encoding is perceived as attractive.
<span class="citation"><a href="#ref-carragher2019limited" role="doc-biblioref">Carragher, Thomas, Gwinn, &amp; Nicholls</a> (<a href="#ref-carragher2019limited" role="doc-biblioref">2019</a>)</span> set out to test this explanation. In one part of the study (Experiment 1), they let participants rate the attractiveness of a face when presented by itself (the Alone condition), as part of a group of different faces (the Different condition), or as part of a group of similar faces (the Similar condition). There were two variants of the latter condition, and participants encountered only one of them in the experiment. In the Identical condition, the group consisted of three copies of exactly the same photo. In the Variant condition, the group consisted of three different photos of the same face.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> The authors argued that if the hierarchical-encoding explanation for the cheerleader effect is true, then the cheerleader effect should not be observed in the Identical condition. This is because the average of three identical photos is just the photo itself, so there is no difference between an encoded average face and the face itself. In the Variant condition however, variability between the different photos of the same face might still lead to an average face which is more attractive than each individual face. This is an example of a 2 (version: Identical, Same) by 3 (condition: Alone, Different, or Similar) design. The first factor (Version) varied between people, and the second (Condition) within people (each participant rated a face in each of the three conditions). The rated attractiveness of the faces in the different conditions are provided in Figure <a href="repeated-measures-anova.html#fig:cheerleader-raincloud-plot">10.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cheerleader-raincloud-plot"></span>
<img src="_main_files/figure-html/cheerleader-raincloud-plot-1.svg" alt="Attractiveness ratings for photo's of faces when presented alone, as part of a group of dissimilar faces (Control), or as part of a group of similar faces (Distractor), which are either identical (Identical-distractors) or different photos (Self-distractors) ofd the same face." width="672" />
<p class="caption">
Figure 10.1: Attractiveness ratings for photo’s of faces when presented alone, as part of a group of dissimilar faces (Control), or as part of a group of similar faces (Distractor), which are either identical (Identical-distractors) or different photos (Self-distractors) ofd the same face.
</p>
</div>
</div>
<div id="a-oneway-repeated-measures-anova" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> A oneway repeated-measures ANOVA</h2>
<p>To keep matters relatively simple, we will for now just consider participants in the Variant condition. That means that we can treat the study as a oneway design, with three levels (condition: Alone, Different, or Same) that all vary within-participants.</p>
<p>Table <a href="repeated-measures-anova.html#tab:cheerleader-self-attractiveness-table">10.1</a> shows the attractiveness ratings for 10 participants in the Variant condition. You can see there that when a participant rates a face as relatively attractive when presented alone, (s)he also tends to rate the face as relatively attractive when shown in a group of faces. Some participants (e.g. participant 37) rate the face as relatively unattractive in all conditions. This indicates individual differences in how attractive people find a face. People’s tastes differ. Whilst that may be interesting in its own right, for the purposes of the experiment, we do not care about such individual differences. What we want to know is whether the attractiveness of a face increases when presented as part of a group.</p>
<table>
<caption><span id="tab:cheerleader-self-attractiveness-table">Table 10.1: </span>Attractiveness ratings in the Alone, Different, and Same condition for 10 participants in the Variant condition. Also shown is the average over the three ratings for each participant.</caption>
<thead>
<tr class="header">
<th align="left">Participant</th>
<th align="right">Alone</th>
<th align="right">Different</th>
<th align="right">Same</th>
<th align="right">Average</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">36</td>
<td align="right">56.3</td>
<td align="right">55.9</td>
<td align="right">54.3</td>
<td align="right">55.5</td>
</tr>
<tr class="even">
<td align="left">37</td>
<td align="right">13.3</td>
<td align="right">13.5</td>
<td align="right">11.6</td>
<td align="right">12.8</td>
</tr>
<tr class="odd">
<td align="left">39</td>
<td align="right">49.8</td>
<td align="right">49.8</td>
<td align="right">51.2</td>
<td align="right">50.3</td>
</tr>
<tr class="even">
<td align="left">40</td>
<td align="right">47.1</td>
<td align="right">52.5</td>
<td align="right">53.2</td>
<td align="right">50.9</td>
</tr>
<tr class="odd">
<td align="left">41</td>
<td align="right">38.9</td>
<td align="right">41.6</td>
<td align="right">42.1</td>
<td align="right">40.9</td>
</tr>
<tr class="even">
<td align="left">42</td>
<td align="right">60.0</td>
<td align="right">58.9</td>
<td align="right">59.5</td>
<td align="right">59.5</td>
</tr>
<tr class="odd">
<td align="left">44</td>
<td align="right">44.7</td>
<td align="right">47.0</td>
<td align="right">50.5</td>
<td align="right">47.4</td>
</tr>
<tr class="even">
<td align="left">45</td>
<td align="right">45.2</td>
<td align="right">43.9</td>
<td align="right">46.2</td>
<td align="right">45.1</td>
</tr>
<tr class="odd">
<td align="left">46</td>
<td align="right">36.8</td>
<td align="right">37.3</td>
<td align="right">35.8</td>
<td align="right">36.6</td>
</tr>
<tr class="even">
<td align="left">47</td>
<td align="right">53.8</td>
<td align="right">52.7</td>
<td align="right">52.9</td>
<td align="right">53.1</td>
</tr>
</tbody>
</table>
<p>To answer this question, we should however take individual differences into account. If we were to (wrongly!) treat all ratings as independent, and analyse the data with a regular oneway ANOVA, we would obtain the results in Table <a href="repeated-measures-anova.html#tab:cheerleader-oneway-ANOVA-results">10.2</a>. As you can see, this analysis indicates there is no effect of presenting a face alone or in a group.</p>
<table>
<caption><span id="tab:cheerleader-oneway-ANOVA-results">Table 10.2: </span>Linear model predicting attractiveness ratings in the three conditions, wrongly assuming independence. Effect-coding was used for Condition.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="right"><span class="math inline">\(\text{SS}\)</span></th>
<th align="right"><span class="math inline">\(\text{df}\)</span></th>
<th align="right"><span class="math inline">\(F\)</span></th>
<th align="right"><span class="math inline">\(P(\geq \lvert F \rvert)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">47.426</td>
<td align="right">2.09e+05</td>
<td align="right">1</td>
<td align="right">2062.611</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">Condition</td>
<td align="right"></td>
<td align="right">3.55e+01</td>
<td align="right">2</td>
<td align="right">0.175</td>
<td align="right">0.840</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\quad X_1\)</span> (Control)</td>
<td align="right">0.338</td>
<td align="right">5.32e+00</td>
<td align="right">1</td>
<td align="right">0.052</td>
<td align="right">0.819</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\quad X_2\)</span> (Distractor)</td>
<td align="right">0.529</td>
<td align="right">1.30e+01</td>
<td align="right">1</td>
<td align="right">0.128</td>
<td align="right">0.721</td>
</tr>
<tr class="odd">
<td align="left">Error</td>
<td align="right"></td>
<td align="right">9.13e+03</td>
<td align="right">90</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>The problem in this analysis is that it ignores the (sometimes rather large) differences between participants in how attractive they find a face on average (i.e. over the three conditions). This individual variability makes the ratings within each condition also highly variable. By ignoring that we have repeated measures for each participant, the model errors are the differences between each rating in a condition, and the average in that condition. If we let <span class="math inline">\(Y_{i,j}\)</span> denote a response by person <span class="math inline">\(i\)</span> in condition <span class="math inline">\(j\)</span>, the errors in this model are <span class="math inline">\(\hat{\epsilon}_{i,j} = Y_{i,j} - \overline{Y}_{\cdot,j}\)</span>. But, if you go back to Table <a href="repeated-measures-anova.html#tab:cheerleader-self-attractiveness-table">10.1</a>, you might realise that we can much more specific in considering what the effect of displaying a face in a group is. For each participant, we could consider whether the rating in e.g. the Control condition is higher than the rating in the Alone condition. For participant 36, the rating in the Control condition is a little lower than their rating in the Alone condition, and for participant 37, it is a little higher. We can then ask the question whether these differences are, on average, positive (indicating increased attractiveness) or negative (indicating decreased attractiveness). We could perform such an analysis by first computing difference scores, and then using a one-sample t-test to assess whether the difference scores are on average different from 0. We could do this for any pair of conditions, but that might result in a large number of tests. A repeated-measures ANOVA allows us to to essentially this within a single analysis.</p>
<div id="including-effects-for-participants" class="section level3" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Including effects for participants</h3>
<p>We can state an ANOVA model as
<!--
\begin{equation}
Y_{i,j} = \mu + \tau^{(P)}_i + \tau^{(A)}_j + \epsilon_{i,j}
(\#eq:glm-oneway-RM-ANOVA-model)
\end{equation}
But this assumes that the effect of a treatment $j$ is the same for all
individuals $i$. If we would allow the effect of a treatment to be different
for participants, that would amount to including an interaction term (the 
effect of treatment $j$ is moderated by individual $i$). This could be stated as
-->
<span class="math display" id="eq:one-RM-ANOVA-model-ch08">\[\begin{equation}
Y_{i,j} = \mu + \pi_i + \tau_j + (\pi \tau)_{i,j} + \epsilon_{i,j}
\tag{10.1}
\end{equation}\]</span>
There is one complication, however. We only have a single observation for each combination of person and condition. That means that the average for each combination of person and condition, which is what the model predicts, equals the observation <span class="math inline">\(Y_{i,j}\)</span>. As such, the errors in this model would all be equal to <span class="math inline">\(\epsilon_{i,j} = 0\)</span>. Our model would fit the data perfectly!</p>
<p>Although you might think that a perfectly fitting model is ideal (why wouldn’t you be happy when a model predicts the data perfectly?), that is far from the case here. Let’s consider the number of parameters in the model. If we have <span class="math inline">\(P\)</span> people, and <span class="math inline">\(A\)</span> levels for condition, our model estimates <span class="math inline">\(1 + (P-1) + (A-1) + (P-1)\times(A-1) = P \times A\)</span> parameters, which is equal to the total number of observations <span class="math inline">\(n = P \times A\)</span>. A model with as many parameters as observations is bound to <em>overfit</em> the data.</p>
<p>While the model specified in Equation <a href="repeated-measures-anova.html#eq:one-RM-ANOVA-model-ch08">(10.1)</a> makes sense, in practice, we can not distinguish between the interaction term <span class="math inline">\(\tau^{(P\times A)}_{i,j}\)</span> and the error term <span class="math inline">\(\epsilon_{i,j}\)</span>. To signify this, we could rewrite the model as
<span class="math display">\[\begin{equation}
Y_{i,j} = \mu + \tau^{(P)}_i + \tau^{(A)}_j + (\tau^{(P\times A)} + \epsilon)_{i,j}
\end{equation}\]</span></p>
<!--
both $\tau_{i,j}$ and $\epsilon_{i,j}$ target the same difference between observation $Y_{i,j}$ and $\mu + \tau^{(P)}_i + \tau^{(A)}_j$. Whilst it is straightforward to compute this difference
$$Y_{i,j} - (\mu + \tau^{(P)}_i + \tau^{(A)}_j)$$
we cannot determine which part of this belongs to $\tau^{(P\times A)}_{i,j}$, and which part to $\epsilon_{i,j}$.
-->
<!-- It's like a pizza,  where some part is yours, and some part belongs to a random stranger. Halves may seem fair, but what if you haven't eaten for days, and perhaps the stranger is starving. Without knowing anything else, it could be just as fair to give on person everything and the other nothing, as splitting in equal halves. The right way to split is, in that case, indeterminate. When two terms in an equation "fight" for the same slice of the pie, that is similar. There is no way to  determine what belongs to what. -->
<p>To test for the main effect of experimental factor <span class="math inline">\(A\)</span>, we can compare the full model to a reduced model where we set <span class="math inline">\(\tau^{(A)}_j = 0\)</span> for all <span class="math inline">\(j\)</span>. This provides MODEL R:
<span class="math display">\[\begin{equation}
Y_{i,j} = \mu + \tau^{(P)}_i + (\tau^{(P\times A)} + \epsilon)_{i,j}
\end{equation}\]</span></p>
<p>You might wonder if you can or should also test for the main effect of persons (P). The short answer is: no. The longer answer is that the inability to distinguish between the interaction term <span class="math inline">\(\tau^{(P\times A)}_{i,j}\)</span> and the error term <span class="math inline">\(\epsilon_{i,j}\)</span> affects the estimates of the person effects <span class="math inline">\(\hat{\tau}^{(P)}_i\)</span> and treatment effects <span class="math inline">\(\hat{\tau}^{(A)}_j\)</span> differently. Whilst the variance of the sampling distribution of the estimates of the treatment effects <span class="math inline">\(\hat{\tau}^{(A)}_j\)</span> depends on both the variance in the interaction term <span class="math inline">\(\tau^{(P\times A)}_{i,j}\)</span> and the error term <span class="math inline">\(\epsilon_{i,j}\)</span>, the variance of the sampling distribution of the person effects depends only on the variance of the error term <span class="math inline">\(\epsilon_{i,j}\)</span>. Intuitively, you might think of this as follows: whilst the treatment effects may differ between persons (as reflected in the interaction term <span class="math inline">\(\tau^{(P\times A)}_{i,j}\)</span>), these deviations from the average treatment effects (as reflected in the treatment term <span class="math inline">\(\tau^{(A)}_{j}\)</span>) are random, and will cancel each other out.</p>
<p>involves me telling you why, and requires us first to consider the difference between fixed and random effects.</p>
</div>
<div id="fixed-vs-random-effects" class="section level3" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> Fixed vs random effects</h3>
<p>A set of participants in a study is generally a sample from a (much) larger population of potential participants. Insofar as we are interested in differences between participants (i.e., an effect of P), we are not so much interested in whether there are differences between the actual participants included in the study, but whether there are differences between the people in the whole population. In other words, we would like to infer from variability between participants in a study whether there are differences between people in the population of interest. That means we want to generalize person effects in a particular sample of participants to person effects in the general population. To make such a leap from sample to population, we need a DGP for person effects, and commonly we might assume that each person effect is drawn from a Normal distribution:
<span class="math display">\[\tau_i^{(P)} \sim \mathbf{Normal}(0,\sigma_\text{P})\]</span>
Our objective here is to estimate the variability between persons, i.e. <span class="math inline">\(\sigma_\text{P}\)</span>, and <em>not</em> to estimate each <span class="math inline">\(\tau^{P}_i\)</span> as precisely as possible.</p>
<p>The generalization problem for fixed effects (experimental manipulations) is different: our objective is to infer from a sample what effect precisely these experimental manipulations will have in the whole population. The objective here is to get the most precise estimation of each <span class="math inline">\(\tau_j^{(A)}\)</span>.</p>
</div>
</div>
<div id="assumptions-1" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> Assumptions</h2>
<div id="sphericity" class="section level3" number="10.4.1">
<h3><span class="header-section-number">10.4.1</span> Sphericity</h3>
<p><span class="math display">\[\Sigma = \left[ \begin{matrix} \sigma_1^2 &amp; \sigma_{12} &amp; \sigma_{13} \\ \sigma_{21} &amp; \sigma_2^2 &amp; \sigma_{23} \\ \sigma_{31} &amp; \sigma_{32} &amp; \sigma_3^2 \end{matrix} \right]\]</span>
So homogeneity of variance means
<span class="math display">\[\sigma_1^2 = \sigma_2^2 = \sigma_3^2\]</span>
For repeated-measures analyses, an additional assumption is necessary for the covariances, namely that of <strong>sphericity</strong>. This assumption means that the <em>variances of all pairwise differences are equal</em>. The variance of a pairwise difference between two (possibly correlated) variables <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> is <span class="math inline">\(\sigma_1^2 + \sigma_2^2 - 2 \sigma_{12}\)</span>, i.e. the sum of the variances of the two variables, minus twice the covariance. Hence, the assumption of sphericity can be stated as:
<span class="math display">\[\sigma_j^2 + \sigma_k^2 - 2\sigma_{jk} = \sigma_{l}^2 + \sigma_{m}^2 - 2 \sigma_{lm} \quad \quad \text{for all } j,k,l,m\]</span>
For example, for three variables, there are 3 pairwise differences, and hence the assumption is</p>
<p><span class="math display">\[\sigma_1^2 + \sigma_2^2 - 2\sigma_{12} = \sigma_{1}^2 + \sigma_{3}^2 - 2 \sigma_{13} = \sigma_{2}^2 + \sigma_{3}^2 - 2 \sigma_{23}\]</span>
If that seems like a complicated and stringent assumption: it is! And it is not that easy to check. Sphericity holds when a more stringent condition, called <strong>compound symmetry</strong> holds. Compound symmetry means that all variances are identical to each other, and all covariances are identical to each other.</p>
</div>
<div id="correcting-for-non-sphericity" class="section level3" number="10.4.2">
<h3><span class="header-section-number">10.4.2</span> Correcting for non-sphericity</h3>
<p>The assumption of</p>
<p>When the assumption of sphericity does <em>not</em> hold (the assumption is violated) the <span class="math inline">\(F\)</span>-statistic still follows an <span class="math inline">\(F\)</span> distribution, but with a smaller value for <span class="math inline">\(\text{df}_1\)</span> and <span class="math inline">\(\text{df}_2\)</span> than usual. The correct degrees of freedom can be stated as <span class="math inline">\(\zeta \times \text{df}_1\)</span> and <span class="math inline">\(\zeta \times \text{df}_2\)</span>, where <span class="math inline">\(0 \geq \zeta \geq 1\)</span> is a correction fraction.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Whilst the value of <span class="math inline">\(\zeta\)</span> depends on the true (co)variances underlying the data, it’s value can be estimated. The Greenhouse-Geisser estimate. Huynh and Feldt showed that, if the true value is close to or higher than <span class="math inline">\(\zeta = 0.75\)</span>, the Greenhouse-Geisser correction tends to be too conservative. They suggested a correction which provides an upward-adjusted estimate of <span class="math inline">\(\zeta\)</span>, which will increase the power of the tests. Howell suggests to use the Huynh-Feldt correction when the Greenhouse-Geisser estimate of <span class="math inline">\(\zeta\)</span> is close to or higher than <span class="math inline">\(\hat{\zeta} = 0.75\)</span>.</p>
<p>A statistical test for the assumption of sphericity was developed by Mauchly (1940) and is known as Mauchly’s sphericity test. Whilst routinely provided by statistical software, it is not an ideal test, as it rests strongly on the assumption of normality and it commonly has low power. Rather than only correcting the degrees of freedom after a significant Mauchly test, one might consider <em>always</em> adjusting the degrees of freedom according to the Greenhouse-Geisser or Huynh-Feldt correction (whichever is deemed more appropriate).</p>
</div>
</div>
<div id="effect-size-1" class="section level2" number="10.5">
<h2><span class="header-section-number">10.5</span> Effect size</h2>
</div>
<div id="mixed-anova-with-between-and-within-factors" class="section level2" number="10.6">
<h2><span class="header-section-number">10.6</span> Mixed ANOVA with between and within factors</h2>
<p><span class="math display" id="eq:glm-mixed-RM-ANOVA-model">\[\begin{equation}
Y_{i,j,k} = \mu + \tau^{(P)}_i + \tau^{(A)}_j + \tau^{(B)}_k + \tau^{(P \times A)}_{i,j} + \tau^{(P \times B)}_{i,k} + \tau^{(A \times B)}_{j,k} + \epsilon_{i,j,k}
\tag{10.2}
\end{equation}\]</span>
Note that we are excluding the three-way interaction term <span class="math inline">\(\tau^{(P \times A \times B)}_{i,j,k}\)</span> as again, this can not be distinguished from the error term <span class="math inline">\(\epsilon_{i,j,k}\)</span>.</p>
</div>
<div id="in-practice-4" class="section level2" number="10.7">
<h2><span class="header-section-number">10.7</span> In practice</h2>
<div id="defining-and-estimating-the-model" class="section level3" number="10.7.1">
<h3><span class="header-section-number">10.7.1</span> Defining and estimating the model</h3>
</div>
<div id="assessing-the-assumptions" class="section level3" number="10.7.2">
<h3><span class="header-section-number">10.7.2</span> Assessing the assumptions</h3>
</div>
<div id="reporting-the-results" class="section level3" number="10.7.3">
<h3><span class="header-section-number">10.7.3</span> Reporting the results</h3>
</div>
</div>
<div id="summary-6" class="section level2" number="10.8">
<h2><span class="header-section-number">10.8</span> Summary</h2>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-carragher2019limited" class="csl-entry">
Carragher, D. J., Thomas, N. A., Gwinn, O. S., &amp; Nicholls, M. E. (2019). Limited evidence of hierarchical encoding in the cheerleader effect. <em>Scientific Reports</em>, <em>9</em>, 1–13.
</div>
<div id="ref-walker2014hierarchical" class="csl-entry">
Walker, D., &amp; Vul, E. (2014). Hierarchical encoding makes individuals in a group seem more attractive. <em>Psychological Science</em>, <em>25</em>, 230–235.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Other examples of such clustering in data are when data is collected in group settings, such as students within classrooms, or patients within hospitals. In such situations one could expect again that observations within each cluster (i.e., a specific group, classroom, or hospital) are more similar to each other than observations across clusters.<a href="repeated-measures-anova.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><span class="citation"><a href="#ref-carragher2019limited" role="doc-biblioref">Carragher et al.</a> (<a href="#ref-carragher2019limited" role="doc-biblioref">2019</a>)</span> use different names for the conditions. They call the Different condition the Control condition, the Similar condition the Distractor condition, the Identical condition the Identical-distractors condition, and the Variant condition the Self-distractors condition.<a href="repeated-measures-anova.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>The correction factor is usually denoted by <span class="math inline">\(\epsilon\)</span>, but I’m using <span class="math inline">\(\zeta\)</span> (“zeta”) as we are already using <span class="math inline">\(\epsilon\)</span> for the error terms.<a href="repeated-measures-anova.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mixing-categorical-and-metric-predictors-ancova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-mixed-effects-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"source": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
