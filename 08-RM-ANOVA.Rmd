# Repeated-measures ANOVA {#ch-RM-ANOVA}

In this chapter, we will discuss how to deal with the non-independence of errors that can result from repeated measures of the same individual or other forms of groupings in the data, within the context of ANOVA analyses. The resulting class of models is known as repeated-measures ANOVA. 

## Non-independence in linear models

The General Linear Model we have considered thus far can be stated as follows:

$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
The assumptions of this model concern the residuals or errors $\epsilon_i$. They are assumed to be _independent and identically distributed_ (iid), following a Normal distribution with a mean of 0 and a standard deviation $\sigma_\epsilon$.

You can expect violations of the _iid_ assumption if data are collected from "units of observation" that are clustered in groups. A common example of this in psychology is when you repeatedly observe behaviour from the same person.^[Other examples of such clustering in data are when data is collected in group settings, such as students within classrooms, or patients within hospitals. In such situations one could expect again that observations within each cluster (i.e., a specific group, classroom, or hospital) are more similar to each other than observations across clusters.] For example, suppose participants in an experiment perform two tests to measure their working memory capacity. A person with a high working memory capacity would score high on both tests, whilst a person with a low working memory capacity would score low on both. That means that the scores in the two tests are correlated, and hence _not_ independent. If the model does not adequately account for this, then the residuals (errors) would also not be independent. 

<!-- As another example of dependency between model residuals, let's go back to the simplest version of the GLM, the model with just an intercept that was the focus of Chapter \@ref(ch-simple-GLM):
\begin{equation}
Y_i = \beta_0 + \epsilon_i
(\#eq:simple-glm-ch08)
\end{equation}
Suppose the dependent variable $Y$ again reflects judgements of the height of Mount Everest, but that one of the labs in the ManyLabs study that collected the data, was based in Nepal. We might expect participants in Nepal to have a more accurate idea of the height of Mount Everest than participants in Poland (the country the participants were from in the subset of the data we considered in Chapter \@ref(ch-simple-GLM)). As a result, participants in Nepal would likely be less influenced by the low anchor than those with less knowledge of Mount Everest. If $\beta_0$ represents the mean judgement over all participants, then participants in Nepal would generally provide judgements which are higher than $\beta_0$ (i.e positive errors), whilst participants in Poland would generally provide judgements that are lower than $\beta_0$ (i.e. negative errors). If our model does not account for such grouping in the data, then the errors for cases from Nepal would be correlated, because all would tend to be positive. Similarly, the errors for cases from Poland would also be correlated, as all would tend to be negative. 

What this example points to is that the simple model does not account for the fact that participants from different countries might have a different average judgement. As such, the model is misspecified. Here, that leads to dependent errors. Unfortunately, compared to violations of other assumptions, such as the normality assumption or the assumption of variance homogeneity, standard statistical procedures are usually _not_ robust to violations of the independence assumption [@kenny_consequences_1986;@judd_treating_2012]. They often lead to considerably increased Type I errors (i.e., false positives) and more generally can produce overconfident results (e.g., too narrow standard errors).

In the example above, we could alleviate the problem of dependent errors by including an effect for country, using a contrast-coding predictor $X_1$ to reflect the difference between participants from Nepal ($X_1 = \tfrac{1}{2}$) and Poland ($X_1 = -\tfrac{1}{2}$) in an expanded model
$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i$$
This model would allow for a different mean judgement for cases from Nepal ($\beta_0 + \tfrac{1}{2} \times \beta_1$) and cases from Poland ($\beta_0 - \tfrac{1}{2} \times \beta_1$). Effectively, this is the strategy employed in repeated-measures ANOVA: include an additional grouping factor in the model.

<!-- The assumption that the errors are independent means that knowing the value of the error for one case $i$ in the data does not give you any information to determine the value of the error for another case $j$. More formally, statistical independence is defined in terms of conditional probabilities. Remember when we discussed the rules of probability (Section \@ref(sec:02-rules-of-probability))? In particular, we used independence to state rule 7, which is a specific case of the multiplication rule (rule 6). We can define the distribution of the error for case $i$ as a conditional probability $p(\epsilon_i | \epsilon_i, X_{1,i}, \ldots, X_{m,i})$, where we conditionalise on all the predictor values, as well as another error term $\epsilon_j$. The errors are independent when 
$$p(\epsilon_i | \epsilon_j, X_{1,i}, \ldots, X_{m,i}) = p(\epsilon_i |  X_{1,i}, \ldots, X_{m,i})$$
i.e. the distribution of $\epsilon_i$, conditional upon knowing the value of the predictors and the error $\epsilon_j$ of another case $j$, is the same as the distribution conditional upon just knowing the value of the predictors. This means that error $\epsilon_j$ provides no information about the error $\epsilon_i$.
-->

## The cheerleader effect

The attractiveness of a person's face is traditionally considered to be related to physical features, such as how symmetrical the features are, how close a face is to the "average" face over many people, and sexual dimorphism (whether a face looks exclusively male or female). However, there is also evidence that perceived facial attractiveness can vary due to factors outside of the face. In what has become known as the "cheerleader effect", the same face is perceived to be more attractive when seen in a group, as compared to when it is seen alone. @walker2014hierarchical proposed that the cheerleader effect arises due to people encoding faces in a hierarchical manner. When presented with a group of faces, people encode the display by first calculating an average face for the group, and then encoding individual faces through deviations from the group average. As average faces tend to be perceived as more attractive than individual faces, this then results in each face in the group being perceived as more attractive, because the average part of the encoding is perceived as attractive. 
@carragher2019limited set out to test this explanation. In one part of the study (Experiment 1), they let participants rate the attractiveness of a face when presented by itself (the Alone condition), as part of a group of different faces (the Different condition), or as part of a group of similar faces (the Similar condition). There were two variants of the latter condition, and participants encountered only one of them in the experiment. In the Identical condition, the group consisted of three copies of exactly the same photo. In the Variant condition, the group consisted of three different photos of the same face.^[@carragher2019limited use different names for the conditions. They call the Different condition the Control condition, the Similar condition the Distractor condition, the Identical condition the Identical-distractors condition, and the Variant condition the Self-distractors condition.] The authors argued that if the hierarchical-encoding explanation for the cheerleader effect is true, then the cheerleader effect should not be observed in the Identical condition. This is because the average of three identical photos is just the photo itself, so there is no difference between an encoded average face and the face itself. In the Variant condition however, variability between the different photos of the same face might still lead to an average face which is more attractive than each individual face. This is an example of a 2 (version: Identical, Same) by 3 (condition: Alone, Different, or Similar) design. The first factor (Version) varied between people, and the second (Condition) within people (each participant rated a face in each of the three conditions). The rated attractiveness of the faces in the different conditions are provided in Figure \@ref(fig:cheerleader-raincloud-plot).

```{r cheerleader-raincloud-plot, fig.cap="Attractiveness ratings for photo's of faces when presented alone, as part of a group of dissimilar faces (Control), or as part of a group of similar faces (Distractor), which are either identical (Identical-distractors) or different photos (Self-distractors) ofd the same face."}
library(dplyr)
library(ggplot2)
library(sdamr)
data("cheerleader")
dat <- subset(cheerleader, Excluded == 0) %>%
  mutate(Item = factor(Item)) %>%
  mutate(Item = forcats::fct_recode(Item, Different = "Control_Group", Same = "Distractor_Manipulation"),
         Task = forcats::fct_recode(Task, Identical = "Identical-Distractors", Variant = "Self-Distractors"))
dat %>%
  # group_by(Task,Item) %>%
  sdamr::plot_raincloud(y=Response, groups = Item) + facet_wrap(~Task)
```

## A oneway repeated-measures ANOVA

To keep matters relatively simple, we will for now just consider participants in the Variant condition. That means that we can treat the study as a oneway design, with three levels (condition: Alone, Different, or Same) that all vary within-participants. As the cheerleader effect predicts that faces presented as part of a group will be rated as more attractive, and that this effect will be larger if there is more variety in the faces within the group, a reasonable set of contrast codes is:
```{r helmert-contrast-cheerleader}
tab <- data.frame(c1 = c("$-\\tfrac{2}{3}$","$\\tfrac{1}{3}$","$\\tfrac{1}{3}$"),
                  c2 = c("$0$","$\\tfrac{1}{2}$","$-\\tfrac{1}{2}$"))
colnames(tab) <- c("$c_1$","$c_2$")
rownames(tab) <- c("Alone","Different", "Same")
knitr::kable(tab, booktabs=TRUE, linesep="", align="r")
```
The first contrast code reflects the expectation that a face in a group (Different or Same) will be rated as more attractive than faces in the Alone condition. The second contrast reflects the expectation that a face surrounded by different faces will be rated as more attractive than when surrounded by more similar faces.

If we were to (wrongly!) treat all ratings as independent, and analyse the data with a regular oneway ANOVA, we would obtain the results in Table \@ref(tab:cheerleader-oneway-ANOVA-results). As you can see, this analysis indicates there is no effect of presenting a face alone or in a group.

```{r cheerleader-oneway-ANOVA-results}
dat <- dat %>%
  filter(Task == "Variant") %>%
  mutate(Item = factor(Item))
codes <- cbind(c(-2/3,1/3,1/3),
               c(0,1/2,-1/2))
contrasts(dat$Item) <- codes
modg <- lm(Response ~ Item, data=dat)
opts <- options()
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modg)[1],NA,coefficients(modg)[2:3],Error=NA),rbind(car::Anova(modg, type=3)[1:2,],expand_Anova(modg, type=3)[-1,]))
rownames(tab) <- c("Intercept","Condition","$\\quad X_1$ (D+S vs A)","$\\quad X_2$ (D vs S)", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting attractiveness ratings in the three conditions, wrongly assuming independence.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

The problem in this analysis is that it ignores the (sometimes rather large) differences between participants in how attractive they find a face on average (i.e. over the three conditions). Table \@ref(tab:cheerleader-self-attractiveness-table) shows the attractiveness ratings for 10 participants in the Variant condition. You can see there that when a participant rates a face as relatively attractive when presented alone, (s)he also tends to rate the face as relatively attractive when shown in a group of faces. Some participants (e.g. participant 37) rate the face as relatively unattractive in all conditions. This indicates individual differences in how attractive people find a face. Simply put: people's tastes differ. Whilst that may be interesting in its own right, for the purposes of the experiment, we do not care about such individual differences. What we want to know is whether the attractiveness of a face increases when presented as part of a group. But to answer this question, we shouldn't completely ignore individual differences. 

```{r cheerleader-self-attractiveness-table}

dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = Participant, names_from = Item, values_from = Response) %>%
  ungroup() %>%
  filter(row_number() <= 10) %>%
  mutate("Average" = (Alone + Different + Same)/3, 
         "D-A" = Different - Alone,
         "S-A" = Same - Alone) %>%
  mutate(Alone = format(Alone, digits=2, nsmall=2),
         Different = format(Different, digits=2, nsmall=2),
         Same = format(Same, digits=2, nsmall=2)) %>%
  knitr::kable(row.names = FALSE, col.names = c("Participant","Alone", "Different", "Same", "Average", "D-A", "S-A"), digits=2, caption=c("Attractiveness ratings in the Alone, Different, and Same condition for 10 participants in the Variant condition. Also shown is the average over the three ratings for each participant, as well as differences between the rating in the Different and Alone condition (D-A) and between the Same and Alone condition (S-A)."))  

#  summarize(Response = mean(Response)) %>%
#  ggplot(aes(x=Response)) + geom_histogram(bins=10)

```

<!-- This individual variability makes the ratings within each condition also highly variable. By ignoring that we have repeated measures for each participant, the model errors are the differences between each rating in a condition and the average in that condition. --> 
If we let $Y_{i,j}$ denote a response by person $i$ in condition $j$, the errors in this model are $\hat{\epsilon}_{i,j} = Y_{i,j} - \overline{Y}_{\cdot,j}$. But if you consider Table \@ref(tab:cheerleader-self-attractiveness-table), you might realise that we can much more specific in considering what the effect of displaying a face amongst in a group is. For each participant, we could consider whether the rating in e.g. the Different condition is higher than the rating in the Alone condition. These difference scores are provided in the D-A and S-A columns. For participant 36, the rating in the Different condition is a little lower than their rating in the Alone condition, and for participant 37, it is a little higher. These difference scores in some sense remove variability in how attracted people are to a face in general. The difference scores just reflect whether people find a face more or less attractive when presented in the context of a group of faces compared to when presented alone. We can then simply ask whether these differences are, on average, positive (indicating increased attractiveness) or negative (indicating decreased attractiveness). We can answer this question by using a one-sample t-test on the difference scores, comparing a general intercept-only MODEL G to an even simpler MODEL R where we fix the intercept to 0. If we can reject the null hypothesis that the mean of the difference is equal to 0, that is evidence of an effect of the experimental manipulation. 

For example, we can compare attractiveness ratings between the different and alone conditions by comparing a MODEL G for the D-A difference scores:
$$\begin{aligned} (\text{D - A})_i &= Y_{i,\text{D}} - Y_{i,\text{A}} \\ &= \beta_0 + \epsilon_{\text{D-A},i} \end{aligned}$$
to a MODEL R:
$$\begin{aligned} (\text{D - A})_i &= Y_{i,\text{D}} - Y_{i,\text{A}} \\ &= 0 + \epsilon_{\text{D-A},i} \end{aligned}$$
```{r}
wdat <- dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = Participant, names_from = Item, values_from = Response) %>%
  ungroup() %>%
  mutate("Average" = (Alone + Different + Same)/3, 
         "D_A" = Different - Alone,
         "S_A" = Same - Alone)
aov1 <- anova(lm(`D_A`~0, data=wdat), lm(`D_A`~1, data=wdat))
aov2 <- anova(lm(`S_A`~0, data=wdat), lm(`S_A`~1, data=wdat))
```
to assess whether the difference scores are on average different from 0. The result of this test is `r write_anova_results(aov1)`. Using a similar model comparison
approach for the S-A difference scores we obtain a test result of `r write_anova_results(aov2)`. So, if we focus on differences between the conditions "within persons" (by computing differences between the conditions for each person), we find evidence that both the Different and Same condition differ from the Alone condition. On average, participants' rating of the attractiveness of a face was `r  round(coefficients(lm(D_A~1, data=wdat))[1],2)` points higher when presented amongst a group of different faces, and `r  round(coefficients(lm(S_A~1, data=wdat))[1],2)` points higher when presented amongst a group of similar faces, compared to when the face was presented by itself. Given that attractiveness was rated on a scale between 1-100, these are not large differences, but they are statistically significant.

### Within-subjects composite scores

The idea of computing difference scores and then using these in linear models is, essentially, how we will test for effects of manipulations that vary within persons. To do this more generally, we will apply orthogonal contrast codes to compute such difference scores. We will refer to the resulting within-person-difference-scores as __within-subjects composite scores__. 

We will use the same contrast codes as before:
```{r}
tab <- data.frame(c1 = c("$-\\tfrac{2}{3}$","$\\tfrac{1}{3}$","$\\tfrac{1}{3}$"),
                  c2 = c("$0$","$\\tfrac{1}{2}$","$-\\tfrac{1}{2}$"))
colnames(tab) <- c("$c_1$","$c_2$")
rownames(tab) <- c("Alone","Different", "Same")
knitr::kable(tab, booktabs=TRUE, linesep="", align="r")
```
<!-- The first contrast code reflects the expectation that a face in a group (Different or Same) will be rated as more attractive than faces in the Alone condition. The second contrast reflects the expectation that a face surrounded by different faces will be rated as more attractive than when surrounded by more similar faces.-->

For each contrast $c_j$, we compute a within-subjects composite score as:
\begin{equation}
W_{j,i} = \frac{\sum_{k=1}^g c_{j,k} Y_{i,k}}{\sqrt{\sum_{k=1}^q c_{j,k}^2}}
(\#eq:within-subjects-composite-scores-ch08)
\end{equation}
The top part of this equation (the numerator) is just the sum of a participant $i$'s score in each condition $k$ multiplied by the corresponding value of contrast code $c_j$. The bottom part (the denominator) is a scaling factor, computed as the square-root of the sum of the squared contrast values. The reason for applying this scaling factor is to make the sums of squares of the resulting analyses add up to the total sum of squares (i.e. the Sum of Squared Error of an intercept-only model). It is not of theoretical importance. 

As an example, let's compute the within-subjects composite scores for participant 37 in Table \@ref(tab:cheerleader-self-attractiveness-table). For contrast $c_1$, we compute
$$\begin{aligned}
W_{1,36} &= \frac{-\tfrac{2}{3} \times 56.32 + \tfrac{1}{3} \times 55.92 + \tfrac{1}{3} \times 54.30}{\sqrt{ \left(-\tfrac{2}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2)}} \\
&= \frac{-0.81}{\sqrt{\tfrac{6}{9}}} = `r round((-(2/3)*56.32 + (1/3)*55.92 + (1/3)*54.3)/sqrt(6/9),2)`
\end{aligned}$$
For contrast $c_2$, the within-subjects composite is computed as:
$$\begin{aligned}
W_{2,36} &= \frac{\tfrac{1}{2} \times 55.92 + (-\tfrac{1}{2}) \times 54.30}{\sqrt{\left(\tfrac{1}{2}\right)^2 + \left(-\tfrac{1}{2}\right)^2)}} \\
&= \frac{-0.81}{\sqrt{\tfrac{2}{4}}} = `r round(((1/2)*55.92 + (-1/2)*54.3)/sqrt(2/4),2)`
\end{aligned}$$
Table \@ref(tab:cheerleader-self-composite-table) shows the resulting values for other participants as well.
```{r cheerleader-self-composite-table}
#options(pillar.sigfig = 2)
dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = Participant, names_from = Item, values_from = Response) %>%
  ungroup() %>%
  filter(row_number() <= 10) %>%
  mutate("W0" = (Alone + Different + Same)/sqrt(3), 
         "W1" = (-(2/3)*Alone + (1/3)*Different + (1/3)*Same)/sqrt((2/3)^2 + (1/3)^2 + (1/3)^2),
         "W2" = (-(1/2)*Same + (1/2)*Different)/sqrt(1/2)) %>%
  mutate(Alone = format(Alone, digits=2, nsmall=2),
         Different = format(Different, digits=2, nsmall=2),
         Same = format(Same, digits=2, nsmall=2)) %>%
  knitr::kable(row.names = FALSE, col.names = c("Participant","Alone", "Different", "Same", "$W_0$", "$W_1$", "$W_2$"), digits=c(0,2,2,2,2,2,2), caption=c("Attractiveness ratings in the Alone, Different, and Same condition for 10 participants in the Variant condition. Also shown are three within-subjects composite scores."))  

```

```{r}
wdat <- dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = Participant, names_from = Item, values_from = Response) %>%
  ungroup() %>%
  mutate("W0" = (Alone + Different + Same)/sqrt(3), 
         "W1" = (-(2/3)*Alone + (1/3)*Different + (1/3)*Same)/sqrt((2/3)^2 + (1/3)^2 + (1/3)^2),
         "W2" = (-(1/2)*Same + (1/2)*Different)/sqrt(1/2))
aov1 <- anova(lm(W1~0, data=wdat), lm(W1~1, data=wdat))
aov2 <- anova(lm(W2~0, data=wdat), lm(W2~1, data=wdat))
```

The first within-subjects composite variable reflects the difference between the Alone condition and the average of the Different and Same conditions. If the mean of this composite variable is positive, that indicates that faces in the Different and Same condition are on average rated as more attractive than in the Alone condition. If the mean of this composite variable is negative, that indicates that faces in the Different and Same condition are on average rated as less attractive than in the Alone condition. If the mean is equal to 0, that indicates there is no difference in attractiveness. To test whether there is an effect of presenting faces in a group, compared to alone, we can compare a MODEL G
$$W_{1,i} = \beta_0 + \epsilon_i$$
to a MODEL R:
$$W_{1,i} = 0 + \epsilon_i$$
The results of this model comparison are `r write_anova_results(aov1)`. We can thus reject the null hypothesis that there is no difference between the Alone condition and the other two. The estimated intercept of MODEL G is $\hat{\beta}_0 = `r round(coefficients(lm(W1~1, data=wdat))[1],2)`$. Due to the scaling applied in the within-subjects composite, this intercept does _not equal_ the average of $\frac{Y_{i,D} + Y_{i,S}}{2} - Y_{i,A}$. To get this value, we need to __rescale__ the within-subjects composite to the scale of the dependent variable. We do this by dividing the estimated intercept by the scaling factor, which is $\sqrt{ \left(-\tfrac{2}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2} = \sqrt{\tfrac{6}{9}}$. So
$$\frac{\overline{Y}_{D} + \overline{Y}_{S}}{2} - \overline{Y}_{A} = \frac{`r round(coefficients(lm(W1~1, data=wdat))[1],2)`}{\sqrt{\tfrac{6}{9}}} = `r round(coefficients(lm(W1~1, data=wdat))[1]/sqrt(6/9),2)`$$
We can conduct a similar model comparison for the second within-subjects composite, $W_2$. The results of this comparison are `r write_anova_results(aov2)`. Hence, we can not reject the null hypothesis that there is no difference between the Different and Alone condition. The estimated intercept of MODEL G is $\hat{\beta}_0 = `r round(coefficients(lm(W2~1, data=wdat))[1],2)`$. Rescaling this to the scale of the dependent variable indicates that
$$\overline{Y}_{D} - \overline{Y}_{S} = \frac{`r round(coefficients(lm(W2~1, data=wdat))[1],2)`}{\sqrt{\tfrac{2}{4}}} = `r round(coefficients(lm(W2~1, data=wdat))[1]/sqrt(2/4),2)`$$.
Comparing the results to the oneway ANOVA of Table \@ref(tab:cheerleader-oneway-ANOVA-results) shows that we get the same parameter estimates (after rescaling), but different test results. That is because the analyses using within-subjects composite scores consider the effects of presenting a face in isolation or a group of faces, regardless of whether someone is attracted to that face or not. 

## Partitioning the variance

$W_0$

Show graph with partitioning

## A 2 (between) by 3 (within) mixed ANOVA

Up to now, we just considered the data from the Variant conditions. Having worked out how to perform effective comparisons of the within-subjects conditions, we are now in a position to consider all the data. Whether the other faces in the Similar condition where exact copies of a single photograph, or different photographs of the same person, was a manipulation that varied _between participants_ (i.e., a participant was only assigned to one of these manipulations, not both). The full design of the study is thus a 2 () by 3 design, where the last factor varied within participants.

<!--
We could do this for any pair of conditions, but that might result in a large number of tests. A repeated-measures ANOVA allows us to to essentially this within a single analysis.

### Including effects for participants

In a repeated-measures ANOVA, we include Person as another factor in the design, in addition to the Condition factor. As the design includes each combination of Person and Condition, this can be treated as a Person by Item factorial design. We can state an ANOVA model as
<!--
\begin{equation}
Y_{i,j} = \mu + \tau^{(P)}_i + \tau^{(A)}_j + \epsilon_{i,j}
(\#eq:glm-oneway-RM-ANOVA-model)
\end{equation}
But this assumes that the effect of a treatment $j$ is the same for all
individuals $i$. If we would allow the effect of a treatment to be different
for participants, that would amount to including an interaction term (the 
effect of treatment $j$ is moderated by individual $i$). This could be stated as

\begin{equation}
Y_{i,j} = \mu + \pi_i + \tau_j + (\pi \tau)_{i,j} + \epsilon_{i,j}
(\#eq:one-RM-ANOVA-model-ch08)
\end{equation}
Here, I'm using a somewhat different notation than in Equation \@ref(eq:glm-factorial-ANOVA-model). Here, $\pi_i$ stands for the person-effect of participant $i$:
$$\pi_i = \mu_{i,\cdot} - \mu$$
and as usual, $\tau_j$ represents the treatment-effect of level $j$ of the experimental factor (Condition):
$$\tau_j = \mu_{\cdot, j} - \mu$$
$(\pi \tau)_{i,j}$ reflects the person-by-treatment interaction effect:
$$(\pi \tau)_{i,j} = \mu_{i,j} - (\mu+\pi_i + \tau_j)$$
There is one complication, however. We only have a single observation for each combination of person and condition. That means that the average for each combination of person and condition, $\overline{Y}_{i,j} = \hat{\mu}_{i,j}$, which is what the model aims to predict, equals the observation $Y_{i,j}$. As such, the errors of the estimated model would all be equal to $\hat{\epsilon}_{i,j} = 0$. In other words, the model fits the data perfectly!

Although you might think that a perfectly fitting model is ideal (why wouldn't you be happy when a model predicts the data perfectly?), that is far from the case here. Let's consider the number of parameters in the model. If we have $P$ people, and $A$ levels for condition, our model estimates $1 + (P-1) + (A-1) + (P-1)\times(A-1) = P \times A$ parameters, which is equal to the total number of observations: $n = P \times A$. A model with as many parameters as observations is bound to _overfit_ the data.

The model of Equation \@ref(eq:one-RM-ANOVA-model-ch08) makes intuitive sense. In practice, however, because there is just one observation for each participant/condition combination, we can not distinguish between the interaction term $\pi\tau_{i,j}$ and the error term $\epsilon_{i,j}$. To make this explicit, we could rewrite the model as
\begin{equation}
Y_{i,j} = \mu + \pi_i + \tau_j + (\tau\pi + \epsilon)_{i,j}
\end{equation}
This is just notational, and does not resolve anything. A more practical version is the following MODEL G:
\begin{equation}
Y_{i,j} = \mu + \pi_i + \tau_j + \epsilon_{i,j}
\end{equation}
which is a model that can be estimated by assigning appropriate contrasts to the Item and Participant factors. 
<!--
both $\tau_{i,j}$ and $\epsilon_{i,j}$ target the same difference between observation $Y_{i,j}$ and $\mu + \tau^{(P)}_i + \tau^{(A)}_j$. Whilst it is straightforward to compute this difference
$$Y_{i,j} - (\mu + \tau^{(P)}_i + \tau^{(A)}_j)$$
we cannot determine which part of this belongs to $\tau^{(P\times A)}_{i,j}$, and which part to $\epsilon_{i,j}$.
-->

<!-- It's like a pizza,  where some part is yours, and some part belongs to a random stranger. Halves may seem fair, but what if you haven't eaten for days, and perhaps the stranger is starving. Without knowing anything else, it could be just as fair to give on person everything and the other nothing, as splitting in equal halves. The right way to split is, in that case, indeterminate. When two terms in an equation "fight" for the same slice of the pie, that is similar. There is no way to  determine what belongs to what. -->

<!--
To test for the main effect of Condition, we can compare MODEL G above to a reduced model where we set $\tau_j = 0$ for all $j$. This provides MODEL R:
\begin{equation}
Y_{i,j} = \mu + \pi_i + \epsilon_{i,j}
\end{equation}

```{r}
dat$Participant <- factor(dat$Participant)
contrasts(dat$Participant) <- contr.sum(length(unique(dat$Participant)))
anova(lm(Response ~ Participant, data=dat), lm(Response ~ Participant + Item, data=dat))
```

Comparing MODEL G to MODEL R provides an appropriate (omnibus) test for the main effect of Condition. [INCLUDE RESULTS] You might wonder if you can or should also test for the main effect of Person. The short answer is: no. The longer answer is that the inability to distinguish between the interaction term $(\pi\tau)_{i,j}$ and the error term $\epsilon_{i,j}$ affects the sampling distribution of the estimates of the person effects $\hat{\pi}_i$ and treatment effects $\hat{\tau}_j$ differently. That is because Person should usually be treated as a __random effect__, whilst an experimental manipulation such as Condition should be treated as a __fixed effect__.

### Random vs fixed effects

Unlike Condition, Person is not really an experimental manipulation. Whilst the levels of Condition (i.e. whether participants viewed a photo by itself, in the context of a group of different faces, or in the context of a group of similar faces) were carefully designed to test a hypothesis of interest, the persons participating in a study are generally sampled from a wider population. We don't want to make claims just about those persons who happened to be available as participants in a study. We want to generalize the results to the wider population. For the levels of Condition, however, we are just interested in claims about these carefully designed levels. We wouldn't pretend that the study would also speak to what would have happened if the face was presented surrounded by photos of a participant's own face, for example. That implies that if we were to replicate the experiment a second or third time, the true effects of Condition would be the same, as we would use exactly the same condition. However, as we would include different participants, the effects of Person would vary from replication to replication. As the Data Generating Process concerns how data varies over repetitions of an experiment or scientific procedure, the status of fixed and random effects is different. Obtaining more observations means we should have better estimates of fixed effects. But by adding other people to an experiment, we cannot obtain a better estimate of how one particular participant differs from the average person. 

Insofar as we want to make any claims about random effects, these would concern the variability of those effects, rather than each specific effect for each person. To make such leaps from sample to population, we need a DGP for person effects. Commonly, we might assume that each person effect is drawn from a Normal distribution:
$$\pi_i \sim \mathbf{Normal}(0,\sigma_\pi)$$
Similarly, for the interaction effect, which is also random (as the persons are sample from the population), we might make a similar assumption:
$$(\pi\tau)_{i,j} \sim \mathbf{Normal}(0,\sigma_{\pi\tau})$$
For fixed effects, we would not need to make any assumption about their distribution, because we don't need to generalize beyond the specific effects included in the design.

Whilst the variance of the sampling distribution of the estimated treatment effects $\hat{\tau}_j$ depends on both the variance in the interaction term $(\pi\tau)_{i,j}$ and the error term $\epsilon_{i,j}$, whilst the variance of the sampling distribution of the person effects depends only on the variance of the error term $\epsilon_{i,j}$. 
<!--
$$\hat{\tau}_j \sim \mathbf{Normal}(\tau_j,\sqrt{\sigma^2_{\pi\tau} + \sigma^2_\epsilon})$$
$$\hat{\pi}_i \sim \mathbf{Normal}(\pi_i,\sigma_\epsilon)$$
-->

<!-- 
Intuitively, you might think of this as follows: whilst the treatment effects may differ between persons (as reflected in the interaction term $(\pi\tau)_{i,j}$), these deviations from the average treatment effects (as reflected in the treatment term $\tau_{j}$) are random, and will cancel each other out.  

<!--
A set of participants in a study is generally a sample from a (much) larger population of potential participants. Insofar as we are interested in differences between participants (i.e., an effect of P), we are not so much interested in whether there are differences between the actual participants included in the study, but whether there are differences between the people in the whole population. In other words, we would like to infer from variability between participants in a study whether there are differences between people in the population of interest. That means we want to generalize person effects in a particular sample of participants to person effects in the general population. To make such a leap from sample to population, we need a DGP for person effects, and commonly we might assume that each person effect is drawn from a Normal distribution:
$$\tau_i^{(P)} \sim \mathbf{Normal}(0,\sigma_\text{P})$$
Our objective here is to estimate the variability between persons, i.e. $\sigma_\text{P}$, and _not_ to estimate each $\tau^{P}_i$ as precisely as possible. 

The generalization problem for fixed effects (experimental manipulations) is different: our objective is to infer from a sample what effect precisely these experimental manipulations will have in the whole population. The objective here is to get the most precise estimation of each $\tau_j^{(A)}$. 
-->

## Assumptions

### Omnibus tests and sphericity

$$\Sigma = \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} & \sigma_{13} \\ \sigma_{21} & \sigma_2^2 & \sigma_{23} \\ \sigma_{31} & \sigma_{32} & \sigma_3^2 \end{matrix} \right]$$ 
So homogeneity of variance means
$$\sigma_1^2 = \sigma_2^2 = \sigma_3^2$$
For repeated-measures analyses, an additional assumption is necessary for the covariances, namely that of __sphericity__. This assumption means that the _variances of all pairwise differences are equal_. The variance of a pairwise difference between two (possibly correlated) variables $Y_1$ and $Y_2$ is $\sigma_1^2 + \sigma_2^2 - 2 \sigma_{12}$, i.e. the sum of the variances of the two variables, minus twice the covariance. Hence, the assumption of sphericity can be stated as:
$$\sigma_j^2 + \sigma_k^2 - 2\sigma_{jk} = \sigma_{l}^2 + \sigma_{m}^2 - 2 \sigma_{lm} \quad \quad \text{for all } j,k,l,m$$
For example, for three variables, there are 3 pairwise differences, and hence the assumption is 

$$\sigma_1^2 + \sigma_2^2 - 2\sigma_{12} = \sigma_{1}^2 + \sigma_{3}^2 - 2 \sigma_{13} = \sigma_{2}^2 + \sigma_{3}^2 - 2 \sigma_{23}$$
If that seems like a complicated and stringent assumption: it is! And it is not that easy to check. Sphericity holds when a more stringent condition, called __compound symmetry__ holds. Compound symmetry means that all variances are identical to each other, and all covariances are identical to each other.

### Correcting for non-sphericity

When the assumption of sphericity does _not_ hold (the assumption is violated) the $F$-statistic still follows an $F$ distribution, but with a smaller value for $\text{df}_1$ and $\text{df}_2$ than usual. The correct degrees of freedom can be stated as $\zeta \times \text{df}_1$ and $\zeta \times \text{df}_2$, where $0 \geq \zeta \geq 1$ is a correction fraction.^[The correction factor is usually denoted by $\epsilon$, but I'm using $\zeta$ ("zeta") as we are already using $\epsilon$ for the error terms.] Whilst the value of $\zeta$ depends on the true (co)variances underlying the data, it's value can be estimated. The Greenhouse-Geisser estimate. Huynh and Feldt showed that, if the true value is close to or higher than $\zeta = 0.75$, the Greenhouse-Geisser correction tends to be too conservative. They suggested a correction which provides an upward-adjusted estimate of $\zeta$, which will increase the power of the tests. Howell suggests to use the Huynh-Feldt correction when the Greenhouse-Geisser estimate of $\zeta$ is close to or higher than $\hat{\zeta} = 0.75$.

A statistical test for the assumption of sphericity was developed by Mauchly (1940) and is known as Mauchly's sphericity test. Whilst routinely provided by statistical software, it is not an ideal test, as it rests strongly on the assumption of normality and it commonly has low power. Rather than only correcting the degrees of freedom after a significant Mauchly test, one might consider _always_ adjusting the degrees of freedom according to the Greenhouse-Geisser or Huynh-Feldt correction (whichever is deemed more appropriate).

<!-- ## Effect size -->

## Mixed ANOVA with between and within factors

\begin{equation}
Y_{i,j,k} = \mu + \tau^{(P)}_i + \tau^{(A)}_j + \tau^{(B)}_k + \tau^{(P \times A)}_{i,j} + \tau^{(P \times B)}_{i,k} + \tau^{(A \times B)}_{j,k} + \epsilon_{i,j,k}
(\#eq:glm-mixed-RM-ANOVA-model)
\end{equation}
Note that we are excluding the three-way interaction term $\tau^{(P \times A \times B)}_{i,j,k}$ as again, this can not be distinguished from the error term $\epsilon_{i,j,k}$.

## In practice

<!-- 
### Defining and estimating the model

### Assessing the assumptions

### Reporting the results

## Summary

-->



