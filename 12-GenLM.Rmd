```{r, echo=FALSE}
set.seed(20221010)
```

# Generalized Linear Models {#ch-generalized-linear-models}

<!-- References: McCullagh and Nelder, Dobson, Agresti. -->

Up to now, we have considered models for which the residuals (model errors) could be assumed to follow a Normal distribution. This is equivalent to modelling the conditional distribution of a dependent variable $Y$, given the values of predictor variables $X_j$, as a Normal distribution. Many of the techniques and ideas of such models can be generalised to situations where we can assume that the distribution of a dependent variable $Y$, conditional upon the values of predictor variables $X_j$, follows a different distribution from a class of probability distributions known as the _exponential family_. Such models are known as _generalised linear models_ [@mccullagh2019generalized]. This name makes for easy confusion with the _general linear model_, so make sure you don't ignore the presence or absence of "-ised"!

The Normal distribution is part of the exponential family of distributions, as are many other distributions. These other distributions include the Binomial, Poisson, and Gamma distributions. Some of these distributions are defined for non-negative integer variables (i.e. variables with values $0, 1, 2, \ldots$), and others for positive continuous variables (i.e. variables with values $>0$), and others for unbounded continuous variables (i.e. variables with values between $-\infty$ and $\infty$). Exponential-family distributions can all be expressed in the same form, but we won't go into the details of that here. For now, what is important is that there are many variables which are *not* continuous and unbounded. It generally does not make sense to assume a Normal distribution for such variables. But because exponential-family distributions share important characteristics with the Normal distribution, assuming a different exponential-family distributions allows you to choose a distribution which is more natural for the variable in question, whilst still being able to formulate useful models for it.

A generalized linear model consists of three components:

1. A *random* part which describes the variability in the data by means of an exponential-family probability distribution, with a conditional mean $\mu_{Y|X_1, \ldots, X_m}$.
2. A *structural* part which is a linear model of the form $\beta_0 + \sum_{j=1}^m \beta_j \times X_j$
3. A *link function* which connects the structural and random part.

## Link functions

```{r, eval=FALSE}
data.frame(distribution = c("Normal", "Gamma", "Poisson", "Binomial"),
           dv = c("Continuous, unbounded","Continuous, positive", "Non-negative integer", "Non-negative integer between 0 and $N$"),
           link = c("Indentiy", "Negative inverse", "Log", "Logit"))

```

When we previously discussed applying the general linear model to "tricky" data, we have suggested to transform the dependent variable and/or the predictor variables, in order to "force" the data to approximately conform to the assumptions of the GLM. Generalised linear models also use transformations, which are called link functions. However, link functions are not applied the variables themselves, but rather to the predictions of a linear model. 

In the general linear model, we model the conditional mean of a Normal-distributed variable as a linear function of predictor variables:
$$\mu_{Y|X_1, \ldots, X_m} = \beta_0 + \sum_{j=1}^m \beta_j \times X_j$$
In a generalised linear model, we model a transformation of the conditional mean of a wider class of distributions as a linear function of predictor variables:
\begin{equation}
g(\mu_{Y|X_1, \ldots, X_K}) = \beta_0 + \sum_{j=1}^m \beta_j \times X_j
(\#eq:glm-link-function)
\end{equation}
where $g$ denotes the so-called **link function**. This link function can take many forms. Some common examples are

* Identity: $g(y) = y$
* Log: $g(y) = \log(y)$
* Logistic: $g(y) = \log \left(\frac{y}{1-y}\right)$

The key thing is that the right-hand-side of Equation \@ref(eq:glm-link-function) is a *linear function* that can, in principle, take any value between $-\infty$ and $\infty$. For variables that are bounded, such a linear model could provide predictions that are outside these bounds. The link function essentially makes such out-of-bounds predictions impossible. This might be easier to understand when we consider the inverse transformation: 
\begin{equation}
\mu_{Y|X_1, \ldots, X_K} = h(\beta_0 + \sum_{j=1}^m \beta_j \times X_j)
(\#eq:glm-inverse-link-function)
\end{equation}
where $h$ denotes the so-called **inverse link function**. The inverse link-functions corresponding to the list of common link functions above are:

* Inverse-Identity: $h(x) = x$
* Inverse-Log: $h(x) = \exp(x)$
* Inverse-Logistic: $h(x) = \frac{\exp(x)}{1+ \exp (x)}$

Intuitively, you can think of a link function and its inverse as follows: Suppose you have ten British pounds and need to convert this into US dollars. The conversion rate (at the time of writing this) is Â£1 = \$1.08. So if $g$ is meant to convert pounds ($y$) to dollars ($x$), we would have 
$$g(y) = 1.08 \times x$$
If we were to convert our dollars back to pounds, we would need a different function, which is the inverse of our function $g$. Calling this $h$, we would need
$$h(x) = \frac{1}{1.08} \times y \approx 0.926 \times y$$
Such transformations are also called _one-to-one-mappings_: for every amount in pounds, there is an equivalent amount in dollars, and vice versa. If I know the amount in one currency, there is only a single amount in the second currency that this refers to. The link function needs to be such a one-to-one mapping, but otherwise, we are free to choose it. The example is a linear function, but we can also choose a nonlinear function, e.g.
$$\begin{aligned}
y &= \exp x \\
\log y &= x
\end{aligned}$$
You can also think of the inverse link function as "undoing" the link function, in the sense that applying the inverse link function to the link function itself, we get the values put into the link function:
$$h(g(x)) = x$$
Whilst the choice of a link function is essentially up to the analyst, it is common to use a **canonical link function**. All distributions from the exponential family can be expressed by a single common formula. stated in 

## Estimation

As usual, we will focus on maximum likelihood estimates of the model parameters. Unlike for the parameters of the general linear model, analytical solutions for maximum likelihood parameters of generalized linear models of usually not available. They are obtained by an iterative procedure called *Iteratively Weighted Least Squares*. You don't need to understand the details of this. For present purposes, it is enough to know that this method starts with a "guess" of the parameter estimates, and then updates this guess repeatedly to increase the likelihood of the estimated model, until no further improvement is obtained, at which point the algorithm is said to converge. Whilst modern statistical software generally has robust implementations of this procedure, estimation difficulties may arise in cases. This may be due to the model being a poor representation of the data, but potentially also because of a poor initial guess for the parameter values. 

## Inference in generalised linear models

As usual, we are not just interested in the estimated model parameters, as we know data is noisy and the parameter estimates are likely to differ from the true parameter values of the Data Generating Process. As for the general linear model, there are different ways to test whether a parameter differs from a hypothesised value. We can focus on the sampling distribution of the parameter estimates, or compare a MODEL R where we restrict a parameter to take a particular value to a MODEL G where we estimate the parameter. 

### Wald test

For large samples, the test statistic
\begin{equation}
Z = \frac{\hat{\beta_j}}{\text{SE}(\beta_j)}
\end{equation}
approximately follows a standard Normal distribution. Note that this $Z$ statistic is similar to the $t$-statistic for the General Linear Model. What is missing however are the degrees of freedom. For large samples, as the number of observations $n$ approaches infinity, the $t$-distribution converges to the standard Normal distribution.

As an alternative to the $Z$ statistic, the Wald statistic:
\begin{equation}
W = \left(\frac{\hat{\beta_j}}{\text{SE}(\beta_j)}\right)^2
\end{equation}
approximately follows (again for large samples) a Chi-squared distribution with $\text{df} = 1$.

Both tests are approximate and only exact for an infinite sample size. They give entirely equivalent results, although the $Z$ statistic can be used to obtain one-sided tests, whilst the Wald statistic can only be used to obtain two-sided tests.  

### Likelihood-ratio test

The Wald test is easy to compute and works well for large-enough samples. A more powerful and reliable test for smaller samples is the likelihood ratio test. To test whether a parameter differs from a hypothesized value, this involves a comparison between a MODEL R where the parameter is fixed to that hypothesised value, and a MODEL G where the parameter is estimated. For linear models, the $t-$ or $F$-statistics could be used to perform such likelihood ratio tests. Unfortunately, the wider-class of distributions covered by generalized linear models does not allow for these precise tests. Instead, we will need to rely on the the approximate Chi-squared test that we introduced in the context of linear mixed-effects models.

The (log) likelihood-ratio test statistic is commonly denoted by $G^2$, and is defined as:
$$\begin{aligned}
G^2 = -2 \log \left(\text{likelihood-ratio}\right) &= -2 \log p(\text{DATA}|\text{MODEL R}) - (-2 \log p(\text{DATA}|\text{MODEL G}))
\end{aligned}$$
For large-enough samples, the sampling distribution of this statistic is approximately equal to a Chi-squared distribution:
$$G^2 \sim \mathbf{chi-squared}(\text{npar}(G) - \text{npar}(R))$$
with degrees-of-freedom equal to the difference in the number of estimated parameters between the models.

### Confidence intervals

Another option for inference is to compute confidence intervals. As straightforward way to compute (approximate) confidence intervals is to rely on the asymptotically Normal distribution of parameter estimates (as in the $Z$-statistic for testing parameter values). 

More accurate confidence intervals can be computed through the **profile likelihood**. Roughly, this method ascertains what happens to the likelihood when the maximum likelihood estimates are subject to small changes. If these changes are relatively small, that would lead to relatively large confidence intervals. Conversely, if the changes are relatively large, this would lead to relatively small confidence intervals.  

## Assessing model fit

For the General Linear Model, the $R^2$ statistic reflects the "proportion of variance explained" and provides a useful way to assess the fit of the model to the data. For generalized linear models, a straightforward measure of variance explained is not available.^[Pseudo-$R^2$ measures have been suggested for generalized linear models, such as McFadden's, Cox and Snell's, and Nagelkerke's. These measures are based on the log likelihood of the fitted model and a null model with only an intercept. Whilst the aim of these measures is to have a statistic with a similar scale as the traditional $R^2$ for linear models, they don't generally succeed in this. You can find more information on these measures [here](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/).]

A common test for overall model fit for generalized linear models is to compare the estimated MODEL R to a *saturated* MODEL G. A **saturated model** fits the data perfectly. It has as many parameters as there are unique patterns in predictor values.

Hosmer-Lemeshow

## Logistic regression

Logistic regression is used for *dichotomous* or *binary* dependent variables, such as whether a question in a test is answered correctly ($Y = 1$) or incorrectly ($Y=0$). As a binary dependent variable can take only two values, assuming it follows a Normal distribution is nonsensical. This also applies to the model errors or residuals. 

Binary data can be assumed to follow a so-called **Bernoulli distribution**:
\begin{equation}
p(Y = k) = \theta^k \times (1-\theta)^{1-k}
(\#eq:Bernoulli-distribution)
\end{equation}
with $k=0, 1$. Note that this is a simple case of the Binomial distribution we encountered in the Chapter X.^[As we are only considering a single outcome, $n = 1$ and we do not need to worry about how many possible outcomes could constitute $k = 0, 1, 2, \ldots$ correct ones, which is why we don't need the $\left(\right)$ term.] Also note that as $x^0 = 1$, and as $k$ can only be 0 or 1, this distribution simplt states that 
$$\begin{aligned}
p(Y = 1) &= \theta^1 \times (1-\theta)^{0} = \theta \times 1 \\ &= \theta \\
p(Y = 0) &= \theta^0 \times (1-\theta)^{1} = 1 \times (1 - \theta) \\ &= 1 - \theta
\end{aligned}$$

The average of $Y$ is $\theta = \mu_Y$. Modelling this average with predictor variables $X_j$, the conditional mean is:
$$\begin{aligned}
\mu_{Y|X_1, \ldots, X_m} &= p(Y=1|X_1, \ldots, X_m) \\
&= \pi_1
\end{aligned}$$
As above, due to the binary nature of $Y$, we also know that
$$\begin{aligned}
p(Y=0|X_1, \ldots, X_m) &= 1 - p(Y=1|X_1, \ldots, X_m) \\
&= 1 - \mu_{Y|X_1, \ldots, X_m} \\
\pi_0 &= 1-\pi_1
\end{aligned}$$
This means that the probability distribution for $Y$ is completely determined by $\mu_{Y|X_1, \ldots, X_m} = \pi_1$, and our model can focus on just this.

The canonical link function for Bernoulli-distributed data is the so-called **logit link** function:
\begin{equation}
g(\mu_{Y|X_1, \ldots, X_m}) = \log \left( \frac{\mu_{Y|X_1, \ldots, X_K}}{1-\mu_{Y|X_1, \ldots, X_K}} \right)
\end{equation}
This may be easier to understand by replacing $\mu_{Y|X_1, \ldots, X_m}$ with the shorthand $\pi_1$:
\begin{equation}
g(\pi_1) = \log \left( \frac{\pi_1}{1-\pi_1} \right)
(\#eq:glm-logit-link-function)
\end{equation}
The logistic regression model is then
\begin{equation}
\log \left( \frac{\pi_1}{1-\pi_1} \right) = \beta_0 + \beta_1 \times X_1 + \ldots + \beta_m \times X_m
(\#eq:glm-logistic-regression-function)
\end{equation}

The inverse link function is called the **inverse logit link** function:
\begin{equation}
h(\beta_0 + \sum_{j=1}^m \beta_j) = \frac{\exp \{ \beta_0 + \sum_{j=1}^m \beta_j X_{j} \}}{1 + \exp \{ \beta_0 + \sum_{j=1}^m \beta_j X_{j} \}}
(\#eq:glm-inverse-logit-link-function)
\end{equation}
So
\begin{equation}
\mu_{Y|X_1,\ldots,X_m} = \pi_1 = \frac{\exp \{ \beta_0 + \sum_{j=1}^m \beta_j X_{j} \}}{1 + \exp \{ \beta_0 + \sum_{j=1}^m \beta_j X_{j} \}}
\end{equation}

Some examples for $m=1$ predictor and different values of $\beta_0$ and $\beta_1$ are shown in Figure \@ref(fig:logit-link-plot).

```{r logit-link-plot}
library(dplyr)
library(ggplot2)
x <- seq(-5,5,length=100) 
rbind(data.frame(b0 = 0, b1 = 1, x=x, p = 1/(1+exp(-x))),
      data.frame(b0 = 0, b1 = 2, x=x, p = 1/(1+exp(-2*x))),
      data.frame(b0 = 1, b1=1, x=x, p = 1/(1+exp(-(1 + x)))),
      data.frame(b0 = 1, b1=-1, x=x, p = 1/(1+exp(-(1 -x))))) %>%
  mutate(case = paste0("b0 = ", b0, ", b1 = ", b1)) %>%
  ggplot(aes(x=x,y=p, colour=case)) + geom_line() + ylab("p(Y = 1)")
```

For this 1-predictor model, the intercept $\beta_0$ reflects the point at which $\pi_1 = 0.5$. The slope $\beta_1$ reflects the steepness of the curve.

### Example: Metacognition in visual perception

Metacognition refers to the ability ascertain how (un)certain we are in our inferences about the world. The "meta" qualifier indicates that this is a cognitive process that is about (i.e. reflects upon) other cognitive processes. For example, @rausch2016visibility asked participants to judge whether a stimulus (a so-called grating pattern) was directed horizontally to vertically. To make this task difficult for their participants, they presented each stimuli for a brief time (200 ms) and with a variety of relatively low-contrast displays. They also asked participants to indicate their certainty that their judgement was correct. 


```{r metacognition-confidence-correct-plot, fig.caption="Confidence and proportion correct for two participants in the study by @rausch2016visibility on metacognition.", fig.width = 8, fig.height=4}
library(sdamr)
data("metacognition")

dat <- subset(metacognition, id %in% c(1,2))

dat %>%
  mutate(confidence = if_else(confidence == 0, .0001, confidence)) %>%
  mutate(conf_b = cut(confidence, c(0,20,40,60,80,100))) %>%
  mutate(id = paste("ID =", id)) %>%
  ggplot(aes(x=conf_b, y=correct)) + stat_summary(fun.data = "mean_cl_boot") + ylim(c(0,1)) + xlab("confidence") + ylab("proportion correct") + facet_wrap(~id)
```

```{r}
mod_s1 <- subset(dat, id == 1) %>%
  mutate(confidence = scale(confidence),
         contrast = scale(contrast)) %>%
  glm(correct ~ confidence * contrast, family=binomial(), data=.)
mod_s2 <- subset(dat, id == 2) %>%
  mutate(confidence = scale(confidence),
         contrast = scale(contrast)) %>%
  glm(correct ~ confidence * contrast, family=binomial(), data=.)

tab <- summary(mod_s1)$coefficients
rownames(tab) <- c("Intercept","Confidence", "Contrast","Confidence $\\times$ Contrast")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SE}(\\hat{\\beta})$", "$z$", "$P(\\geq \\lvert z \\rvert)$")
tab[,4] <- sapply(tab[,4], format_p)

knitr::kable(tab, caption = "Null-hypothesis significance tests for a logistic regression model using the z statistic.", escape = FALSE, booktabs=TRUE, linesep="")

#knitr::kable(papaja::apa_print(mod_s1)$table)

#summary(mod_s1)

#car::Anova(mod_s1, type=3)
```

### Parameter interpretation

In the general linear model, $\beta_j$, the slope of a predictor variable, reflects the predicted change in the dependent variable for a one-unit increase in the predictor variable $X_j$. Because the model is linear, the predicted change is the same, no matter what base value we choose for $X_j$. That means that $Y$ is predicted to change by $\beta_j$, whether we increase $X_j$ from 0 to 1, or from 100 to 101. This is not the case when we consider the predicted change in $p(Y=1|X_j)$ for different values of $X_j$. For example, for a model with $\beta_0 = 0$ and $\beta_1 = 1$, Figure XXX shows that this probability increases more rapidly when we move from $X_1 = 0$ to $X_1 = 1$ than when we move from $X_1 = 3$ to $X_1 = 4$. 

We can make use of the following properties of exponents:
$$\begin{aligned}
a^{(b + c)} &= a^b \times a^c \\
a^{(b \times c)} &= \left(a^{b}\right)^c = \left(a^{c}\right)^b
\end{aligned}$$
(remember that $\exp (a) = e^a$, where $e$ denotes the natural number), to rewrite the odds as
$$\begin{aligned} \frac{p(Y=1|X_1)}{1-p(Y=1|X_1)} &= \exp\left(\beta_0 + \beta_1 X_1\right) \\ &= \exp(\beta_0) \times \left(\exp(\beta_1) \right)^{X_1} \end{aligned}$$
In the above, you can see that every one-unit increase in $X_1$ has the effect of multiplying the odds by $\exp(\beta_1)$. So the odds at $X_1 = 1$ is the odds at $X_1 = 0$, multiplied by $\exp(\beta_1)$. Similarly, the odds at $X_1 = 2$ is the odds at $X_1 = 1$ multiplied by $\exp(\beta_1)$.

In the log-odds scale, every one-unit increase in $X$ *adds* $\beta_1$ to the log-odds. In the odds scale, every one-unit increase in $X$ *multiplies* the odds by $\exp(\beta_1)$. In terms of interpreting the effect of $X_1$, you can choose whether you find this easier to do in the scale of log-odds, or in the scale of odds. The most natural scale is probably the scale of the response $p(Y=1|X_1)$ itself. Unfortunately, the non-linear effect of $X_1$ on the response prevents a simple interpretation in this scale.

### Probit regression: Using a different link function

The logit link is the canonical link function for the Bernoulli and Binomial distribution. But you are not restricted to choose the canonical link function. In principle, any (monotone and one-to-one) function $g(Y)$ that transforms the variable $Y$ which is bounded between 0 and 1 to an unbounded scale between $\-infty$ and $\infty$. 

An alternative to the logit link is the **Probit link function**:

$$\begin{aligned} p(Y \leq y) && Y \sim \mathbf{Normal}(\mu = 0, \sigma = 1) \end{aligned}$$ 

```{r}
library(sdamr)
data("metacognition")

dat <- subset(metacognition, id == 1)
mod <- glm(correct ~ scale(confidence) * scale(contrast), family=binomial(link = "probit"), data=dat)
summary(mod)

car::Anova(mod, type=3)
```

## Poisson regression

<!-- https://online.ucpress.edu/collabra/article/7/1/27242/118591/Poisson-Regressions-A-Little-Fishy -->
<!-- https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12439 -->

Poisson regression is useful when the total number of possible occurrences of an event in not given. For example, suppose you let a participant solve as many algebra problems as they can within 1 hour. Some participants may be able to solve a problem every minute, whilst others may need longer (e.g. 5 minutes per problem, on average). 

The **Poisson distribution** is stated as:
\begin{equation}
p(Y=k) = \frac{\mu^k \exp (-\mu )}{k!}
(\#eq:Poisson-distribution)
\end{equation}
with $k=0, 1, 2, \ldots$ denoting the number of occurrences, which is a non-negative integer.

The canonical link function for Poisson-distributed data is the *log* link function:
$$g(\mu_{Y|X_1, \ldots, X_K}) = \log\left(\mu_{Y|X_1, \ldots, X_K}\right)$$
And the Poisson regression model is thus
$$\log\left(\mu_{Y|X_1, \ldots, X_K}\right) = \beta_0 + \sum_{j=1}^m \beta_j \times X_{j}$$

The inverse link function is the *exponential* function:
$$h\left(\beta_0 + \sum_{j=1}^m \beta_j \times X_{j}\right) = \exp \left( \beta_0 + \sum_{j=1}^m \beta_j \times X_{j} \right)$$

### Example: Gestures in different social contexts

```{r}
data("gestures")
gestures %>%
  ggplot(aes(x=gestures)) + geom_histogram() + facet_wrap(context~language)

```

```{r}
contrasts(gestures$context) <- c(1,-1)
contrasts(gestures$language) <- c(1,-1)
contrasts(gestures$gender) <- c(1,-1)
gestures$log_d <- log(gestures$dur)
pois_mod <- glm(gestures ~ context*language*gender, data=gestures, family=poisson(), offset=log_d)

```

$$\log (\lambda) = \log\left(\frac{\mu}{\tau}\right)$$
$$\log (\mu) = \log(\tau) + \beta_0 + \sum_{j=1}^m \beta_j X_j$$
Note that the term $\log(\tau)$ above has no slope. You can think of $\log(\tau)$ as a predictor with a fixed slope of 1. 

### Overdispersion

In a Poisson distribution, the variance is identical to the mean. In real data, the variance is often larger than the mean. When the variance is larger than theoretically expected, this is called **overdispersion**. 

```{r}
qpois_mod <- glm(gestures ~ context*language*gender, data=gestures, family=quasipoisson(), offset=log_d)
nbinom_mod <- MASS::glm.nb(gestures ~ offset(log_d) + context*language*gender, data=gestures)
```

## Log-linear models

Log-linear models are used to analyse relations in multivariate categorical data. Such data can be expressed in the form of *multiway contingency tables* that contain counts for combinations of categorical classifications. For example, the political party someone has last voted for and newspaper they read most often. We can count how often each combination of vote and newspaper occurs in a random sample.

Let $\mu_{i,j}$ denote the expected frequency of combination where $A = i$ and $B = j$. Assuming independence in the observations, the probability that one observation equals this combination is $\pi_{i,j} = P(A = i, B = j)$. For a total of $n$ independent observations, the expected frequency is then: 
$$\mu_{i,j} = n \times \pi_{i,j}$$
Log-linear models are similar in spirit to a factorial ANOVA, in the sense that the counts are decomposed into main effects, two-way interactions, three-way interactions, etc. For example, 

$$\log \mu_{i,j} = \lambda + \lambda^{(A)}_i + \lambda^{(B)}_j + \lambda^{(AB)}_{i,j}$$
Just like a factorial ANOVA model can perfectly fit the sample means, this log-linear model can perfectly fit the sample counts. This is hence an example of a **saturated model**, where the total number of parameters equals the total number of possible combinations minus 1.^[We need one parameter less as the last remaining cell can be filled in as $n$ minus the frequencies in the other cells.]

For a random sample of people from the general population, we can then count how often people who report voting for the Conservative Party also report reading the Daily Mail, and how many report reading the Guardian, etc (see e.g. https://www.statista.com/chart/9897/general-election-voting-by-newspaper-readership/ and https://yougov.co.uk/topics/politics/articles-reports/2017/06/13/how-britain-voted-2017-general-election for an example of such data).

This is an extension of the Chi-squared test for a two-way contingency table.



### Example: Newspapers and voting

After the 2017 UK general elections, a large survey was conducted where people were asked which party they voted for, and amongst other things, the newspaper they most often read. Table \@ref(tab:papervotes-table) shows the results. In the UK, many newspapers have strong links to political parties. We could ask whether such links also exist for their readers. 

```{r papervotes-table}
data("papervotes")
tab <- xtabs(n ~ ., data=papervotes, drop.unused.levels = TRUE, exclude = c("Plaid Cymru"))
knitr::kable(tab, caption = "Party voted for and most-read newspapers for the 2017 UK general election.")
```

For each combination of political party $i = 1, \ldots, 8$ and newspaper $j = 1, \ldots, 10$, Table \@ref(tab:papervotes-table) shows the corresponding count $n_{i,j}$. The total number is $n = \sum_i \sum_j n_{i,j}$, reflecting the sample size of the survey. The loglinear model concerns the expected counts for each combination, $\mu_{i,j} = n \times P(\text{party} = i, \text{newspaper} = j)$. These expected counts can be estimated by the observed counts $\hat{\mu}_{ij} = n_{i,j}$.

The full model 
$$\log \mu_{i,j} = \lambda + \lambda^{(\text{party})}_i + \lambda^{(\text{newspaper})}_j + \lambda^{(\text{party} \times \text{newspaper})}_{i,j}$$
has $8-1 = 7$ parameters $\lambda^{(\text{party})}_i$ for the "main effect" of party, $10-1 = 9$ parameters $\lambda^{(\text{newspaper})}_i$ for the "main effect" of newspaper, and $7 times 9 = 63$ parameters $\lambda^{(\text{party} \times \text{newspaper})}_{i,j}$ for the "interaction" between party and newspaper. Including the intercept $\lambda$, the total number of parameters is thus $1 + 7 + 9 + 63 = 80$, which equals the number of observed counts (i.e. combinations of party and newspaper). This saturated model therefore will fit the observed counts perfectly. Its main purpose is to serve as a comparison for simpler models. For this example, the comparison model of interest is a model in which political party voted-for is independent of the newspaper read:

$$\log \mu_{i,j} = \lambda + \lambda^{(\text{party})}_i + \lambda^{(\text{newspaper})}_j$$

```{r}
mod0 <- loglin(tab, margin = list(1, 2))
mod1 <- loglin(tab, margin = list(c(1, 2)))
```

### A three-way table: Rock-Paper-Scissors

@guennouni2022transfer let participants repeatedly play the well-known game of Rock-Paper-Scissors against artificial (AI) opponents, who were endowed with different human-like strategies. The "level-1" AI player would assume the human player to repeat their last chosen action, and then would pick the action to beat this. For example, if the human player had previously chosen "Rock", then they would expect the human player to play "Rock" again, therefore choosing "Paper" as this is the action that beats "Rock". The "level-2" AI player would expect the human player to adopt a "level-1" strategy, choosing actions to beat this. For example, if the AI player had previously chosen "Rock", the human would be expected to next play "Paper", as this beats "Rock". The AI player would therefore play "Scissors", as this beats "Paper" (the expected action of the human player). Results showed that people could pick-up on the strategy of the AI player, using this to their advantage. Moreover, they could transfer this learned strategy to other games.

If we focus on the "level-1" AI player, we can say that their actions depend completely on the previous action of the human player.^[In the experiment, some noise was added to the actions of the AI player, who would choose a completely random action on 10% of the rounds.] If the human player is sensitive to the AI player's strategy, then their own strategy should also completely depend on their previous actions. For example, they should choose to play "Scissors" after playing "Rock", as they should expect the AI player to play "Paper". If this is the case, then we might expect the current action of the human player to be independent of the previous action of the AI player. We can test this hypothesis with log-linear models.

Table \@ref(tab:rps-table) shows the actions chosen by the human player (in the last half of the game) as a function of the previous actions of the AI and human player. 

```{r rps-table}
data("rps")
library(dplyr)
dat <- rps %>%
  group_by(id) %>%
  mutate(previous_human = lag(human_action),
         previous_ai = lag(ai_action)) %>%
  filter(round > 25, ai_strategy == "Level1") %>%
  mutate(previous_human = factor(previous_human, levels=c("rock", "paper", "scissors")),
         previous_ai = factor(previous_ai, levels=c("rock", "paper", "scissors")),
         human_action = factor(human_action, levels=c("rock", "paper", "scissors")))
tab <- table(dat[,c("previous_human", "previous_ai","human_action")])
data.frame(ftable(tab)) %>%
  tidyr::pivot_wider(names_from = human_action, values_from= Freq) %>%
  arrange(previous_human) %>%
  knitr::kable(caption = "Action chosen by human players in Rock-Paper-Scissors by previous actions chosen by both human and AI player.")
```

Table \@ref(tab:rps-loglm-results)

```{r rps-loglm-results}
mod_1_2_3 <- MASS::loglm(~ 1 + 2 + 3, data=tab)
mod_12_3 <- MASS::loglm(~ 1*2 + 3, data=tab)
mod_13_2 <- MASS::loglm(~ 1*3 + 2, data=tab)
mod_23_1 <- MASS::loglm(~ 2*3 + 1, data=tab)
mod_12_23 <- MASS::loglm(~ 1*2 + 2*3, data=tab)
mod_12_13 <- MASS::loglm(~ 1*2 + 1*3, data=tab)
mod_13_23 <- MASS::loglm(~ 1*3 + 2*3, data=tab)
mod_12_13_23 <- MASS::loglm(~ 1*2 + 1*3 + 2*3, data=tab)
mod_123 <- MASS::loglm(~ 1*2*3, data=tab)

mod_comp <- as.data.frame(matrix(unlist(lapply(list(mod_1_2_3, mod_12_3, mod_13_2, mod_23_1, mod_12_23, mod_12_13, mod_13_23, mod_12_13_23, mod_123), function(x) c(x$df, x$lrt, 1-pchisq(x$lrt, x$df), x$pearson, 1-pchisq(x$pearson, x$df)))), ncol=5, byrow = TRUE))
mod_comp[,3] <- sapply(mod_comp[,3], format_p)
mod_comp[,5] <- sapply(mod_comp[,5], format_p)
rownames(mod_comp) <- c("$(H_p, A_p, H_c)$", "$(H_p \\cdot A_p, H_c)$", "$(A_p, H_p \\cdot H_c)$", "$(H_p, A_p \\cdot H_c)$", "$(H_p \\cdot A_p, A_p \\cdot H_c)$", "$(H_p \\cdot A_p, H_p \\cdot H_c)$", "$(H_p \\cdot H_c, A_p \\cdot H_c)$", "$(H_p \\cdot A_p, H_p \\cdot H_c, A_p \\cdot H_c)$", "$(H_p \\cdot A_p \\cdot H_c)$")
rownames(mod_comp) <- c("$(H, A, C)$", "$(HA, C)$", "$(HC,A)$", "$(AC, H)$", "$(HA, AC)$", "$(HA, HC)$", "$(HC, AC)$", "$(HA, HC, AC)$", "$(HAC)$")
knitr::kable(mod_comp, row.names = TRUE, col.names = c("df", "$G^2$", "$p(\\gt G^2)$", "$X^2$", "$p(\\gt X^2)$"), caption="Results from all possible log-linear models for the Rock-Paper-Scissors data of Table \\@ref(tab:rps-table)")
```

### Sparse data and empty cells

Problems can arise when tables have many small counts. This can happen when the total sample size is small, or when the number of cells in the table is relatively large. 

A *structural zero* refers to a case where the expected count is truly zero, because a combination of values can not occur (i.e. the probability of that combination equals 0). A *sampling zero* occurs when the expected count is larger than 0, but a particular combination has not been observed in the data due to chance.

For saturated models, @agresti2018introduction recommends to add a value of $\tfrac{1}{2}$ to all cells. For non-saturated models, this correction is too large however, and it is recommended to add a much smaller value to each cell (e.g. .00000001, or .0001). 

## Multinomial logistic regression

<!-- https://www.bookdown.org/chua/ber642_advanced_regression/multinomial-logistic-regression.html -->

Logistic regression concerns modelling a dichotomous or binary dependent variable. There are many situations in which we would like to model a polytomous (i.e. non-binary) categorical dependent variable. For instance, in a marketing application, we might be interested in determining the factors that underlie consumers' product choice.  

A simple, but suboptimal solution would be to treat the data as a set of binary variables. For instance, if we consider four products, we might construct 4 binary variables, where the first one encodes whether product $A$ was chosen ($Y_{1,i} = 1$) or not ($Y_{1,i} = 0$), the second one whether product $B$ was chosen ($Y_{2,i} = 1$) or not ($Y_{2,i} = 0$), etc. For each of these variables, we might construct a logistic regression model with the same set of predictors.

One problem with this approach is that it ignores one fundamental aspect: these variables are not independent! If we know that $Y_{1,i} = 1$, then all the other variables ($Y_{2,i}, Y_{3,i}, \ldots$) have to be 0.

In multinomial logistic regression, a set of binary logistic regression models is estimated simultaneously, and the dependencies between these models are taken into account appropriately. 

### Baseline category logit

The most common multinomial logistic regression model uses a so-called *baseline category logit* link function. This approach takes one of the categories (or levels) of the dependent variable as a baseline, and compares all other levels to that baseline category. 

$$\log \left( \frac{\pi_j}{\pi_k} \right) = \beta_{j,0} + \beta_{j,1} \times X_1 + \ldots + \beta_{j,m} \times X_m \quad \quad \text{for all } j \neq k$$
We could also parametrise the comparison of the baseline category against itself, i.e. $\log \left( \frac{\pi_k}{\pi_k} \right)$, but as this will always equal 0, we can just treat these parameters as fixed, i.e $\beta_{k,0} = \beta_{k,1} = \ldots = \beta_{k,m} = 0$.

### Example: Rock-Paper-Scissors

As an example, we reanalyse the Rock-Paper-Scissors data, focusing on a model of the human player's current action, as a function of the previous actions of the human and AI player. We choose "paper" as the baseline category. We use effect coding for the two predictors (human and AI previous action). As each predictor has three possible values, we need two contrast codes for each. We will estimate effects for previous actions "rock" and "scissors" (again, treating "paper" as a baseline). Our two logits are then

$$\begin{aligned}
\log \left(\frac{p(\text{rock})}{p(\text{paper})} \right) &= \beta_{R, 0} + \beta_{R, HR} \times C_{H: R} +  \beta_{R, HS} \times C_{H: S} + \beta_{R, AR} \times C_{A: R} +  \beta_{R, AS} \times C_{A: S} \\ 
\log \left(\frac{p(\text{scissors})}{p(\text{paper})} \right) &= \beta_{S, 0} + \beta_{S, HR} \times C_{H: R} +  \beta_{S, HS} \times C_{H: S} + \beta_{S, AR} \times C_{A: R} +  \beta_{S, AS} \times C_{A: S} \end{aligned}$$
Parameter estimates and (approximate) $Z$-tests are provided in Table \@ref(tab:rps-multinomial-logit).
```{r rps-multinomial-logit, warning=FALSE, message=FALSE}
data("rps")
dat <- rps %>%
  group_by(id) %>%
  mutate(previous_human = lag(human_action),
         previous_ai = lag(ai_action)) %>%
  filter(round > 1, ai_strategy == "Level1")
contrasts(dat$previous_ai) <- contrasts(dat$previous_human) <- cbind(c(-1,1,0),c(-1,0,1))
rps_mod_multinom <- nnet::multinom(human_action ~ previous_human + previous_ai, data=dat, model = TRUE, trace = FALSE)

tab <- data.frame(coef = as.numeric(t(summary(rps_mod_multinom)$coefficients)), sd = as.numeric(t(summary(rps_mod_multinom)$standard.errors)))
tab$Z <- tab[,1]/tab[,2]
tab$p <- sapply(2*(1-pnorm(abs(tab$Z))), format_p)
tab <- cbind(tab[1:5,], tab[6:10,]) 
rownames(tab) <- c("Intercept", "H: rock", "H: scissors", "A: rock", "A: scissors")
tab %>%
  knitr::kable(digits=3, col.names = rep(c("$\\hat{\\beta}$","$\\text{SE}(\\hat{\\beta})$", "$z$", "$P(\\geq \\lvert z \\rvert)$"),2), align="r", caption="Results of a baseline category multinomial logistic regression model for human current actions as a function of human and AI previous actions.") %>%
  kableExtra::add_header_above(c(" "=1, "C: rock" = 4, "C: scissors" = 4))

rps_multinom_test <- MASS::dropterm(rps_mod_multinom, test="Chisq", trace = FALSE)

coef <- round(coefficients(rps_mod_multinom),3)
#rps_multinom_confint <- confint(rps_mod_multinom)
```

Instead of the Z-test for the individual coefficients in Table , we may wonder whether a predictor has "any effect" on the dependent variable. For example, if the AI's previous action has no effect on the human's current action, then that would mean that the null-hypothesis $$H_0: \beta_{R, AR} = \beta_{R, AS} = \beta_{S, AR} = \beta_{S, AS} = 0$$ is true. We can test this hypotheses by a model comparison, comparing the full model to one where we fix these parameters to 0 (effectively removing the AI previous action predictor from the model).
This model comparison provides the following test results:
$-2 \log(\text{likelihood-ratio}) = `r rps_multinom_test$LRT[2]`$, $\text{df} = 4$, $p < .001$. Hence, we can conclude that the human players' current actions do depend on the AI's previous actions.

### Reconstructing probabilities of responses


$$\hat{\pi_j} = \frac{\exp \left( \beta_{j,0} + \beta_{j,1} \times X_1 + \ldots + \beta_{j,m} \times X_m \right)}{\sum_{l=1}^K \exp \left(\beta_{l,0} + \beta_{l,1} \times X_1 + \ldots + \beta_{l,m} \times X_m \right)}$$
Note that we also sum over the baseline category $k$ in the denominator, setting the parameters here to 0.

For example, if the human player previously played "Rock" ($C_{H: R} = 1$, $C_{H: S} = 0$) and the AI previously player "Paper" ($C_{A: R} = -1$, $C_{A: S} = -1$), we can work out the probability of each possible current action is by computing the following quantities:

```{r}
d_coef <- function(x) {
  if(x < 0) return(paste0("(",x,")")) else return(x)
}
tab <- data.frame(
  action = c("C: R", "C: P", "C: S"),
  linear = c(paste0("$", d_coef(coef[1,1])," + ",d_coef(coef[1,2])," \\times 1 + ",d_coef(coef[1,3])," \\times 0 + ",d_coef(coef[1,4]),"\\times -1 + ", d_coef(coef[1,5]),"\\times -1$"), "0", paste0("$",d_coef(coef[2,1])," + ",d_coef(coef[2,2]), " \\times 1 + ",d_coef(coef[2,3]), " \\times 0 + ",d_coef(coef[2,4]), " \\times -1 + ",d_coef(coef[2,5])," \\times -1$")),
  value = c(sum(coefficients(rps_mod_multinom)[1,]*c(1,1,0,-1,-1)),
    0,
    sum(coefficients(rps_mod_multinom)[2,]*c(1,1,0,-1,-1)))
)
tab$exp <- exp(tab$value)
tab$p <- tab$exp/sum(tab$exp)
tab %>%
  knitr::kable(digits=3, col.names = c("action","linear equation","value","exp","prob"))
```

For each action, we start with computing the value of the linear equation. These values are then transformed by taking the natural exponent. Finally, we can divide each of these exponentiated values by the sum of all exponentiated values to arrive a the calculated probability. For example, the probability that the action is "Rock" is computed as
$$\begin{aligned} p(\text{rock}) &= \frac{\exp(`r round(tab$value[1],3)`)}{\exp(`r round(tab$value[1],3)`) + \exp(0) + \exp(`r round(tab$value[3],3)`)} \\ 
&= $$


### Alternative logit models for ordinal categories

When the categories of the dependent variable can be meaningfully ordered, this ordering can be used to construct a different set of logit models. All these formulations work with odds, but these odds are differentially defined. Some common examples are:

* Cumulative logit. $\log \left( \frac{\pi_1}{\pi_2 + \pi_3 + \ldots} \right)$, $\log \left( \frac{\pi + \pi_2}{\pi_3 + \pi_4 + \ldots} \right)$, $\log \left( \frac{\pi_1 + \pi_2 + \pi_3}{\pi_4 + \ldots} \right)$, etc.
* Adjacent category logit. $\log \left( \frac{\pi_1}{\pi_2} \right)$, $\log \left( \frac{\pi_2}{\pi_3} \right)$, $\log \left( \frac{\pi_3}{\pi_4} \right)$, etc.
* Continuation ratio logit. $\log \left( \frac{\pi_1}{\pi_2} \right)$, $\log \left( \frac{\pi_1 + \pi_2}{\pi_3} \right)$, $\log \left( \frac{\pi_1 + \pi_2 + \pi_3}{\pi_4} \right)$, etc.

We can choose any set of logits, as long as we don't include too many and it is possible to reconstruct the probability of each outcome from the set. In principle, we could choose to use the same logit formulations for *unordered categories*. The model as a whole would fit the data equally well, no matter what valid set of logits is chosen. Similarly to the approach we took in Repeated-Measures ANOVA, these logits essentially define contrasts on a dependent variable. Any valid contrast will do, but choosing a good set of contrasts will provide a more informative set of parameters.

The main usefulness of alternative logit formulations for ordered categories is when using these in combination with restrictions on the parameters. In particular, assuming that the slope of predictors is identical for each logit. For example, the **Proportional Odds Model** uses the cumulative logit formulation and assumes the effect of a predictor to be identical for each of these cumulative logits:

$$\log \left( \frac{\sum_{k=1}^l \pi_k}{\sum_{k=l+1}^K \pi_k} \right) = \beta_0 + \sum_{j=1}^m \beta_j \times X_j \quad \quad l = 1, \ldots, K$$
This reduces the number of parameters quite drastically compared to an unconstrained multinomial logistic regression model.

## Generalized linear mixed-effects models

As in the linear model, the generalized linear model can be extended by including random effects. Whilst estimation of such generalized linear mixed-effects models is (a lot) more involved than for linear mixed-effects models, conceptually, the models are very similar. Again, we can write each intercept and slope as consisting of a fixed and a random part, and assume that each random effect is drawn from a (multivariate) Normal distribution, with mean 0.

```{r metacognition-glmer, cache=TRUE}
library(sdamr)
data("metacognition")
glmer_mod <- metacognition %>%
  mutate(confidence_c = scale(confidence),
         contrast_c = scale(contrast)) %>%
  lme4::glmer(correct ~ confidence_c * contrast_c + (confidence_c + contrast_c||id), family=binomial(), data=.)

car::Anova(glmer_mod, type=3)
```

### Item response theory

Item-response theory concerns the measurement of *latent variables* through a set of items that differ in difficulty. Whole books have been written on this topic, and we will only provide a relatively straightforward example here.

The so-called Rasch or 1-parameter logistic IRT model can be stated as a generalized linear mixed-effects model as follows:
$$\begin{aligned}
\text{logit}(p(Y_{i,j} = 1)) &= \beta_j + \gamma_i \\
\gamma_i &\sim \mathbf{Normal}(0,\sigma_{\gamma_i})
\end{aligned}$$
where $p(Y_{i,j} = 1)$ is the probability that person $i$ answers item $j$ correct, $\beta_j$ is a (fixed) effect for item $j$, and $\gamma_i$ a (random) effect for person $i$. 

```{r rasch-model, cache=TRUE}
#data("VerbAgg", package="lme4")
data("LSAT7", package="mirt")
rasch_mod <- mirt::expand.table(LSAT7) %>%
  mutate(id = factor(row_number())) %>%
  tidyr::pivot_longer(cols = starts_with("item"), names_to = "item", values_to = "response") %>%
  lme4::glmer(response ~ -1 + item + (1|id), family=binomial(), data=.)
#rasch_mod <- lme4::glmer(r2 ~ -1 + item + (1|id), family=binomial(), data=dat)
```

## In practice

As you will have noticed, generalised linear models are even more general than the general linear model, and can a useful way to model a wide variety of data. 

1. Choose a sensible distribution (family) for the random part of the model. This can usually be based on the nature of the dependent variable. For binary data, a Bernoulli distribution is generally the right choice. For count (frequency) data, if the maximum frequency is fixed by design, a Binomial distribution makes sense. If the maximum not fixed, a Poisson distribution makes sense.

2. Choose a useful link function. The link function defines how the predictors relate to the mean of the dependent variable. Hence, the choice is important. But 

3. Estimate and assess the model. Overdispersion can be an issue.

4. Report the results

