---
title: "Statistics: Data analysis and modelling"
author: "Maarten Speekenbrink"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: krantz
bibliography: [book.bib, packages.bib]
biblio-style: apalike
csl: apa-no-doi-no-issue.csl
link-citations: yes
github-repo: mspeekenbrink/sdam-book
description: "A book about statistics for data analysis, with a main focus on statistical modelling."
---
```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```

# Preface {-}

<!-- 
```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      out.width="95%")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

library(sdamr)
```

```{r myFunctions, echo=FALSE}
options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}
```

-->
This book concerns __statistics__, and more specifically __data analysis__ through __modeling__. It aims to be a self-contained source, requiring little prior background in mathematics or statistics. That said, it is not necessarily an "easy" book. It contains some mathematical notation and ideas. I do my best to properly explain how to read and interpret these in both an intuitive as well as a more formal way. I don't do this to torture you: Neither you with little mathematical background, who might have to read some explanations twice or more, nor you with more mathematical background, who might find my intuitive explanations tedious, superfluous, and utterly pointless, nor you in the middle, who, like me, will agree with both (but hopefully not simultaneously). What I hope to do is to bring you some sense of the beauty of a formal system that stems from its simplicity and exactness, as well as some of the excitement of grasping it. I'm under no illusion that this is an easy task, for you or for me. But I hope you will try, like I do writing this book.

We will focus on building models of data in order to evaluate claims about how the data came to be what it is. Statistics is often taught with a "cookbook" approach. The "ingredients" are characteristics of the data (e.g. a variable is metric) and the goal of the analysis (e.g. compare the mean of two groups). The "cookbook" will then provide the recipe (e.g., a two-sample t-test). Whilst straightforward to teach, the limitations of this approach become quickly apparent when you need to analyse data from a less standard design. The cookbook can also become somewhat of a straightjacket, forcing you to change the design of a study in order to allow the subsequent analysis to be covered in the cookbook. Another limitation of the approach is treating the different analyses and tests in isolation, making it difficult to see the strong connections between the models and statistical principles underlying them. By contrast, the modelling approach adopted here is much more flexible. My aim is to give you the confidence to, when given the ingredients, design your own recipe.

Unlike some other books, we don't discuss using software such as R, JASP, or SPSS. Instead, there are companion books which discuss how to do analyses with specific software (R or JASP). Separating theory and practice in this way has as main benefit that each can be focused on fully. An additional benefit is that it is also easier to add companions for other software. A downside is that you might need to cross-reference between two books. However, by keeping the structure of the companions mostly in line with the structure of this book, this cross-referencing should be straightforward enough. As a long time R user, my personal preference is towards R, and this is held as the "golden standard". Practically, that means that some of the analyses employed might be more difficult to do in other software. 

## Acknowledgements {-}

This book is inspired by lots of other statistics books I have read over the years. Some books deserve special mention. "Data analysis: A model comparison approach" [@judd2011data] is a book I have used for many years as required reading for the MSc level statistics course I convened. It is extremely clear in its coverage of the General Linear Model, and consistent in its use of the model comparison approach we adopt here as well. "Doing Bayesian data analysis" [@kruschke2014doing] gave me the idea to cover all the basics of statistical modelling with a simple binomial example. ["Learning statistics with R"](https://learningstatisticswithr.com/) by Danielle Navarro convinced me to release this book online under a creative commons licence (CC BY-SA 4.0).  

A large part of this book was written between September and December of 2020, whilst moving all teaching online during the world-wide Covid-19 pandemic.^[Revising this section in September 2021, we are by no means out of the pandemic, but in the UK things are slowly improving.] For a variety of reasons, that meant writing at high-speed and -- lacking any touch typing skills and mostly relying on three out of ten fingers -- many typos. I therefore would also like to thank everyone who spotted these typos and otherwise unclear parts, in particular Sabine Topf and Laura Bourne, as well as the students in "PSYC0146".

## Notation {-}

In a book with mathematical details, notation is important. Mathematics is, if anything, a very precise language. For some of you, this may be a very foreign language. But languages can be learned. Like words, mathematical symbols represent important concepts and relations. My first aim, notation wise, is to be consistent, so that each symbol has a clearly defined meaning. My second aim is to stay as close as possible to prevailing conventions, so that it will be easier to understand published journal articles and books. Combining these aims is not always easy, or possible, because in different areas of statistics, the same symbol may have very different conventional meanings. Mathematics, in all its beauty, allows one to develop an entirely new dialect within a single paper, whilst still conveying to the agreed-upon structure of general mathematics. In writing this book, I have generally chosen for internal consistency, changing some commonly-used symbols for less-widely used ones, but will note the more commonly adopted symbols where necessary. In any case, I will try to be very clear when introducing notation.

You will come across symbols from the Greek alphabet. Generally, these symbols are used to reflect parameters of statistical models, and sometimes to reflect other fundamental aspects of statistical analyses. So it will be useful to familiarize yourself with the Greek alphabet. In the following table, each Greek letter is shown, with the corresponding name in English (useful if you want to read the symbols aloud), as well as the common parameter or concept the symbol refers to in this book:

```{r greek-symbol-table}
greek_symbols <- c("alpha","beta","gamma","delta","epsilon","zeta","eta","theta","iota","kappa","lambda","mu","nu","xi","pi","rho","sigma","tau","upsilon","phi","chi","psi","omega")
knitr::kable(
  data.frame(
    symbol = paste0("$\\",greek_symbols,"$"),
    name = greek_symbols,
    usage = c("significance level","parameter (slope or intercept) in the General Linear Model","random effect in a linear mixed-effects model","","error","","","a parameter in general","","","","mean","","","","correlation","standard deviation","","","","Chi-squared distribution","","")
  ),
  escape = FALSE,
  booktabs = TRUE
)
```

```{r}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Introduction {#ch-intro}

```{r setup2, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      fig.align = "center")
library(sdamr)
```

In this chapter we will introduce some fundamental concepts, such as experiments, data, analysis, and modelling. We will also introduce you to Paul the octopus.

## Paul the octopus

> We know that all octopus have nine brains so we know he has exceptional powers.   
> _Oliver Walenciak_, Paul's keeper   
> Source: [Irish central](https://www.irishcentral.com/sports/paul-the-psychic-octopus-picks-all-the-world-cup-winners-98010019-238038511)

Paul the Octopus (26 January 2008 â€“ 26 October 2010) was born in the Sea Life Centre in Weymouth, England, and subsequently moved to the aquarium chain's centre in Oberhausen, Germany. It was there he found world-wide fame as a cephalopod oracle. 

```{r paul-the-octopus-image, fig.cap="Paul the octopus predicts Spain to win the final of the 2010 FIFA World Cup. Source: [The Guardian](https://www.theguardian.com/football/2010/jul/09/psychic-octopus-paul-picks-spain)"}
if(knitr::is_html_output()) {
  knitr::include_graphics("https://static.guim.co.uk/sys-images/Guardian/Pix/pictures/2010/7/15/1279193684789/Paul-the-octopus-005.jpg")
} else {
  download.file("https://static.guim.co.uk/sys-images/Guardian/Pix/pictures/2010/7/15/1279193684789/Paul-the-octopus-005.jpg", "Paul-the-octopus-005.jpg", mode = 'wb')
  knitr::include_graphics("Paul-the-octopus-005.jpg")
}
```

Paul began his career during the UEFA Euro 2008 football tournament. In the lead-up to Germany's international football matches, Paul was presented with two clear plastic boxes, each containing food (a mussel or an oyster). Each container was marked with the flag of a team, one the flag of Germany, and the other the flag of Germany's opponent. The box which Paul opened first (and ate its contents) was deemed to be the predicted winner of the match. Paul predicted Germany to win all of their games, a correct prediction in 4 out of 6 cases. He failed to predict their defeats by Croatia in the group stage, and by Spain in the championship's final.

```{r uefa2008, echo=FALSE}
data(uefa2008)
knitr::kable(uefa2008,caption = "Paul's predictions for the UEFA Euro 2008",label = "uefa2008", booktabs = TRUE, linesep="")
```

Two years later, during the 2010 FIFA World Cup football tournament, Paul obtained celebrity status and his divinations were broadcast live on German TV. This time, Paul made correct predictions for all matches in which Germany played, as well as the final between Spain and the Netherlands. 

```{r fifa2010, echo=FALSE}
data(fifa2010)
knitr::kable(fifa2010,caption = "Paul's predictions for the 2010 FIFA World Cup",  booktabs = TRUE, linesep="")
```

Paul's record of 8/8 correct predictions in the 2010 world cup is quite amazing. The common octopus (_Octopus vulgaris_) is indeed quite an intelligent species. But was Paul really psychic? Could he really foresee the future?

## Experiments and observations

We can view Paul's divinations as the result of an experiment to test his psychic abilities. Paul's predictions were derived under more or less controlled conditions: The keeper didn't put Paul's favourite food in the box representing the team he'd hope to win, and didn't always place the box representing Germany on one side of the tank. Had he done one or more of these things, he might have biased the results. 

A paradigmatic example of an experiment is a randomized-control trial, as common in studies assessing the effectiveness of medication or other therapeutic interventions. In such experiments, a sample of participants is taken from a population of interest, and each participant is randomly assigned to either an experimental condition or control condition. For example, participants in the experimental condition might be given a new drug, while participants in the control condition receive a placebo. Ideally, this is done in a "double-blind" setting, where neither the participant nor the person delivering the treatment or otherwise interacting with the participant knows which condition the participant is assigned to. Randomization is key here: By letting chance determine who gets which treatment, characteristics of the participants (e.g. age, sex, severity of symptoms, etc.) will not be tied to the treatment they get. As such, it will be reasonable to assume that the experimental and control condition differ only in their treatment, such that any differences in outcome can be attributed to the treatment, and not to other differences between the groups of participants. 

<!-- Causality -->

Data obtained under less stringent conditions is often referred to as observational data. For instance, rather than random assignment, we could let the doctor choose which treatment to give to which patient. The doctor might choose to give the new drug to the more severe cases, and the placebo to the less severe cases. Initial severity of disease is now __confounded__ with the treatment, and any difference in outcome success observed between the experimental and control condition might be due to the treatment, or to the initial differences in severity. Or even to a third variable such as age, if it is itself related to both initial severity of disease as well as the effect of the drug. We wouldn't be able to tell, and hence the conclusions we can draw about the effects of the treatment are much less straightforward. We are not completely helpless though. Using statistics, we can attempt to control for pre-existing differences in age and initial disease severity. As we will see in due course, we can attempt to build a model which includes both treatment and initial disease severity as predictors of outcome. In doing so, we can attempt to estimate the unique effect of treatment on outcome. By considering only those differences in outcome that cannot be attributed to initial severity or age, we can still say something meaningful about treatment effects. Nevertheless, there could always be other confounding factors that we haven't though of. As such, conclusions about causality (whether it is truly the difference in medication that caused a difference in outcome), are not as straightforward in observational studies.

## Data

__Data__ are a collection of measured or otherwise observed or known values of variables. A __variable__ is, as the name implies, something that can take a variety of values. In the medical example, it might be the severity of the symptoms after 3 weeks of treatment, while in Paul's case, it might be the correctness of his predictions. In a statistical model, a __dependent variable__ is a variable of interest, a variable which we aim to predict, explain, or describe. An __independent variable__ is a variable which we use to help predict, explain, or describe the dependent variable. For instance, in the medical example above, the dependent variable would be the severity of symptoms, while independent variables would be the treatment (medication or placebo), and potentially other factors such as patients' age, sex, etc. Whether a variable is a dependent or independent variable may differ from model to model. As such, it is an aspect of the analysis, and not so much of the design of a study. 

Paul has provided us with two sets of data. The first contains his predictions for the UEFA Euro 2008 cup, and the second his predictions for the 2010 FIFA World Cup. The dependent variable in both sets is the accuracy of Paul's predictions. Paul's "predictions" were of course not directly predictions who was going to win each game. Paul made a choice which of two containers to open first. But we're not really interested in whether Paul opened the left or right container in his tank, or whether Paul chose the container with the Spanish flag or the container with the Dutch flag. What we are interested in is whether the first container to be opened contained the flag of the team that would win the match played that day. If it did, Paul made a correct prediction, if not, Paul made an incorrect prediction. The data then contains information about what we deem relevant to our purposes. In a sense, looking just at the accuracy of Paul's predictions, we have already abstracted quite a bit from the experiment. We ignore whether Paul went left or right first, how long it took him to open the first container, ... These are all aspects of Paul's behaviour which we could have focused on, and hence could be variables in our data. These variables have different characteristics. For instance, accuracy (correct or incorrect) and direction (left or right container) both have only two possible values, while the duration until opening the first container has many possible values. Variables with a finite (limited) number of values are called __discrete__, while variables with an infinite (unlimited) number of values are called __continuous__.^[Actually, while a variable which can only have a finite number of possible values is necessarily discrete, it is not true that a variable with an infinite number of possible values is necessarily continuous. Both discrete and continuous variables can have an infinite number of values. For instance, a variable which can have as values any of the natural numbers $0, 1, 2, \ldots$ is discrete because it cannot take a value between say 1 and 2. But there are _countably infinite_ natural numbers, so this variable will have an infinite number of possible values. The difference with a continuous variable is however that it is possible to count, or read aloud, the number of possible values in order of magnitude without ever skipping a possible value. This is not possible for continuous variables. For instance, if you would count $0.0001, 0.0002, ...$, you would miss $0.00011$, and $0.00012$, and many other possible values.] In practice, all data is discrete. Even if we were to measure the time it took Paul to open the left or right box first, when we have to store the data, or otherwise make it available for analysis, we simply wouldn't be able to express such a number in finite time or an otherwise finite manner which can be represented in a computer. It would take an infinite amount of time to read a continuous number aloud, and an infinite amount of memory to store this on a computer. 

### Measurement scales

In addition to whether data is discrete or continuous, it is useful to distinguish between different scales of measurement. The measurement scale of a variable determines how we can assign numbers to its values, and how meaningful computations with those numeric values are. For instance, if you determine whether people are male or female, you can assign numbers to this variable "biological sex", such as $\text{female} = 2$ and $\text{male} = 1$, but it doesn't make that much sense to treat these numbers in the usual way. For instance, that we assigned a higher value to females does not imply that they have more of "biological sex". And if we calculate an average of these numbers for a group of people, it might be 1.67. But a statement like "the average biological sex in this group was 1.67" is pretty meaningless.

There are four scales of measurement:

* A __nominal scale__ allows identifying values as "identical" or "different", but nothing more. Examples of variables on a nominal scale are pizza toppings, eye colour, etc. While we can assign numbers to the values (e.g., brown = 1, blue = 2, green = 3), these are arbitrary and can be changed without much consequence (e.g. brown = 21, blue = 7, green = 1). 
* An __ordinal scale__ allows values to be identified as "smaller", "identical", or "larger", but nothing more. For instance, first, second, or third to cross the finish line in a race. Variables on an ordinal scale can be meaningfully ordered or ranked. This means there is some constraint to numbers we can assign to values. 
* An __interval scale__ allows items to be assigned more meaningful numbers. On an interval scale, the differences between numbers can themselves be meaningfully ordered. Temperature is a common example. The difference between 16 and 14 degrees Celsius is smaller than the difference between 19 and 16 degrees Celsius. But we can not say that 30 degrees Celsius is twice as warm as 15 degrees Celsius. This is because an interval scale doesn't have a fixed 0 point. For degrees Celsius, 0 degrees is the point at which water freezes. It is not the point at which there is no temperature. 
* A __ratio scale__ has all characteristics an interval scale has, with the addition of an absolute 0 point. Weight is a common example. Weight cannot be smaller than 0, and 0 indicates no weight at all. Not only can we say that the difference between 3 and 2 kilograms is smaller than the difference between 10 and 5 kilograms, we can also say that 10 kilograms is twice as heavy as 5 kilograms.

Sometimes it is tricky to determine what the measurement scale is. For instance, are "correct" and "incorrect" ordered (correct is better than incorrect) or nominal? Most of the time, a courser distinction is enough, namely whether data is __categorical__ (nominal or ordinal) or __metric__ (interval or ratio).

### The Data Generating Process

In addition to considering the type or measurement scale of the data, you will generally also need to consider the process that in the end led to the data at hand. This process can be called the __Data Generating Process__. 

As the section on experimental vs observational data indicated, the way the data were obtained can affect and limit the inferences that can be made. It is also important to consider to which group or population our data speaks. This is generally related to how we have selected the subjects of a study. For instance, in the case of Paul, we have data from a single octopus, and we are mostly interested in assessing whether he was psychic. We are not interested in Paul's psychic abilities for just those games for which he made predictions though. If Paul indeed had psychic abilities (but let's restrict that to the context of football games), we would expect him to make accurate predictions for other games as well. So here we would like to generalize beyond the data. We would like to generalize from the games for which Paul actually made predictions to all games in the UEFA Euro 2008 and 2010 FIFA World cups, or even to all international football matches. We then need to consider how __representative__ the data is for the more general group we want to make claims about. As Paul mainly made predictions for games in which one of the teams was Germany, the sample is not a good representation of all the games played in both cups. If it is in one way or the other easier to determine the winner in games involving Germany than it is to determine the winner in other games, then the test of Paul's abilities that was conducted is different than the test that would be conducted if we included other games. This, then, limits the claims we can make from the test. If we were interested in whether octopuses in general have psychic abilities, then using a single octopus would not be a great choice. Paul may be unique in many ways, so again he might not be representative of all octopuses. 

As in experiments, where random assignment to conditions can avoid confounds, random sampling can be used to make it more likely that data collected is representative. If we are interested in Paul's predictions for all international matches, it is practically undoable to get him to make predictions for all of these. But by randomly sampling a set of international matches, we will not be able to let this set depend on certain characteristics (e.g. Germany playing) which might bias the results. 

Determining the representativeness of available data for a larger group of possible observations is usually not something that can be done with absolute certainty. And unless we have access to more data, it is generally also not something that statistical analysis can help with. Considerations of representativeness usually rest on (ideally sound) reasoning. 

The Data Generating Process (or DGP for short) is a concept we will encounter repeatedly throughout this book. The DGP contains everything of importance when considering the variability of the data in the context of other data that could have been collected. This includes how subjects were sampled (e.g., if we randomly sample octopuses then other possible data sets would contain data from other octopuses), how subjects are assigned to conditions (if any), but most importantly also how subjects generate responses (or otherwise observable events), and any possible source of randomness in this generation. For instance, during a particular prediction, Paul might initially go for the box on the left, but then a ray of sunlight hits the box on the right, leading Paul to open that box first. Such small events can be considered a source of random noise in the experiment, that lead to variability in Paul's behaviour, and ultimately to variability in the correctness of his predictions. As a concept, the purpose of the DGP is to describe all such sources of variability as they impact the variability of a particular variable of interest. The DGP then describes not only how the data set that we have access to was actually generated, but also _how all possible other data sets would have been generated_. This makes the DGP a rather abstract concept. Nevertheless, we see it is something real, something "out there in the world", in contrast to statistical models, which inhabit the world of mathematics. For statistics, the most important aspect of the DGP is that it provides a distribution over the values of a variable if the DGP was "let loose" and generated all possible data sets (of which the actual data set we have access to is only one). For instance, this would be the frequency of correct and incorrect predictions that Paul makes in every possible version of the experiment (so every possible match, every possible assignment of teams to the left or right box, and perhaps every possible small disturbance in light conditions, etc.) This distribution is sometimes also referred to as the __population distribution__. If this all sounds a little too abstract at the moment, don't worry. We will discuss the DGP in more detail later.

## Exploring and describing data

Once you have collected data, you may feel the urge to immediately apply some statistical model or test to it. But really, a first step should be to explore the data. Statistical tests are designed to confirm or reject hypotheses, and part of __confirmatory data analysis__. They are a very important part of scientific research, but they always rely on assumptions about the data generating process. Before conducting these analyses, it is crucial to assess whether these assumptions are reasonable, and this is where __exploratory data analysis__ can help. @Tukey1977EDA, a major proponent of exploratory data analysis, points out that exploratory data analysis can also suggest new hypotheses about the causes of observed phenomena, and with that provide a basis for further experimentation and data collection.

As the distribution of Paul's predictions is not so interesting to look into exploration of data, we will focus on other data. We will stay in the context of the 2010 FIFA World Cup, and consider team statistics such as the number of goals scored and the number of games played.

```{r fifa2010teams-table}
data("fifa2010teams")
dat <- fifa2010teams[,c(2,3,4,6,9,11)]
dat <- dat[order(dat$team),]
knitr::kable(dat, col.names = stringr::str_replace_all(stringr::str_to_sentence(colnames(dat)),"_"," "), caption = "Team statistics in the 2010 FIFA World cup", booktabs = TRUE, row.names = FALSE)
```

### Summary statistics

Summary statistics are calculated values that aim to summarize key aspects of the distribution of observed values of a variable. Two main aspects of such distributions are (a) where the values are generally located within the range of possible values, and (b) the variation in the values. Measures of the __location__ of the values aim to reflect the central value, in some form or another, of a distribution. Measures of __spread__ aim to reflect the level of variability around the central value.  

#### Measures of location

There are three common measures of location: the mean, median, and mode. Let's consider each in turn.

The __mean__ is the average of a set of values, computed by adding all the values up and then dividing this sum by the total number of values. More formally, the mean of a variable $Y$ is usually denoted by drawing a straight line over the variable (i.e. as $\overline{Y}$), and the formula for its computation is 
\begin{equation}
\overline{Y} = \frac{\sum_{i=1}^n Y_i}{n}
(\#eq:definition-sample-mean)
\end{equation}
Perhaps this is the first time you have seen the "summation operator" $\Sigma$, so let's go over how to work with this. Basically, the summation operator states that you should sum up every value of a variable that is placed to the right of it. Below the summation operator (or on the lower-right side) is an _iterator_ $i$, which gives more information about which values to sum. The iterator goes from the value it is given below or on the bottom-right side (here, the iterator is assigned the value 1), to the value on the top (or top-right) of the summation operator (in this case $n$). So here, the operator states that we should sum up all values $Y_i$, where $i$ goes from 1 to $n$, i.e. all values $Y_1, Y_2, \ldots, Y_n$:
$$\sum_{i=1}^n Y_i = Y_1 + Y_2 + Y_3 + \ldots + Y_n$$ 
```{r}
y <- dat$goals_for
```
For the FIFA 2010 World Cup, we have $n=32$ teams, so to compute the mean number of goals scored, we would sum from $Y_1$ (the number of goals scored by the first team) to $Y_{32}$ (the number of goals scored by the final team), and get
$$\overline{Y} = \frac{`r y[1]` + `r y[2]` + `r y[3]` + `r y[4]` + `r y[5]` + \ldots + `r y[31]` + `r y[32]`}{32} = \frac{`r sum(y)`}{32} = `r sum(y)/32`$$

The __median__ is defined as the value in the set of values such that 50% of the values are lower than it. It is easy to compute if the number of observations ($n$) is odd. We simply order the values in increasing or decreasing magnitude. The median is then simply the middle value in this ordered set. If $n$ is  an even number, the median cannot be uniquely determined. But it is common to take the value that is halfway between. 

The goals scored by each team, ordered in increasing magnitude are:
```{r}
y <- dat$goals_for[order(dat$goals_for)]
```
`r paste(y,collapse=", ")`

There are $n=32$ values, so the first 16 values in the ordered sequence above are the lowest 50% of the values, and the second 16 values are the highest 50% values. If we had one value less, i.e. if $n=31$, then the 16th value would be the median. In our case, the median is a value between the 16th and 17th value. If you go along the sequence and stop at the 16th value, you will see that it equals `r y[16]`. The next value is the 17th value, which also equals `r y[17]`. So in this case, the median must be `r  y[16]`, as this is the only value between `r y[16]` and `r y[17]`. If the 17th value happened to be `r y[17] + 1`, then we would have determined the median as the value halfway between `r y[16]` and `r y[17] + 1`, i.e. as `r y[16] + .5`.

The __mode__ is the most frequently occurring value. This measure is really only useful for relatively large data sets with a limited number of possible values. For a continuous variable, there is generally no value that occurs more than once in a data set, and hence all values could be considered a mode. If we look at the total goals scored, we see that the mode is 3, as this is the most frequent value (there are 8 teams who scored 3 points, while there are 4 teams who scored 2 points, and 4 teams who scored 4 points). 

#### Measures of spread

There are three common measures of spread: the inter-quartile range, the variance, and the standard deviation.

The __inter-quartile range__ is the distance between the 25th and the 75th percentile. A percentile is a value such that a specified proportion of the values are lower than it. So the 25th percentile is the lowest value in the data such that 25% are even lower. If you go back to the definition of the median, you may notice that the median is actually the 50th percentile. The 25th, 50th, 75th and 100th percentiles are also respectively called the 1st, 2nd, 3rd, and 4th quartile, hence the name inter-quartile range. To compute percentiles, it is handy to order the values in increasing magnitude (just like we did for computing the median). A sample percentile can then be computed by determining the appropriate rank (the place of a value in the ordering) that corresponds to the required percentile:
$$\text{rank}(\text{percentile}) = \text{ceiling}\left(\frac{\text{percentile}}{100} \times n\right)$$
Here, $\text{percentile}$ is the required percentile (e.g. 25, or 75). The ceiling function stands for rounding up to the nearest integer, so e.g. $\text{ceiling}(2.1) = 3$. To compute the inter-quartile range for total goals scored, we compute the needed ordinal ranks as 
$$\text{rank}(25) = \text{ceiling}\left(\frac{25}{100} \times 32 \right) = 8$$
which means we can take the 8th value in the ordered list as the 25th percentile, which equals `r y[8]`, and
$$\text{rank}(75) = \text{ceiling}\left(\frac{75}{100} \times 32 \right) = 24$$
which means we can take the 24th value in the ordered list, which equals `r y[24]`. The inter-quartile range is then
$$\text{IQR} = [`r y[8]`, `r y[24]`]$$

```{r}
y <- dat$goals_for
```
The __variance__ is the average squared deviation of values from the mean. One reason for using _squared_ deviations rather than the average deviation itself is that the average deviation 
$$\frac{\sum_{i=1}^n (Y_i - \overline{Y})}{n}$$
is always 0. An alternative is of course to then calculate the average of the _absolute_ deviations. While this is a sensible measure of the spread of values, there are technical reasons to prefer the average squared deviation (i.e. the variance), which we won't go into now. Let's just plough on with how to calculate the variance. The variance of a collection of observed values, which is also referred to as the sample variance, is usually denoted as $S^2_Y$, and defined as:
\begin{equation}
S^2_{Y} = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}
(\#eq:definition-sample-variance)
\end{equation}
So you simply subtract from each value the average value (the sample mean), and then raise each deviation to the power of 2. Then you add those squared deviations up, and divide the sum by the total number of observations. For the total goals scored, the variance is calculated as:
$$\begin{aligned}
S^2_Y &= \frac{(`r y[1]` - `r mean(y)`)^2 + (`r y[2]` - `r mean(y)`)^2 + (`r y[3]` - `r mean(y)`)^2 + \ldots + (`r y[31]` - `r mean(y)`)^2 + (`r y[32]` - `r mean(y)`)^2}{`r length(y)`} \\
&= \frac{(`r y[1]-mean(y)`)^2 + (`r y[2]- mean(y)`)^2 + (`r y[3]- mean(y)`)^2 + \ldots + (`r y[31]- mean(y)`)^2 + (`r y[32]-mean(y)`)^2}{`r length(y)`} \\
&= \frac{`r (y[1]-mean(y))^2` + `r (y[2]-mean(y))^2` + `r (y[3]-mean(y))^2` + \ldots + `r (y[31]-mean(y))^2` + `r (y[32]-mean(y))^2`}{`r length(y)`} \\
&= \frac{`r sum((y - mean(y))^2)`}{`r length(y)`} \\
&= `r sum((y - mean(y))^2)/length(y)`
\end{aligned}$$

A downside of the variance is that its value is difficult to interpret. By squaring each deviation, the value of the variance is on a different scale of magnitude than the original values. The __standard deviation__ aims to provide a more interpretable measure of spread, by transforming the variance back to the scale of the original values. The standard deviation of a collection of observed values, usually denoted as $S_Y$, is simply the square-root of the variance, so
$$S_Y = \sqrt{S^2_Y}$$
or written out in more detail as
\begin{equation}
S_{Y} = \sqrt{\frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}}
(\#eq:definition-sample-standard-deviation)
\end{equation}
For the total goals scored, the standard deviation is
$$S_Y = \sqrt{`r sum((y - mean(y))^2)/length(y)`} = `r sqrt(sum((y - mean(y))^2)/length(y))`$$


### Visual exploration

Exploring a data set is generally done visually, using various plots such as histograms, boxplots, 
scatterplots, etc. A good plot can immediately provide a lot more information than summary measures. So before proceeding with statistical analysis: plot your data! Visualizing data in the most effective and pleasant way is an art form in itself. But here are some standard plots that are helpful and relatively easy to produce:

```{r example-histogram-and-boxplot, fig.show='hold',fig.width=4,out.width='49%',fig.cap="A histogram and boxplot of the total goals scored by each team"}
library(ggplot2)
ggplot(dat,aes(x=goals_for)) + geom_histogram(binwidth=1,colour="black") + xlab("Goals scored")

ggplot(dat,aes(x="",y=goals_for)) + geom_boxplot(width=.5) + ylab("Goals scored") + xlab("")
```

A __histogram__ (left plot in Figure \@ref(fig:example-histogram-and-boxplot)) counts the number of occurrences of values, or ranges of values in the case of continuous variables with a lot of possible values. For the latter, it divides the range of possible values into "bins" (equally sized ranges of values). The count of observations in each bin is then plotted as the height of a bar, and the width of the bar reflects the range (in the case of discrete values, the range is defined by the midpoints between these values. A histogram is useful to depict the distribution of values of a variable. The size of the bins is important though, as it determines how smooth or rough the histogram looks, and often requires some tweaking. If you make the bins too wide, it may be hard to spot the intricacies in a distribution. If you make the bins too narrow, on the other hand, each bin will contain only a small number of values and you won't summarize the distribution in a visually meaningful way. In the extreme case, each bin will either contain just one observation, or none.

A __boxplot__ (right plot in Figure \@ref(fig:example-histogram-and-boxplot)), also called a box and whiskers plot, depicts the inter-quartile range as a box, in which a line is placed to depict the median. Two lines extend from either side of the box. These are the "whiskers" and they depict the range of values in the data excluding any potential outliers. More precisely, the endpoints of the whiskers are the smallest and largest value in the data which are no more than $1.5 \times \text{IQR}$ away from the 25% and 75% quartiles, respectively. If there are values in the data which extend beyond these endpoints, they are considered "outliers", and plotted as points outside of the whiskers. In the example boxplot, you can see that there are three such outliers. 

A __scatterplot__ is useful to see the relation between the values of two variables. For instance, it is likely that the total goals scored by each team will depend on the number of games they played. We can visually inspect this by plotting each team as a point with the value of the x-axis representing the value of one variable, and the value on the y-axis as the value of the other variable. This is done in Figure \@ref(fig:example-scatter-plot), from which it seems quite clear that those teams who played more games generally also scored more points. 
```{r example-scatter-plot,fig.width=4,out.width='50%',fig.cap="A scatter plot of total goals scored by total matches played"}
ggplot(dat,aes(x=matches_played,y=goals_for)) + geom_point() + xlab("Matches played") + ylab("Goals scored")
```

As all the plots above can tell us something meaningful about the data, and they often highlight different aspects of the data, it can be useful to combine them. One form of such a combination plot has been called a  __raincloud plot__ [@allen2019raincloud]. A raincloud plot combines a boxplot with a scatterplot of the observed values and a type of continuous histogram called a nonparametric density plot. In my version of such plots (\@ref(fig:example-raincloud-plot)), a boxplot is placed in the middle, with an additional point representing the sample mean. On the left are the observed values, where the points are "jittered" (randomly displaced on the x-axis) to try to avoid overlapping points. On the right of the boxplot is the nonparametric density plot. This part aims to depict the distribution in a smooth, continuous way. 

```{r example-raincloud-plot,fig.width=4,out.width='50%',fig.cap="A raincloud plot of the average goals scored per match"}
set.seed(5678)
dat$average_goals_for <- dat$goals_for/dat$matches_played
plot_raincloud(dat,y = average_goals_for, point_size=1) + ylab("Average goals scored")
```

## Analysis and modelling

Analysing data can mean lots of things. According to [WikiPedia](https://en.wikipedia.org/wiki/Data_analysis) "Data analysis is a process of inspecting, cleansing, transforming and modelling data with the goal of discovering useful information, informing conclusions and supporting decision-making." That is a reasonably accurate, but also a quite lengthy and boring definition. An alternative to this I quite like is that data analysis is _identifying a signal within noise_. 

Data is noisy for many reasons. For one thing, the data that we have access to is generally limited, in the sense that we could have obtained other and/or more data. Other sources of noise include measurement error. Such sources of noise imply that the data set we have only gives a limited, incomplete, and corrupted view on the process that generated it (the Data Generating Process). We are not interested solely in describing the data set we have, we generally want to infer something about the Data Generating Process. We don't just want to state that Paul made 8 out of 8 accurate predictions in the 2010 FIFA World Cup, we want to use this data to assess whether Paul had psychic abilities. Whether, had Paul's keeper allowed Paul to make predictions for other matches, he would have also been correct. Such inferences go beyond the data that we have. Perhaps Paul was just lucky in those 8 predictions, and his 100% hit rate is not a good reflection of his hit rate in general. 

Statistical modelling can be viewed as describing the Data Generating Process in the language of mathematics. At its core, a statistical model defines the distribution of possible values of variables. In an ideal world, this model distribution is identical to the true distribution that follows from the Data Generating Process. In general, though, there will be some mismatch between the model distribution and the DGP distribution. Models are simplifications and idealizations. As statistician George Box famously noted "all models are wrong, but some are useful" [@Box1987]. What do we mean by this? Consider the famous London tube map (Figure \@ref(fig:london-tube-map)). This map is "wrong" because as any map, it is a simplification of reality. It does not provide a completely accurate representation of the world. But what is special about the tube map is that, unlike standard maps, it does not even attempt to reflect the geographical location of the different underground (or "tube") lines and stations. Its initial designer, Harry Beck, realised that because the lines run mostly underground, the exact physical locations of the stations were largely irrelevant to the traveller wanting to know how to get from one station to another. What matters to them is the relation between the stations, and which lines connect them. Hence, he designed the map as a schematic diagram which clearly shows the lines connecting stations. If you used the map to determine the walking distance between stations, you'd be very disappointed. But to determine your underground journey, the map is very useful. 

```{r london-tube-map, fig.cap="London tube map. Source: [TfL](https://tfl.gov.uk/maps/track/tube)"}
if(knitr::is_html_output()) {
  knitr::include_graphics("https://tfl.gov.uk/cdn/static/cms/images/tube-map.gif")
} else {
  webshot::webshot("https://tfl.gov.uk/cdn/static/cms/images/tube-map.gif", "tube-map.png")
  knitr::include_graphics("tube-map.png")
}
#knitr::include_graphics("https://tfl.gov.uk/cdn/static/cms/images/tube-map.gif")
```

Statistical models are a bit like the London tube map: their usefulness will depend on your goals. In order to derive properties of various statistical analyses and tests, we have to assume that a model is true (in the sense that it truly reflects the DGP distribution). While this assumption is inherently uncertain, the derived properties may hold, at least approximately, as long is the model distribution is an "accurate enough" representation of the DGP distribution. While in mathematics, statements are either true or false, in applied statistics, we have to deal with statements that are only approximately true. 

## Summary

The objective of statistical data analysis is to go beyond a given data set, and make claims about aspects of the process that generated the data (the Data Generating Process). To make inferences beyond the limited data we have collected, we need statistical models which aim to describe the distribution of all possible data that we could have collected. Data can be of different types. The measurement scale of a variable determines the way in which numbers assigned to values of the variable can be interpreted and manipulated. Aspects of the data collection, such as whether data was collected in an experiment with random assignment, or whether the data was observed in a more naturalistic setting, are important aspects of the Data Generating Process, and determine to what extent we can rule out confounds that may bias our conclusions. Paul was one amazing octopus.

<!--chapter:end:01-intro.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Statistical modelling {#ch-modeling}

Was Paul truly psychic, an oracle of some kind? Or was Paul -- or his keeper, as what does fame really mean to an octopus? -- just really lucky? How do we know? And what really are the chances of predicting the winner of 8 matches correct? These are questions we will focus on now.

In this chapter, we will start defining statistical models and using them to make inferences about the Data Generating Process. We will cover some basic probability theory, and use this to define two alternative models of Paul's predictions. One in which he is randomly guessing, and one in which his predictions can be more (or less) accurate than if he were randomly guessing, while we are unsure about how accurate Paul really is in that case. We will discuss how to estimate the unknown accuracy of Paul's predictions in this model. After this, we will try to answer the question: was Paul (somewhat) psychic? Our answer will be based on whether we can refute the hypothesis that he was randomly guessing. This is generally done with a procedure called a null-hypothesis significance test. <!-- We will also consider another way in which to look at this, in the form of so-called confidence intervals. -->

## Coin flipping: Defining a statistical model

What does it mean for Paul to be psychic, to be an oracle? Sometimes it is easier to answer this question by considering the opposite: When would Paul _not_ be an oracle? If Paul had no way to tell the future, he would be "merely guessing". Purely random guessing between two options can be implemented by flipping a coin. Suppose Paul made his choices by "flipping a mental coin", and choosing the box on the left when the outcome was "heads", and the box on the right when the outcome was "tails". Great, we now have a model of the Data Generating Process! Coin flipping can be seen as a physical model. It is something that we can do in the real world, as many times as we want. Because we understand the physical properties of coin flips reasonably well -- it is really difficult to predict the outcome of a coin flip -- it works well as a model of random guessing. But flipping coins over and over is rather tedious. If we have a mathematical description, a statistical model, of flipping coins, we can spare ourselves this trouble. To do so, we first need to know a bit more about probability.  

## Probability

Let's simulate Paul's decisions for the 2010 FIFA World cup. Grab a coin and flip it `r nrow(fifa2010)` times. What are the outcomes? When I did this, the outcomes were:

`r set.seed(123456); paste(sample(c("heads","tails"),size=nrow(fifa2010),replace=TRUE),collapse=", ")`

Remember, Paul would go left upon heads, and right upon tails. Following these rules, Paul's decisions are then:

`r set.seed(123456); paste(sample(c("left","right"),size=nrow(fifa2010),replace=TRUE),collapse=", ")`

To know whether these guesses are correct or incorrect, we need to know which team was placed in which box. This information is provided in Table \@ref(tab:fifa2010): the first country under `Match` is the box on the left, and the second the box on the right. Using this, our simulation then provides the following:  

```{r, echo=FALSE}
library(sdamr)
data(fifa2010)
set.seed(123456)
x <- sample(c(1,2),size=nrow(fifa2010),replace=TRUE)
matchpred <- matrix(unlist(strsplit(fifa2010$Match,"-")),ncol=2,byrow=TRUE)[cbind(1:nrow(fifa2010),x)] == fifa2010$Prediction
simcor <- matchpred & fifa2010$Outcome == "correct" | !matchpred & fifa2010$Outcome != "correct"
simcor <- ifelse(simcor,"correct","incorrect")
```

`r paste(simcor,collapse = ", ")`

So in our simulation, Paul made `r sum(simcor == "correct")` correct predictions out of `r length(simcor)`. Not quite as good as the real Paul, but still a reasonable performance. 

Notice how we went through quite a few steps. We defined a model of non-psychic-and-guessing Paul as making decisions by flipping a coin, we then simulated Paul's decisions by flipping a coin, then transformed these coin flips into a variable containing the decisions to open the left or right box, and finally transformed this variable into another variable containing the accuracy of the predictions. The first transformation is easy. We could have simply put stickers on the coin relabelling "heads" as "left" and "tails" as "right". The second transformation is not as obvious as it depends on a second variable (whether the winning team was represented by the left or right box). In this case, we could have saved ourselves a bit of work by immediately relabelling our coin flips as "correct" or "incorrect". To see why, we need to know a bit more about how to calculate probabilities.

Let's start at the beginning. For one coin flip, there are two possible outcomes: the coin lands on heads or on tails. The outcome can't be neither (we are assuming the coin is thin enough to not land on its side), nor two heads, or something else. The coin must land on either heads or tails, so there are two outcomes. We call the set of all possible outcomes the __outcome space__, and we'll use a nice curly $\mathcal{S}$ to denote it:

$$\mathcal{S} = \{\text{heads},\text{tails}\}$$

<!--
```{definition, outcome-space, name="outcome space", echo=TRUE}

The outcome space is the set of all possible events.

```
-->

Our coin flip is a __random variable__ that takes one of the values in the outcome space. It is traditional to denote random variables with capital Roman letters, and the values with small Roman letters. When we want to make generic statements that apply to different random variables (such as a coin flip, the roll of a die, the outcome of a prediction), we tend to use this notation, and $P(Y=y)$ then means 'The probability that random variable $Y$ has value $y$'. Other times, it may be easier to use more descriptive names. In this book, when we use a name for a variable, we will use a computer font, such as `coin_flip`, for the random variable. So $P(\texttt{coin\_flip} = \text{heads})$ means 'The probability that random variable `coin_flip` has the value "heads"'.   

### What is probability? {#sec:02-probability-definition}

```{r, echo=FALSE}
library(ggplot2)
set.seed(456)
n_flip <- 2000
flip <- rbinom(n_flip,size=1,prob=.5)
dat <- data.frame(flip = 1:n_flip, y = cumsum(flip)/(1:n_flip))
```

I have just introduced the word "probability" without telling you what it means. You probably have some intuition yourself, for instance that probability is the chance of something happening. You might be surprised to know that even though statisticians can all perform probability calculations comfortably, there is quite some disagreement on what probability means. According to the traditional, __Frequentist view__, probability means _long-run relative frequency_. For instance, the probability of a coin flip landing on heads is defined as the proportion of times a coin lands heads when I flip a coin for a very, very large number of times. Table \@ref(tab:long-run-frequency-table) shows the outcome and relative frequency calculations for 15 coin flips.

```{r long-run-frequency-table, echo=FALSE}
tab <- dat[1:15,]
tab$outcome <- c("tails","heads")[flip[1:15] + 1]
tab$frequency <- cumsum(flip)[1:15]
tab <- tab[,c(1,3,4,2)]
colnames(tab) <- c(colnames(tab)[1:2],"cumulative frequency heads", "relative frequency heads")
knitr::kable(tab, caption = "Relative frequency of heads for 1 to 15 flips. The cumulative frequency is the number of heads thus far, and the relative frequency the cumulative frequency divided by the number of flips.", booktabs = TRUE)
```

Figure \@ref(fig:coin-flip-relative-frequency) shows the relative frequency (i.e. the proportion) of heads after each coin flip, when flipping a coin for `r n_flip` times.  As you can see, the relative frequency fluctuates quite a bit, but becomes more stable with more flips. But even after `r n_flip` flips, it is not equal to 0.5. 

```{r coin-flip-relative-frequency, dev='png', echo=FALSE,fig.cap=paste("Relative frequency of heads when flipping a coin for",n_flip,"times"), cache=TRUE}
if(knitr::is_html_output()) {
  library(gganimate)
  ggplot(dat,aes(x=flip,y=y)) + geom_line() + ylim(c(0,1)) + ylab("relative frequency of heads") + transition_reveal(flip)
} else {
  ggplot(dat,aes(x=flip,y=y)) + geom_line() + ylim(c(0,1)) + ylab("relative frequency of heads")
}
```

```{r, echo=FALSE}
set.seed(456)
n_flip <- 5000
n_sim <- 5
dat <- data.frame()
for(i in 1:n_sim) {
  dat <- rbind(dat,
               data.frame(simulation=i,flip = 1:n_flip, y = cumsum(rbinom(n_flip,size=1,prob=.5))/(1:n_flip))
  )
}
```

Flipping a coin can be viewed as an experiment, that we can repeat. Figure \@ref(fig:coin-flip-relative-frequency-multiple) shows the results of flipping a coin for `r n_flip` times when repeating the experiment `r n_sim` times. As you can see, the results of our experiments (the relative frequencies) are different each time. The differences are quite marked for a small number of flips, but become more alike after flipping the coin more times. This illustrates something known as __The Law of Large Numbers__, to which we will come back soon. For now, let's focus on a different matter. If probability is a relative frequency, then which one is it? Each repetition of the experiment, as well as each number of throws, would give us a different answer! 

```{r coin-flip-relative-frequency-multiple, dev='png', echo=FALSE,fig.cap=paste("Relative frequency of heads when flipping a coin",n_sim,"times for",n_flip,"times"), cache=TRUE}
if(knitr::is_html_output()) {
  ggplot(dat,aes(x=flip,y=y,colour=factor(simulation))) + geom_line() + ylim(c(0,1)) + ylab("relative frequency of heads") + theme(legend.position="none") + transition_reveal(flip)
} else {
  ggplot(dat,aes(x=flip,y=y,colour=factor(simulation))) + geom_line() + ylim(c(0,1)) + ylab("relative frequency of heads") + theme(legend.position="none")
}
```

To give a single answer, a "long-run relative frequency" must be the relative frequency when flipping the coin for an infinite number of times. That may seem disappointing. A nice aspect of the Frequentist View is that probabilities are something of the real world, but surely, we can't flip a coin for an infinite number of times in the real world. That is true, but to understand what probability is, we don't necessarily have to perform such infinite experiments. When we have to estimate the probability from a given (non-infinite) number of coin flips, we do have to worry about these issues. This, indeed, is what statistics is about: dealing with sample variability to estimate and infer unknown things. But for defining what a probability is, we can use mathematics to show that the long run relative frequency converges to a single number as the number of coin flips gets closer and closer to infinity. Which, in other words, means there is a single long-run relative frequency, and hence the probability is mathematically well-defined.     

There is an alternative to the Frequentist View, called the __Bayesian view__ or __Subjective view__, according to which probability means a _rational degree of belief_. This, in a sense, takes probability out of the real world, and into our minds. By doing so, it allows statements of probabilities for single events, such as what is the probability that it will rain tomorrow. In the Frequentist View, it either rains tomorrow or not. We can ask what the probability is that it rains on the 25th of July in general, by considering the long-run relative frequency of rain on every 25th of July in past, present, and future years. But there is no long-run relative frequency for the 25th of July in 2020.

#### The rules of probability {#sec:02-rules-of-probability}

<!-- https://bolt.mph.ufl.edu/6050-6052/unit-3/module-6/ -->
<!-- https://bolt.mph.ufl.edu/6050-6052/unit-3/module-7/ -->

Although the interpretation of probability is debated, the mathematical rules of calculating probabilities are generally agreed upon. Suppose we have an outcome space $\mathcal{S} = \{E_1, E_2, E_3, \ldots\}$. That is, the outcome space consists of (abstract) events $E_1$, $E_2$, $E_3$, etc. To refer to any of these, we can use the notation $E_i$, where $i$ can equal 1, 2, 3, etc. The rules of probability are then as follows: 

1. $0 \leq P(E_i) \leq 1$. The probability of any event $E_i$ is greater or equal to 0 and smaller than or equal to 1. This rule requires little further explanation. Probabilities are chosen to lie on a scale between 0 and 1.
2. $P(E_1 \text{ or } E_2 \text{ or } E_3 \text{ or } \ldots) = P(\mathcal{S}) = 1$. In words, this means that the probability of at least one event in the outcome space occurring is 1. 
3. If $E_i$ and $E_j$ are _mutually exclusive_ events (if one of them occurs, the other cannot occur), then $P(E_i \text{ or } E_j) = P(E_i) + P(E_j)$. This is also called the __sum rule__.
4. $P(\neg E_i) = 1 - P(E_i)$. Here, $\neg$ means "not", so the probability of "not $E_i$" is 1 minus the probability of $E_i$ occurring. This is also called the __complement rule__.
5. For any events $E_i$ and $E_j$, $P(E_i \text{ or } E_j) = P(E_i) + P(E_j) - P(E_i \text{ and } E_j)$. This rule holds whether the events are mutually exclusive or not, and can be called the __general sum rule__.

Now, all these rules seem overkill when we are dealing with a simple coin flip. So let's consider a slightly more complex situation: betting on the outcome of a roll of a six-sided die. In this betting game, like in roulette, you are allowed to bet on many things, such as the exact number, but also whether the number is odd or even, whether the number is greater than 3, whether the number is greater than 1, etc. All these things are events in the outcome space:

$$\mathcal{S} = \{1, 2, 3, 4, 5, 6, \text{even}, \text{odd}, >3, < 1, \geq 4, 1 \text{ or } 3, \ldots \}$$

The first six events in this outcome space (the numbers 1 to 6) are called __elementary events__: they are mutually exclusive (a single roll cannot result in both a 1 and a 3), and one of them _must_ occur. So we know that $P(1) + P(2) + P(3) + P(4) + P(5) + P(6) = 1$. But wait, there are more events in the outcome space, and $P(\mathcal{S}) = 1$, and these six events are only a small part of the outcome space, so does that imply that they all have probability 0? No! That is because not all events in the outcome space are mutually exclusive. For instance, if the outcome is 5, then the outcome is also odd, and it is also $>3$, $\geq 4$, etc. As you might have noticed, the other events are actually themselves sets of the elementary events.  For instance,  $\text{odd} = \{1, 3, 5\}$, $\text{even} = \{2, 4, 6\}$, and $> 3 = \{4, 5, 6 \}$. Figure \@ref(fig:euler-diagram-betting-die) depicts the relations between these events, as well as the elementary events.

```{r euler-diagram-betting-die,fig.cap="A diagram depicting the relations between 9 events in the outcome space of the die betting game"}
library(ggplot2)
library(ggforce)
library(viridis)

ggplot() + theme_void() + 
  geom_ellipse(aes(x0 = 3, y0 = 2, a = 3, b = .98, angle = 0), fill = viridis(3)[1], alpha = .4) + geom_text(aes(x=2,y=2.7,label="odd")) + 
  geom_ellipse(aes(x0 = 3, y0 = 0, a = 3, b = .98, angle = 0), fill = viridis(3)[2], alpha = .4) + geom_text(aes(x=2,y=.7,label="even")) + 
  geom_ellipse(aes(x0 = 4.7, y0 = .8, a = 2.8, b = 1.4, angle = pi/4.5), fill = viridis(3)[3], alpha=.4) + geom_text(aes(x=6,y=1,label=">3")) +
  geom_circle(aes(x0=1,y0=2,r=.5)) + geom_text(aes(x=1,y=2, label="1")) +
  geom_circle(aes(x0=1,y0=0,r=.5)) + geom_text(aes(x=1,y=0, label="2")) +
  geom_circle(aes(x0=3,y0=2,r=.5)) + geom_text(aes(x=3,y=2, label="3")) +
  geom_circle(aes(x0=3,y0=0,r=.5)) + geom_text(aes(x=3,y=0, label="4")) +
  geom_circle(aes(x0=5,y0=2,r=.5)) + geom_text(aes(x=5,y=2, label="5")) +
  geom_circle(aes(x0=5,y0=0,r=.5)) + geom_text(aes(x=5,y=0, label="6"))

```

We just really need two more rules and then we're done. These rules are to compute the probability of a conjunction of events (the probability of both events occurring). To be able to state these rules, we first need to consider the concept of __conditional probability__. In words, such a probability refers to the probability of one event _given that another event occurred_. For instance, we can consider the probability of the event "greater than 3" given that the event "odd" occurred (i.e., the probability that the outcome was greater than 3 given that the outcome was an odd number). If we know that the outcome was odd (i.e. 1, 3, or 5), we can rule out all occurrences where the outcome was even (i.e. 2, 4, or 6). To now consider the probability that the outcome was greater than 3, we only need the probability of an outcome greater than 3 within the set of odd numbers. If we are sure that the outcome was odd, we can set $P(\text{odd}) = 1$. There are three outcomes, each with equal probability, so $P(1|\text{odd}) = P(3|\text{odd}) = P(5|\text{odd}) = \frac{1}{3}$. In the set of odd outcomes, there is only one number greater than 3, namely 5. That means that the conditional probability $P(>3|\text{odd}) = P(5|\text{odd}) = \frac{1}{3}$. Conversely, we can also work out the conditional probability $P(\leq 3|\text{odd}) = P(1|\text{odd}) + P(3|\text{odd}) = \frac{2}{3}$. In a sense, a conditional probability is just looking at the relative occurrence of one event $E_i$ (e.g., the event "$>3$") within the set of outcomes defined by another event (e.g., the event "odd"). 

$$P(E_i|E_j) = \frac{P(E_i \text{ and } E_j) }{P(E_j)}$$

6. $P(E_i \text{ and } E_j) = P(E_i) \times P(E_j | E_i)$. __Multiplication rule__
7. For two __independent__ events $E_i$ and $E_j$, $P(E_i \text{ and } E_j) = P(E_i) \times P(E_j)$

Kolmogorov, a rather brilliant mathematician, showed that you only need rules 1, 2 and 3; all the other rules follow from the first three. In mathematical terms, rules 1, 2, and 3 are _axioms_, statements which we assume are true without being able to prove them, while the remaining rules are _theorems_, statements which follow from the axioms and can be proven to be true.

#### A model of flipping an unbiased coin

Now that we know more about probability, let's get back to our model for Paul. With a balanced coin (and a not too cunning flipper), the probability of the coin landing heads should equal the probability of the coin landing tails. In mathematical notation, we can state this as

$$P(\text{heads}) = P(\text{tails})$$

Furthermore, as heads and tails are the only two possible outcomes, we know that

$$P(\text{heads} \text{ or } \text{tails}) = 1$$
Finally, because heads and tails are two mutually exclusive and equally likely events, we then infer that

$$P(\text{heads}) = P(\text{tails}) = 0.5,$$
and given the way Paul bases his decisions on this coin flip, also that
$$P(\text{paul left}) = P(\text{paul right}) = 0.5.$$

Interestingly, if Paul made his decision for the left or right box by flipping a coin, it doesn't really matter whether the team representing the left or right box was chosen randomly, or in some other way. Suppose that Paul's keeper was very knowledgeable about football, or that he was the real psychic. Furthermore, let's suppose that he thinks Paul might generally prefer the right box (as the light is particularly nice on that side of the tank, for instance). Then $P(\text{winner right}) = 1$ and $P(\text{winner left}) = 0$. Because Paul's decisions are independent from which team was placed in which box (he's merely flipping a coin), we know that $P(\text{paul left} | \text{winner left}) = P(\text{paul left})$, and similarly $P(\text{paul right} | \text{winner left}) = P(\text{paul right})$. There are two ways in which Paul can make a correct prediction: he opens the right box, and the winning team was in the right box, or he opens the left box and the winning team was in the left box. So
$$P(\text{correct}) = P(\text{paul left} \text{ and } \text{winner left}) + P(\text{paul right} \text{ and } \text{winner right})$$
Using the multiplication rule, we can write this as
$$P(\text{correct}) = P(\text{paul left} | \text{winner left}) \times P(\text{winner left}) + \\ P(\text{paul right} | \text{winner right}) \times P(\text{winner right})$$
And from the independence between Paul's decisions and the assignment of teams to boxes, we can simplify this further as
$$P(\text{correct}) = P(\text{paul left}) \times P(\text{winner left}) + P(\text{paul right}) \times P(\text{winner right})$$
which, filling in the appropriate values, becomes
$$P(\text{correct}) = .5 \times 0 + .5 \times 1 = .5$$
Actually, you can fill in any valid probability for $P(\text{winner left})$ and $P(\text{winner right})$, and $P(\text{correct})$ will always be .5! Let's say, in abstract terms, that $P(\text{winning left}) = w$. From the complement rule, we know that $P(\text{winning right}) = 1 - w$. Then
$$\begin{aligned}
P(\text{correct}) &= P(\text{paul left}) \times w + P(\text{paul right}) \times (1-w) \\
&= .5 \times w +  .5 \times (1-w) \\
&= .5 \times w - .5 \times w + .5\\
&= .5
\end{aligned}$$
So according to our model, no matter what bias Paul's keeper might have in placing winning teams in the left or right box, Paul's accuracy for each prediction would be $P(\text{correct}) = .5$.

<!--
$$P(\text{correct}) = P(\text{incorrect}) = 0.5$$

$$P(\texttt{prediction}_i = \text{correct}) = P(\texttt{prediction}_i = \text{incorrect}) = 0.5$$

$$P(Y_i = \text{correct}) = P(Y_i = \text{incorrect}) = 0.5$$

which you can translate into words as 

> The probability that observation $i$ of variable $Y$ equals 'correct' is identical to the probability that observation $i$ of variable $Y$ equals 'incorrect', and both equal 0.5.
-->

### Distributions

A probability distribution, as the name suggests, defines how probability is distributed over all possible values of a variable. You can think of "all probability" as a pie, and the probability distribution as slicing up the pie in (possibly unequal) pieces. Each possible value of a variable is given its own slice, and the size of the slice relative to the overall pie is the probability. Looking at the correctness of a single prediction, the pie is divided into two equal slices: half for $P(\text{correct})$, and the other half for $P(\text{incorrect})$. But Paul made more than one prediction (8 in the case of the 2010 FIFA World cup), and what we're really interested in is Paul's accuracy in all his predictions: What is the probability that Paul made 8 out of 8 correct predictions if he was merely guessing? Using the rules of probability, we can work out this probability as well. 

According to our coin-flipping model, it does not matter which of Paul's prediction we look at (e.g. whether the first, the second, the last, or any other one). In fact, we could generate the data in the reverse order, or start with the Germany-Australia match and then the Spain-Germany match. Let's write Paul's predictions as a variable $\texttt{pred}_i$, where the index $i$ can be a value between 1 and 8, corresponding to the first, second, etc., prediction he made. The probability that any prediction is correct is $P(\texttt{pred}_i = \text{correct}) = .5$, and whether that prediction is correct is independent from whether any earlier (or later) prediction is correct. In fancy terms, this means that the observations $Y_i$ are _independent and identically distributed_ (IID).

When considering the probability of a sequence of events (e.g. a consecutive run of 8 correct predictions), it can be useful to draw all possible outcomes in the form of a tree. In Figure \@ref(fig:heads-tails-tree), we show such a tree for the first three of Paul's predictions according to our model in which he was randomly guessing. Starting at the "root node" on the left, the first guess can be either correct or incorrect, and these two outcomes are represented as separate branches. If the first guess was correct, we take the "correct" branch. The next guess can be either correct or incorrect, which splits this branch into two further branches, and so forth. 

```{r heads-tails-tree, fig.cap="Outcome tree for three random guesses."}
library(DiagrammeR)

treePlot <- "
  digraph tree_plot {

    graph [layout = dot,
           rankdir = LR]

    node [shape = box] 
    S [label = 'start']
    
    node [shape = oval, style = filled]
    H [label = 'correct', fillcolor = purple ]
    T [label = 'incorrect', fillcolor = yellow]
    HH [label = 'correct', fillcolor = purple ]
    HT [label = 'incorrect', fillcolor = yellow]
    TH [label = 'correct', fillcolor = purple ]
    TT [label = 'incorrect', fillcolor = yellow]
    HHH [label = 'correct', fillcolor = purple ]
    HHT [label = 'incorrect', fillcolor = yellow]
    HTH [label = 'correct', fillcolor = purple ]
    HTT [label = 'incorrect', fillcolor = yellow]
    THH [label = 'correct', fillcolor = purple ]
    THT [label = 'incorrect', fillcolor = yellow]
    TTH [label = 'correct', fillcolor = purple ]
    TTT [label = 'incorrect', fillcolor = yellow]
    
    node [shape = plaintext, style = unfilled]
    HHHe [label = '3 correct']
    HHTe [label = '2 correct, 1 incorrect']
    HTHe [label = '2 correct, 1 incorrect']
    HTTe [label = '1 correct, 2 incorrect']
    THHe [label = '2 correct, 1 incorrect']
    THTe [label = '1 correct, 2 incorrect']
    TTHe [label = '1 correct, 2 incorrect']
    TTTe [label = '3 incorrect']
    
    edge [label = '.5']
    S -> {H T}
    H -> {HH HT}
    T -> {TH TT}
    HH -> {HHH HHT}
    HT -> {HTH HTT}
    TH -> {THH THT}
    TT -> {TTH TTT}
    
    edge [label = '']
    
    HHH -> HHHe
    HHT -> HHTe
    HTH -> HTHe
    HTT -> HTTe
    THH -> THHe
    THT -> THTe
    TTH -> TTHe
    TTT -> TTTe
    
}
"
DiagrammeR::grViz(treePlot)

# treePlot <- "
#   graph LR
#     
#   S[start]--.5-->H(Cor)
#   S--.5-->T(Inc)
#   H--.5-->HH(Cor)
#   H--.5-->HT(Inc)
#   T--.5-->TH(Cor)
#   T--.5-->TT(Inc)
#   HH--.5-->HHH(Cor)
#   HH--.5-->HHT(Inc)
#   HT--.5-->HTH(Cor)
#   HT--.5-->HTT(Inc)
#   TH--.5-->THH(Cor)
#   TH--.5-->THT(Inc)
#   TT--.5-->TTH(Cor)
#   TT--.5-->TTT(Inc)
#   HHH---HHHe[3 Cor]
#   HHT---HHTe[2 Cor 1 Inc]
#   HTH---HTHe[2 Cor 1 Inc]
#   HTT---HTTe[1 Cor 2 Inc]
#   THH---THHe[2 Cor 1 Inc]
#   THT---THTe[1 Cor 2 Inc]
#   TTH---TTHe[1 Cor 2 Inc]
#   TTT---TTTe[3 Inc]
# "
#if(knitr::is_html_output()) {
# DiagrammeR(treePlot)
#} else {
  #treePlot <- mermaid(treePlot)
#  export_graph(render_graph(mermaid(treePlot)),file_name = "g.pdf")
  #tmp <- capture.output(rsvg::rsvg_png(charToRaw(DiagrammeRsvg::export_svg(mermaid(treePlot))),file='g.png'))
  #tmp<-capture.output(rsvg_png(charToRaw(export_svg(stnds.qa.d2)),'stnds.qa.png'))
#  cat('![](g.pdf)\n\n')
#}
  
```

For three coin flips, there are 8 unique sequences of outcomes (the tree ends in 8 branches). Because the probability of "heads" and "tails" are identical, and each coin flip is independent of the others, each unique sequence such as "`r set.seed(45); paste0(sample(c("correct","incorrect"),size=3,replace=TRUE),collapse=", ")`" has exactly the same probability, namely $0.5 \times 0.5 \times 0.5 = (0.5)^3 = `r (.5)^3`$. However, if we are not interested in the probability of a unique sequence of outcomes, but rather the total number of correct guesses, the corresponding probabilities are not equal. For example, there is only one sequence in which the number of correct guesses is 3, but there are three sequences in which the number of correct guesses equals 2. Each of these sequences has the sample probability, so we can just add these up
$$P(\text{2 correct and 1 incorrect}) = 3 \times `r (.5)^3` = `r 3*.5^3`$$
Similarly, the probability $P(\text{1 correct and 2 incorrect}) = `r 3*.5^3`$, as there are three sequences in which the number of correct guesses equals 1. 

Note that we have used a distribution over the outcomes of a single variable (a single prediction) to effectively construct three variables ($\texttt{pred}_1$, $\texttt{pred}_2$, and $\texttt{pred}_3$) reflecting Paul's first, second, and third prediction, respectively. We can take the outcomes of each of these three variables and turn this into a new variable reflecting the outcome of all three predictions. This new variable has 8 possible outcomes (unique sequences), each with an equal probability of occurring. Finally, we can then use this sequence variable to construct yet a new variable, the one we are really interested in, reflecting the number of correct predictions. This variable (in the case of three predictions), has four unique outcomes (0, 1, 2, or 3 correct) with _unequal_ probabilities. Perhaps all these variables make your head spin. But a main thing to realize is that from a quite simple model of the Data Generating Process (the outcome of each prediction is an independent coin flip), we can start to say something about how likely Paul's performance of 8 out of 8 correct predictions really was.

To work out the probability distribution of the number of correct predictions out of 8, we could expand the tree of \@ref(fig:heads-tails-tree), but the tree would become very large. There is a simpler way to count the number of sequences with a particular number of correct predictions in them. The number of sequences of $n$ predictions where $k$ predictions are correct can be computed as
$$\frac{n!}{k!(n-k)!} ,$$
where the exclamation mark refers to the so-called factorial function, where a positive integer is multiplied by all positive integers which are smaller than it. So
$$n! = n \times (n-1) \times (n-2) \times (n-3) \times \ldots \times 2 \times 1$$
Furthermore, there is the convention that
$$0! = 1$$

So we can write 
$$P(k \text { correct and }  n-k \text{ incorrect}) = \frac{n!}{k! \times (n-k)!} (0.5)^n$$
For example, we can work out the probability of $k=8$ correct and $n-k=0$ incorrect as
$$\begin{aligned}
P(8 \text { correct and }  0 \text{ incorrect}) &= \frac{8 \times 7 \times ... \times 2 \times 1}{(8 \times 7 \times ... \times 2 \times 1) \times 1} (0.5)^8 \\
&= \frac{1}{1} (0.5)^8 = `r (.5)^8`
\end{aligned}
$$
As another example, the probability of $k=3$ correct and $n-k=5$ incorrect is
$$\begin{aligned}
P(8 \text { correct and }  0 \text{ incorrect}) &= \frac{8 \times 7 \times ... \times 2 \times 1}{(3 \times 2 \times 1) \times (5 \times 4 \times 3 \times 2 \times 1)} (0.5)^8 \\
&= \frac{`r prettyNum(factorial(8))`}{`r factorial(3)` \times `r factorial(5)`} \times (0.5)^8 = `r factorial(8)/(factorial(5)*factorial(3))` (0.5)^8 = `r factorial(8)/(factorial(5)*factorial(3))*(.5)^8`
\end{aligned}
$$
We can follow the same procedure to compute the probability of all other outcomes. The resulting distribution is depicted in Figure \@ref(fig:distribution-8-coin-flips-correct).
```{r distribution-8-coin-flips-correct, fig.cap="probability distribution over the number of correct predictions out of 8, assuming random guessing."}
ggplot(data.frame(x=0:8,y=dbinom(0:8,size=8,prob=.5)),aes(x=as.factor(x),y=y)) + geom_col(width=1,col="black") + xlab("Number correct") + ylab("Probability")
```

## Flipping a biased coin: An alternative model {#sec:02-binomial-model}

Suppose that Paul did not make his decisions by flipping a mental coin. Perhaps Paul had extensive knowledge of all things football, or perhaps he could see into the future. In that case, we might expect Paul to have a higher rate of correct answers than predicted by our coin flipping model. But perhaps Paul, although an oracle, wasn't so benevolent to his caretakers, or didn't want to spoil the match for them. In that case, we might expect the rate of correct answers to be lower than predicted by the coin flipping model. In both cases, we'd expect the probability of a correct answer to be different from .5: $P(\text{correct}) \neq .5$. But whilst not .5, we don't otherwise know the value. We call such an unknown value which determines a probability distribution a __parameter__. We will generally use symbols from the Greek alphabet to denote such parameters. Here, we will use $\theta$ (pronounced as "thee-ta"). Our alternative to the coin flipping model can now be stated as:

$$P(\texttt{pred}_i = \text{correct}) = \theta$$

Like a _variable_, a parameter such as $\theta$ can take different values. It belongs to a model, and its value can not be observed, only inferred. As our parameter reflects a probability, we know that it can never be lower than 0, and never larger than 1. So we know that:

$$0 \leq \theta \leq 1 .$$

<!-- ("zero is less than or equal to $\theta$, which is less than or equal to 1").--> Note that the possible values in this range includes $\theta = 0.5$. Our (unbiased) coin flipping model is actually one of the possible versions (a "special case") of our new, more general model. Some other ways of saying this are that the coin flipping model is contained within, or nested under, the more general new model.  As we will see, __nested models__ play an important role in statistical modelling. In particular, we will often compare how well a special-case or nested model describes the data compared to a more general model. In such model comparisons, we will usually refer to the special-case model as __MODEL S__, and the more general model as __MODEL G__. It is important to realise that these names (like variables) are containers and we can put any models in these, as long as the one we put into MODEL S is a special case of the one we put into MODEL G.

To work out the probability of the number of correct predictions according to this model, we can draw a similar tree as in Figure \@ref(fig:heads-tails-tree), but instead of the probability of each branch being 0.5, we'd use $\theta$ for any branch that leads to correct, and $1-\theta$ for any branch leading to incorrect. That means that the probability of a particular sequence, such as correct, incorrect, correct, is not $(0.5)^{3}$, but rather $\theta \times (1-\theta) \times \theta = \theta^2 \times (1-\theta)$. Apart from this, the logic of constructing the probability distribution of $k$ out of $n$ correct predictions is the same, and can be computed as
\begin{equation}
P(k \text { correct and }  n-k \text{ incorrect}) = \frac{n!}{k! \times (n-k)!} (\theta)^k \times (1-\theta)^{(n-k)}
(\#eq:definition-binomial-distribution)
\end{equation}
This distribution is quite common in statistics, and is called the __Binomial distribution__. Note that if we fill in $\theta = 0.5$, then $(1-\theta) = 0.5$ and the resulting distribution is exactly the one we worked out for the unbiased coin-flipping model earlier. So our new, more general model can indeed exactly produce the distribution of the simpler model, and this is what we mean when we say that the simpler model is nested under the more general model.

To work out the probability that Paul made 8 out of 8 correct predictions according this new model, we need to know the value of the parameter $\theta$. But what should this be? There are infinite possible values that might be reasonable (e.g., $\theta = .9$, or $\theta = .947873$). Luckily, we don't need to pluck one of these values out of thin air: we can use the data we have to estimate the value of this parameter.

## Estimation

<!-- 
From data produced by the Data Generating Process (DGP), we can estimate the value of a parameter. Remember, when we collect data in real life, it is obtained from DGP. Our model, on the other hand, provides a mathematical abstraction of the DGP. The DGP is The Truth. It is how the real world generated the data. Our model, on the other hand, is almost surely an approximation to The Truth. Think about it. Can you summarize Paul's choices by a mathematical formula with a distribution that takes a single parameter ($\theta$)? Probably not when you see Paul swimming in his tank, catching a glimmer of light shining from one of the boxes, than being distracted by the sound of a distant door opening, then going back and sensing a vibration in the water. How would all these serendipitous things not affect Paul's decision at the time? Our model ignores all these factors, and for a reason. Imagine we had to take all sensory input of Paul into account. We would not only have to take into account the actual stimuli present at the time, but also how they affected Paul's neuronal reactions to those. Even if we only take into account the neurons responding directly to external input, the number of possible neuronal excitation patterns quickly become enormous. When you think about it, you'd also want to take into account the activation state of all the neurons at the time just before (and possibly before that), which makes the number of possibilities even more enormous. And what about the chemical consistency of the water at the time? Learning a model of particular decision by a particular person at a particular time seems simply impossible without assuming that some of these things don't really matter to the decision at hand.

So we can view our biased coin-flipping model as a simplification, but potentially useful model of Paul's accuracy if he was (somewhat) psychic. 
-->

Estimation, in some sense, means an "educated guess". It is a guess, because we cannot be completely sure that we have picked the true value. It is educated because we don't just pluck the value from thin air: we use information in the data to guide our guess.

There are different ways in which we can use the information in the data to estimate parameters, such as the method of moments, minimum mean square estimation, and maximum likelihood estimation. Here, we will focus on the latter. 

### Maximum likelihood estimation

The general idea of maximum likelihood estimation can be summarized as follows: Find that value of a parameter $\theta$ such that the probability assigned to the data at hand by the model based on $\theta$ is highest. 

Let's think this through. If we pick any particular value for $\theta$, we can use the Binomial distribution (Equation \@ref(eq:definition-binomial-distribution)) to work out the probability that Paul made 8 out of 8 predictions. For instance, if we pick $\theta = .4$, then 
$$\begin{aligned}
P(\text{8 correct and 0 incorrect}) &= \frac{8!}{8! \times 0!} (0.4)^8 \times (0.6)^0 \\
&= 1 \times (0.4)^8 \times 1 = `r format(dbinom(8,8,.4), scientific=FALSE)`
\end{aligned}$$
If we pick $\theta = .95$, on the other hand, we'd get
$$P(\text{8 correct and 0 incorrect}) = 1 \times (0.95)^8 \times 1 = `r format(dbinom(8,8,.95), scientific=FALSE)`$$
which is obviously higher. If we plot $P(\text{8 correct} | \theta)$, the probability of 8 correct _given_ or _conditional upon_ a particular value of $\theta$, for all possible values of $\theta$, we get the curve shown in Figure \@ref(fig:plot-likelihood-fifa2010). 

```{r plot-likelihood-fifa2010, fig.cap = "Probability of Paul's performance in the 2010 FIFA World Cup for all possible values of $\\theta$ in the biased coin-flipping model", out.width='50%',fig.width=4, fig.height=3}
ggplot(data.frame(x=seq(0,1,length=100), y = dbinom(8,8,seq(0,1,length=100))),aes(x=x,y=y)) + geom_line() + xlab(expression(theta)) + ylab(expression("P(8 correct | " * theta * ")"))
```

Looking at this plot, is it obvious that the probability of Paul's observed performance is highest in the model with $\theta = 1$, thus, our maximum likelihood estimate becomes $\hat{\theta} = 1$. As can be shown mathematically, the maximum likelihood estimate of $\theta$ for our model and any data set with $k$ correct out of $n$ is _always_
\begin{equation}
\hat{\theta} = \frac{k}{n} ,
(\#eq:maximum-likelihood-estimator-binomial)
\end{equation}
i.e. equal to the proportion of correct. Estimating a probability as a proportion seems intuitively reasonable. But how good is it really? 


### Properties of good estimators

An __estimator__ like specified in Equation \@ref(eq:maximum-likelihood-estimator-binomial) is an algorithm which takes data as input and produces a parameter estimate as output. Evaluating the quality of the resulting estimates is generally done under the assumption that our model represents the Data Generating Process accurately, i.e. our model is a true representation of the DGP. If our model has unknown parameters, that means that there are values for those parameters such that our model produces data with exactly the same distribution as the DGP. We can call those parameter values the true parameter values. Now, even if we have access to such a true model, estimates of its parameters from a given data set are unlikely to be identical to the true parameters. The main reason is that any actual data set will not cover all of the values that are possible from the Data Generating Process, and/or not in their expected proportions. But the fact is that in practice we have to rely on such limited data sets; we never have access to all the data that could be produced by the Data Generating Process. So we will have to work with estimates that are different from the true parameters. One thing we can do is quantify how much our estimates are likely to deviate from the true parameters. This, in a nutshell, is what statistical estimation theory is all about.

Let's focus on the estimator of $\theta$ in Equation \@ref(eq:maximum-likelihood-estimator-binomial) a bit more. For any fixed sample size $n$ and a particular true value of $\theta$, the variability in the estimates will be completely determined by the variability in $k$, the number correct. The variability in $k$ directly follows from the Binomial distribution itself. If you look at the distribution depicted in Figure \@ref(fig:distribution-8-coin-flips-correct), which is the distribution of $k$ for $\theta = .5$, you can see there are 9 possible values for $k$, and hence also 9 possible values of $\frac{k}{n}$, the estimator of $\theta$. Assuming the true parameter equals $\theta = .5$, each time we generate a new data set of 8 predictions, it is possible that the estimate of $\theta$ will equal any value $\left\{\frac{0}{8}, \frac{1}{8}, \frac{2}{8}, \ldots, \frac{7}{8}, \frac{8}{8}\right\}$, because for each new data set, the value of $k$ can be any value between 0 and 8. Exactly like the values of $k$, not each of these estimates will be equally likely. For most of the new data sets, the estimate will equal $\hat{\theta} = \frac{4}{8}$, the true value of $\theta$. If the true $\theta = 1$, then there would be no variability in $k$. It is simply impossible to make a wrong prediction, and hence the estimate would always equal the true value. In this case, how variable the estimates are depends on the true value of $\theta$. There is no variability whenever $\theta = 0$ or $\theta = 1$, and the variability increases the closer $\theta$ gets to .5. 

Because in this case, the variability in the estimates depends on the true value of $\theta$, which is unknown and the reason we have to estimate this parameter in the first place, we cannot easily say how far off we might expect our estimate to be from the true parameter. Nevertheless, we can still assess how good our estimator is by considering some important properties of good estimators in general: unbiasedness, consistency, and efficiency.

An __unbiased__ estimator gives, on average, estimates which are equal to the true parameter.  

A good estimator should become more precise as we have more data. The larger the number of observations from the Data Generating Process, the closer the resulting estimates should be to the true parameters. A __consistent__ estimator is one which provides less variable estimates if we give it more data.

An __efficient__ estimator is an estimator which provides the least possible variable estimates, meaning it is the most consistent possible. Variability in estimates is unavoidable, as data sets are always limited samples from the Data Generating Process. It can be proven mathematically that there is level of variance of the estimates that no unbiased estimator can go below, or no biased estimator with a given level of bias can go below. An estimator for which the variance in the estimates is equal to this lower bound is called efficient. Such an estimator is, in some sense, the Holy Grail of statistical estimation, especially an unbiased one.

In general, maximum likelihood estimators are consistent and (asymptotically) efficient. They are not always unbiased though, although the bias (the difference between the mean estimate and the true value) decreases when the number of observations increases. Given these desirable properties, maximum likelihood estimators are often the estimators of choice.

## Comparing models: Null-hypothesis significance testing {#sec:02-likelihood-ratio}

So, at this point, we have three models of Paul's accuracy in predictions. The biased coin-flipping model with an unknown parameter $\theta$. The unbiased coin-flipping, or random guessing, model (which is a special case of the first one, with $\theta = 0.5$). And an _estimated_ version of the biased coin-flipping model, with $\hat{\theta} = 1$ (which is also a special case). Now what?

What we would like to know is whether Paul was randomly guessing, or whether he had some psychic abilities. In other words, whether the unbiased coin-flipping model or the biased coin-flipping model provides a better description of the Data Generating Process. While it is reasonable to define a model as providing a better description of the DGP when the distribution it proposes is a better match to the true DGP distribution, this definition is not usable in practice, as we simply don't know the true DGP distribution. We have to work with the data at hand, and there are many possible ways in which we can use the data to evaluate whether one statistical model provides a better description of the DGP. One principled way is to compare the probability assigned to the observed data by each model. This is usually done by computing what is called the likelihood ratio. Suppose we have two versions of the general biased-coin flipping model, one where we assume that the parameter $\theta$ has a particular value $\underline{\theta}$, and one where we assume $\theta$ takes a different specified value $\underline{\theta}'$. Note that by placing a line underneath a parameter, we denote that it is a particular value of interest. In this case, we are interested in a single variable, namely the number of correct predictions Paul made, which we will denote by $Y$, our general symbol for the dependent variable. We can then write the likelihood ratio as:
$$\frac{P(Y = k | n, \theta = \underline{\theta})}{P(Y = k | n, \theta = \underline{\theta}')}$$
At the top (the _numerator_), is the probability of $k$ out of $n$ correct when the probability of a single prediction correct equals $\underline{\theta}$. The bottom part (the _denominator_), is the same probability if the probability of a single correct prediction equals another value $\underline{\theta}'$. Suppose $\underline{\theta} = .5$ (as in the random guessing model), and $\underline{\theta}' = 1$, as in the _estimated_ biased coin-tossing model. The likelihood ratio is then
$$\frac{P(Y = 8 | n = 8, \theta = 0.5)}{P(Y = 8 | n = 8, \theta = 1)} = \frac{`r format(dbinom(8,8,.5),scientific=FALSE)`}{`r format(dbinom(8,8,1),scientific=FALSE)`} = `r format(dbinom(8,8,.5)/dbinom(8,8,1),scientific=FALSE)`$$
What does this mean? Well, if both models assigned the same probability to the observed data, the likelihood ratio would equal one. If the probability according to the model of the numerator is higher, the value of the likelihood ratio would be _greater_ than 1. And if the probability according to the model of the denominator is higher, the value of the likelihood ratio would be _smaller_ than 1. So, as is clear, the model with the estimated value of $\theta$ assigns a higher probability to the observed data. But that is no wonder! We estimated $\theta$ by maximum likelihood, meaning that no other value of $\theta$ could assign a higher probability to the data! If it did, our estimate would simply _not_ be the maximum likelihood estimate. So, if the model in the denominator is the one estimated by maximum likelihood, we know the likelihood ratio could _never_ be larger than 1. 

So that's a little annoying. What seemed like a reasonable way to compare two models describe a particular data set, now seems pretty useless. We cannot use the general biased coin-flipping model to compute the likelihood ratio, because without choosing a particular value for $\theta$, the model can not be used to compute the probability of the data. And we can not just use the estimated value of $\theta$, because then that model would always win the "likelihood competition". But computing likelihood ratio's is not, in fact, completely useless. We just need to take into account the fact that the estimated model will always win. What then matters is: by how much?

The null-hypothesis significance test provides an answer to "by how much should the estimated model be better to refute an alternative model?". The logic is roughly as follows: Suppose the restricted model (MODEL R, e.g. the one where $\theta = .5$) is the true model. Then we can use this model to simulate lots of data sets. For each data set, we can estimate $\theta$ to give us an estimated version of the general model (i.e. MODEL G, the one with $\theta$ unknown). Then we can use both models to compute a value of the likelihood ratio. For each data set, the likelihood ratio may take a different value. What we end up with is thus a distribution of values of the likelihood ratio, produced by MODEL R. We will know that this likelihood ratio is always below 1, but by inspecting this distribution, we will roughly know what the usual values are, in the case where MODEL R is true. We can then try to find a range of values which are relatively _unusual_, given that MODEL R is true. If the value of the likelihood ratio we found for the real data is within this range, we might reason that this value is "unusual enough" to reject MODEL R, and accept that MODEL G provides a better description.

So let's do this. I used MODEL R (the one with $\theta = .5$) to simulate 100,000 data sets in which Paul made 8 predictions. For each of these, I estimated MODEL G, and then computed the likelihood ratio for that dataset, comparing MODEL R to the estimated MODEL G:
$$\frac{P(Y = k | n = 8, \theta = 0.5)}{P(Y = k | n = 8, \theta = \frac{k}{8})}$$
Note that $k$ is the main thing that varies from simulated dataset to simulated dataset, and that for $\theta$ in the model of the denominator, I have filled in the maximum likelihood estimate $\frac{k}{8}$, which also varies over the simulations. I then determined the proportion (out of all 100,000) simulations of each value of the likelihood ratio. You can see the result in Figure \@ref(fig:simulate-LR-coin-flipping).

```{r simulate-LR-coin-flipping, fig.cap = "Likelihood ratio values for 100,000 simulated data sets from MODEL R with $\\theta = .5$", out.width="60%",fig.width=4.5,fig.height=3.5}
set.seed(123)
sim <- rbinom(100000,size=8,prob=.5)
LR <- dbinom(sim,size=8,prob=.5)/dbinom(sim,size=8,prob=sim/8)
LRvals <- table(LR)/1000
ggplot(data.frame(k=sim,LR=LR),aes(x=LR)) + stat_count(width=.01) + xlab("likelihood ratio") +  ylab("proportion") + scale_y_continuous(labels=function(x) x/100000)
```

A first thing to notice is that there are only 5 unique values of the likelihood ratio in all 100,000 simulations. This is not a strange coincidence. While there are 9 possible values of $k$ for datasets with $n=8$, some of these will provide exactly the same likelihood ratio. For example, the likelihood ratio is identical for $k=3$ and $k=5$:
$$\frac{P(Y = 3 | n = 8, \theta = 0.5)}{P(Y = 3 | n = 8, \theta = \frac{3}{8})} = \frac{P(Y = 5 | n = 8, \theta = 0.5)}{P(Y = 5 | n = 8, \theta = \frac{5}{8})} = `r dbinom(5,8,.5)/dbinom(5,8,5/8)`$$
Similarly, the value of the likelihood ratio is identical for $k=2$ and $k=6$, $k=1$ and $k=7$, and $k=0$ and $k=8$. With that out of the way, let's look at how often each of these 5 values occurred. As MODEL R was the true model (we used it as the Data Generating Process for our simulations), you might expect that a likelihood ratio of 1 would be the most frequent value, as that would imply that MODEL R and (estimated) MODEL G provide an equally good description of a data set. But while this value occurs relatively frequently (`r LRvals[5]`%), the most common value is actually `r dbinom(5,8,.5)/dbinom(5,8,3/8)` (`r LRvals[4]`%). Only the value of `r format(dbinom(0,8,.5)/dbinom(0,8,0/8),scientific=FALSE)` is rather unlikely (`r LRvals[1]`%). In all our simulations, less than 1% had a value of the likelihood ratio as low as the one we found for the real data. Thus, if MODEL R is true, we would not expect an estimated MODEL G as well very often. But is that reasonable grounds to conclude MODEL G provides a better description than the random-guessing MODEL R?

Before we go into this, let me point out that I used the idea of simulating data sets mainly for educational purposes. Although simulation is useful, it is imprecise. Even if we simulate 100,000 data sets, this is still a limited number, and hence any results will differ (even if only slightly) when we simulate 100,000 other data sets. In this case, it is actually quite straightforward to avoid simulation completely. Using the Binomial distribution, it is -- with a little thinking -- quite easy to work out the distribution of the likelihood ratio comparing MODEL R to estimated versions of MODEL G. Recall that for $n=8$, there are 9 possible values of $k$. If MODEL R is true, we know the probability of each of these values from the Binomial distribution. Moreover, each of these values will lead to a different estimate of $\theta$ in MODEL G, which in turn, paired with the corresponding value of $k$, each provide a value of the likelihood ratio. Let's collect all these things in a table:

```{r table-binomial-likelihood-ratio}
tab <- data.frame(
  k = 0:8,
  lik1 = dbinom(0:8,size=8,prob=.5),
  est = 0:8/8,
  lik2 = dbinom(0:8,size=8,prob=0:8/8)
)
tab$LR <- tab$lik1/tab$lik2
colnames(tab) <- c("$k$","$P(k \\mid \\theta = .5)$", "$\\hat{\\theta}$", "$P(k \\mid \\theta = \\hat{\\theta})$", "likelihood ratio")
knitr::kable(tab, escape=FALSE, digits = 6, caption = "Possible outcomes and their probabilities according to MODEL R, and estimates of $\\theta$, probabilities of outcomes according to estimated MODEL G, and likelihood ratio values.", booktabs = TRUE, linesep="")
```

The column $P(k|\theta = .5)$ contains the probability of each possible value of $k$ according to MODEL R. As this is for the moment the true model, it is both the actual probability of $k$, and the "model assigned" value which goes into the likelihood ratio. In the table, we see that $k=0$ and $k=8$ both give the same value of the likelihood ratio. We know the probability of each value of $k$ is $P(k|\theta = .5)$ (because MODEL R generated the data), and therefore we can work out that the probability of obtaining this value of the likelihood is simply the sum of the probabilities of $k=0$ and $k=8$:
$$\begin{aligned}
P(\text{likelihood ratio} &= P(k=0) + P(k=8) \\
&= `r format(tab[1,5], digits=3)`) = `r format(tab[1,2], digits=3)` + `r format(tab[9,2], digits=3)` = `r format(tab[1,2] + tab[9,2], digits=3)`
\end{aligned}$$
Similarly, for the other possible values of the likelihood, we can work out the probabilities as:
$$\begin{aligned}
P(\text{likelihood ratio} = `r format(tab[2,5], digits=4)`) &= `r format(tab[2,2], digits=4)` + `r format(tab[8,2], digits=4)` = `r format(tab[2,2] + tab[8,2], digits=4)` \\
P(\text{likelihood ratio} = `r format(tab[3,5], digits=5)`) &= `r format(tab[3,2], digits=5)` + `r format(tab[7,2], digits=5)` = `r format(tab[3,2] + tab[7,2], digits=5)` \\
P(\text{likelihood ratio} = `r format(tab[4,5], digits=5)`) &= `r format(tab[4,2], digits=5)` + `r format(tab[6,2], digits=5)` = `r format(tab[4,2] + tab[6,2], digits=5)` \\
P(\text{likelihood ratio} = `r format(tab[5,5], digits=5)`) &= `r format(tab[5,2], digits=5)` 
\end{aligned}$$
This is similar to the example of throwing dice before. There, we had elementary events (each number that can be thrown), but we also had other events, such as "greater than 3", which we could compute the probability of by using the probability of the elementary events. Here, the number of correct guesses $k$ are the elementary events, and values of the likelihood ratio can be considered compound events. You can view a plot of the resulting distribution of likelihood ratio values in Figure \@ref(fig:true-likelihood-ratio-distribution-coin-flip).
```{r true-likelihood-ratio-distribution-coin-flip, fig.width=4.5,fig.height=3.5,out.width="60%",fig.cap="The true distibution of likelihood ratio's comparing MODEL R with estimated MODEL G, when the true model is MODEL R."}
ggplot(data.frame(
  lr = tab[1:5,5],
  p = tab[1:5,2] + c(tab[9:6,2],0)  
),aes(x=lr,y=p)) + geom_col(width=.01) + xlab("likelihood ratio") + ylab("P(likelihood ratio | MODEL R)")
```
When you compare this plot to Figure \@ref(fig:simulate-LR-coin-flipping), you see there are indeed very alike. So we don't need to simulate data to determine what values of the likelihood ratio we might see -- and how often -- if MODEL R is true. MODEL R allows us to easily compute the probability of each possible number of correct guesses, $k$. This number, in turn, determines everything else of interest: the estimates of $\theta$ for MODEL G, the probability of the data according to (the estimated) MODEL G, and the likelihood ratio between MODEL R and (the estimated) MODEL G. When possible, it is better to work out the distribution like we did here, instead of relying on simulation. Simulation is inherently noisy, even with 100,000 samples. 

<!-- As I indicated before, each value of $k$ leads to a different estimate of $\theta$ in MODEL G. So $P(k|\theta = .5)$ is identical to the probability of that estimate, i.e. 
$$P(k|\theta = .5) = P\left(\hat{\theta} = \frac{k}{8}|\theta = .5\right).$$
In other words, because MODEL R is the true model, it is the one that generates the data, and with that the estimates of $\theta$ in MODEL G. Because we need an estimate of $\theta$ to determine the probability of the data according to (the estimated) MODEL G, MODEL R also determines the probability of the data assigned by (the estimated) MODEL G. And with that, MODEL R also determines the likelihood ratio between MODEL R and (the estimated) MODEL G. I have said this now about three times, but that is because it is really important to understand this. By assuming MODEL R is true, we assume we know the Data Generating Process, and this will generate not only data, but also distributions of many things, including estimates, and likelihood ratio's.   

Right, so if we go back to the table above,-->

That's all well and good, you might object, but you have done all this by assuming that MODEL R is true. But we don't know whether MODEL R is true, we don't know that Paul was randomly guessing. In fact, the question whether he was actually psychic (and not randomly guessing) is what brought us to this point in the first place! That is indeed very true. The issue is that there are many ways in which MODEL R can be false and MODEL G true. In fact, there are an infinite number of values for $\theta$ which aren't equal to 0.5. Each of these implies a different distribution of the likelihood ratio where we compare MODEL R (with $\theta = .5$) to the _estimated_ MODEL G (with $\hat{\theta} = \frac{k}{n}$). Some examples are given in Figure \@ref(fig:true-likelihood-ratio-distribution-biased-coin-flip). If we assume that MODEL R is false, then any of these distributions would be the true distribution of the likelihood ratio. But without knowing the exact value of $\theta$, we simply don't know which one. <!--Such indeterminism makes it practically impossible to derive -->

```{r true-likelihood-ratio-distribution-biased-coin-flip, fig.cap="The true distibution of likelihood ratio's comparing MODEL R with estimated MODEL G, when the true model is MODEL G with different values of $\\theta$."}
ggplot(data.frame(
  theta = factor(c(rep(.6,5),rep(.7,5),rep(.8,5),rep(.9,5)),labels=c(expression(theta == .6),expression(theta == .7),expression(theta == .8),expression(theta == .9))),
  lr = c(tab[1:5,5]),
  p = c(c(dbinom(0:3,size=8,prob=.6) + dbinom(8:5,size=8,prob=.6),dbinom(4,size=8,prob=.6)),
        c(dbinom(0:3,size=8,prob=.7) + dbinom(8:5,size=8,prob=.7),dbinom(4,size=8,prob=.7)),
        c(dbinom(0:3,size=8,prob=.8) + dbinom(8:5,size=8,prob=.8),dbinom(4,size=8,prob=.8)),
        c(dbinom(0:3,size=8,prob=.9) + dbinom(8:5,size=8,prob=.9), dbinom(4,size=8,prob=.9)))
),aes(x=lr,y=p)) + geom_col(width=.01) + xlab("likelihood ratio") + ylab(expression("P(likelihood ratio | " * theta * ")")) + facet_wrap(~theta) + facet_wrap(~theta,labeller = label_parsed)

```

### Decisions and types of error

Let's relocate ourselves. We started by defining a model in which Paul was randomly guessing (MODEL R). We then defined another model in which Paul could be (somewhat) psychic, but as we don't know _how_ psychic and  whether Paul would want to make correct predictions or incorrect predictions. In this model, Paul's probability of making a correct prediction could be anything. Because it is so general, MODEL G can effectively predict anything. We can use the data to estimate MODEL G. But doing that, it makes MODEL G a bit like an annoying friend, who always says "I knew that was going to happen" _after_ it happened, but never before. Because MODEL G is so fickle, our procedure focuses almost exclusively on MODEL R, the version of MODEL G (remember, MODEL R is a special case of MODEL G) which is much more strident, and does tell us how likely things are to happen _before_ they happen.

In the null hypothesis significance test, the hypothesis that MODEL R is true is called the __null hypothesis__, or $H_0$, in the conventional notation:
$$H_0: \text{MODEL R is true}$$
This hypothesis itself may be either true or false. If $H_0$ is true, then MODEL R is true. If $H_0$ is false, then the statement "MODEL R is true" is false, so MODEL R is false. These are the two possibilities that we are faced with, but we don't know which of these is actually the case.^[Perhaps it is a little confusing to first talk about models being true, and then about hypotheses stating that these models are true. As it is conventional to talk about the null hypothesis ($H_0$) in the context of significance testing, it is important to know what this concept implies though, hence that's why I'm using it here also.] Based on data, we can decide to accept $H_0$ as a true statement, or reject $H_0$. But these decisions can be correct, or incorrect. Combining the actual state of the world with our decisions, we have the following four possibilities: 

```{r table-decisions-errors}
tab <- data.frame(x = c("correct","error (Type 2)"),
                  y= c("error (Type 1)", "correct"))
colnames(tab) <- c("accept $H_0$","reject $H_0$")
rownames(tab) <- c("$H_0$ is true","$H_0$ is false")
knitr::kable(tab, escape = FALSE, booktabs = TRUE, linesep="")
```

If $H_0$ is actually true, and we accept $H_0$, we can rejoice, as we made a correct decision. Similarly, if $H_0$ is actually not true, and we reject $H_0$, we can also rejoice, as this is another correct decision. If, on the other hand, $H_0$ is true, but we reject $H_0$, we made a wrong decision. This is called a Type 1 error. A type 2 error refers to making the other wrong decision, namely if we accept $H_0$, but $H_0$ is in fact false.

__Null hypothesis significance testing__ is a procedure to make a decision based on data such that we can limit the chances of making one of these errors, namely the Type 1 error. 

### Significance and power

In null hypothesis significance testing, we focus on the probability of making a Type 1 error, which is usually denoted as $\alpha$:
\begin{equation}
\alpha = P(\text{reject } H_0| H_0 \text{ is true})
(\#eq:significance-definition)
\end{equation}
It is also referred to as the __significance level__. If $H_0$ is true, then we can define a decision procedure to either accept or reject $H_0$, such that we know the probability of a Type 1 error, and moreover, we can set this probability to a desired value.

Let's consider Figure \@ref(fig:true-likelihood-ratio-distribution-coin-flip) again. This is the theoretical distribution of the likelihood ratio values when MODEL R is true, i.e., when $H_0$ is true. Large values of the likelihood ratio indicate that MODEL G does not provide a much better fit to the data than MODEL R, so it makes sense that such large values should not persuade us to reject MODEL R. Values close to 0, on the other hand, indicate that MODEL G does provide a substantially better fit to the data than MODEL R. Hence, it makes sense to decide to reject $H_0$ if the likelihood ratio is "close enough" to 0. Clearly, such low values can occur even if $H_0$ is true (as Figure \@ref(fig:true-likelihood-ratio-distribution-coin-flip) attests). But they are unlikely to do so. If $H_0$ is false, such low values become more likely (see Figure \@ref(fig:true-likelihood-ratio-distribution-biased-coin-flip)).

The idea is now to pick a cutoff value for the likelihood ratio such that if we find a value equal to or smaller than it, we reject the null hypothesis. We choose this cutoff value, called the __critical value__,  by considering the probability of the values, _assuming MODEL R is true_ (as this is the only model that can provide us with a proper distribution). More precisely, we choose the critical value such that the probability of making a Type 1 error does not exceed a chosen value, called the significance level, which is usually set to $\alpha = .05$. That means that, if $H_0$ is true, we are allowing at most 5% of our tests to result in a Type 1 error. So how do we pick this critical value? Let's again consider the theoretical distribution of the likelihood ratio we worked out and depicted in Figure \@ref(fig:true-likelihood-ratio-distribution-coin-flip). As it is difficult to see the precise probabilities in this figure, let's first create another table like the one we did before, but now focussing specifically on the values of the likelihood ratio. Table \@ref(tab:cumulative-distribution-likelihood-ratio-table) doesn't really contain any more information than Table \@ref(tab:table-binomial-likelihood-ratio). We have simply added up some of the numbers in that table. A new concept though is that of a __cumulative probability__, by which we mean _the probability of a given value or any lower value_. Here, we focus on the cumulative probability for the likelihood ratio, $P(\text{likelihood ratio} \leq \text{value})$. Such cumulative probabilities are interesting here, because we want to find a cutoff value, and then limit the probability of getting any values lower than it to the significance level $\alpha$. 

```{r cumulative-distribution-likelihood-ratio-table}
tab <- data.frame(
  k = 0:8,
  lik1 = dbinom(0:8,size=8,prob=.5),
  est = 0:8/8,
  lik2 = dbinom(0:8,size=8,prob=0:8/8)
)
tab$LR <- tab$lik1/tab$lik2
colnames(tab) <- c("$k$","$P(k \\mid \\theta = .5)$", "$\\hat{\\theta}$", "$P(k \\mid \\theta = \\hat{\\theta})$", "likelihood ratio")

tab2 <- data.frame(
  lr = tab[1:5,5],
  p = tab[1:5,2] + c(tab[9:6,2],0),
  ps = cumsum(tab[1:5,2] + c(tab[9:6,2],0))
)
knitr::kable(
  tab2,col.names = c("value","$P(\\text{likelihood ratio} = \\text{value})$","$P(\\text{likelihood ratio} \\leq \\text{value})$"), caption="Possible values of the likelihood ratio, their probability and cumulative probability, assuming MODEL R is true.", escape=FALSE, digits=6, booktabs=TRUE, linesep="")
```

Let's look at the cumulative probabilities (simply computed by adding, on each row of the table, the value of $P(\text{likelihood ratio} = \text{value})$ on that row, and all rows above it). How should we place our critical value to ensure that $\alpha \leq .05$? If we picked the value `r format(tab2[2,1],digits=4)`, and rejected the null hypothesis if we obtained a value equal to or lower than that, then the probability of a Type I error would be `r format(tab2[2,3],digits=4)`. Thus, if we don't want that probability to be higher than .05, then we can't pick that value. If, on the other hand, we pick the value `r format(tab2[1,1],digits=3)`, then the probability of a Type I error would be `r format(tab2[1,3],digits=4)`, which is lower than $\alpha = .05$. As we should not go higher than our desired value of $\alpha$, that would be the appropriate critical value.^[Of course, another way to ensure that we never go above $\alpha$ is to set the critical value to 0. But that would mean we _never_ reject $H_0$, which makes the test pointless.] Indeed, we could place the critical value somewhere between `r format(tab2[1,1],digits=4)` and `r format(tab2[2,1],digits=4)`, as I have done in Figure \@ref(fig:critical-value-likelihood-ratio-coin-flip) to make the plot easier to read. As, with $n=8$, there are no possible values of the likelihood ratio between `r format(tab2[1,1],digits=4)` and `r format(tab2[2,1],digits=4)` anyway, the precise placement of the critical value does not really matter. 

```{r critical-value-likelihood-ratio-coin-flip, fig.cap="Critical values (dotted lines) and the true distibution of likelihood ratios comparing MODEL R with estimated MODEL G, when the true model is MODEL R (left) and the true model is one particular version of MODEL G (right).", fig.height=3}
ggplot(data.frame(
  theta = factor(c(rep(.5,5),rep(.9,5)), labels=c(expression(theta == .5),expression(theta == .9))),
  lr = c(tab[1:5,5],tab[1:5,5]),
  p = c(tab[1:5,2] + c(tab[9:6,2],0),c(dbinom(0:3,size=8,prob=.9) + dbinom(8:5,size=8,prob=.9), dbinom(4,size=8,prob=.9)))
),aes(x=lr,y=p)) + geom_col(width=.01) + xlab("likelihood ratio") + ylab("P(likelihood ratio | MODEL R)") + facet_wrap(~theta, labeller=label_parsed) + geom_vline(xintercept=.04,lty=2)
```

The plot on the left of Figure \@ref(fig:critical-value-likelihood-ratio-coin-flip) shows the situation considered by the null hypothesis significance test, namely the situation in which MODEL R is true. If MODEL R is false, then $\theta$ can, in principle, take any value $\theta \neq 0.5$. If $\theta$ does not equal 0.5, the probability $P(\text{likelihood ratio} \leq \text{critical value})$ will be _higher_ than when $\theta = 0.5$. As an example, the right side of Figure \@ref(fig:critical-value-likelihood-ratio-coin-flip) shows that 
$$P(\text{likelihood ratio} \leq \text{critical value} | \theta = 0.9) = `r dbinom(0,8,.9) + dbinom(8,8,.9)`$$
This probability of obtaining a value equal to or lower than the critical value, _given that $H_0$ is false_, is also called the __power__ of a test:

\begin{equation}
\text{power} = P(\text{test value} \leq \text{critical value} | H_0 \text{ is false} )
(\#eq:power-definition)
\end{equation}
Note that I'm using "test value" rather than likelihood ratio, because there will be times when we use a different test statistic than the likelihood ratio, and I want the definition of power here to cover those situations too. Furthermore, the complementary probability, $P(\text{value} > \text{critical value} | H_0 \text{ is false})$, is the probability of a Type 2 error. Now, while power is a very important concept in hypothesis testing, it is also elusive. If you look at Figure \@ref(fig:true-likelihood-ratio-distribution-biased-coin-flip), you can see that the power is different for different values of $\theta$. 

Power is an important concept because usually, it is less interesting if the null hypothesis is true than if it is false. For instance, it is less interesting if Paul was merely guessing than if he was psychic. So usually, we are more interested in rejecting the null hypothesis than in accepting it. Disappointingly, in this case, the power is pretty low even when $\theta = .9$, and it is even lower for values of $\theta$ closer to 0.5. We can increase the power by increasing $\alpha$. You can visualize this in Figure \@ref(fig:critical-value-likelihood-ratio-coin-flip) as moving the critical value to the right, for instance to the right of the second possible value of the likelihood ratio. If $\theta = .9$, that would increase the power to `r dbinom(0,8,.9) + dbinom(8,8,.9) + dbinom(1,8,.9) + dbinom(7,8,.9)`. But conversely, the probability of a Type 1 error would now be `r dbinom(0,8,.5) + dbinom(8,8,.5) + dbinom(1,8,.5) + dbinom(7,8,.5)`. Generally speaking, we don't want the probability of a Type 1 error so high. A better way of increasing the power of a test is to collect more data; e.g., by letting Paul make more predictions than 8.

```{r}
pwr_theta <- optimise(function(x) {(dbinom(0,8,x) + dbinom(8,8,x) - .8)^2}, c(0.5,1))$minimum
```

The tricky thing about the power of a test is that it depends on the unknown parameter $\theta$, and can never really be known. We can calculate the power of a test for various values of $\theta$, but in a sense, the choice of such values is arbitrary if we don't know the true value of $\theta$. Of course, using our data, we could estimate $\theta$, and then calculate what some have called the "observed power" of a test. In this example, $\hat{\theta} = \frac{8}{8} = 1$, and the "observed power" is 1. But, as we know, the estimate of $\theta$ is not identical to the true value of $\theta$, and hence the estimated power is unlikely to be equal to the true power, unless we have a lot of data, but then the power would be reasonably large in the first place. As such, "observed power" is rather useless. Perhaps the most useful way (at least in my opinion) to consider power is to determine the value of $\theta$ that would provide a reasonable level of power. Conventionally, a power of $P(\text{test value} \leq \text{critical value} | H_0 \text{ is false} ) = 0.8$ is considered reasonable. By plotting the power for different values of $\theta$, as done in Figure \@ref(fig:power-curve-biased-coin-flip), we can then determine the values of $\theta$ which would obtain this level of power. Such a plot is also called a power-curve. As you can see, values of $\theta$ close to 0 and 1 provide a high power. To reach a power of at least 0.8, it would need to be the case that $\theta \leq `r 1-pwr_theta`$ or $\theta \geq `r pwr_theta`$. Thus, Paul would have to be rather good at giving correct predictions (or very good at giving incorrect predictions), to make it reasonably likely that we would reject the null hypothesis. If you consider $\theta \leq `r 1-pwr_theta`$ or $\theta \geq `r pwr_theta`$ too extreme to be likely, then it would be a good idea to get more data in order to make the required value of $\theta$ more reasonable.

```{r power-curve-biased-coin-flip, fig.width=4, fig.height=3,out.width='60%', fig.cap="A power-curve, plotting the power of the test for different values of $\\theta$. The broken line indicates a power of 0.8, and the dotted lines indicate the values of $\\theta$ which obtain this power."}
ggplot(
  data.frame(
    theta = seq(0,1,length=100),
    power = dbinom(0,8,seq(0,1,length=100)) + dbinom(8,8,seq(0,1,length=100))
  ),
  aes(x=theta,y=power)
) + geom_line() + geom_hline(yintercept=.8,lty=2) + geom_line(data = data.frame(theta = c(1-pwr_theta,1-pwr_theta),power=c(0,.8)),aes(x=theta, y = power),lty=3) + geom_line(data = data.frame(theta = c(pwr_theta,pwr_theta),power=c(0,.8)),aes(x=theta, y = power),lty=3) + xlab(expression(theta))
```

<!-- ## Confidence intervals -->

### Testing whether Paul was guessing

Finally then, let's actually test the null hypothesis that Paul was guessing. When just considering his performance in the 2010 FIFA World cup, Paul made $n=8$ predictions and $k=8$ of these were correct. This gives a likelihood ratio of MODEL R ($\theta = .5$) over estimated MODEL G ($\hat{\theta} = 1$) of `r format(tab[1,5], digits=3)`. Using a significance level of $\alpha = .05$, the critical value is set to `r format(tab[1,5], digits=3)`, and as the likelihood ratio is equal to or smaller than this value, we reject $H_0: \theta = .5$, and conclude that it is rather unlikely that Paul was merely guessing. In fact, $P(\text{likelihood ratio} \leq `r format(tab[1,5], digits=3)` | \theta = .5) = `r format(dbinom(0,8,.5) + dbinom(8,8,.5), digits=3)`$. This latter probability, the probability of a test value equal to more extreme than the test value calculated for the data, is generally also called the __p-value__. If the p-value is smaller than the required significance level, the test result is called significant, and we reject $H_0$. What "more extreme" means can vary, depending on how the test is performed.

Now, in the preceding discussion, we have completely ignored Paul's performance in the UEFA Euro 2008 cup. If we can assume that Paul's psychic abilities did not change between the UEFA Euro 2008 and the 2010 FIFA World cup, it makes sense to pool both data sets to get a more reliable and more powerful test. As the logic is entirely the same as before, we will go through this quickly. Pooling both datasets, we now have $n=14$ predictions, of which $k= 12$ were correct. This provides a likelihood ratio of
$$\text{likelihood ratio} = \frac{P(k=12|n=14, \theta = .5)}{P(k=12|n=14, \hat{\theta} = \frac{12}{14})} = `r dbinom(12,14,.5)/dbinom(12,14,12/14)`$$
```{r theoretical-likelihood-ratio-distribution-pooled-data, eval=FALSE}
tab3 <- data.frame(
  k = 0:14,
  lik1 = dbinom(0:14,size=14,prob=.5),
  est = 0:14/14,
  lik2 = dbinom(0:14,size=14,prob=0:14/14)
)
tab3$LR <- tab3$lik1/tab3$lik2
colnames(tab3) <- c("$k$","$P(k \\mid \\theta = .5)$", "$\\hat{\\theta}$", "$P(k \\mid \\theta = \\hat{\\theta})$", "likelihood ratio")
ggplot(data.frame(
  theta = factor(c(rep(.5,8),rep(.9,8)), labels=c(expression(theta == .5),expression(theta == .9))),
  lr = c(tab3[1:8,5],tab3[1:8,5]),
  p = c(tab3[1:8,2] + c(tab3[15:9,2],0),c(dbinom(0:6,size=14,prob=.9) + dbinom(14:8,size=14,prob=.9), dbinom(7,size=14,prob=.9)))
),aes(x=lr,y=p)) + geom_col(width=.005) + xlab("likelihood ratio") + ylab("P(likelihood ratio | MODEL R)") + facet_wrap(~theta, labeller=label_parsed) + geom_vline(xintercept=sum(tab3[c(3,4),5])/2,lty=2)

```

For $\alpha = .05$, the critical value is `r dbinom(2,14,.5)/dbinom(2,14,2/14)`, so again, we can reject 
$H_0: \theta = .5$. In fact, the probability of obtaining a likelihood ratio of `r dbinom(2,14,.5)/dbinom(2,14,2/14)` or smaller if $H_0$ is true, the p-value, is `r pvalue(2*pbinom(2,14,.5))`. 

## Hypothesis testing directly with the Binomial distribution

In the preceding, we have focused on the likelihood ratio as the test statistic for our hypothesis tests. The likelihood ratio is a very general measure to compare how well two models describe the data. And it clearly shows how hypothesis testing can be seen as a form of model comparison. That said, the likelihood ratio can also be a bit cumbersome to work with, and deriving the distribution of the likelihood ratio under the null hypothesis is not always straightforward. As we will see later, we can often use a different measure in place of the likelihood ratio, which will provide testing procedures that are equivalent to it.

As we saw when we worked out the theoretical distribution of the likelihood ratio (Table \@ref(tab:table-binomial-likelihood-ratio)), there is a direct relation between the number of correct guesses and the value of the likelihood ratio. In a way, you can think of the likelihood ratio as a transformation of the number of correct guesses into a new measure (the likelihood ratio). For example, $k=8$ out of $n=8$ correct guesses becomes a likelihood ratio of `r format(tab[9,5],digits=3)`, and $7$ out of $8$ correct guesses becomes a likelihood ratio of `r format(tab[8,5],digits=4)`. Similarly, $k=0$ out of $n=8$ correct guesses becomes a likelihood ratio of `r format(tab[1,5],digits=3)`, and $1$ out of $8$ correct guesses becomes a likelihood ratio of `r format(tab[2,5],digits=4)`. So different values of $k$ may provide the same likelihood ratio value (in mathematical terms, this is also called a many-to-one mapping). This is similar to outcomes 1, 3, and 5 of a die roll all mapping onto "odd number". 

Because both the value and the probability of likelihood ratio values depend solely on the number of correct guesses, the number of correct guesses contains all the information about the likelihood ratio. Because of this, we can define our hypothesis tests directly in terms of correct guesses, without losing any information. For instance, when we considered the pooled data from the UEFA Euro 2008 cup and the 2010 FIFA World cup, we determined that any likelihood ratio equal to or smaller than `r format(dbinom(2,14,.5)/dbinom(2,2,12/14), digits=4)` would lead to a rejection of the null hypothesis. There are three possible values of the likelihood ratio in this critical region (values between 0 and `r format(dbinom(2,14,.5)/dbinom(2,2,12/14), digits=4)`), and these are associated with $k=0,1,2,12,13,\text{ or } 14$. Thus, we can equivalently define the critical region in terms of the number of correct guesses: $k \leq 2$ and $k \geq 12$. Figure \@ref(fig:binomial-distribution-critical-values-pooled) depicts the distribution of $k$ under $H_0$ and the critical region for $\alpha=.05$.  

```{r binomial-distribution-critical-values-pooled, fig.cap = "Distribution of the number of correct predictions ($k$) out of $n=14$ under MODEL R ($\\theta = .5$), and the critical regions for $\\alpha = .05$."}
ggplot(data.frame(x=0:14,y=dbinom(0:14,size=14,prob=.5),critical = c(rep("yes",3),rep("no",9),rep("yes",3))),aes(x=as.factor(x),y=y,fill=critical)) + geom_col(width=1,col="black") + xlab("Number correct (k)") + ylab("Probability") + theme(legend.position = "none") + geom_vline(xintercept=3.5, lty=2) + geom_vline(xintercept=12.5, lty=2) + annotate("text", x=2,xmin=1,xmax=3,y=.20,hjust = 0.5, vjust=1,label="lower \n critical region") + annotate("text", x=14,xmin=13,xmax=15,y=.20,hjust = .5, vjust=1,label="upper \n critical region")
```

Note that both this new way of testing $H_0$, as well as the likelihood ratio test we performed earlier, base their decisions on Paul's number of correct guesses (i.e. $Y = 12$). This new way checks directly whether the value of $Y$ is in the critical region, while the likelihood ratio test first transforms $Y$ into a likelihood ratio, and then checks whether this value is in the critical region. Both ways of testing provide exactly the same decisions. But testing $Y$ directly is somewhat easier, as we don't have to compute anything else. In addition, using $Y$ directly also makes it more straightforward to perform a one-sided hypothesis test.

Up to now, we have compared a model with $\theta = .5$ to a model that merely assumed $0 \leq \theta \leq 1$. The latter model can not only account for Paul making very accurate predictions, but also for Paul making very inaccurate predictions. But perhaps it is more reasonable to expect that if Paul was indeed psychic, that he would make correct predictions, rather than deceiving his keeper by making inaccurate predictions. In other words, if Paul was psychic, we would expect the probability of him making a correct prediction to be larger than if he were just guessing, so $\theta > .5$. We can still test this model against the null hypothesis of random guessing, but now small numbers of correct guesses should not move us to reject the null hypothesis, as such small numbers would be even less likely if a model with $\theta > .5$ were true. If we'd use the right critical region of Figure \@ref(fig:binomial-distribution-critical-values-pooled), the actual significance level would be only $\alpha = `r 1-pbinom(11,14,.5)`$ (the probability $P(k \geq 12 | \theta = .5)$). If we'd like a significance level of $\alpha = .05$, we can actually move the critical value to 11, as $P(k \geq 11 | \theta = .5) = `r 1-pbinom(10,14,.5)`$, which is still below the desired $\alpha = .05$, but it is as close as we can get here. The resulting right-sided test of the hypothesis $H_0: \theta = .5$ versus the alternative that $\theta > .5$ is depicted in Figure \@ref(fig:binomial-distribution-critical-values-pooled-one-sided). The result of this test, with $Y = 12$, is again to reject $H_0$.

```{r binomial-distribution-critical-values-pooled-one-sided, fig.cap="Distribution of the number of correct predictions ($k$) out of $n=14$ under MODEL R ($\\theta = .5$), and the critical region for a one-sided test and $\\alpha = .05$."}
ggplot(data.frame(x=0:14,y=dbinom(0:14,size=14,prob=.5),critical = c(rep("no",11),rep("yes",4))),aes(x=as.factor(x),y=y,fill=critical)) + geom_col(width=1,col="black") + xlab("Number correct (k)") + ylab("Probability") + theme(legend.position = "none") + geom_vline(xintercept=11.5, lty=2) + annotate("text", x=13.5,y=.20,hjust = .5, vjust=0,label="critical region")
```

It is possible to formulate this one-sided test in terms of a likelihood ratio test as well, but that wouldn't necessarily make things clearer, and I think this chapter is long enough already. 

## Summary

Statistical models define a probability distribution for the Data Generating Process. To test a particular hypothesis about an aspect of the DGP, such as that the probability of a correct prediction by Paul equals .5, we can define two alternative models, one which assumes the DGP has that characteristic, and one which doesn't. When it is difficult to decide upon a reasonable alternative value for the hypothesised characteristic of the DGP, the model can leave this value unspecified as a parameter. Generally, apart from the value of the parameter(s), the models are identical in other respects. In that case, the model where we assume we know the value of the parameter(s) is a special case of the more general model where we leave the parameter(s) unspecified. The specified model (e.g., one in which the probability of a correct prediction is $\theta = .5$) is then said to be nested within the more general model (e.g., the one in which $\theta$ can take any value between 0 and 1). We refer to the more general model as MODEL G (the "general" model), and the specified model as MODEL R (the "restricted" model, in which we restrict the value of a parameter to have a specified value). 

Unknown parameters of statistical models can be estimated from the data. Maximum likelihood estimation is a widely used and useful method to estimate parameters of statistical models. It estimates parameters by finding the value which maximises the probability of the observed data according to the model. Maximum likelihood estimators are consistent and efficient, although they can be biased. 

When comparing statistical models, we want to know how good each is in describing the data. A useful and general measure for this is the likelihood ratio. However, when comparing an estimated model to a model where we restrict the value of the parameter(s) to take a particular value, we know the likelihood ratio will never show the estimated model is worse than the restricted model, as we estimated the model to maximise the likelihood. The idea of null hypothesis significance testing is to consider how the parameter estimates and the likelihood ratio values are distributed if the null hypothesis (the restricted model) is true. The procedure relies on a critical value such that the null hypothesis is rejected if the likelihood ratio is equal to or more extreme than the critical value. the critical value is chosen such that the probability of rejecting the null hypothesis when it is true (a Type 1 error) does not exceed a particular value, called the significance level ($\alpha$). The power of a test, the probability of rejecting the null hypothesis when it is false, is also an important concept, but in practice, it is difficult to determine, as it requires knowing the true value of the parameter(s) of the general model.  

## Epilogue

Was Paul really psychic? Quite likely not. Paul's accuracy is quite unlikely if he were randomly guessing, and we have rejected this null hypothesis. That doesn't mean that the null hypothesis is definitely not true; the null hypothesis significance test merely limits the probability of a wrong rejection to a maximum of e.g. $\alpha = .05$. In statistics, we can never really be sure of anything. But we shouldn't use this argument to allow us to conclude whatever we want, because then it becomes rather pointless to test a hypothesis in the first place. So we should really conclude that Paul's performance was higher than expected if he were guessing. But there are various ways to explain his performance which don't involve him being psychic. For instance, Germany was a team in a rather disproportionate number of the matches for which Paul provided predictions, and in many of these, he opened the box with the German flag on it first. Perhaps Paul preferred the German flag over most other flags? Germany was also a strong team in both the UEFA Euro 2008 and the 2010 FIFA World cup. These factors could easily combine to allow Paul a large number of correct predictions. You can probably think of other confounds to explain his performance as well.

Paul was not the only animal making predictions about sports matches. Mani the parakeet, Nelly the elephant, Flopsy the kangaroo, Madame Shiva the guinea pig, Big Head the sea turtle, Shaheen the camel, and Achilles the cat, are some other ones with more or less success. And there are probably many more who no one has heard about. If you think of all these animals making predictions as repetitions of a larger experiment (e.g. testing whether animals in general are psychic), it does not seem surprising that in some of these, an animal will make a large number of correct predictions. 

But then again, Paul was not just any animal... He was an octopus...with nine brains!


<!--
## Exercises

1. Maybe Paul has a preference to go to the right hand container. 
    a. Suppose Paul always opens the right container first. So $P(\text{Paul chooses right}) = 1.0$. Furthermore, suppose that his keeper has ability to predict the winner of a match, in that he is correct on 60% of his predictions, i.e. $P(\text{keeper is correct}) = .6$. Finally, let's suppose that the keeper wants to help Paul, so he always puts the flag of the country he predicts to win on the right container. What is the probability that Paul's "predictions" are correct? And what if Paul's spatial preference was less marked at $P(\text{Paul chooses right}) = .6$?
2. 
  
-->

<!--chapter:end:02-modeling.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# A model with a mean (one sample t-test) {#ch-simple-GLM}

In this chapter, we will start our journey into the General Linear Model with the most basic example within the GLM: a model concerning a single Normal distribution, where we are interested in statements about the mean of that distribution. The resulting test is also known as the one-sample t-test. We will look at the Normal distribution in detail, and cover estimation of its parameters, before focusing on testing hypotheses about the mean. We will also introduce confidence intervals as an alternative, but mostly equivalent means to make inferences about parameters. We end the chapter by looking at the assumptions of the model, and what happens to the testing procedure if those assumptions are false. <!-- with a look ahead towards the other chapters, by providing a brief overview of the General Linear Model. -->

## Numeric judgement and anchoring

In 1906, Sir Francis Galton attended The West of England Fat Stock and Poultry Exhibition in Plymouth. One of the attractions at the fair was a contest to guess the weight of a "dressed" ox on display. The nearest guess to the actual weight of the ox would win a prize, and a total of 800 attendees participated for a small fee. Not only was Francis Galton a eugenicist^[He founded the Galton Laboratory of National Eugenics at UCL.] -- and rightly condemned for this in more modern times -- he was also a statistician, and bought the used tickets off the stall holder. Back home he analysed the 787 usable tickets and later published his findings [@galton1907vox]. What surprised Galton was that while the individual guesses were often far from the true weight of the ox, the average guess of the group as a whole was highly accurate. Such findings are now often referred to as the "Wisdom of the crowds".

Although the wisdom of the crowds has been demonstrated in various domains, from decades of research, we also know that human judgement is subject to a long list of biases. One such bias is called "anchoring", and it pertains to the influence of prior exposure to a more or less arbitrary numerical value on the subsequent numerical judgement. For instance, if I ask you "How tall do you think Mount Everest is?", but first state that Mount Everest is taller than 2000 feet, you are likely to give a smaller number than if I first state that Mount Everest is shorter than 45,500 feet. 

In this example, we will focus on data obtained from a large scale replication study, covering the above described anchoring manipulation, as well as many other experiments [@klein2014investigating]. This "Many Labs" study contained data collected by a large number of labs from different countries, and here we will use data collected by Konrad Bocian and Natalia Frankowska from the University of Social Sciences
and Humanities Campus in Sopot, Poland. In particular, we will focus on the judged height of Mount Everest (in meters, as their participants were Polish) after a low anchor (610 meters^[This would be the equivalent of 2000 feet, but I'm not entirely sure that this is the value they used.]). From the wisdom of the crowds idea, we might expect the average judgement to be identical (or at least very close) to the true height of Mount Everest, which is 8848 meters above sea level. On the other hand, if the low anchor biased the judgements, we might expect the average judgement to deviate from the true height.

```{r load-anchoring, echo = FALSE}
library(sdamr)
library(ggplot2)
data(anchoring)
dat <- subset(anchoring,referrer %in% c("swps","swpson") & anchor == "low")
```

### Exploring the data

Before diving into statistical modelling, as discussed, it is always good to explore the data. There were judgements by $n=`r nrow(dat)`$ people in this dataset. A graphical overview in the form of a histogram and raincloud plot is given in Figure \@ref(fig:anchoring-eda-plots). The minimum was judged height was `r min(dat$everest_meters)` meters (so quite close to the anchor), and the maximum was `r pretNum(max(dat$everest_meters))` meters. The median judgement was `r median(dat$everest_meters)` and the mean `r mean(dat$everest_meters)`. The sample variance of the judgements was `r pretNum((length(dat$everest_meters) -1)*var(dat$everest_meters)/length(dat$everest_meters))` and the sample standard deviation `r sqrt((length(dat$everest_meters) -1)*var(dat$everest_meters)/length(dat$everest_meters))`. 

```{r anchoring-eda-plots, echo=FALSE, fig.cap="Histogram and boxplot of participants' judgments", fig.show="hold", out.width="48%", fig.width=3, fig.height=3}
ggplot(dat,aes(x=everest_meters)) + geom_histogram(binwidth=500) + xlab("Height of Mount Everest in meters")

# ggplot(dat,aes(y=everest_meters, x="")) + geom_violin() + geom_boxplot(width=.1) + ylab("Height of Mount Everest in meters") + xlab("") + stat_summary(fun=mean, geom="point", shape=16, size=2)
plot_raincloud(dat,y = everest_meters, point_size=1) + ylab("Height of Mount Everest in meters")
```

Looking at Figure \@ref(fig:anchoring-eda-plots), you can see a peak in the distribution around the true height of Mount Everest (8848 meters), but the judgements are also quite varying, with a large number of judgements substantially below the actual height. This may be due to some people knowing the correct answer, whilst others don't and have to guess. Those guessing would likely be more influenced by the low anchor. The question is whether, on average, the group's judgements are equal to the actual height of Mount Everest.

## A statistical model of judgements

Clearly, not everyone gave a correct answer, and there is substantial variation in people's judgements. The goal of a statistical model is to account for this variation. In this chapter, we will consider one of the simplest statistical models for metric data such as the judgements, namely the Normal distribution. In this model, we will assume that the variation in judgements is entirely random. We may also assume that, on average, the judgements are equal to the actual height of Mount Everest, but some people will overestimate, and some people underestimate the height in their judgements. Moreover, we might assume that the probability of an underestimation is equal to the probability of an overestimation, and that very large over- or underestimations are less likely than smaller over- or underestimations. The Normal distribution encapsulates such assumptions.

### The Normal distribution

The __Normal distribution__ is the well-known bell-shaped curve depicted in Figure \@ref(fig:normal-density-plot).

```{r normal-density-plot, fig.cap="The Normal density function for $\\mu = 0$ and $\\sigma = 1$", fig.height=4,out.width="80%"}
ggplot() + stat_function(fun=function(x) dnorm(x,mean=0,sd=1)) + xlim(-4,4) + ylab("p(y)") + xlab("y")
```

The curve is given by the following function:

\begin{equation}
p(y) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(y-\mu)^2}{2\sigma^2}}
(\#eq:normal-density-function)
\end{equation}

If that looks complicated, don't worry. First, let me point out that $\pi$ refers to the mathematical constant pi, i.e. $\pi= 3.141593\ldots$, and $e$ refers to the mathematical constant also known as Euler's number, i.e. $e =  2.718282\ldots$. They are known constants and not parameters. The Normal distribution does have two parameters: $\mu$ ("mu"), the mean, and $\sigma$ ("sigma"), also called the standard deviation. Confusingly, the formula above is not actually the formula for the Normal probability distribution. Rather, it is the formula of the Normal __density function__. The Normal distribution applies to continuous variables. Technically, you can't assign a probability to a particular value of a continuous variable, you can only assign probabilities to ranges of values. This is because a continuous variable has an infinite number of values (even if we restrict ourselves to a range). If we gave each unique value a probability of anything larger than 0, the total probability would not be 1, but infinite. If you add up an infinite amount of values, no matter how tiny they are, at some point you would reach a sum of 1, and then you'd have to keep on adding more. No matter how large the sum becomes, you would have to keep on going. Adding up an infinite number of values is not light work...

While we can't define probability of any particular value, the probability that an observation is within a particular range is well-defined. So, rather than asking "What is the probability that the height is exactly 8567.46384634748763...?", we can ask "What is the probability that the height is between 8567 and 8568?", or "What is the probability that the height is between 8567.575 and 8567.576?". These probabilities are defined by the "area under the curve" within that range. Using calculus, they can be computed as:

$$P(a \leq Y \leq b) = \int_a^b \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(y-\mu)^2}{2\sigma^2}} d y$$

If you are unfamiliar with calculus and have never seen an integration sign ($\int$), don't worry, you don't have to solve equations like this, and there won't be many more like this. You can just see it as a mathematical way of referring to the area under the curve between point $a$ and $b$. A proper probability density function respects the rule of total probability. The Normal distribution is defined over all real numbers (i.e. all possible numbers between minus infinity and infinity). Thus, the rule of total probability rquires that $P(-\infty \leq Y \leq \infty) = 1$, and this is true for the Normal distribution. Figure \@ref(fig:normal-distribution-plot) shows the probability of three ranges, each symmetrical around the mean. The probability of a value falling in the range between $\mu - 3 \times \sigma$ and $\mu + 3 \times \sigma$ is very close to 1. Hence, values more than three standard deviations from the mean are extremely unlikely.

```{r normal-distribution-plot, fig.cap="Normal distribution and the probability of a value falling in one of three overlapping ranges. Note that the ranges are overlapping, so that the range between $\\mu - 3 \\sigma$ and $\\mu + 3 \\sigma$ also covers the range between $\\mu - 2 \\sigma$ and $\\mu + 2 \\sigma$, but that this isn't shown in the colours.",fig.height=3}
library(scales)
library(dplyr)
library(tidyr)
#levels <- c(bquote("mu %+-% sigma %~~% .683"),bquote("mu %+-% 2 %*% sigma %~~% .954"),bquote("mu %+-% 3 %*% sigma %~~% .997"))[3:1]
levels <- c(bquote(mu %+-% sigma %~~% .683),bquote(mu %+-% 2 %*% sigma %~~% .954),bquote(mu %+-% 3 %*% sigma %~~% .997))[3:1]
rbind(data.frame(interval=1,x=c(-3,seq(-3,3,length=100),1),y=c(0,dnorm(seq(-3,3,length=100)),0)),
      data.frame(interval=2,x=c(-2,seq(-2,2,length=100),2),y=c(0,dnorm(seq(-2,2,length=100)),0)),
      data.frame(interval=3,x=c(-1,seq(-1,1,length=100),1),y=c(0,dnorm(seq(-1,1,length=100)),0))) %>%
  ggplot(aes(x=x,y=y,fill=factor(interval,labels=levels))) + stat_function(aes(fill=NULL),fun=function(x) dnorm(x,mean=0,sd=1)) + xlim(-4,4) + ylab("p(y)") + xlab("y") + geom_polygon(alpha=.8) + scale_fill_discrete(name="interval",labels = parse_format()) + theme(legend.title=element_text("interval"))
```

Because the Normal distribution is symmetric, with a single peak in the middle at $\mu$, the parameter $\mu$ not only equals the mean, but also the median (50% of all values are lower than $\mu$, and 50% are higher), and the mode.

The Normal distribution is rather popular in statistical modelling. One important reason we'll mention later (the Central Limit Theorem). Here, we'll focus on two properties that make the Normal distribution quite convenient to work with. Before we continue, let's introduce some new notation. As before, we will use $Y$ to denote the dependent variable (i.e. people's judgements of the height of Mount Everest), and $Y_i$ to denote the $i$-th value of the dependent variable (i.e. the judgement by participant $i$), where $i=1,\ldots,n$. We will state the assumption that $Y$ follows a Normal distribution as
$$Y_i \sim \mathbf{Normal}(\mu,\sigma)$$
You can read this as "$Y_i$ is sampled from a Normal distribution with mean $\mu$ and standard deviation $\sigma$". So the tilde ($\sim$) stands for "sampled from". When we denote distributions, such as the Normal distribution, we will use a bold font for the name of the distribution, and between the parentheses that follow we will indicate the parameters of the distribution.

### Two useful properties of the Normal distribution

__1. A linear transformation of a Normal-distributed variable is also Normal-distributed__.
More formally, if 
$$Y \sim \mathbf{Normal}(\mu_y, \sigma_y)$$
and $$Y' = a + b \times Y$$ i.e. a new variable $Y'$ is constructed by multiplying $Y$ by $b$ and then adding a constant $a$, the distribution of this new variable $Y'$ is
$$Y' \sim \mathbf{Normal}\left(a + b \times \mu_y, | b | \times \sigma_y\right)$$
i.e. a Normal distribution with mean equal to the linear transformation of the mean of $Y$, and standard deviation equal to the standard deviation of $Y$ multiplied by the absolute value of $b$. 

For example, suppose a fast-food chain is known for its "mega-megalicious" burger. The weight of each handmade patty is Normal-distributed with a mean of 1.2 pounds, and a standard deviation of 0.16 pounds. We can work out the distribution of the weight in kilograms by first noting that 1 kilogram = 2.2046 pounds and conversely, that one pound is $1/2.2046 = 0.4536$ kilograms. Weight in kilograms is thus a linear transformation of weight in pounds: 
$$\text{kg} = 0 + 0.4536 \times \text{lbs}$$ with $a=0$ and $b=0.4536$. The distribution of the weight in kilograms is then a Normal distribution with mean $0 + 0.4536 \times 1.2 = 0.5443$ kilograms, and standard deviation $|0.4536| \times .16 = 0.0725$ kilograms. 

As another example, we can standardize a Normal variable by subtracting the mean and then dividing by the standard deviation. The resulting variable is conventionally denoted as 
\begin{equation}
Z = \frac{Y-\mu}{\sigma}
(\#eq:standardized-Z-definition)
\end{equation}
and the transformation of $Y$ to $Z$ is also called the Z-transformation. We can view $Z$ as a linear transformation of $Y$ with $a=-\frac{\mu_y}{\sigma_y}$ and $b = \frac{1}{\sigma_y}$. So $Z$ has a mean of $\mu_z = -\frac{\mu_y}{\sigma_y} + \frac{1}{\sigma_y} \times \mu_y= 0$ and a standard deviation of $\sigma_z = |\frac{1}{\sigma_y}|\sigma_y = 1$. A Normal-distribution with a mean of 0 and a standard deviation of 1 is also called a __standard Normal distribution__. 

__2. The sum of two Normal-distributed variables is also Normal-distributed__. 
More formally, if $$Y_1 \sim \mathbf{Normal}(\mu_1,\sigma_1)$$ and $$Y_2 \sim \mathbf{Normal}(\mu_2,\sigma_2)$$
and $Y_s$ is the sum of $Y_1$ and $Y_2$, i.e. $$Y_s = Y_1 + Y_2$$, then $Y_s$ is distributed as 
$$Y_s \sim \mathbf{Normal}\left(\mu_1 + \mu_2, \sqrt{\sigma_1^2 + \sigma^2_2}\right)$$
i.e. a Normal distribution with mean equal to the sum of the mean of $Y_1$ and the mean of $Y_2$, and standard deviation equal to the square-root of the sum of the variance of $Y_1$ and the variance of $Y_2$.

As we will see, these properties will come in handy later. For instance, when working out the sampling distribution of the mean.

### Back to anchoring

If people's judgements are on average correct, this implies that $\mu = 8848$. Alternatively, the average judgement might be biased, for instance because of the low anchor. If this is so, what would the average judgement be? It is difficult to make a clear prediction about this. If we can't really come up with anything sensible, we may as well assume that this parameter can have any value, as we did for Paul's probability of a correct prediction if he were (somewhat) psychic. We can then formulate our two alternative models for people's judgements as:

- MODEL R: $\quad Y_i \sim \mathbf{Normal}(8848, \sigma)$
- MODEL G: $\quad Y_i \sim \mathbf{Normal}(\mu, \sigma)$

MODEL R is the restricted model, a special case of the more general MODEL G. The restriction is on $\mu$. In MODEL G, the mean can take any value, so $-\infty \leq \mu \leq \infty$ (remember that $\infty$ stands for _infinity_). MODEL R picks a specific value $\underline{\mu}$ from this infinite range, namely $\mu = \overline{\mu} = 8848$. In both models, the standard deviation is not specified, so is considered unknown. Standard deviations, like variances, can never be negative, so at least we know that $\sigma \geq 0$, but this is all that we will specify in advance for that parameter.

## Parameter estimation

MODEL G has two parameters: the mean $\mu$ and the standard deviation $\sigma$. The maximum likelihood estimate of $\mu$ is the sample mean:

\begin{equation}
\hat{\mu} = \overline{Y} = \frac{\sum_{i=1}^n Y_i}{n}
(\#eq:ml-estimator-mean-simple-model)
\end{equation}

The maximum likelihood estimate of the variance is the sample variance (see Equation \@ref(eq:definition-sample-variance)), i.e. $\hat{\sigma}^2_\text{ML} = S^2$. However, this estimator of the variance is biased. It is therefore common to estimate the variance with the unbiased estimator:
\begin{equation}
\hat{\sigma}^2 = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}
(\#eq:unbiased-estimator-variance-simple-model)
\end{equation}
The difference is to divide the sum of squared deviations by $n-1$, rather than by $n$. Intuitively, you can think of the reason for this as follows: When we use the sample mean $\overline{Y}$ rather than the true mean $\mu$ in computing the variance, we don't take into account that the sample mean is a noisy estimate of the true mean $\mu$. Compared to $\overline{Y}$, the true mean could be somewhat higher or lower, and therefore the sample variance is likely to be an _underestimate_ of the true variance. In other words, it is biased. By dividing by $n-1$ instead of $n$, the resulting estimate is a little higher. It so happens this completely removes the bias. When $n$ is large (i.e. there are a large number of observations), there will be little difference between the unbiased estimate and the sample variance, but for small $n$ the difference will be more marked. 

Note that the estimator above is for the variance $\sigma^2$ (sigma squared, i.e. sigma raised to the power of 2), not for teh standard deviation $\sigma$. If we want an estimator of the standard deviation, we can simply take the square-root to get the following estimator of $\sigma$:

$$\hat{\sigma} = \sqrt{\hat{\sigma}^2}$$

If the mean is known to equal a particular value, $\mu = \underline{\mu}$, as in MODEL R, then we should use that value $\underline{\mu}$ instead of $\overline{Y}$ to estimate the variance and standard deviation. The unbiased estimate of the standard deviation for MODEL R is
$$\hat{\sigma}_R = \sqrt{\frac{\sum_{i=1}^n (Y_i - \underline{\mu})^2}{n}}$$
Perhaps confusingly, we can now divide by $n$ instead of by $n-1$ to get an unbiased estimate. The reason for this is that when we know the true mean, there is no estimation error like we had for $\overline{Y}$, and hence no additional source of variability which would bias the estimate. 

<!-- Usually, the mean $\mu$ is of more interest than the standard deviation $\sigma$. The standard deviation (or equivalently, the variance) are often so-called _nuisance parameters_. -->

### Sampling distribution of the estimated mean

Remember that the estimator of the mean is an algorithm that provides estimates from data. Different datasets will give different estimates, even though all these datasets are generated from the same Data Generating Process. Our model of the Data Generating Process is the Normal distribution, which has two parameters: $\mu$ and $\sigma$. Let's pick some values for these parameters, so that we can use our model to simulate data. For instance, suppose the judgements are on average equal to the true value, so $\mu = 8848$. Individual judgements are quite variable, however, so let's take $\sigma = 2000$. Figure \@ref(fig:histogram-sampling-mean-simple) shows the distribution of the estimated mean of 10,000 data sets each consisting of $n=109$ observations (the same number of observations as our anchoring data). Also shown (as a dotted line) is the Normal distribution of the model we used to generate the data. Clearly, the estimated means are much less variable than the simulated judgements themselves.

```{r histogram-sampling-mean-simple, fig.cap="Estimated means for 10000 simulated data sets of $n = 109$, drawn from a Normal distribution with mean 8848 and standard deviation 2000. The dotted line represents the Normal distribution from which the data sets were generated, and the solid line the theoretical distribution of the estimated means.",fig.height=4,out.width="80%"}
set.seed(145)
mu <- 8848
sigma <- 2000
n_sim <- 10000
n <- 109
sim <- matrix(rnorm(n*n_sim, mean=mu, sd = sigma),ncol=n)
estimates <- rowMeans(sim)
ggplot(data.frame(estimates=estimates),aes(x=estimates)) + geom_histogram(binwidth=20) + xlim(8848-+ 2000,8848 + 2000) + stat_function(fun = function(x) 
    dnorm(x, mean = 8848, sd = 2000) * 20 * 10000, linetype=2) + stat_function(fun = function(x) 
    dnorm(x, mean = 8848, sd = 2000/sqrt(109)) * 20 * 10000, linetype=1)

```

Simulating data and then looking at the resulting distribution of estimates is straightforward, but also noisy. Luckily, it is quite easy to derive the true distribution of the estimated means (i.e. the sample means). Using the two properties of the Normal distribution, we know that the sum of two Normal-distributed variables also follows a Normal distribution. This is easily generalized to the sum of any number of Normal-distributed variables. For instance, we can construct the distribution of the sum of three variables by adding the sum of two Normal-distributed variables (which we know is a Normal-distributed variable) to a third Normal-distributed variable: $Y_s = (Y_1 + Y_2) + Y_3$. Hence, the sum of three Normal-distributed variables is also Normal-distributed. We can then take the sum of three variables, and add a fourth one, and this sum will also be Normal-distributed: $Y_s = ((Y_1 + Y_2) + Y_3) + Y_4$, and so will the sum of five, six, and any number of Normal-distributed variables. This is a nice example of a _recursive function_.

In short, if each $Y_i \sim \mathbf{Normal}(\mu,\sigma)$, then $$\sum_{i=1}^n Y_i \sim \mathbf{Normal}(n \mu, \sqrt{n}\sigma)$$
Moreover, as we discussed earlier, we can view the mean as a linear transformation of this sum with $a = 0$ and $b=\frac{1}{n}$, from which we can derive that
 $$\overline{Y} \sim \mathbf{Normal}\left(\mu, \frac{\sigma}{\sqrt{n}}\right)$$
Hence, the sampling distribution of the estimated means is a Normal distribution with a mean equal to $\mu$. The estimator is thus _unbiased_. The standard deviation of the sampling distribution of the estimated means is equal to $\frac{\sigma}{\sqrt{n}}$,  i.e. the standard deviation of the dependent variable divided by $\sqrt{n}$. The standard deviation of the sampling distribution of estimates is also called the __standard error__ of the estimates. Dividing by $\sqrt{n}$ implies that the standard deviation of the sample means is smaller than the standard deviation of the dependent variable. And, as $n$ increases, it becomes smaller and smaller. The estimator is therefore also _consistent_. And, as it turns out, the estimator is also _efficient_.
<!--
\begin{equation}
\overline{Y} \sim \mathbf{Normal}(\mu, \frac{\sigma}{\sqrt{n}})
(\#eq:sampling-distribution-mean-simple)
\end{equation}
-->

To simulate judgements, we had to pick an arbitrary value of $\sigma$. But how can we simulate the data when $\sigma$ is unknown? We could of course use the unbiased estimate $\hat{\sigma}$. For MODEL R and the present data, that would be $\hat{\sigma} = `r sqrt(sum((dat$everest_meters - 8848)^2)/nrow(dat))`$, which is obviously substantially larger than the value of 2000 we used before. If we'd use this value to simulate data sets and look at the distribution of the sample mean, we'd get a similar plot to the one of Figure \@ref(fig:histogram-sampling-mean-simple), but with a larger standard deviation. 

Suppose that MODEL R is true, that the Data Generating Process indeed results in a Normal distribution of people's judgements with a mean $\mu = 8848$ and a standard deviation $\sigma$. If we knew the value of $\sigma$, then we'd know everything there is to know about the distribution of the DGP. And knowing this, we'd know everything there is to know about the distribution of the sample means. But we don't know $\sigma$. While it makes sense to use an estimate of $\sigma$, this estimate will be noisy. We shouldn't just pretend that our estimate $\hat{\sigma}$ is identical to the true $\sigma$. The problem is that different values of $\sigma$ lead to different sampling distributions of the mean. Key to working out the sampling distribution of the mean when $\sigma$ is unknown is to also take into account the sampling distribution of the estimates of $\sigma$. Roughly, the idea is that, for a given data set, we can work out how likely different values of $\sigma$ are, and we can then derive the average of all the Normal distributions that follow from each possible value of $\sigma$. The resulting distribution is not a Normal distribution. It was derived by William Sealy Gosset  (1876â€“1937) in 1904. Gosset worked as Head Experimental Brewer for Guinness in Dublin, and the company had a rule forbidding their chemists to publish their findings [@zabell2008student]. Gosset was able to convince his boss that his mathematical work was of no practical use to competing brewers, and was allowed to publish them in @student1908probable, but under a pseudonym to avoid his colleagues getting similar ideas. "Student" was the pseudonym chosen by the managing director of Guinness, and hence the distribution is now known as Student's t-distribution. 

```{r comparing-normal-and-t-simple, fig.cap=paste0("(Understandardized) Student t-distribution which is the true sampling distribution of the mean under MODEL R with unknown $\\sigma$ (solid line) and the Normal distribution that would be the sampling distribution if it were known that $\\sigma = \\hat{\\sigma}$ (broken line). Note that these curves are drawn for the case of $n=10$ observations, to make the differences between the distributions more marked. For $n=",nrow(dat),"$, the t-distribution is almost identical to the Normal distribution."),fig.height=4,out.width="80%"}
mu <- 8848
sigma <- sqrt(sum((dat$everest_meters - mu)^2)/nrow(dat))
n <- 10
ggplot(data.frame(x=c(mu - 2*sigma,mu + 2*sigma),y=c(0,dnorm(mu,mean=mu,sd=sigma/sqrt(n)))),aes(x=x,y=y)) + stat_function(fun = function(x) dnorm((x-mu)/(sigma/sqrt(n))), linetype=2) + stat_function(fun = function(x) dt((x-mu)/(sigma/sqrt(n-1)), df=n-1), linetype=1) + ylab("density") + xlab("Height of Mount Everest in meters")
```

You can see a comparison of the t-distribution and the Normal distribution in Figure \@ref(fig:comparing-normal-and-t-simple). The main thing to notice is that the t-distribution is also bell-shaped and symmetric, but it is wider (has fatter tails) than the Normal distribution. The difference between the t-distribution and Normal distribution depends on the value of $n-1$. When $n>30$, the difference is, for most practical purposes, negligible.

## Testing whether $\mu$ has an specific value

Our main interest here is to assess whether it is reasonable to assume that, in the Data Generating Process, the average judgement equals the true height of Mount Everest. More bluntly, we simply want to know whether $\mu = 8848$.

### The classical way

Now we know what happens to the estimate $\hat{\mu}$ of MODEL G when MODEL R is in fact true, we can use a similar logic as before to decide when $\hat{\mu}$ is different enough from $\underline{\mu}$ to reject MODEL R. To work this out, it is convenient to standardize the sample mean by subtracting the (supposed) true average, and dividing by the standard error of the sample mean:
\begin{equation}
t = \frac{\hat{\mu} - \underline{\mu}}{\hat{\sigma}/\sqrt{n}} = \frac{\overline{Y} - \underline{\mu}}{\hat{\sigma}/\sqrt{n}}
(\#eq:definition-one-sample-t)
\end{equation}
where 
$$\hat{\sigma} = \sqrt{\frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}}$$
is the unbiased estimator of $\sigma$. We call the resulting standardized value the $t$ statistic. It is much like the $Z$-transformation, but now we're using an _estimated_ $\hat{\sigma}$ rather than a known value. The standardized $t$ statistic follows a $t$ distribution with $n-1$ __degrees of freedom__, which reflects the amount of data we used to estimate $\sigma$. What we showed in Figure \@ref(fig:comparing-normal-and-t-simple) was an "unstandardized" t-distribution, which also has a location and scale parameter. By standardizing $t$, we don't need to worry about these. Generally, when someone mentions the t-distribution, they refer to the standardized version, which has a single parameter, the degrees of freedom (commonly denoted with the Greek symbol $\nu$, pronounced as "nu"; I will just use "degrees of freedom", or $df$ though). Back in the times of Gosset, this was particularly important, as there was no access to modern computers to quickly work out the probabilities for different versions of unstandardized t-distributions. Statisticians had to rely on tables for a standard version of a distribution, which were painstakingly computed by hand. In fact, the term "computer" originally referred to a person performing mathematical calculations. Complex, long, and often rather tedious calculations were performed by teams of such computers.

I won't bore you with the mathematical details of the t-distribution. As I already mentioned, it looks quite like a Normal distribution. When the degrees of freedom get larger, the distributions looks more and more like the standard Normal distribution. Because we know that if MODEL R is true, the $t$ statistic follows a t-distribution with $n-1$ degrees of freedom, we know all there is to know about the sampling distribution of $t$. If $t$ is far away from 0, that means that the estimate $\hat{\mu}$ of MODEL G is far away from the assumed value $\underline{\mu}$ in MODEL R, and hence we have reason to reject MODEL R. Following the logic of the null-hypothesis significance test, we will limit the number of wrong rejections of MODEL R by choosing critical values such that
$$P(t \leq \text{lower critical value} | \text{MODEL R}) + P(t \geq \text{upper critical value} | \text{MODEL R}) = \alpha$$
The resulting critical regions are depicted in Figure \@ref(fig:critical-values-t-distribution).

```{r critical-values-t-distribution, fig.cap="Critical regions for rejecting $H_0$: $\\mu = 8848$ with $\\alpha = .05$", out.width="80%", fig.height=4}
library(scales)
library(dplyr)
library(tidyr)
#levels <- c(bquote("mu %+-% sigma %~~% .683"),bquote("mu %+-% 2 %*% sigma %~~% .954"),bquote("mu %+-% 3 %*% sigma %~~% .997"))[3:1]
levels <- c(bquote(mu %+-% sigma %~~% .683),bquote(mu %+-% 2 %*% sigma %~~% .954),bquote(mu %+-% 3 %*% sigma %~~% .997))[3:1]
data.frame(interval=1,x=c(-5,seq(-5,qt(.025,nrow(dat)-1),length=100),qt(.025,nrow(dat)-1),qt(.975,nrow(dat)-1),seq(qt(.975,df=nrow(dat)-1),5,length=100),5),y=c(0,dt(seq(-5,qt(.025,nrow(dat)-1),length=100),df=nrow(dat)-1),0,0,dt(seq(qt(.975,nrow(dat)-1),5,length=100),df=nrow(dat)-1),0)) %>%
  ggplot(aes(x=x,y=y,fill=factor(interval,labels=""))) + stat_function(aes(fill=NULL),fun=function(x) dt(x,df=nrow(dat)-1)) + xlim(-4,4) + ylab("p(y)") + xlab("y") + geom_polygon(alpha=.8) + theme(legend.position="none") + xlab("t") + ylab("p(t)") + geom_vline(xintercept = qt(.025,nrow(dat)-1), lty=2) + geom_vline(xintercept = qt(.975,nrow(dat)-1), lty=2) + annotate("text", x=-3,xmin=-4,xmax= qt(.025,nrow(dat)-1), y=.4,hjust = 0.5, vjust=1,label="lower \n critical region") + annotate("text", x=3, xmax=4, xmin= qt(.975,nrow(dat)-1),y=.4,hjust = .5, vjust=1,label="upper \n critical region")

```

So how about the judgements in the anchoring experiment? Is it reasonable to assume that $\mu = 8848$? Let's perform the test and see. For MODEL G, our estimated mean is $\hat{\mu} = `r mean(dat$everest_meters)`$, and the unbiased estimate of the standard deviation is $\hat{\sigma} = `r sd(dat$everest_meters)`$. The $t$ statistic is then
$$t = \frac{`r mean(dat$everest_meters)` - 8848}{`r sd(dat$everest_meters)`/\sqrt{`r nrow(dat)`}} = `r  (mean(dat$everest_meters) - 8848)/(sd(dat$everest_meters)/sqrt(nrow(dat)))`$$
For a significance level of $\alpha = .05$, we can use the t-distribution with `r nrow(dat) - 1` degrees of freedom to work out the lower and upper critical value as `r qt(.025,nrow(dat)-1)` and `r qt(.975,nrow(dat)-1)`. Clearly, the value of the $t$-statistic that we computed is far below the lower critical value, which means we reject the null hypothesis. The associated $p$-value with this two-sided test (we would reject $H_0$ if we had very large negative, as well as very large positive values of the $t$ statistic), is $P(t \leq `r (mean(dat$everest_meters) - 8848)/(sd(dat$everest_meters)/sqrt(nrow(dat)))`| \text{df}  = `r nrow(dat) - 1`) + P(t \geq `r -1*(mean(dat$everest_meters) - 8848)/(sd(dat$everest_meters)/sqrt(nrow(dat)))`| \text{df}  = `r nrow(dat) - 1`) = P(|t| \geq `r -1*(mean(dat$everest_meters) - 8848)/(sd(dat$everest_meters)/sqrt(nrow(dat)))`| \text{df} = `r nrow(dat) - 1`) < .001$. So, in conclusion, the $t$-statistic is in the critical region for our chosen significance level of $\alpha = .05$, which leads us to reject the null hypothesis. Equivalently, the $p$-value, the probability of obtaining a $t$ statistic equal to or more extreme than the one we found, is smaller than our chosen significance level $\alpha=.05$, so we reject the null hypothesis. In our anchoring data, the "wisdom of crowds" doesn't seem to hold, because the average judgement does not equal the true height of Mount Everest.

### The model comparison way

There is an equivalent way to perform the hypothesis test that $\mu = \underline{\mu}$, by directly comparing two versions of our statistical model, one in which we assume we know the value of $\mu$ to be $\underline{\mu}$ (MODEL R), and one in which we don't (MODEL G). As we did for Paul the Octopus, we then look at the likelihood ratio to see whether MODEL R is tenable. Note that in both models, the standard deviation is considered unknown and will have to be estimated. 

The procedure is essentially the same as before. We find a way to calculate the likelihood ratio, and work out the distribution of the likelihood ratio values we would get for repetitions of an experiment _assuming MODEL R is true_. Using this distribution, we can then work out a critical value, such that the probability of obtaining a likelihood ratio value equal to or smaller than it is equal to a significance level $\alpha$, i.e. , $P(\text{likelihood ratio } \leq \text{critical value} | \text{MODEL R}) = \alpha$. The derivation of the likelihood ratio is a little more complicated than before though. Feel free to glance over the following, as the derivation is not that important. The paragraphs marked with a vertical line in the left margin are more advanced and can be skipped without too much consequence.

```{block2, type = "advancedtext", echo=TRUE}
When all observations $Y_i$, $i=1, \ldots, n$ are independent draws from a Normal distribution, then we can use the product rule to work out the joint "probability" (density value really) of all observations. That is, we use Equation \@ref(eq:normal-density-function) to compute $p(Y_i)$, the density value of each observation, and then multiply these together:
$$
\begin{aligned}
p(Y_1, Y_2, \ldots, Y_n) &= p(Y_1) \times p(Y_2) \times \ldots \times p(Y_n) \\
&= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(Y_i-\mu)^2}{2\sigma^2}} \\
&= \left( \frac{1}{\sqrt{2 \pi} \sigma} \right)^n e^{ \frac{n}{2\sigma^2} \sum_{i=1}^n (Y_i - \mu)^2}
\end{aligned}
$$
The $\prod$ sign on the second line is similar to the summation sign, but tells you to _multiply_ all the values that follow it, so e.g.
$$\prod_{i=1}^3 Y_i = Y_1 \times Y_2 \times Y_3$$
On the third line, we make use of the fact that $e^a \times e^b = e^{a + b}$. We can write the likelihood ratio of MODEL R over MODEL G as
$$
\begin{aligned}
L\!R(R,G) &= \frac{\left( \frac{1}{\sqrt{2 \pi} \hat{\sigma}_{ml,R}} \right)^n e^{ \frac{n}{2\hat{\sigma}^2_{ml,R}} \sum_{i=1}^n (Y_i - \mu)^2}}{\left( \frac{1}{\sqrt{2 \pi} \hat{\sigma}_{ml,G}} \right)^n e^{ \frac{n}{2\hat{\sigma}^2_{ml,G}} \sum_{i=1}^n (Y_i - \mu)^2}} \\
&=  \frac{ \left(\sqrt{\frac{2\pi}{n} \sum_{i=1}^n(Y_i - \underline{\mu})^2}\right)^{-n} e^{\frac{\sum_{i=1}^n(Y_i - \underline{\mu})^2}{2 \sum_{i=1}^n(Y_i - \underline{\mu})^2/n}}}{\left(\sqrt{\frac{2\pi}{n} \sum_{i=1}^n(Y_i - \overline{Y})^2}\right)^{-n} e^{ \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{2 \sum_{i=1}^n (Y_i - \overline{Y})^2/n}} } \\
&= \left(\frac{\sum_{i=1}^n(Y_i - \underline{\mu})^2}{\sum_{i=1}^n(Y_i - \overline{Y})^2}\right)^{-n/2}
\end{aligned}
$$

```

What is interesting to point out  is that, on the last line of the equation above, we see that the two main ingredients of the likelihood ratio are _sums of squared deviations_. Moreover, as the deviations are between observations and the mean (either assumed mean or sample mean), these sums of squared deviations are very closely related to variances. As we will see later, "sums of squares" are important quantities when comparing General Linear Models.

```{block2, type = 'advancedtext', echo=TRUE}
While the likelihood ratio is reasonably straightforward to compute, it is not easy to derive the sampling distribution of the likelihood ratio. Previously, we could work out the distribution of the likelihood ratio when MODEL R is true relatively easily from the Binomial distribution of the outcomes. We can do something similar here, which effectively means transforming the likelihood ratio into a statistic we do know the distribution of, or conversely, transforming a statistic we know the distribution of into the likelihood ratio. To do this, first we can rewrite 
$$\sum_{i=1}^n(Y_i - \underline{\mu})^2 = \sum_{i=1}^n(Y_i - \overline{Y} + \overline{Y} - \underline{\mu})^2 = n(\overline{Y} - \underline{\mu})^2 + \sum_{i=1}^n(Y_i - \overline{Y})^2$$
and filling this in the numerator of the likelihood ratio gives
$$L\!R(R,G) = \left(1 + \frac{n(\overline{Y} - \underline{\mu})^2}{\sum_{i=1}^n(Y_i - \overline{Y})^2}\right)^{-n/2}$$
For a given experiment or study, we can treat the number of observations $n$ as a given fixed number. What we are really interested in is the part of the likelihood ratio that depends on the data. If we take the square root of this part, we get 
$$\frac{|\overline{Y} - \underline{\mu}|}{\sqrt{\sum_{i=1}^n(Y_i - \overline{Y})^2}} = \sqrt{n (n-1)} \times | t |$$
with $t$ defined as in Equation \@ref(eq:definition-one-sample-t). When the absolute value of the $t$ statistic becomes larger, the likelihood ratio value becomes smaller (less supportive of MODEL R). We can thus view the likelihood ratio as a (rather complicated) transformation of the absolute value of the $t$ statistic. The _distribution_ of the likelihood ratio is then also a (rather complicated) transformation of the _distribution_ of the $t$-statistic. Just like the coin tossing model, where the likelihood ratio was effectively a transformation of the statistic $k$ (the number of correct guesses), and the distribution of the likelihood ratio a transformation of the distribution of $k$, i.e. the Binomial distribution.

```

What all this means is that the one-sample t-test, as worked out before, is equivalent to a likelihood ratio test comparing MODEL R to MODEL G. Whether you perform the test in the "classic" way by computing the $t$ statistic and assessing the magnitude of this in the $t$ distribution, or whether you compute a likelihood ratio and assess the magnitude of this in the distribution of the likelihood ratio under MODEL R, you will get exactly the same outcome.

## Confidence intervals {#ch3-confidence-interval}

Confidence intervals are __interval estimates__. That is, rather than providing a single point estimate of a parameter, they specify a range of values for a parameter. That range is chosen in a precise, but somewhat counter-intuitive and difficult to understand manner. Ideally, what you might want to obtain is a range such that you can be certain that, with a specified probability, the true parameter is within this range. Unfortunately, this is _not_ what confidence intervals do. Like much of Frequentist statistics, confidence intervals concern "the long run", they are based on the idea of an infinite number of possible (simulated) datasets from a true model. Confidence intervals are the result of an algorithm which is constructed in such a way that the ranges it produces will, for a specified proportion of all those possible datasets, contain the true value of the parameter. 

While difficult to interpret properly, the confidence interval for the mean of Normal interval with unknown standard deviation is straightforward to compute:
\begin{equation}
\hat{\mu} \pm t_{n-1; 1- \frac{\alpha}{2}} \frac{\hat{\sigma}}{\sqrt{n}}
(\#eq:confidence-interval-one-sample-t-test)
\end{equation}
where $t_{n-1; 1- \frac{\alpha}{2}}$ stands for the upper critical value of a test with significance level $\alpha$. In other words, we take our estimate of the mean (i.e. $\hat{\mu} = \overline{Y}$), and add or subtract from that a multiple of the __estimated standard error of the mean__ (which is $\frac{\hat{\sigma}}{\sqrt{n}}$; remember that the standard error of a statistic is the standard deviation of the sampling distribution of that statistic). The multiple is derived from the t-distribution. As explained earlier, for the Normal distribution, we know that 95% of the observations lie in the range between $\mu - 1.96 \times \sigma$ and $\mu + 1.96 \times \sigma$. Similarly, for the Normal distribution, 95% of the sample means lie in the range between $\mu - 1.96 \times \frac{\sigma}{\sqrt{n}}$ and $\mu + 1.96 \times \frac{\sigma}{\sqrt{n}}$. The value  $t_{n-1; \frac{\alpha}{2}}$ is effectively a replacement for 1.96, which takes into account that we have estimated $\sigma$. 
```{r simulated-confidence-intervals-one-sample-t-test, fig.cap="$\\text{95%}$ confidence intervals for 100 simulated datasets with $\\mu=8848$ and $\\sigma=2000$. Most of the intervals contain the true value $\\mu$, but some do not. If we would simulate an infinite number of datasets, then exactly $\\text{95%}$ of the intervals would contain $\\mu$, and exactly $\\text{5%}$ would not."}
set.seed(238476234)
n <- nrow(dat)
nsim <- 100
mu <- 8848
sigma <- 2000
conf_dat <- data.frame(sim=1:nsim,lower=rep(0.0,nsim),est=rep(0.0,nsim),upper=rep(0.0,nsim),contains=TRUE)
for(sim in 1:nsim) {
  x <- rnorm(n,mean=mu,sd=sigma)
  conf_dat$lower[sim] <- mean(x) + qt(.025,n-1)*sd(x)/sqrt(n)
  conf_dat$est[sim] <- mean(x)
  conf_dat$upper[sim] <- mean(x) + qt(.975,n-1)*sd(x)/sqrt(n)
}
conf_dat$contains <- (conf_dat$lower <= mu & conf_dat$upper >= mu)
conf_dat$contains <- !conf_dat$contains
ggplot(conf_dat,aes(x=sim,y=est,ymax=upper,ymin=lower,colour=contains)) + geom_point() + geom_linerange() + geom_hline(yintercept = mu,lty=2) + xlab("simulated data set") + ylab("confidence interval") + theme(legend.position = "none")
```

Earlier on, we simulated datasets of size $n=`r nrow(dat)`$ from a Normal distribution with mean $\mu = 8848$ and $\sigma = 2000$. In Figure \@ref(fig:simulated-confidence-intervals-one-sample-t-test) we show the resulting 95% confidence intervals (now estimating $\sigma$ from the data) for `r nsim` of such simulated datasets. As you can see, the location, and the width of each confidence interval is different. Moreover, you can see that while most of the confidence intervals contain the true value $\mu = 8848$, some don't. Indeed, `r sum(conf_dat$contains)` out of `r nsim` confidence intervals do not include the true value of $\mu$, which is pretty close to 5%. <!-- You can see a particularly nice animation of how  confidence intervals work on the [Seeing Theory](https://seeing-theory.brown.edu/frequentist-inference/index.html#section2) website. -->

For the judgements in the anchoring experiment, the estimated mean was $\hat{\mu} = `r mean(dat$everest_meters)`$, the unbiased estimate of the standard deviation was $\hat{\sigma} = `r sd(dat$everest_meters)`$, and the upper critical value was $t_{`r nrow(dat)-1`, .975} = `r qt(.975, nrow(dat)-1)`$. Thus, the confidence interval is
$$
\begin{aligned}
\text{95% confidence interval} &= \hat{\mu} - t_{n-1; 1 - \frac{\alpha}{2}} \times \frac{\hat{\sigma}}{\sqrt{n}} \leq \mu \leq \hat{\mu} + t_{n-1; 1- \frac{\alpha}{2}} \times \frac{\hat{\sigma}}{\sqrt{n}} \\
&= `r mean(dat$everest_meters)` - `r qt(.975, nrow(dat)-1)` \times \frac{`r sd(dat$everest_meters)`}{\sqrt{`r nrow(dat)`}} \leq \mu \leq `r mean(dat$everest_meters)` + `r qt(.975, nrow(dat)-1)` \times \frac{`r sd(dat$everest_meters)`}{\sqrt{`r nrow(dat)`}} \\
&= `r mean(dat$everest_meters) - qt(.975, nrow(dat)-1)*(sd(dat$everest_meters)/sqrt(nrow(dat)))` \leq \mu \leq `r mean(dat$everest_meters) + qt(.975, nrow(dat)-1)*(sd(dat$everest_meters)/sqrt(nrow(dat)))` 
\end{aligned}
$$

Confidence intervals can be used to perform null hypothesis significance tests. In fact, a confidence interval contains all values of $\underline{\mu}$ for which MODEL R would not be rejected with the current data. In other words, if the confidence interval includes the value of $\underline{\mu}$ of interest (i.e. $\underline{\mu} = 8848$ in our example), that means that the null hypothesis would _not_ be rejected. In addition, we can also determine a whole range of other values of $\underline{\mu}$ which, if purported as a null hypothesis, would also not be rejected. In that sense, a confidence interval provides more information than a simple "reject" or "not reject" decision of a null hypothesis significance test. If a confidence interval is very wide, that implies the significance test has low power. Of course, what "very wide" is, depends on the scale of the data, and a subjective evaluation by the researcher.

There have been calls to abandon hypothesis tests in favour of estimation and reporting confidence intervals [e.g. @cumming2014new]. Given the tricky interpretation of confidence intervals [@morey2016fallacy], and their rather close ties to significance tests, such ideas seem to promise more than they can deliver. There is no need to abandon either significance tests or confidence intervals, provided you know how to interpret them.

## Effect size

Whether an observed deviation between the sample mean $\overline{Y}$ and assumed mean $\underline{\mu}$ is significant, depends on the power of the test. The power of a test, as you may recall, is the probability of a test statistic exceeding the critical value(s) when the null hypothesis is false (i.e., when the true mean $\mu$ does not equal the assumed mean $\underline{\mu}$). This, in turn, depends to a large part on the number of observations ($n$). If you look closely at the definition of the $t$-statistic (Equation \@ref(eq:definition-one-sample-t)), you might see that, for any given values of  $\overline{Y}$, $\underline{\mu}$, and $S$, the value of $t$ increases with $n$. This is because the more observations we have, the more precisely we can estimate the true mean $\mu$ from the sample mean $\overline{Y}$. So that's all good. But as a consequence, if we have enough data, even the tiniest deviation between $\overline{Y}$ and $\underline{\mu}$ would give a significant test result. Whilst that is "all good" statistically speaking, in practical terms, we might not be all that interested in such small deviations from theoretical predictions. In addition to considering significance, it is useful to consider the *size of the effect* we have observed. By "effect", we simply mean the deviation between the sample mean and assumed mean. As this deviation is dependent on the scale of measurement, it is useful to standardize the deviation in some way. A common way to standardize effect sizes is to make them relative to the variability in the data, in the same way as is done in $Z$ scores. This provides us with what is commonly known as Cohen's $d$:
\begin{equation}
\text{Cohen's } d = \frac{\overline{Y} - \underline{\mu}}{\hat{\sigma}}
(\#eq:cohen-d-one-sample-t-test)
\end{equation}
For our anchoring example, Cohen's $d$ is
$$d = \frac{`r mean(dat$everest_meters)` - 8848}{`r sd(dat$everest_meters)`} = `r  (mean(dat$everest_meters) - 8848)/(sd(dat$everest_meters))`$$

Cohen's $d$ quantifies an effect in units of the standard deviation in the sample data. As a rule of thumb, an absolute value of $|d| = 0.2$ is considered a small effect, an absolute value of $|d| = 0.5$ a medium effect, and an absolute value of $|d| = 0.8$ a large effect [@cohen1988statistical]. Thus, the effect-size we found can be classified as large. These rule-of-thumb values, relative to the Normal distribution, are depicted in Figure \@ref(fig:one-sample-cohen-d-plot). As you can see, even "large" effect sizes are still quite close to the center of the distribution. For example, `r round(100*(1 - pnorm(.8)),2)`% of observations from the Data Generating Process would exceed the deviation specified as a large positive effect size. But you should keep in mind that the objective of an effect size here is to quantify the deviation between a sample mean and an assumed mean, not between observations and that assumed mean.

```{r one-sample-cohen-d-plot, fig.cap="Small, medium, and large values of Cohen's $d$ in context of the data distribution.", fig.height=4,out.width="80%"}
data.frame(d = c(.2,-.2,.5,-.5,.8,-.8), effect = rep(c("small","medium","large"),each=2)) %>% ggplot() + stat_function(fun=function(x) dnorm(x,mean=0,sd=1)) + xlim(-4,4) + ylab("p(y)") + xlab("y") + geom_vline(aes(xintercept=d, colour=effect))
```

For our anchoring example, Cohen's $d$ is
$$d = \frac{`r mean(dat$everest_meters)` - 8848}{`r sd(dat$everest_meters)`} = `r  (mean(dat$everest_meters) - 8848)/(sd(dat$everest_meters))`$$

Comparing Equation \@ref(eq:cohen-d-one-sample-t-test) to \@ref(eq:definition-one-sample-t), we can see that these look almost the same. The only difference is that the $t$ statistic divides the estimated standard deviation by $\sqrt{n}$, as this provides the standard error of the mean. For a one-sample t-test, Cohen's $d$ can then also be computed as $t/\sqrt{n}$. This shows that, to an extent, Cohen's $d$ removes the effect of $n$ from the $t$ statistic, and is not "inflated" as sample size increases. In addition to providing test results, it is generally a good idea to also provide the effect size as additional information. 

## Assumptions {#sec:02-assumptions}

When we started modelling the data, we assumed each judgement was effectively an independent random draw from a Normal distribution. In MODEL R, we assumed that the mean of that Normal distribution was $\mu = 8848$, while in MODEL G, we left both the mean and the standard deviation unknown. Both models thus assume the judgements follow a Normal distribution. Looking at Figure \@ref(fig:anchoring-eda-plots), you might wonder whether that's a reasonable assumption. The histogram doesn't look exactly Normal. With real and limited data, histograms can be deceiving though. A __Q-Q (quantile-quantile) plot__ aims to provide a better means to visually inspect whether data (approximately) follows an assumed distribution. Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Percentiles are an example of quantiles, which divide a probability distribution into ranges each containing .01 of the total probability (i.e. 1%). By ordering the values of a variable in a data set in increasing order of magnitude, you can also obtain "empirical quantiles". Suppose you have 5 observations ordered in increasing magnitude:

1, 4, 4.5, 9, 20

For each value, you can determine the proportion of values in the data which are equal or smaller than it. For instance, the proportion of observations equal to or smaller than 1 is $1/5 = 0.2$,  the proportion of observations equal to or smaller than 4 is $2/5 = 0.4$, the proportion of observations equal to or smaller than 4.5 is $3/5 = 0.6$, etc. For each of these proportions, we can also work out the corresponding value of the variable for the probability of a value equal to smaller than it equals this proportion. For instance, for a Normal distribution with mean $\mu=7$ and standard deviation $\sigma = 7$, the quantile such that $P(\text{value } \leq \text{ quantile}) = .2$ is `r qnorm(.2,mean=7,sd=7)`, and the quantile such that $P(\text{value } \leq \text{ quantile}) = .4$ is `r qnorm(.4,mean=7,sd=7)`. In a Q-Q plot, the values of the observed data are plotted against these theoretical quantiles in a scatter-plot. If the data follows the assumed distribution, the empirical quantiles should generally match the theoretical quantiles, and hence the points in the plot should roughly lie on a straight line. Figure \@ref(fig:anchoring-qq-plot) shows such a Q-Q plot for the judgements. As you can see, rather than on a straight-line, the points seem to lie on an s-shaped curve, which indicates that the assumption of a Normal distribution might not be a good match to the data.

```{r anchoring-qq-plot, out.width="80%", fig.width=5, fig.height=5, fig.cap="A Quantile-Quantile plot for the judgments of the height of Mount Everest"}
ggplot(dat, aes(sample = everest_meters)) + stat_qq() + stat_qq_line()
```

There are also two common tests for the null hypothesis that the distribution is Normal: the Kolmogorov-Smirnov, and the Shapiro-Wilk test. The former is a generic test, which can be used to check for other distributions besides the Normal one as well. The second one only focuses on the Normal distribution, and because it was specifically designed for this distribution, it is more powerful. I'm not going to describe how these tests actually work. You can look that up elsewhere if you like. I will tell you that both tests reject the null-hypothesis of a Normal distribution. For both tests, the $p$-value (the probability of obtaining a test result equal to or more extreme, given that the null hypothesis of a Normal distribution is true), is $p<.001$. I should also caution you in relying too much on such tests. If you have large datasets, the tests are rather powerful, and may therefore reject the null hypothesis of a Normal distribution when there is just a small deviation from Normality in the data. Such small deviations might be rather inconsequential, so you should use your judgement and also rely on visual inspection of histograms and Q-Q plots. For the current data, both the plots and the tests indicate quite some deviation from a Normal distribution...

Perhaps there are really two groups of participants: those who know the height of the Mount Everest, and those who don't. The first group will provide a highly accurate answer (allowing for small deviations, as their knowledge may not be exact), while the second group will guess, providing much more variable judgements which are more likely to be influenced by the anchor. If the judgements of each of these groups can be represented by a Normal distribution, but with different means and variances, then the distribution as a whole might look a bit like the one depicted in Figure \@ref(fig:anchoring-mixture-plot). This is also called a _mixture_ distribution and clearly, it is not a Normal distribution. How bad would it be if this were there case?

```{r anchoring-mixture-plot, out.width="80%", fig.height=4, fig.cap="A bimodal distribution, arising when there are two groups of participants: those with good knowledge who provide answers close to the true height of Mount Everest, and those with poor knowledge, providing much more variable judgements. The distribution for each group is depicted by dotted lines, and the distribution over both groups as a solid line."}
mu1 <- 8848
sigma1 <- 250
mu2 <- 4000
sigma2 <- 3000

ggplot(data.frame(x=c(min(dat$everest_meters),max(dat$everest_meters)),y=c(0,.2*dnorm(mu1,mu1,sigma1))),aes(x=x,y=y)) + stat_function(fun = function(x) .2*dnorm(x,mu1,sigma1), linetype=3) + stat_function(fun = function(x) .8*dnorm(x,mu2,sigma2), linetype=3) + stat_function(fun = function(x) .2*dnorm(x,mu1,sigma1) + .8*dnorm(x,mu2,sigma2), linetype=1) + ylab("p(y)") + xlab("Height of Mount Everest in meters")
```

This is where things get a bit messy. One thing to realise is that we are mainly interested in the mean judgement, and whether this equals 8848 meters. We needed to make an assumption about the distribution of the judgements to (1) determine an appropriate estimator of the mean, and (2) derive a sampling distribution of the estimated means. In making inferences about the true mean of the judgements, we only really need to focus on this sampling distribution. And as it happens, an important result in statistics tells us that, no matter what distribution the data itself follows, if we use enough observations, the sampling distribution of the mean will be very close to Normal. This result is called the Central Limit Theorem. 

## The Central Limit Theorem

A main reason that the Normal distribution is used so often (and perhaps called "Normal", rather than "Abnormal"), is due to a mathematical fact known as the __Central Limit Theorem__:

```{definition, clt, name="Central Limit Theorem", echo=TRUE} 
The distribution of the sum of $n$ independent variables approaches the Normal distribution as the number of variables approaches infinity ($n \rightarrow \infty$).

```

This is quite an amazing theorem. Although you might wonder why you should care about sums of an infinite number of random variables, there are two things to note. Firstly, the rate at which the distribution "approaches" the normal distribution can be rather fast. Secondly, as we showed when we discussed two useful properties of the Normal distribution, the sample mean can be viewed as a normalized sum of $n$ variables: $\overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i$. So the Central Limit Theorem applies also to the sample mean, and implies that as long as the size of the samples is large enough, the sample mean will follow a Normal distribution.

### The Central Limit Theorem in action

To see the central limit theorem in action, let's pick an arbitrary probability distribution over 5 values, as shown in Figure \@ref(fig:weird-prob-dist). This will be the true distribution (i.e. the Data Generating Process) from which we can draw of simulate values.   

```{r weird-prob-dist,fig.cap="An aribitrary probability distribution over 5 values, clearly not a Normal distribution!",fig.width=4,fig.height=3,out.width="50%"}
library(dplyr)
library(tidyr)
library(purrr)

values <- c(1,2,3,4,5)
prob <- c(.5,.2,.1,.6,.3)
prob <- prob/sum(prob)

data.frame(value=values,probability=prob) %>% 
  ggplot(aes(x=value,y=probability)) + geom_col() + ylim(c(0,1))
```

When we repeatedly draw 5 values and calculate the mean over these 5 observations, we can look at the distribution of these sampled means. This is shown in Figure \@ref(fig:clt-animation-1). You can see that even when we draw just 5 values from a decidedly non-Normal distribution, the sample distribution of the resulting average already has the characteristic bell-shaped curve. 
<!-- Figure \@ref(fig:clt-animation-2) shows the same when we repeatedly sample 20 values and calculate the mean. -->

```{r clt-animation-1, dev='png', echo=FALSE, cache=TRUE, fig.cap="The distribution of the mean of 5 observations by simulating 200 data sets (from the distribution in Figure \\@ref(fig:weird-prob-dist))", out.width='50%', fig.width=4}

library(gganimate)
library(dplyr)
library(tidyr)
library(purrr)

set.seed(345)
values <- c(1,2,3,4,5)
prob <- c(.5,.2,.1,.6,.3)
prob <- prob/sum(prob)

n <- 5
n_sim <- 200
clt_dat <- data.frame(sim=1:n_sim,size=n,outcome=sapply(1:n_sim,function(x) sum(sample(values,size=n,prob=prob,replace=TRUE))/n))

if(knitr::is_html_output()) {
clt_dat %>%
  pull(sim) %>% 
    map_df(~ clt_dat %>% filter(sim %in% 1:.x) %>% mutate(sim = .x)) %>% 
  ggplot(aes(x=outcome)) + geom_histogram(binwidth=.2) + xlim(c(1,5)) + ylim(c(0,30)) + transition_manual(sim) + ease_aes("linear") + enter_fade() + exit_fade()
} else {
  clt_dat %>%
  #pull(sim) %>% 
  #  map_df(~ clt_dat %>% filter(sim %in% 1:.x) %>% mutate(sim = .x)) %>% 
  ggplot(aes(x=outcome)) + geom_histogram(binwidth=.2) + xlim(c(1,5)) + ylim(c(0,30))
}

```

<!--
```{r clt-animation-2, dev='png',cache=TRUE, fig.cap="The distribution of the mean of 20 samples from the distribution in Figure XX"}

set.seed(567)
n_sim <- 1000
n <- 20
clt_dat <- data.frame(sim=1:n_sim,size=n,outcome=sapply(1:n_sim,function(x) sum(sample(values,size=n,prob=prob,replace=TRUE))/n))

clt_dat %>%
  pull(sim) %>% 
    map_df(~ clt_dat %>% filter(sim %in% 1:.x) %>% mutate(sim = .x)) %>%
  ggplot(aes(x=outcome)) + geom_histogram(binwidth=.05) + xlim(c(1,5)) + ylim(c(0,75)) + transition_manual(sim) + ease_aes("linear") + enter_fade() + exit_fade()
```
-->

From the Central Limit Theorem, we can conclude that if we focus on the mean of a sufficiently large number of independent observations, we can reasonably assume that the sampling distribution of the mean will be a Normal distribution, even when the distribution from which the actual values were drawn is far from Normal. We can also use this to argue that the $t$-statistic is a valid means to test our hypothesis even when the data is not Normal-distributed. Indeed, simulations show that hypothesis tests with the $t$-statistic are valid for sufficiently large data sets even when the distribution of the observations is far from Normal [e.g. @lumley2002importance].

Finally, the Central Limit Theorem may also be applied to justify the assumption of a Normal distribution, at least for certain variables. For instance, in (cognitive) psychology, we might imagine that processes such as perception and judgement rely on processing and transforming information, and that various processing and transformation steps are subject to many independent sources of noise (e.g., things we might call visual, neural, and memory noise). If these sources of noise combine additively, we could argue that the resulting percept or judgement will follow a Normal distribution.

### Bootstrapping a statistic

The crucial thing to realise is that the assumption that the data is Normal-distributed is used to derive the sampling distribution of the test statistic. The sampling distribution of the test statistic allows us to calculate a $p$-value, and if the true sampling distribution of the statistic is different from what is assumed, then the $p$-value will be wrong. If the data is Normal-distributed, then we are guaranteed that the sampling distribution of the $t$ statistic follows a t-distribution. If the data is not Normal-distributed, the sampling distribution of the $t$ statistic may still be indistinguishable from a t distribution. In that case, the $p$-value would still be highly accurate, and we would not need to worry that the data is not Normal-distributed. In short, what we really want to know is whether the assumed distribution of a test statistic is approximately correct.

This is not something we can know for sure. All we have is a sample from the Data Generating Process; we do not know the true distribution of the data. But we can use this sample to do something similar as I did above to show the Central Limit Theorem in action: we can repeatedly draw samples and check the resulting distribution. In this case, we will treat the sample as what can be termed an _empirical distribution_. We can draw, with replacement, a large number of samples of size $n$ from the data, and then check the distribution of the sample mean, or even better, the distribution of the $t$ statistic. We need to draw samples with replacement (which means that the same observation can occur multiple times in our generated samples) to ensure that we obtain different generated samples. This procedure is also called a **nonparametric bootstrap** [@davison1997bootstrap]. It can be useful for a variety of purposes, including the computation of robust confidence intervals and hypothesis testing. Here, we use it to assess whether the assumed distribution by a parametric test such as the $t$-test seems reasonable.

Figure \@ref(fig:bootstrap-sample-mean-and-t) shows the distribution of the sample mean and the $t$-statistic for a total of 100,0000 bootstrap samples, drawn with replacement from the empirical distribution of the data (Figure \@ref(fig:anchoring-eda-plots)). As these plots indicate, whilst the empirical distribution looks far from Normal, the sampling distribution of the mean does not appear to deviate too far from a Normal distribution. Whilst not perfect, both the histogram and Q-Q plot show that the distribution is approximately Normal. The sampling distribution of the $t$ statistic matches the assumed t distribution somewhat less well, with signs of left-skew. As such, the results of the $t$-test may be somewhat biased. This would be of particular concern when the observed effect was small and the test result close to the border of significance. Given the large magnitude of the $t$-statistic we computed for the actual data, and the very low $p$-value, it doesn't seem likely that these are due to wrongly assuming a t distribution. 

```{r bootstrap-sample-mean-and-t, cache=TRUE, fig.cap="Boostrapped sample means and $t$ statistics, using 100,000 bootstrap samples from the empirical distribution. For both the sample mean and $t$ statistic, we show a histogram with the assumed theoretical distribution overlaid, and a Q-Q plot.", dev='png'}
nsim <- 100000
set.seed(23456)
boot_mean <- rep(0.0,nsim)
boot_t <- rep(0.0,nsim)
s_mean <- mean(dat$everest_meters)
y <- matrix(dat[sample(1:nrow(dat),size=nsim*nrow(dat),replace=TRUE),]$everest_meters,ncol=nsim)
boot_mean <- colMeans(y)
boot_t <- apply(y,2,function(x) t.test(x - s_mean,mu=0)$statistic)

plots <- list()

plots[[1]] <- data.frame(mean=boot_mean) %>%
  ggplot(aes(x=mean)) + geom_histogram(binwidth=30) + stat_function(fun = function(x) 
    dnorm(x, mean = mean(boot_mean), sd = sd(boot_mean)) * 30 * nsim, linetype=2) + ylab("count")

plots[[2]] <- data.frame(mean=boot_mean) %>%
  ggplot(aes(sample = mean)) + stat_qq() + stat_qq_line() + ylab("sample") + xlab("theoretical")

plots[[3]] <- data.frame(t=boot_t) %>%
  ggplot(aes(x=t)) + geom_histogram(binwidth=.1) + stat_function(fun = function(x) 
    dt(x, df = nrow(dat) - 1) * .1 * nsim, linetype=2) + ylab("count")

plots[[4]] <- data.frame(t=boot_t) %>%
  ggplot(aes(sample = t)) + stat_qq(distribution = stats::qt, dparams = list(df = nrow(dat) - 1)) + stat_qq_line(distribution = stats::qt, dparams = list(df = nrow(dat) - 1)) + ylab("sample") + xlab("theoretical")

gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], ncol=2)

```

## In practice

The one-sample t-test is perhaps one of the simplest statistical hypothesis tests. Simplicity is not a bad thing. If you can formulate a hypothesis in terms of an exact value of the true mean should take, that is a sign of an advanced science. Unfortunately, such precise predictions are rare in psychology. But in those occasions where you can use a one-sample t-test, here is some practical advice on how to proceed:

1. __Explore the data__. In particular, check for **outliers**, which in this case are observations very far removed from the sample mean. Remember that in a Normal distribution, values which are more than 3 standard deviations removed from the mean are very rare. Whilst not impossible, such rare observations can severely bias the estimated mean. For example, if one participant in the anchoring experiment would have estimated the height of Mount Everest as one million meters, the sample mean would change rather dramatically. Consequently, the result of a statistical test could be entirely due to a single observation, effectively rendering all other observations in the data meaningless. In such cases, there is good reason to remove these unduly influential observations from the data. But this should always be done with caution. We will discuss outliers in more detail in Section \@ref(sec:04b-outliers).
2. After removing outliers if necessary, assess whether the assumptions underlying the test are reasonable. This is, in my opinion, best done graphically. Plot the distribution of the variable of interest with e.g. a histogram, raincloud and Q-Q plot. Inspect these carefully. What you are looking for is relatively large deviations from a Normal distribution. 
3. Conduct the analysis and report the results. When reporting the results, make sure that you include all relevant statistics. For example, one way to write the results of anchoring analysis is as follows:

> To assess bias in participant's judgements of the height of Mount Everest, we conducted a one-sample $t$-test, against the null hypothesis that the average judgement equals the true height of Mount Everest (i.e. 8848 meters). In our sample of $n=`r nrow(dat)`$ participants, the average judgement was `r papaja::apa_print(t.test(dat$everest_meters,mu=8848))$estimate`. This is a significant difference from the null hypothesis of $\mu = 8848$, `r papaja::apa_print(t.test(dat$everest_meters,mu=8848))$statistic`, with an effect size of Cohen's $d = `r round(effectsize::effectsize(t.test(dat$everest_meters,mu=8848))$Cohens_d,3)`$, which can be considered large.

<!--
## Summary
-->

<!--
http://www.stats.ox.ac.uk/~dlunn/b8_02/b8pdf_8.pdf
http://www.stat.ucla.edu/~hqxu/stat105/pdf/ch09.pdf
https://www.math.arizona.edu/~jwatkins/v-anova.pdf
-->




<!-- 
## The General Linear Model

$$Y_i = \beta_0 + \beta_1 X_{1i} + \ldots + \beta_K X_{Ki} + \epsilon_i \quad \quad \epsilon_i \sim N(0,\sigma^2)$$
-->

<!--chapter:end:03-GLM-simple.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Simple linear regression {#ch-simple-regression}

In this chapter, we continue our journey into the General Linear Model by extending the very simple model of the previous chapter to include a predictor. The resulting model assumes there is a linear relation between the dependent variable and the predictor, and is also known as a simple linear regression model. We will look at the parameters of the model in detail, and discuss their estimation, as well as testing whether their values are equal to a priori ones of interest.

```{r load-trump-data}
library(sdamr)
data("trump2016")
dat <- subset(trump2016,state != "District of Columbia")
```

## Trump, votes, and hate groups

Donald Trump was perhaps the most divisive president in American history. The 2016 US elections were mired in controversy. [Some reports](https://www.theguardian.com/world/2020/mar/18/white-nationalist-hate-groups-southern-poverty-law-center) indicate the number of white nationalist hate groups have gone up by 55% in the Trump era. One study found a strong relation between Trump support and anti-immigrant sentiment, racism, and sexism [@schaffner_understanding_2018]. 

[Paul Ricci](https://www.csiwithoutdeadbodies.com/2017/02/hate-groups-and-trumps-vote-predictive.html) collated data about the number of hate groups in the different US states and votes for Trump. A scatterplot of these two variables is provided in Figure \@ref(fig:scatterplot-trump-votes-hate-groups). You can see that in states where there are relatively little hate groups, there appear to also be relatively little votes for Trump. In states where there are a relatively large number of hate groups, the appear relatively many votes for Trump.

```{r scatterplot-trump-votes-hate-groups,fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens"}
library(ggplot2)

if(knitr::is_html_output()) {
  plotly::ggplotly(ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes,name=state)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump"), tooltip=c("name","x","y"))
} else {
  ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump")
}
```

To assess the relation between Trump votes and the number of hate groups, we will use a __simple linear regression__ model. We are interested in a model which allows us to predict the percentage of votes for Trump from the number of hate groups. In other words, votes for Trump is the dependent variable, and hate groups a __predictor__ variable. A predictot variable is sometimes also referred to as an independent variable. 

As the name suggests, a linear regression model involves a line, a straight one in fact. This straight line represents the average or expected value of the dependent variable $Y$ (i.e. the percentage of Trump votes) over all cases which have a particular value on the predictor (or independent) variable $X$ (i.e. the number of hate groups). For example, we may aim to infer the average percentage of Trump votes in all states with 5.55 hate groups per 1 million citizens, or the average percentage of Trump votes in states with 0 hate groups. The model allows the values of the dependent variable for individual cases to vary around the average. So the percentage of Trump votes for two different states, each with 5.55 hate groups, may differ from the average as well as each other. The model assumes that such variability can be described through a Normal distribution. Importantly, the model assumes that the variance of this normal distribution is the same, no matter the value of on predictor. 

both be different from the average and from each to vary around the average. In particular, it assumes that the deviations between actual and mean percentages are Normal-distributed. 

## The model

In the previous chapter, we used a simple statistical model:
$$Y_i \sim \mathbf{Normal}(\mu,\sigma)$$
This model assumes that each observation is independently drawn from a Normal distribution, with a mean $\mu$ and a standard deviation $\sigma$. We can state this model in an equivalent way as:
\begin{equation}
Y_i = \mu + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma)
(\#eq:simple-model-as-glm)
\end{equation}
In this two-part formulation, the first part (formula on the left side) decomposes each observation $Y_i$ in a _structural part_ (here the mean, $\mu$), and a _random part_ that we usually call the error ($\epsilon_i$). We don't know much about this error. We just assume that is is drawn from a Normal distribution with a mean of 0 and standard deviation $\sigma_\epsilon$. This assumption about the distribution forms the second part (the formula on the right side of Equation \@ref(eq:simple-model-as-glm)).^[If you are wondering why this formation provides the same model as in the previous chapter, you can show this using the properties of the Normal distribution. In the second formulation, you can view $Y$ as a linear transformation of the error $\epsilon$: $Y_i = a + b \epsilon$, with $a = \mu$ and $b=1$. The mean of $Y$ is then $a + \mu_\epsilon = \mu + 0 = \mu$, and the standard deviation of $Y$ equals $\sqrt{|b| \sigma} = 1 \times \sigma = \sigma$, which are the same parameters as when directly specifying the Normal distribution for $Y$.]

A model like this doesn't allow for very precise predictions. As the error term is assumed to be completely random, it is unpredictable. So if you were asked to predict the value of $Y$, all you could really do is to use $\mu$ as your prediction. If, however, the dependent variable (e.g. votes for Trump) is related to a predictor variable (e.g. number of hate groups), then we should be able to use to use the predictor to make more precise predictions. __Linear regression__ allows us to to this in a straightforward way. 

The bivariate regression model is depicted in Figure \@ref(fig:simple-regression-explanation-plot). More formally, the model can be defined as follows:

\begin{equation}
Y_i = \beta_0 + \beta_1 \times X_{i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
(\#eq:bivariate-regression-model-definition)
\end{equation}

Here, $Y_i$ is the value of the $i$-th observation of the dependent variable (with $i = 1, \ldots, n$) and $X_i$ the value of the $i$-th observation of the predictor variable. These are the observable parts of the data. The model contains three important parameters:

* The __intercept__ $\beta_0$, which is the mean of $Y$ when $X=0$. 
* The __slope__ $\beta_1$, which reflects the increase or decrease in the mean of $Y$ for every _1-unit increase_ in the predictor $X$. By a 1-unit increase, we simply mean that the value of $X$ goes up by 1, e.g. from $X=2$ to $X=3$, or from $X=12.63$ to $X=13.63$.
* The __standard deviation__ $\sigma_\epsilon$ of the __error__ or __residual__ terms $\epsilon_i$. The errors are assumed to be drawn independently from the same Normal distribution, with a mean of 0 and standard deviation $\sigma_\epsilon$.

```{r simple-regression-explanation-plot,fig.cap="The simple regression model. $\\textbf{A}$: Simple regression aims to capture the relation between two variables, a dependent variable ($Y$) and a predictor variable ($X$). Each case in a dataset provides a pair of observations on both variables. $\\textbf{B}$: The intercept is the mean of $Y$ when $X=0$ and is the point at which the regression line crosses the y-axis. The slope determines the steepness of the regression line and represents the increase (or decrease) in the mean of $Y$ for every 1-unit increase in the predictor $X$. $\\textbf{C}$: the error terms or residuals are the vertical distances of each observed Y-value from the regression line. $\\textbf{D}$: The errors in a regression model are assumed to follow a Normal distribution around the regression line.",fig.show="hold",fig.width=6,fig.height=6,out.width="100%"}

#layout(matrix(c(1,4,1,3,2,3,2,5),nrow=2))
layout(matrix(c(1,3,2,4),nrow=2))
par(mar=c(4,4,2,2))
x <- seq(-1,5)
y <- seq(-1,5)
xp <- c(-.5,.7,1.4,2.5,3.5,4)
yp <- c(1.2,1,3,4,4.5,3)

plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5), main="A")
points(xp,yp)

plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5), main="B")
abline(a=2,b=.5)
abline(v=0,lty=2)
abline(h=0,lty=2)
arrows(0,0,0,2,lwd=2,code=2,length=0.1)
text(0,.5*2,labels=expression(beta[0]),pos=4)
lines(c(0,1),c(2,2),lty=3)
arrows(1,2,1,.5 + 2,lwd=2,code=2,length=0.1)
text(1,2 + .5*.5,labels=expression(beta[1]),pos=4)
lines(c(1,2),c(2.5,2.5),lty=3)
arrows(2,2.5,2,.5 + 2.5,lwd=2,code=2,length=0.1)
text(2,2.5 + .5*.5,labels=expression(beta[1]),pos=4)
points(xp,yp)

plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5), main="C")
abline(a=2,b=.5)
#abline(v=0,lty=2)
#abline(h=0,lty=2)
for(i in 1:length(xp)) {
  lines(c(xp[i],xp[i]),c(yp[i],.5*xp[i]+2))
  text(xp[i],2 + .5*xp[i] + .5*(yp[i] - (2 + .5*xp[i])),labels=expression(epsilon[i]),pos=4)
}
points(xp,yp)

plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5), main="D")
abline(a=2,b=.5)
#abline(v=0,lty=2)
#abline(h=0,lty=2)
#points(xp,yp)
yt <- seq(0,6,length=100)
xt <- c(0,.75*dnorm(yt,mean=3,sd=.5),0)
yt <- c(0,yt,6)
polygon(-1+xt,-1.5+yt,col=rgb(0,0,0,alpha=.5))
polygon(0+xt,-1+yt,col=rgb(0,0,0,alpha=.5))
polygon(1+xt,-.5+yt,col=rgb(0,0,0,alpha=.5))
polygon(2+xt,yt,col=rgb(0,0,0,alpha=.5))
polygon(3+xt,.5+yt,col=rgb(0,0,0,alpha=.5))
polygon(4+xt,1+yt,col=rgb(0,0,0,alpha=.5))
polygon(5+xt,1.5+yt,col=rgb(0,0,0,alpha=.5))
```

Remember, unlike variables, parameters are _not_ observable. We can infer their value from the data by estimation and/or performing hypothesis tests. But we can never be completely sure that such inferences are correct. Let's consider the structural part of the simple regression model again:
$$Y_i = \beta_0 + \beta_1 \times X_{i} + \epsilon_i$$
This formula involves both observable variables ($Y$ and $X$) and unobservable parameters ($\beta_0$, $\beta_1$, and $\epsilon_i$). Indeed, the errors $\epsilon_i$ are really parameters as well, because they cannot be directly observed. There are many of these (one for each observation $i = 1, \ldots, n$), and they are not of primary concern. The other part, $\beta_0 + \beta_1 \times X_i$, determines the regression line, representing the average value of $Y$ for all cases with a particular value on the predictor variable $X$. These average values are __conditional means__, they are the mean of $Y$, conditional upon a particular value of the predictor $X$. "Conditional upon" here means that we only consider cases with a particular value on the predictor variable. For instance, we may consider all states with 5.55 hate groups. The conditional mean is then just the average of the dependent variable (votes for Trump) in this particular group of states. We can use $\mu_{Y|X}$ to denote the mean of $Y$ conditional upon $X$. If we then use $\mu_{Y|X_i}$ to denote the mean of $Y$ conditional upon $X$ having the value $X_i$ (i.e. the value of $X$ for case $i$ in the dataset), we can define the conditional mean in terms of the regression model as:
\begin{equation}
\mu_{Y|X_{i}} = \beta_0 + \beta_1 \times X_{i}
(\#eq:conditional-mean-simple-regression)
\end{equation}

Now, plugging the conditional mean into Equation \@ref(eq:bivariate-regression-model-definition) we get
$$Y_i = \mu_{Y|X_{i}} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
which, in many respects, is very similar to the simple model of Equation \@ref(eq:simple-model-as-glm). The key difference is the use of a conditional mean $\mu_{Y|X_{i}}$ instead of an unconditional (constant) mean $\mu$. Effectively, within each group of cases with the same value on the predictor variable $X$ (e.g. all states with 5.55 hate groups), there is just a single conditional mean. Within such a group, the model is equivalent to the simple model of  model simple model of Equation \@ref(eq:simple-model-as-glm). What is new here is that we now also take into account differences between different groups. In particular, we assume that these groups differ just in the conditional mean $\mu_{Y|X}$, whilst the standard deviation of the errors ($\sigma_\epsilon$) is the same for all groups. We finally assume that there is a simple, linear relation between the value of $X$ and the conditional mean $\mu_{Y|X}$. So the regression model extends the simple model of the previous chapter by using a straight line to represent the relation between the (conditional) mean of the dependent variable and a predictor variable. <!-- In fact, the model of the previous chapter is a special case of the regression model. -->

<!--
If you were asked to predict the percentage of Trump votes for a state with for example 5.55 hate groups, the conditional mean of the percentage Trump votes for all states with 5.55 hate groups would be a good prediction, because without knowing anything else, it will be impossible to predict whether actual the votes for Trump will be above or below the mean for a given state. Using $\hat{Y}$ to denote the predicted value of the dependent variable, we can thus write:
$$\hat{Y}_i = \beta_0 + \beta_1 \times X_{i}$$

$$Y_i = \hat{Y}_i + \epsilon_i$$
This last equation can be written in words as 
$$\text{data} = \text{prediction} + \text{error}$$
This is just a mathematical truism, you can _always_ make this statement, because the error is simply the difference between the observed values (the data) and the predicted values. But perhaps it helps in thinking about what error is. Moreover, if you look at Equation \@(eq:bivariate-regression-model-definition) again, you might notice that the probabilistic part of the model (i.e., the part involving probability distributions) really only concerns the errors $\epsilon_i$. These are, as already indicated, assumed to be independently drawn from a Normal distribution with a mean of 0 and a standard deviation $\sigma$:
$$\epsilon_i \sim \mathbf{Normal}(0,\sigma)$$
-->

## Estimation {#sec:04-estimation}

It can be shown that for the model specified in Equation \@ref(eq:bivariate-regression-model-definition), the maximum likelihood estimates of the model parameters are
\begin{equation} 
\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \times \overline{X}
(\#eq:simple-regression-intercept-estimate)
\end{equation}
for the intercept, and
\begin{equation}
\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}
(\#eq:simple-regression-slope-estimate)
\end{equation}
for the slope. Note that to estimate the intercept, we first need the estimate of the slope. So let's focus on
this one first. The top part of the division (the numerator) contains a sum of 
deviations of the predictor values ($X_i$) from its average ($\overline{X}$) multiplied by deviations
of the values $Y_i$ of the dependent variable from its average ($\overline{Y}$). Let's consider these
multiplied deviations in more detail. Each deviation is positive (larger than 0) when the value is 
higher than the average, and negative (smaller than 0), when the value is lower than the average.
So the multiplied deviations are positive whenever both values are larger than their average, and 
whenever both values are below their average (a negative value multiplied by another negative value 
is positive). If we were to divide the sum of the multiplied deviations by $n$ (the number of observations), 
we'd get the average of these multiplied deviations. This average is also called the __covariance__ between 
$X$ and $Y$: 
$$\text{Cov}(X,Y) =  \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{n}$$
Note that, as an estimator, this provides biased estimates of the true covariance. An unbiased estimator
of the true covariance is obtained by dividing by $n-1$ instead of $n$ (just as for the variance). Going back to our example, the covariance between Trump votes and hate groups would be positive whenever states with higher-than-average Trump votes are generally also states with higher-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally also states with lower-than-average hate groups. 
The covariance would be negative, on the other hand, whenever states with higher-than-average Trump votes are generally states with lower-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally states with higher-than-average hate groups. A positive or negative covariance is indicative of a relation between $X$ and $Y$. Indeed, the well-known
Pearson correlation coefficient is a standardized covariance, where the standardization scales the correlation
to always be between -1 and 1 and involves dividing the covariance by the product of the standard deviations
of both variables:
\begin{align}
r_{X,Y} &= \frac{\text{Cov}(X,Y)}{S_X \times S_Y} \\
&= \frac{\frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{n}}{\sqrt{\frac{\sum_{i=1}^n (X_i - \overline{X})^2}{n}} \times \sqrt{\frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}}}
\end{align}

Going back to the estimate of the slope (Equation \@ref(eq:simple-regression-slope-estimate)), we can view this as a different way of standardizing the covariance. Looking at the bottom part of the division (the denominator), we can see it consists of the sum of squared deviations of the predictor values from its mean. If we were to divide this sum by the number of observations, we'd get the variance of $X$. As dividing both the top part (numerator) and bottom part (denominator) in a division by the same value does not affect the outcome of the division (i.e. $\frac{a}{b} = \frac{a/c}{b/c}$), we can choose to divide both by $n$ so the numerator becomes the covariance and the denominator the variance (we could also divide both by $n-1$ so they become unbiased estimators of the covariance and variance). So an alternative way of computing the estimate of the slope is
$$\hat{\beta}_1 = \frac{\text{Cov}(X,Y)}{S^2_X}$$
Note that the variance of $X$ equals the product of the standard deviation of $X$ and itself, as $S^2_X = S_X \times S_X$. So the slope estimate looks quite a bit like the correlation coefficient, where instead of the standard deviation of $Y$, we use the standard deviation of $X$ twice. With a little algebraic manipulation, we can also state the slope estimate in terms of the correlation as 
$$\hat{\beta}_1 = \frac{S_Y}{S_X} r_{X,Y}$$
The reason for going into these alternative formulations is to show you that the slope tells us something about the relation between $X$ and $Y$, just like the covariance and correlation do. If the sample correlation is 0, then so is the estimated slope. It is important to realise that we have been discussing the _estimate_ of the slope, not the true value itself. But the same relations hold for the true values. If we denote the true correlation as $\rho_{X,Y}$, then the true value of the slope can be defined as
$$\beta_1 = \frac{\sigma_Y}{\sigma_X} \rho_{X,Y}$$
The true value of the slope is 0 when the true correlation between $X$ and $Y$ equals $\rho_{X,Y} = 0$. It would also be 0 if the true standard deviation of $Y$ equals $\sigma_Y = 0$, but this implies that $Y$ is a constant and that is not a very interesting situation.

That was perhaps a little tortuous, and we haven't even discussed the estimate of the intercept! Remember that the intercept represents the mean value of $Y$ for all cases where $X=0$. This average is often not so interesting itself, although in our example, we might be interested in what the average percentage of votes for Trump would be in the absence of any hate groups. Equation \@ref(eq:simple-regression-intercept-estimate) shows that we can estimate this value by adjusting the sample average $\overline{Y}$ by subtracting $\hat{\beta}_1 \times \overline{X}$ from it. How come? Well, it can be shown that __the regression line always passes through the point $(\overline{X},\overline{Y})$__. The derivation for showing that this has to be the case is not that interesting, so you'll just have to trust me, or look it up elsewhere. But this implies that $$\overline{Y} = \beta_0 + \beta_1 \times \overline{X}$$
and then we can simply subtract $\beta_1 \times \overline{X}$ from both sides to get $\overline{Y} - \beta_1 \times \overline{X} = \beta_0$.

Finally, we can also estimate $\sigma_\epsilon$, the standard deviation of the error. As usual for variances and standard deviations, the maximum likelihood estimate is biased, so we'll focus on an unbiased estimator. Like before, an estimate of the variance is computed from a sum of squared deviations from an estimated mean. In this case, we need to use the estimated _conditional_ means
$$\hat{\mu}_{Y|X_i} = \hat{\beta}_0 + \hat{\beta}_1 \times X_i$$
to compute the following estimate of the error variance:
\begin{equation}
\hat{\sigma}^2_\epsilon = \frac{\sum_{i=1}^n (Y_i - \hat{\mu}_{Y|X_i})^2}{n-2}
(\#eq:simple-regression-sd-error-variance-estimate)
\end{equation}
Note that we are dividing by $n-2$ here, rather than by $n-1$ as we did when estimating the variance of $Y$ in a model without predictors. The reason for this is that we are now using two noisy parameter estimates (i.e. $\hat{\beta}_0$ and $\hat{\beta}_1$), rather than just one. As usual, to get the estimate of $\hat{\sigma}_\epsilon$, we can just take the square root of $\hat{\sigma}^2_\epsilon$.

### Estimating the relation between Trump votes and hate groups

So, what are the estimates of the model predicting Trump votes by hate groups? We can calculate the estimates relatively easily by first computing the sample means of predictor and dependent variable, their sample standard deviations, and the covariance between the predictor and dependent variable:

```{r}
tab <- data.frame(a=mean(dat$hate_groups_per_million), b=mean(dat$percent_Trump_votes), c=(nrow(dat)-1)/nrow(dat)*var(dat$hate_groups_per_million), d = (nrow(dat)-1)/nrow(dat)*var(dat$percent_Trump_votes), e = (nrow(dat)-1)/nrow(dat)*cov(dat$hate_groups_per_million, dat$percent_Trump_votes))
colnames(tab) <- c("$\\overline{\\mathtt{hate}}$", "$\\overline{\\mathtt{votes}}$", "$S^2_\\mathtt{hate}$", "$S^2_\\mathtt{votes}$", "$\\text{Cov}(\\mathtt{hate},\\mathtt{votes})$")
knitr::kable(tab, row.names = FALSE, escape = FALSE, digits=6, booktabs = TRUE, linesep="")
```

The estimate of the slope is then
$$\hat{\beta}_1 = \frac{`r tab[,5]`}{`r tab[,3]`} = `r tab[,5]/tab[,3]`$$
and the estimate of the intercept is 
$$\hat{\beta}_0 = `r tab[,2]` - `r tab[,5]/tab[,3]` \times `r tab[1]` = `r tab[,2] - tab[,5]/tab[,3] * tab[,1]`$$
```{r estimate-simple-regression-trump2016}
mod <- lm(percent_Trump_votes ~ hate_groups_per_million,data=dat)
```

Of course, we would not normally bother with calculating these estimates "hy hand" in this way. We would rely on statistical software such as R or JASP to calculate the estimates for us. In any case, we can write the  estimated model as:

$$\texttt{votes}_i = `r round(coef(mod)[1],3)` + `r round(coef(mod)[2],3)` \times \texttt{hate}_i + e_i$$
According to this model, the average percentage of Trump votes in states without any hate groups is `r round(coef(mod)[1],3)`. For every one additional hate group (per million citizens), the percentage of Trump votes increases by `r round(coef(mod)[2],3)`. You can view the resulting regression line (the conditional means of votes for Trump as predicted by the model) in Figure \@ref(fig:scatterplot-trump-votes-hate-groups-with-regression).

```{r scatterplot-trump-votes-hate-groups-with-regression, fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens with the estimated regression line."}
library(ggplot2)

if(knitr::is_html_output()) {
  plotly::ggplotly(ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes,name=state)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1]), tooltip=c("name","x","y"))
} else {
  ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1])
}
```

## Hypothesis testing

The estimated model indicates a positive relation between the number of hate groups and votes for Donald Trump. But the slope is an _estimated_ parameter. It might be that, in reality, there is no relation between hate groups and Trump votes. But because we have limited and noisy data, sometimes the estimated slope might be positive, and other times it might be negative. So the question is whether we have enough evidence to reject the null hypothesis that $\beta_1 = 0$.

As before, there are two main ways in which to look at testing whether the parameters of the model are equal to a priori values. The first is to consider how variable the parameter estimates are under the assumption that the true parameter is identical to the a priori value. The second way is to compare two models, one in which the parameter is fixed to the a priori value, and one where it is freely estimated. Both of these ways will provide us with the same outcome. The model comparison approach is more flexible, however, as it also allows you to test multiple parameters simultaneously. We'll start with discussing the first method, but it will be the last time until a while. After that, we'll focus on model comparison throughout. 

### Sampling distribution of estimates

Remember, an estimate (whether of the mean or another parameter) is a noisy reflection of the true value of that parameter. The noise comes from having access only to limited data, not _all_ the data the data generating process can produce. Suppose in reality there is no relation between hate groups and Trump votes, so that the true slope is $\beta_1 = 0$. In that case, the model becomes
\begin{align}
Y_i &= \beta_0 + 0 \times X_i + \epsilon \\
&= \beta_0 + \epsilon_i
\end{align}
which is identical to the simple model in Equation, renaming $\beta_0 = \mu$ and $\sigma_\epsilon = \sigma$. Then, the true value of the intercept would be $\beta_0 = \mu$. If we'd know the true value of the standard deviation $\sigma_\epsilon$, we'd have a fully specified model (a Normal distribution) which we can use to generate as many alternative data sets as we'd like. By generating these data sets, and estimating the slope of our model for each, we can get an overview of the variability of the estimates when the true slope equals $\beta_1 = 0$. An unbiased estimator ensures that on average, these estimates equal the true value. The main thing of interest is then the variability around this value. A consistent estimator ensures that this variability becomes smaller with the larger data sets. But to understand how well we can estimate the parameter for the present data, we would simulate data sets with the same number of observations (so $n=`r nrow(dat)`$). Unfortunately, we don't know the true value of $\sigma$. Our data provides an estimate of $\sigma$, but we know this estimate is noisy itself. Thinking in the same way about the sampling distribution of $\hat{\sigma}$, we could first sample a values of $\sigma$, and then use each of these to generate a data set for which to estimate $\beta_1$. Doing this many (many many!) times would give us a good overview of the variability of the estimates. As before, we don't have to actually simulate the data sets. If the model corresponding to the null hypothesis is true, then we can derive that the sampling distribution of the estimates follows a t-distribution. Thus, we should look for the $t$ value our data provides, and evaluate this value within the context of the sampling distribution derived under the model where $\beta_1 = 0$. For both parameters (intercept and slope), the same logic applies. 

Actually, the same procedure can be used to test the hypothesis that a parameter takes any given value, i.e. $H_0: \beta_j = \underline{\beta}_j$. Often, the chosen null-value is $\underline{\beta}_j = 0$, but that does not need to be the case. In general then, for parameters $\beta_j$ (where $j = 0$ or 1), the $t$-statistic to test the null-hypothesis $H_0: \beta_j = \underline{\beta}_j$ is computed as 
\begin{equation}
t = \frac{\hat{\beta}_j - \underline{\beta}_j}{\text{SE}(\hat{\beta}_j)} \quad \quad \quad t \sim \mathbf{T}(n-2)
\end{equation}
where $\mathbf{T}(n-2)$ denotes a standard Student t-distribution with $n-2$ degrees of freedom, and $\text{SE}(\hat{\beta}_j)$ is the standard error of the estimate, which you should remember is the standard deviation of the sampling distribution of the estimates. I won't bore you with how to compute this standard error; the equations aren't overly insightful, and statistical software does a good job at computing standard errors. One thing to realise though is that _the computed standard error is valid for datasets with exactly the same values on the predictor_. In other words, it assumes the predictor values are fixed (a part of the Data Generating Process). If you'd collect a different dataset with different values for e.g. the number of hate groups, the standard error would also be different.

R would for instance provide the following results for this regression model:
```{r simple-regression-trump2016-model-summary}
knitr::kable(summary(mod)$coefficients, digits=c(3,3,3,3), booktabs=TRUE, linesep="")
```
The values listed for `(intercept)` concern $\beta_0$, and the values listed for `hate_groups_per_million` concern the slope $\beta_1$. You can see the parameter estimates are identical to those computed earlier (phew, no mistakes there!). You can see that the standard errors for the two parameters are quite different in magnitude. This is not so surprising, as they reflect quite different things, the intercept being a particular value of the dependent variable, while the slope represents an increase in the dependent variable for an increase in the predictor. If you'd change the scale of the predictor (e.g., from hate groups per million to hate groups per 10,000 citizens), the slope would change, as well as the corresponding standard error. Changing the scale in this way would not affect the standard error of the intercept. The values of the $t$-statistic are those for tests where the null-hypotheses are $\beta_0 = 0$ and $\beta_1 = 0$. These test statistics are obtained by simply dividing each estimate by the corresponding standard error (for a different test value $\underline{\beta}_j \neq 0$, you would first subtract this value from the estimate, and then divide this difference by the standard error. <!--) , you obtain the value of the $t$-statistic (just like we did earlier in Equation \@ref(eq:definition-one-sample-t)).--> 

In a manner analogous to that depicted in Figure \@ref(fig:critical-values-t-distribution), you then determine critical values for the $t$-statistic (based on the degrees of freedom and the significance level $\alpha$), and determine whether the value you computed for the parameters lies within the critical range. If it does, the test result is called significant, and you reject the null hypothesis. If not, then the result is called not significant, and you don't reject the null hypothesis. Now that these are easy to compute with statistical software, it is more common to check the $p$-values. Remember, the $p$-value is the probability of the computed $t$-statistic _or a more extreme value_, assuming the null hypothesis is true. If the $p$-value is smaller than the chosen significance level (e.g. $\alpha = .05$), that means that the test is significant, and the null hypothesis rejected. In the results above, you can see that both $p$-values are below the significance level $\alpha = .05$. Hence, we can reject the null hypothesis that $\beta_0 = 0$ and the null hypothesis that $\beta_1 = 0$. In other words, there is good evidence that, in the absence of hate groups, the percentage of people voting for Trump is _not_ equal to 0.^[It's probably unwise to mix statistics and politics, but wouldn't that have been an utopia?] Also, there is good evidence that there is a relation between the number of hate groups and Trump votes.

Personally, I find the $t$ statistic quite intuitive in the context of a one-sample t-test. Generalizing the concept to a standardized estimate (dividing the estimate by its standard error) is also reasonably intuitive. However, in a multi-parameter model such as here, the sampling distribution of a single parameter is dependent on the estimation of all the other parameters. For instance, the test of the null hypothesis $H_0: \beta_1 = 0$ (i.e., no effect of hate groups on Trump votes) is based on deriving the sampling distribution of $\hat{\beta}_1$ in a model where $\beta_1 = \underline{\beta}_1$, but all the other parameters (i.e. $\beta_0$ and $\sigma_\epsilon$) are not assumed known. Hence, for each possible sample, these values would need to be estimated. This uncertainty is dealt with similarly as before, resulting in a $t$-distribution, but now there are two sources of uncertainty (two parameters to estimate). Hence, the degrees of freedom are $n-2$ here.

### Model comparison

The fact that, in a multi-parameter model, a test of one parameter is _not_ conducted in isolation, but rather in the context of all the other parameters in the model, is more explicit in the model comparison approach. When we compare two models, we have to make clear what the parameters are in each: what are the unknown quantities which we will have to estimate, and what are quantities which we can assume a precise value for? 

As before, we will consider comparing _nested models_, in which a restricted MODEL R is a _special case_ of a more general MODEL G. Sticking to simple linear regression models, the most general model we have is that of Equation \@ref(eq:bivariate-regression-model-definition). 

Just like for the simple model of the previous chapter, it turns out we can compute the likelihood of a model as a function of a _sum of squared deviations_. We will not go through the derivation of this again, as it is rather similar and equally tedious. But, because we are now calling these deviations _errors_, will start to refer to them as a __Sum of Squared Errors__ (SSE). The SSE is an overall measure of model error, whilst the likelihood is an overall measure of model fit. The SSE is inversely related to the likelihood: the higher the SSE, the lower the likelihood of the model. 

Let's call the model of Equation \@ref(eq:bivariate-regression-model-definition) MODEL G. Before we go on, I want to warn you that what we call MODEL G, and what we call MODEL R, can change from situation to situation. Basically, the identity of MODEL G and MODEL R are "local" to the particular model comparison. You can think of MODEL G as a parent, and MODEL R as a child. While the relation between them is similar, within an extended family, someone can be both a parent to one family member, and be the child of another. This is the form of flexibility that you will need when thinking about nested models. A model can be both more general than one model, and more restricted than another. I will come back to this soon.

First, let's consider what the Sum of Squared Errors of a model is. The easiest way to define this is in terms of the __predictions__ of each model. Recall that a regression model has a structural and random part. The structural part defines the conditional mean of the dependent variable, while the random part concerns the random variation of the actual observations around the conditional means. As the random part is by definition unpredictable, there is not much we can do with that in terms of forming predictions. So we're stuck with the structural part. Although not a universal principle, there are many situations in which it makes sense to predict outcomes by the (conditional) mean. If we do this, then we can predict the outcomes with MODEL G as

$$\hat{Y}_{G,i} = \hat{\beta}_{G,0} + \hat{\beta}_{G,1} \times X_i$$
where $\hat{Y}$ stands for a predicted value. We're using the same "hat" for this as an _estimated value_, because it is really also a good estimate of what the value of $Y_i$ might have been if it was another observation with the same value for $X$. We are also assigning the subscript "G" to all the estimates, to distinguish them from those of a different model. Now let's consider a MODEL R in which we assume that there is no relation between Trump votes and hate groups, so that $\beta_1 = 0$. The predictions for this model would be

$$\hat{Y}_{R,i} = \hat{\beta}_{R,0} + 0 \times X_i = \hat{\beta}_{R,0}$$
Now we have two models to make predictions, we can write the corresponding Sum of Squared Error of each as:
\begin{equation}
\text{SSE}(M) = \sum_{i=1}^n \left(Y_i - \hat{Y}_{M,i} \right)^2
(\#eq:SSE-GLM-general)
\end{equation}
where we can replace the general letter $M$ (for Model) with either $G$ of $R$, to get $\text{SSE}(G)$ or $\text{SSE}(R)$, respectively. So the Sum of Squares is based on a difference between each observation and the model prediction for that observation. These are thus the _prediction errors_. If the prediction was equal to the the prediction of the true model (e.g. $\beta_0 + \beta_1 X_{i}$), then these would be equal to the true error terms $\epsilon_i$. But because we only have an estimated model, they are effectively estimates of the true errors. You can see these (unsquared) errors for the two models in Figure \@ref(fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison). 

```{r scatterplot-trump-votes-hate-groups-with-regression-model-comparison, fig.cap="Estimated regression lines for MODEL G (left) and MODEL R (right) with $\\beta_1 = 0$ and the errors.", fig.width=8, fig.height=4, out.width="100%"}
library(ggplot2)
library(dplyr)

mod0 <- lm(percent_Trump_votes ~ 1, data=dat)

ldat <- data.frame(model = rep(c("MODEL G","MODEL R"), each = 2), hate_groups_per_million = rep(c(min(dat$hate_groups_per_million),max(dat$hate_groups_per_million)),2), percent_Trump_votes = c(coef(mod)[1] + coef(mod)[2]*min(dat$hate_groups_per_million), coef(mod)[1] + coef(mod)[2]*max(dat$hate_groups_per_million), coef(mod0)[1], coef(mod0)[1]), ymin=50,ymax=50)

rbind(dat %>% mutate(model = "MODEL G", ymin = if_else(percent_Trump_votes > predict(mod), predict(mod), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod), predict(mod), percent_Trump_votes)),
      dat %>% mutate(model = "MODEL R", ymin = if_else(percent_Trump_votes > predict(mod0), predict(mod0), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod0), predict(mod0), percent_Trump_votes))) %>%
        ggplot(aes(x=hate_groups_per_million,y=percent_Trump_votes, ymin = ymin, ymax = ymax)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_line(data=ldat) + geom_linerange(alpha=.5) + facet_wrap(~model)

# 
# dat %>% mutate(ymin = if_else(percent_Trump_votes > predict(mod), predict(mod), percent_Trump_votes),
#                ymax = if_else(percent_Trump_votes < predict(mod), predict(mod), percent_Trump_votes)) %>%
#   ggplot(aes(x=hate_groups_per_million,y=percent_Trump_votes, ymin = ymin, ymax = ymax)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1]) + geom_linerange(alpha=.5) + ggtitle("MODEL G")
#            
# 
# mod0 <- lm(percent_Trump_votes ~ 1, data=dat)
# dat %>% mutate(ymin = if_else(percent_Trump_votes > predict(mod0), predict(mod0), percent_Trump_votes),
#                ymax = if_else(percent_Trump_votes < predict(mod0), predict(mod0), percent_Trump_votes)) %>%
#   ggplot(aes(x=hate_groups_per_million,y=percent_Trump_votes, ymin = ymin, ymax = ymax)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_abline(slope = 0, intercept = coef(mod0)[1]) + geom_linerange(alpha=.5) + ggtitle("MODEL R")

```

When you compare the errors between the models, you can see that MODEL G does not provide a better prediction for _each_ observation. Sometimes the distance from an observation to the regression line is larger for MODEL G than for MODEL R. However, MODEL G does appear to provide a better prediction for _most_ observations. This is unsurprising. MODEL G is estimated by maximising the likelihood, and for models with Normal-distributed errors, maximising the likelihood is equivalent to minimising the Sum of Squared Errors. As the likelihood of MODEL G can never be lower than that of MODEL R, the Sum of Squared Errors of MODEL G can never be higher than that of MODEL R. In other words, it is _always the case that_
$$\text{SSE}(R) \geq \text{SSE}(G)$$
The Sum of Squared Errors of MODEL G will never be higher than that of MODEL R. So we cannot just select the model with the lowest SSE, as this would mean we'd always select MODEL G. We need to find a way to determine whether $\text{SSE}(G)$ is sufficiently lower than $\text{SSE}(R)$ to make us believe that MODEL G is indeed superior to MODEL R.

In the previous chapter, we discussed that the $t$-statistic can be used to perform a test which is equivalent to the likelihood ratio test, and that this was useful because the sampling distribution of the $t$-statistic is known, while the sampling distribution of the likelihood ratio is difficult to determine. For the general linear models, there is a similar argument that leads to a new statistic, which can be viewed as a generalization of the $t$-statistic. This is the $F$-statistic. If was given the letter in honour of Sir Ronald A. Fisher, a rather brilliant and very influential statistician, who derived the statistic in the 1920s.

For comparing two linear models, where MODEL R is a special case of MODEL G, we can define the $F$-statistic as:
\begin{equation}
F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar(G)}}}
(\#eq:glm-f-definition)
\end{equation}
Here, $\text{npar}(G)$ stands for the number of parameters which are estimated in MODEL G, and $\text{npar}(R)$ for the number of parameters which are estimated in MODEL R. In counting the number of estimated parameters, we are excluding the standard deviation of the errors $\sigma_\epsilon$.^[Although $\sigma_\epsilon$ is really a parameter in the models, it is generally not of direct interest. It is a so-called "nuisance parameter", something that we need to take into account, but we'd rather forget about.] In the present example, MODEL G then has two estimated parameters: $\hat{\beta}_{G,0}$ and $\hat{\beta}_{G,1}$, while MODEL R has just one: $\hat{\beta}_{R,0}$. If we compute the SSE for each model (i.e. taking the vertical lines to the regression line in Figure \@ref(fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison) and squaring them), we get $\text{SSE}(G) = `r sum(residuals(mod)^2)`$ and $\text{SSE}(R) = `r sum(residuals(mod0)^2)`$. Putting the values into Equation \@ref(eq:glm-f-definition) gives as the following value for the $F$-statistic:

$$\begin{aligned}
F &= \frac{\frac{`r sum(residuals(mod0)^2)` -  `r sum(residuals(mod)^2)`}{`r length(coef(mod))` - `r length(coef(mod0))`}}{\frac{`r sum(residuals(mod)^2)`}{`r nrow(dat)` - `r length(coef(mod))`}} \\
&= \frac{`r  (sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0)))`}{`r sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))`} \\
&= `r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))`
\end{aligned}$$

If MODEL R is true, the sampling distribution of the $F$-statistic follows an $F$-distribution. This distribution has two parameters, and both are degrees of freedom. We will refer to these as $\text{df}_1$ and $\text{df}_2$. The first one reflects the difference in the number of estimated parameters between the models
\begin{equation}
\text{df}_1 = \text{npar}(G) - \text{npar}(R)
(\#eq:glm-df1-general)
\end{equation}
and the second one the number of observations minus the number of estimated parameters of the more general model
\begin{equation}
\text{df}_2 = n - \text{npar}(G)
(\#eq:glm-df2-general)
\end{equation}
```{r f-distribution-plot, fig.cap="The F distribution with $\\text{df}_1 = 1$ and $\\text{df}_2 = 48$, and the critical value for $\\alpha = .05$", out.width="70%", fig.width=6, fig.height=.75*6}
xlim <- c(0,5)
data.frame(interval=1,x=c(qf(.95,1,48), seq(qf(.95,1,48),xlim[2],100),xlim[2]), y = c(0,df( seq(qf(.95,1,48),xlim[2],100),1,48),0)) %>%
  ggplot(aes(x=x,y=y,fill=factor(interval,labels=""))) + stat_function(aes(fill=NULL),fun=function(x) df(x,1,48), n=500) + xlim(xlim) + ylab("p(F)") + xlab("F") + geom_polygon(alpha=.8) + theme(legend.position="none") + geom_vline(xintercept = qf(.95, 1, 48), lty=2) + annotate("text", x=xlim[2]-.5, xmax=xlim[2], xmin=qf(.95,1,48),y=2,hjust = .5, vjust=1,label="upper \n critical region") + ylim(c(0,3))
#ggplot() + stat_function(fun=function(x) df(x,1,48), n=500) + xlim(c(0,5)) + xlab("F") + ylab("p(F)") + geom_vline(xintercept = qf(.95, 1, 48), lty=2)
```

Note that the value of the $F$-statistic can never be negative; the distribution is thus defined only over positive values of $F$. High values of $F$ indicate that the MODEL G has substantially less error than MODEL R. Another thing to note is that, __whenever $\text{df}_1 = 1$, there is a direct relation between the $F$ statistic and the $t$ statistic__: $F = t^2$, or conversely, $\sqrt{F} = t$. It is easy to check that this is indeed the case here: $\sqrt{`r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))`} = `r  sqrt(((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))))`$, which is the value of the $t$ statistic we computed earlier.

The critical value of the $F$ statistic, with $\alpha = .05$ and $\text{df}_1 = 2-1 = 1$ and $\text{df}_2 = 50 - 2 = 48$, is `r qf(.95,1,48)`, and any value of $F$ above this critical value would result in a rejection of the null hypothesis. So, because the $F$ value we computed was `r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))`, which is larger than the critical value, we reject the null hypothesis $H_0: \beta_1 = 0$, and conclude that there is evidence of a relation between the number of hate groups and Trump votes. Instead of checking whether the $F$ value is larger than a critical value, we can also compute the $p$-value, which is the probability of an $F$ value equal to the sample value `r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))` in the distribution depicted in Figure \@ref(fig:f-distribution-plot). The $p$-value of this test can be stated as $P(F \geq `r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))` | \text{df}_1 = 1, \text{df}_2 = 48) =  `r format(1-pf(((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))),1,48),digits=3)`$. As this probability is smaller than the significance level $\alpha = .05$, this again implies we reject the null hypothesis.

```{r}
mod00 <- lm(percent_Trump_votes ~ hate_groups_per_million - 1, data=dat)
```
We can also compare MODEL G to a different MODEL R, in which we assume the intercept equals $\beta_0 = 0$, whilst allowing the slope to take any value. This model forces the regression line to go through the (0,0) point. Estimating this MODEL R gives^[Note that we cannot use Equation \@ref(eq:simple-regression-slope-estimate) to estimate the slope now. There is no simple formula for the slope estimate when you fix the intercept to 0, and so you will have to rely on statistical software to do this.]
$$\begin{aligned}
Y_i &= 0 + \hat{\beta}_{R,1} \times X_i \\
&= `r coef(mod00)[1]` \times X_i
\end{aligned}$$
You can see the resulting regression line and error terms in Figure \@ref(fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2). 
```{r scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2, fig.cap="Estimated regression lines for MODEL G (left) and MODEL R (right) with $\\beta_1 = 0$ and the errors.", fig.width=8, fig.height=4, out.width="100%"}
library(ggplot2)
library(dplyr)

ldat <- data.frame(model = rep(c("MODEL G","MODEL R"), each = 2), hate_groups_per_million = rep(c(min(dat$hate_groups_per_million),max(dat$hate_groups_per_million)),2), percent_Trump_votes = c(coef(mod)[1] + coef(mod)[2]*min(dat$hate_groups_per_million), coef(mod)[1] + coef(mod)[2]*max(dat$hate_groups_per_million), coef(mod00)[1]*min(dat$hate_groups_per_million), coef(mod00)[1]*max(dat$hate_groups_per_million)), ymin=50,ymax=50)

rbind(dat %>% mutate(model = "MODEL G", ymin = if_else(percent_Trump_votes > predict(mod), predict(mod), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod), predict(mod), percent_Trump_votes)),
      dat %>% mutate(model = "MODEL R", ymin = if_else(percent_Trump_votes > predict(mod00), predict(mod00), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod00), predict(mod00), percent_Trump_votes))) %>%
        ggplot(aes(x=hate_groups_per_million,y=percent_Trump_votes, ymin = ymin, ymax = ymax)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_line(data=ldat) + geom_linerange(alpha=.5) + facet_wrap(~model)
```
Visually, this alternative MODEL R seems clearly inferior to MODEL G. The SSE of this model is $\text{SSE}(R) = `r print(sum(residuals(mod00)^2),digits=7)`$. Computing the $F$-statistic gives
$$
\begin{aligned}
F &= \frac{\frac{`r print(sum(residuals(mod00)^2),digits=7)` -  `r sum(residuals(mod)^2)`}{`r length(coef(mod))` - `r length(coef(mod00))`}}{\frac{`r sum(residuals(mod)^2)`}{`r nrow(dat)` - `r length(coef(mod))`}} \\
&= \frac{`r  print((sum(residuals(mod00)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod00))), digits=7)`}{`r sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))`} \\
&= `r  ((sum(residuals(mod00)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod00))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))`
\end{aligned}
$$
Because the test involves the same degrees of freedom, the critical value is the same as before. So we reject the null hypothesis again, which here is $H_0: \beta_0 = 0$. The $p$-value now is $P(F \geq `r  ((sum(residuals(mod00)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod00))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))` | \text{df}_1 = 1, \text{df}_2 = 48) `r pvalue(1-pf(((sum(residuals(mod00)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod00))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))),1,48),limit=TRUE,limit_below = 1e-4)`.$

A nice thing about the $F$-statistic is that it is very general, and can be used to compare any two nested linear models. For instance, we could also compare MODEL G to a model where we assume both $\beta_0 = 0$ _and_ $\beta_1 = 0$. This model would assume that the dependent variable follows a Normal distribution with a mean of 0. That doesn't make much sense here, so we won't compute this test. But the generality of the $F$-test to allow testing of multiple parameters simultaneously comes in very handy in the later chapters.

<!--
I would like to point out that the discrepancy $Y_i - \hat{Y}_{M,i}$ is  a reasonable estimate of the error term $\epsilon_i$, as it is straightforward to rewrite the model as 
$$\begin{align}
\epsilon_i &= Y_i - (\beta_0 + \beta_1 \times X_i) \\
&= Y_i - \mu_{Y|X{_{i}}}
\end{align}$$
and we have just indicated that $\hat{Y}_i = \hat{\mu}_{Y|X{_{i}}}$. Replacing the conditional mean by its estimate, and realizing that ny using an estimate for this, we than also can't be certain about the true value of $\epsilon$, which would also turn that into an estimate, we then write this formally as:  
$$\hat{\epsilon}_{M,i} = Y_i - \hat{Y}_{M,i}$$
This in turn is a somewhat convoluted way to say that the prediction errors would be a reasonable way to estimate the error variance or standard deviation $\sigma_\epsilon$.
-->


<!--


```{r simple-regression-trump2016-anova}
knitr::kable(anova(mod))
```

-->

### Confidence intervals

The way to compute and interpret confidence intervals for the parameters of a simple linear regression model is analoguous to that for the one sample $t$-test (see Section \@ref(ch3-confidence-interval)). The formula to compute confidence intervals for the two parameters can be written as:
$$\hat{\beta}_j \pm t_{n-2; 1-\tfrac{\alpha}{2}} \times \text{SE}(\hat{\beta}_j)$$
where $t_{n-2; 1-\tfrac{\alpha}{2}}$ is the upper critical value in a $t$-distribution with $n-2$ degrees of freedom and a significance level of $\alpha$. Using $\alpha=.05$ gives us the conventional 95%-confidence interval. 


<!-- 
## Assumptions and outliers

```{r boxplots-trump-votes-hate-groups, fig.show='hold', fig.cap="Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens", out.width="50%",fig.width=4}
plot_raincloud(dat,percent_Trump_votes, point_size = 1) + ylab("Percentage of votes for Trump")
plot_raincloud(dat,hate_groups_per_million, point_size = 1) + ylab("Hate groups per million citizens")
#ggplot(dat,aes(y=percent_Trump_votes, x="")) + geom_violin() + geom_boxplot(width=.1) + ylab("Percentage of votes for Trump") + xlab("") + stat_summary(fun=mean, geom="point", shape=16, size=2)
#ggplot(dat,aes(y=hate_groups_per_million, x="")) + geom_violin() + geom_boxplot(width=.1) + ylab("Hate groups per million citizens") + xlab("") + stat_summary(fun=mean, geom="point", shape=1, size=2)
```




```{r simple-regression-trump2016-residuals-plots,fig.show='hold',out.width='50%',fig.width=4,fig.cap="Predicted vs residual plot and a QQ plot of the residuals"}
res <- residuals(mod)
pred <- predict(mod)
ggplot(data.frame(predicted=pred,residual=res),aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept=0)

plot_qq_marginals(res, breaks=20)
```

## Summary

-->

## Summary

A simple linear regression is a model of the relation between two variables: the dependent variable $Y$ and a predictor variable $X$. The model uses a straight line to associate conditional means, which are expected or average value of the dependent variable, to each possible value of the predictor variable. This straight line has two important parameters. The first one is the intercept ($\beta_0$), which is the conditional mean ($\mu_{Y|X}$) of the dependent variable for cases where the predictor variable has the value $X=0$. The second is the slope $\beta_1$, which reflects the increase in the conditional mean for every one-unit increase in the dependent variable. This is then related the steepness of the line. The model assumes variability in the values of the dependent variable around these conditional means to follow a Normal distribution, with a mean of 0 and a constant standard deviation ($\sigma_\epsilon$).

Hypothesis testing for the parameters $\beta_0$ and $\beta_1$ can be based on the sampling distribution of the estimates of these parameters under the null-hypothesis, which leads to a t-test. Alternatively, you can perform these hypothesis tests by model comparison, comparing the Sum of Squared Error of each model with an $F$-test. This is equivalent to a likelihood ratio test, but Sums of Squared Errors and the resulting $F$-distribution are easier to work with.

A simple (bivariate) regression model is a special case of a _multiple regression model_, which we will discuss next. As many things that apply to multiple regression models apply to simple regression models as well, we will discuss things like effect sizes and assessing assumptions, in the next chapter.

<!--chapter:end:04-GLM-regression.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Multiple regression {#ch-multiple-regression}

In this chapter, we extend the simple regression model to include multiple predictor variables. The slopes of the predictors in the resulting multiple regression model reflect the unique effect of each predictor on the dependent variable, after removing the effect of all other predictors from both the dependent and predictor variable. We will look at what that means in some detail. We then go on to discuss parameter estimation and testing. 

```{r load-trump-data-again}
library(papaja)
library(sdamr)
data("trump2016")
dat <- subset(trump2016,state != "District of Columbia")
```

## Trump, votes, and hate groups (again)

In the previous chapter, we found evidence for a relation between the number of hate groups and votes for Trump. This relation was found with an observational study, not an experimental one. Because the number of hate groups was not randomly assigned to states, there are possible confounding factors that could account for the result. The well-known phrase "correlation does not imply causation" should be taken seriously. For instance, it could be that hate groups are especially prevalent in places with relatively low levels of education. And it might be that those with a relatively low level of education are also more likely to vote for Trump. In other words, education level might be a __common cause__ of both hate groups and Trump votes, whilst there is no direct relation between hate groups and Trump votes. The difference between a direct relation, and a spurious one through a common cause, is depicted in Figure \@ref(fig:direct-relation-vs-common-cause). You can see more, often rather amusing examples of spurious relations on the [spurious correlation](https://www.tylervigen.com/spurious-correlations) website.

```{tikz direct-relation-vs-common-cause, fig.cap="A direct relation between hate groups and Trump votes vs a spurious relation through a common cause (education)."}
  \usetikzlibrary{arrows}
	\begin{tikzpicture}
	\pgfsetxvec{\pgfpoint{3cm}{0cm}}
	\pgfsetyvec{\pgfpoint{0cm}{3cm}}
  \tikzstyle{every node}=[minimum size=2cm]

  \draw node at (0,1.5) {\textbf{direct relation}};
	\draw node[circle,draw, align=center] (x1) at (0,0.5) {\texttt{hate} \\ \texttt{groups}};
	\draw node[circle,draw, align=center] (y) at (1.5,0.5) {\texttt{Trump} \\ \texttt{votes}};

  \draw node at (3,1.5) {\textbf{common cause}};
	\draw node[circle,draw, align=center] (x1b) at (0+3,0) {\texttt{hate} \\ \texttt{groups}};
	\draw node[circle,draw, align=center] (yb) at (1.5+3,0) {\texttt{Trump} \\ \texttt{votes}};
	\draw node[circle,draw, align=center] (x2b) at (.75+3,1) {\texttt{education}};

	\draw[->, line width=.5mm] (x1) -- (y);

	\draw[->, line width=.5mm] (x2b) -- (yb);
	\draw[->, line width=.5mm] (x2b) -- (x1b);

	\end{tikzpicture}
```

To assess the relation between hate groups and Trump votes in the possible presence of common causes and other confounds, ideally, we would like to "remove" the effect of such confounds from both the "hate groups" predictor, and the "Trump votes" dependent variable. If, after statistically controlling for confounds in such a way, we still find evidence for a relation between hate groups and Trump votes, that would strengthen our belief that the relation is real, rather than spurious. Multiple regression is a way in which to determine _unique effects_ of predictor variables on the dependent variable. Before we move on to define the multiple regression model, we will work through an indirect way to control for a third variable (e.g. a possible common cause), which will provide insight into what is meant by unique effects in a multiple regression model.

### Controlling for education level

We have indicated that, potentially, the level of education might be a common cause for both the prevalence of hate groups, and voter support for Trump. What if we could assess the relation between hate groups and Trump votes after removing the effect of education level from both? One reasonable way to do this, using the tools we already have, is by means of simple regression models. The idea is reasonably straightforward. In a regression model predicting Trump votes from education level, the error terms (residuals) of that model reflect that part of the variation in Trump votes which can not be predicted from education level. Similarly, in a regression model predicting hate groups from education level, the error terms (residuals) of that model reflect the variation in hate groups which can not be predicted from education level. So, if there is still a relation between the _residual_ Trump vote, and the _residual_ hate groups, then the relation can not be due to education level as a common cause.

```{r}
mod1 <- lm(percent_Trump_votes ~ percent_bachelors_degree_or_higher, data=dat)
tab1 <- summary(mod1)$coefficients
mod2 <- lm(hate_groups_per_million ~ percent_bachelors_degree_or_higher, data=dat)
tab2 <- summary(mod2)$coefficients
```

When we estimate a regression model predicting Trump votes from education level (defined as the percentage of citizens who obtained a Bachelors degree or higher), we obtain the following estimated model
$$\texttt{trump_votes}_i = `r coefficients(mod1)[1]` `r ifelse(coefficients(mod1)[2] >= 0, "+", "-")` `r abs(coefficients(mod1)[2])` \times \texttt{education}_i + \hat{\epsilon}_{\texttt{votes},i}$$
and for the hate groups we obtain
$$\texttt{hate_groups}_i = `r coefficients(mod2)[1]` `r ifelse(coefficients(mod1)[2] >= 0, "+", "-")` `r abs(coefficients(mod2)[2])` \times \texttt{education}_i + \hat{\epsilon}_{\texttt{hate},i}$$
Note that I'm labelling the error terms for both models with a subscript (either $\texttt{votes}$ or $\texttt{hate}$) to denote these are the residual terms for a model predicting Trump votes and hate groups from education levels respectively. Also, I'm adding the hat above them to indicate that these are not the "true errors" (from a model with the true values of the slope and intercept) but rather estimates of these resulting from using estimated parameters. The residuals, as well as the estimated regression lines, are depicted in Figure \@ref(fig:regressions-controlling-for-education). The slope of $\texttt{education}_i$ is significant in both models. In the model for Trump votes, $\hat{\beta}_\texttt{education} = `r tab1[2,1]`$, `r apa_print(mod1)$statistic$percent_bachelors_degree_or_higher`. In the model for hate groups, $\hat{\beta}_\texttt{education} = `r tab2[2,1]`$, `r apa_print(mod2)$statistic$percent_bachelors_degree_or_higher`.
This thus shows a reliable relation between education level and Trump votes, and between education level and hate groups, which is supportive of the idea of a common cause. However, significance is not a prerequisite to statistically control for a third variable.

```{r regressions-controlling-for-education, fig.show="hold", out.width="49%", fig.width=5, fig.height=4, fig.cap="Predicting Trump votes and hate groups by linear regression models with education level as predictor. The residuals of each model are depicted as vertical lines from the observations to the regression line."}
library(ggplot2)
library(dplyr)
dat %>% mutate(ymin = if_else(percent_Trump_votes > predict(mod1), predict(mod1), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod1), predict(mod1), percent_Trump_votes)) %>%
  ggplot(aes(x=percent_bachelors_degree_or_higher, y=percent_Trump_votes, ymin=ymin, ymax=ymax)) + geom_point() + xlab("% bachelors degree or higher") + ylab("% votes for Trump") + geom_abline(intercept = coefficients(mod1)[1], slope=coefficients(mod1)[2]) + geom_linerange(alpha=.5)

dat %>% mutate(ymin = if_else(hate_groups_per_million > predict(mod2), predict(mod2), hate_groups_per_million),
               ymax = if_else(hate_groups_per_million < predict(mod2), predict(mod2), hate_groups_per_million)) %>%
  ggplot(aes(x=percent_bachelors_degree_or_higher, y=hate_groups_per_million, ymin=ymin, ymax=ymax)) + geom_point() + xlab("% bachelors degree or higher") + ylab("Hate groups per million citizens") + geom_abline(intercept = coefficients(mod2)[1], slope=coefficients(mod2)[2]) + geom_linerange(alpha=.5)
```

```{r}
tdat <- data.frame(res_trump_vote=residuals(mod1), res_hate_groups = residuals(mod2))
mod3 <- lm(res_trump_vote ~ res_hate_groups, data=tdat)
```

The regression line in both models represents the variation in the dependent variable (Trump votes and hate groups) that is dependent on education level. In both models, the error term $\epsilon$ is assumed to be completely independent from the predictor. Hence, these are the aspects of Trump votes and hate groups that cannot be explained by education level. To assess whether there is a relation between these residual terms, which would indicate a relation between Trump votes and hate groups that is independent from education level, we can estimate another linear regression model
$$
\hat{\epsilon}_{\texttt{votes},i}  = 0 + `r coefficients(mod3)[2]` \times \hat{\epsilon}_{\texttt{hate},i} + \hat{\epsilon}_i
(\#eq:slope-hate-groups-from-residuals)
$$
Note that the intercept is equal to 0. This is necessarily the case, because __the residuals of estimated regression models always have a mean of 0__. If you go back to the estimate of the intercept in Equation \@ref(eq:simple-regression-intercept-estimate), you will see that if the means are both 0, the result has to be 0 as well:
$$\begin{aligned}
\hat{\beta}_0 &= \overline{Y} - \hat{\beta}_1 \overline{X} \\
&= 0 - \hat{\beta}_1 \times 0 \\
&= 0
\end{aligned}$$

A scatterplot of the dependent variable (the _residual_ Trump votes $\hat{\epsilon}_{\texttt{votes},i}$) and the predictor (the _residual_ hate groups $\hat{\epsilon}_{\texttt{hate},i}$), together with the estimated regression line, is provided in Figure \@ref(fig:scatterplot-residual-votes-and-hate).

```{r scatterplot-residual-votes-and-hate, fig.cap = "Residual Trump votes and hate groups, after statistically removing the effect of education level from both, and the estimated regression line.", out.width="80%", fig.width=6, fig.height=4}
tdat %>%
  ggplot(aes(x=res_hate_groups, y=res_trump_vote)) + geom_point() + geom_abline(intercept=coefficients(mod3)[1], slope = coefficients(mod3)[2]) + xlab("Residual hate groups per million citizens") + ylab("Residual % votes for Trump")
```

We can see that there appears to be a positive relation between residual Trump votes and residual hate groups. If we perform a test of the null hypothesis $H_0: \beta_1 = 0$ in this model, we find that the result is significant, `r apa_print(mod3)$statistic$res_hate_groups`. This thus indicates that there is a relation between Trump votes and hate groups that can not be attributed to education level. While the procedure of removing the effect of education level from both Trump votes and hate groups is appropriate, there however is a slight issue with the hypothesis test we just performed. To remove the effect of education level, we first had to estimate two new models. These estimates are noisy, and this noise may affect the estimates of the third model in which we predicted the residual Trump votes from the residual hate groups. The hypothesis test does not take this additional source of noise into account.

It turns out that we do not need to estimate separate models to remove the effect of education level. By including both education level and hate groups as predictors of Trump votes in a multiple regression model, the estimated slope of hate groups will be _exactly equal_ to the one that we just computed from the residuals. Multiple regression models thus concern the unique effects of each predictor on the dependent variable, removing the effect of all other predictors from that relation.  

## The multiple regression model

The multiple regression model is a straightforward extension of the simple linear regression model, including more than one predictor $X$:
\begin{equation}
Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
(\#eq:multiple-regression-model-definition)
\end{equation}
Note that we are using $m$ to reflect the total number of predictors $X$ in the model. As for the simple regression model, this model consists of a structural part which reflects the conditional mean of the dependent variable $Y$, conditional upon all predictors:
$$\beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_k \times X_{m,i} = \mu_{Y|X_1,\ldots,X_m}$$
and a random part $\epsilon_i$. 

```{r}
mod <- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat)
```

For a model with $m=2$ predictors, such as
$$
\texttt{trump_votes}_i = \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
(\#eq:multiple-regression-votes-by-hate-education)
$$
the data can be represented in a three-dimensional space. In this space, the conditional means $\mu_{Y | X_{1,i}, X_{2,i}}$ can be represented as a __regression plane__. A visual representation is given below.

```{r 3d-regression-plane, fig.cap="Three-dimensional representation of a regression model predicting Trump votes from hate groups and education level. By clicking on the image and moving your mouse, you should be able to rotate the image to explore it further. You can also click on the 'Play' button for an animation of such a rotation.", fig.align='center', cache=TRUE}
library(rgl)
#library(rglwidget)
plot3d(x=dat$hate_groups_per_million, y=dat$percent_bachelors_degree_or_higher, z=dat$percent_Trump_votes, xlab="Hate groups", ylab="% bachelors degree or higher", zlab="% Trump votes", type = "s", col = "purple", size = 1)
coefs <- coef(mod)
a <- coefs["hate_groups_per_million"]
b <- coefs["percent_bachelors_degree_or_higher"]
c <- -1
d <- coefs["(Intercept)"]
planes3d(a, b, c, d, alpha = 0.5)
userMat <- matrix(c(0.04618797,-0.998904467,-0.005153618,0,0.02906432,-0.003813314,0.999554276,0,-0.99849236,-0.046314929,0.028857738,0,0.00000000,0.000000000,0.000000000,1), ncol=4, byrow = TRUE)
rgl.viewpoint(userMatrix = userMat)

fn <- rgl::par3dinterp(time = (0:3)*0.75, userMatrix = list(userMat,
                                      rotate3d(userMat, pi/2, 0, 0, -1),
                                      rotate3d(userMat, pi/2, 0, 0, 1),
                                      rotate3d(userMat, pi/2, 1, 0, 0))) 
widget <- rglwidget(elementId = "interactive-3dplot-multiple-regression") %>%
  playwidget(par3dinterpControl(fn, 0, 2*3*.75, steps=30),
       step = 0.01, loop = TRUE, rate = 0.5)
widget

```

Keeping all predictors constant apart from one, the model becomes a simple linear regression model for the non-constant variable. If you pick a value of "% bachelors degree or higher", you can think of this as slicing the regression plane at that point. The slice of the plane is a single straight line, which is then a simple regression model. For example, if we focus on states with an education level of `r min(dat$percent_bachelors_degree_or_higher)`% with a bachelors degree or higher, then the multiple regression model can be written as
$$\begin{aligned}
\texttt{trump_votes}_i &= \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times `r min(dat$percent_bachelors_degree_or_higher)` + \epsilon_i  \\
&= (\beta_0 + \beta_2 \times `r min(dat$percent_bachelors_degree_or_higher)`) + \beta_1 \times \texttt{hate_groups}_i + \epsilon_i \\
&= \beta_0' + \beta_1 \times \texttt{hate_groups}_i + \epsilon_i
\end{aligned}$$
which is a simple regression model with a new intercept $\beta_0' = \beta_0 + \beta_2 \times `r min(dat$percent_bachelors_degree_or_higher)`$ composed as the sum of the original intercept and `r min(dat$percent_bachelors_degree_or_higher)` times the slope of $\texttt{education\_level}$, and the same slope $\beta_1$ for $\texttt{hate_groups}$. You can see these simple regression lines, for three different values of education level, in Figure \@ref(fig:trump-multiple-regression-scatter-with-multiple-regression-lines). Notice that the regression lines for different education levels are parallel, because they have the same slope. However, the regression lines are at different heights, because they have a different intercept.

```{r trump-multiple-regression-scatter-with-multiple-regression-lines, fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens, with regression lines for three different values of education level. The colour of each data point and regression line reflects the value of education level.", fig.width=5, fig.height=5, out.width='60%'}
ggplot(dat,aes(x=hate_groups_per_million, y=percent_Trump_votes, colour=percent_bachelors_degree_or_higher)) + geom_point() + geom_abline(intercept = coefs[1] + min(dat$percent_bachelors_degree_or_higher)*coefs[3], slope = coefs[2], colour=viridis::viridis_pal()(3)[1]) + geom_abline(intercept = coefs[1] + (min(dat$percent_bachelors_degree_or_higher) + (max(dat$percent_bachelors_degree_or_higher) - min(dat$percent_bachelors_degree_or_higher))/2) *coefs[3], slope = coefs[2], colour=viridis::viridis_pal()(3)[2]) + geom_abline(intercept = coefs[1] + max(dat$percent_bachelors_degree_or_higher)*coefs[3], slope = coefs[2], colour = viridis::viridis_pal()(3)[3]) + theme(legend.position = "bottom") + labs(colour = "% bachelors degree or higher") + xlab("Hate groups per million citizens") + ylab("% votes for Trump")
```

## Estimation {#sec:04b-estimation}

The estimate of the intercept is a direct generalization of that for a simple regression model: 
\begin{equation} 
\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \times \overline{X}_1 -  \hat{\beta}_2 \times \overline{X}_2 - \ldots -  \hat{\beta}_m \times \overline{X}_m
(\#eq:multiple-regression-intercept-estimate)
\end{equation}
Again, it is an adjustment of the mean of the dependent variable, subtracting means multiplied by slopes of each predictor variable. 

The estimates of the slopes are straightforward to compute using matrix algebra.^[A vector with the intercept and slopes can be computed by first constructing a so-called design matrix $\mathbf{X}$, where the first column contains only 1's, and the remaining columns contain values for each predictor. The vector with estimates is then computed as $\hat{\boldsymbol{{\beta}}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}$, where $\mathbf{Y}$ is the vector with the values of the dependent variable, and $^\top$ stands for the matrix transpose, and $\mathbf{X}^{-1}$ for the inverse of matrix $\mathbf{X}$.] However, we won't cover matrix algebra here, so we will leave the estimation of the slopes to statistical software. 

The (unbiased) estimate of the error variance is also a direct generalization of the estimate for a simple regression model:
\begin{equation}
\hat{\sigma}^2_\epsilon = \frac{\sum_{i=1}^n (Y_i - \hat{\mu}_{Y|X_1, \ldots, X_m})^2}{n-\text{npar}(M)}
(\#eq:multiple-regression-sd-error-variance-estimate)
\end{equation}
Here, $\text{npar}(M)$ is the number of parameters in the model, excluding the error variance (i.e., it is the number of $\beta_j$ parameter, where $j=0,\ldots,m$. So $\text{npar}(M) = m + 1$. 

```{r}
coefs <- coefficients(mod)
sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(dat) - length(coefficients(mod))))
```
Estimating the model in which we predict Trump votes from hate groups and education level gives the following estimates:
$$`r write_GLM_equation(mod,dv_name = "trump_votes", iv_names = c("hate_groups","education"), digits=4)`$$
A main thing to note here is that the estimated slope of $\texttt{hate_groups}$ is exactly the same as in Equation \@ref(eq:slope-hate-groups-from-residuals) when first computing the residual Trump votes and hate groups! This is because the slopes in a multiple regression model are __unique effects__ of a predictor on the dependent variable, after removing the effect of all other predictors from both that predictor and the dependent variable. So, equivalently, the slope of $\texttt{education}$ in the model above is identical to the slope we would obtain by first computing the residual Trump votes from
$$\texttt{trump_votes}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \texttt{hate_groups}_i + \hat{\epsilon}_{\text{vote},i}$$

and the residual education from 
$$\texttt{education}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \texttt{hate_groups}_i + \hat{\epsilon}_{\text{education},i}$$
and then determining the slope of the residual $\texttt{education}$ in the model
$$\hat{\epsilon}_{\text{vote},i} + \beta_0 + \beta_1 \times \hat{\epsilon}_{\text{education},i} + \epsilon_i$$

## Inference {#sec:04b-inference}

Testing hypotheses regarding the parameters of multiple regression models is analogous to testing the parameters of simple regression models. So one approach is to look at the sampling distribution of the estimates under the null hypothesis and compute the appropriate $t$ statistic. In a multiple regression model, the $t$-statistic follows a $t$-distribution with $n-(m+1)$ degrees of freedom, where as before $m$ stands for the number of predictors in the model. As we estimate a slope for each predictor, as well as an additional intercept parameter, the total number of estimated parameters is $\text{npar} = m + 1$. Hence, the degrees of freedom equals $n-\text{npar}$. 

Testing, for each parameter, the null hypothesis $H_0: \beta_j = 0$, where $j = 0,\ldots,2$, we obtain the results given in \@ref(tab:t-test-results-multiple-regression-table).
```{r t-test-results-multiple-regression-table}
tab <- summary(mod)$coefficients
rownames(tab) <- c("Intercept","Hate groups per million", "% bachelors degree or higher")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SE}(\\hat{\\beta})$", "$t$", "$P(\\geq \\lvert t \\rvert)$")
knitr::kable(tab, caption = "Null-hypothesis significance tests using the t statistic.", escape = FALSE, booktabs=TRUE, linesep="")
```

Alternatively, we can compare models in which we either fix the parameter to the value assumed in the null hypothesis or not. As discussed in the previous chapter, this is a more general approach. When comparing two multiple regression models, we use the $F$-statistic defined in Equation \@ref(eq:glm-f-definition):
$$F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar}(G)}}$$
For instance, to test the null hypothesis $\beta_1 = 0$ in the model defined in Equation \@ref(eq:multiple-regression-votes-by-hate-education), we compare this general model
$$\text{MODEL G: } \quad \texttt{trump_votes}_i = \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i$$  
to a restricted model where we fix $\beta_1 = 0$:
$$\begin{aligned}
\text{MODEL R: } \quad \texttt{trump_votes}_i &= \beta_0 + 0 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \\
&= \beta_0 + \beta_1 \times \texttt{education}_i + \epsilon_i
\end{aligned}$$  
In other words, we would compare the multiple regression model to a simple regression model where we don't include $\texttt{hate_groups}$, as the null hypothesis assumes that this predictor has no _unique_ effect on the dependent variable. If the Sum of Squared Error of MODEL R is not substantially higher than that of MODEL G, then there would be no good evidence that $\beta_1 \neq 0$. Remember that the Sum of Squared Error of MODEL R can never be lower than that of MODEL G, i.e.  $\text{SSE}(R) \geq \text{SSE}(G)$. This is because __estimating the parameters of MODEL G by maximum likelihood is equivalent to minimizing the Sum of Squared Error__. If $\hat{\beta}_1 = 0$ in MODEL G, that would mean that the SSE of both models is exactly the same. If $\hat{\beta}_1 \neq 0$ in MODEL G, then $\text{SSE}(R) > \text{SSE}(G)$, because otherwise the estimated MODEL G would not have maximised the likelihood (or equivalently minimised the SSE).

We have already shown results for MODEL G. The Sum of Squared Error for that model is $\text{SSE}(G) = `r sum(residuals(mod)^2)`$. Estimating MODEL R gives 
$$`r modg <- mod; modr <- lm(percent_Trump_votes ~ percent_bachelors_degree_or_higher, data=dat); write_GLM_equation(modr,dv_name = "trump_votes", iv_names = c("education"), digits=4)`$$
with $\text{SSE}(R) = `r sum(residuals(modr)^2)`$. The $F$ statistic then becomes:
\begin{align}
F &= \frac{\frac{`r sum(residuals(modr)^2)` -  `r sum(residuals(modg)^2)`}{`r length(coef(modg))` - `r length(coef(modr))`}}{\frac{`r sum(residuals(modg)^2)`}{`r nrow(dat)` - `r length(coef(modg))`}} \\
&= \frac{`r  (sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr)))`}{`r sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg)))`} \\
&= `r  ((sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr))))/(sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg))))`
\end{align}
To determine whether this value is significant, we need to compare it to an $F$-distribution with $\text{df}_1 = 3-2 = 1$ and $\text{df}_2 = `r nrow(dat)` - 3 = `r nrow(dat) - 3`$ degrees of freedom. For this distribution and a significance level of $\alpha = .05$, the critical value is `r qf(.95, df1 =  1, df2=nrow(dat) - 3)`, which is lower than the computed $F$ value, and hence the result is significant and we reject the null hypothesis.

Similarly, to test the null-hypothesis $\beta_2 = 0$ in MODEL G as defined above, we compare this model to a _new_ restricted MODEL R where we fix $\beta_2 = 0$:
$$
\begin{aligned}
\text{MODEL R: } \quad \texttt{trump_votes}_i &= \beta_0 + \beta_1 \times \texttt{hate_groups}_i + 0 \times \texttt{education}_i + \epsilon_i \\
&= \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \epsilon_i
\end{aligned}
$$  
This is the model we estimated in the previous chapter, where we determined the estimates as 
$$`r modr <- lm(percent_Trump_votes ~ hate_groups_per_million, data=dat); write_GLM_equation(modr,dv_name = "trump_votes", iv_names = c("hate_groups"), digits=4)`$$ 
with an associated $\text{SSE}(R) = `r sum(residuals(modr)^2)`$. The $F$ statistic is then computed as 
$$\begin{aligned}
F &= \frac{\frac{`r sum(residuals(modr)^2)` -  `r sum(residuals(modg)^2)`}{`r length(coef(modg))` - `r length(coef(modr))`}}{\frac{`r sum(residuals(modg)^2)`}{`r nrow(dat)` - `r length(coef(modg))`}} \\
&= \frac{`r  (sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr)))`}{`r sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg)))`} \\
&= `r  ((sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr))))/(sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg))))`
\end{aligned}$$
The critical value is the same as before, and hence we also reject this null hypothesis.

To test whether the intercept $\beta_0 = 0$, we would compare MODEL G to yet another MODEL R:
$$
\begin{aligned}
\text{MODEL R: } \quad \texttt{trump_votes}_i &= 0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \\
&=\beta_1 \times \texttt{hate_groups}_i + \beta_1 \times \texttt{education}_i + \epsilon_i
\end{aligned}
$$  
which is estimated as 
$$`r modr <- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher - 1, data=dat); write_GLM_equation(modr,dv_name = "trump_votes", iv_names = c("hate_groups","education"), digits=4)`$$ 
with $\text{SSE}(R) = `r sum(coefficients(modr)^2)`$. The $F$ statistic is computed in the same way as before:
\begin{align}
F &= \frac{\frac{`r sum(residuals(modr)^2)` -  `r sum(residuals(modg)^2)`}{`r length(coef(modg))` - `r length(coef(modr))`}}{\frac{`r sum(residuals(modg)^2)`}{`r nrow(dat)` - `r length(coef(modg))`}} \\
&= \frac{`r  (sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr)))`}{`r sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg)))`} \\
&= `r  ((sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr))))/(sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg))))`
\end{align}

The results of all three tests are collected in Table \@ref(tab:trump-multiple-regression-ANOVA-table).
```{r trump-multiple-regression-ANOVA-table}
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(mod),Error=NA),car::Anova(mod, type=3))
rownames(tab) <- c("Intercept","Hate groups per million", "% bachelors degree or higher","Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$",  "$P(\\geq F)$")
knitr::kable(tab, caption = "Null-hypothesis significance tests by model comparisons and the F statistic.", escape = FALSE, booktabs = TRUE, linesep="")
options(opts)
```

When performing such model comparisons, it is very important to remember that what we call MODEL G and MODEL R will depend on the null hypothesis tested. Generally, MODEL G will remain the same within the context of an analysis, but MODEL R will vary from test to test.

There is one more comparison that we could perform, namely to compare MODEL G to a MODEL R without any predictors. This is a so-called __omnibus test__ of the null hypothesis about two parameters simultaneously, namely $H_0: \beta_1 = 0 \text { and } \beta_2 = 0$. This results in another version of the restricted MODEL R:
$$\begin{aligned}
\text{MODEL R: } \quad \texttt{trump_votes}_i &= \beta_0 +  0 \times \texttt{hate_groups}_i + 0 \times \texttt{education}_i + \epsilon_i \\
&= \beta_0 + \epsilon_i
\end{aligned}$$  
This is like the alternative model of the one-sample t-test we considered earlier. Here, the model is estimated as 
$$`r modr <- lm(percent_Trump_votes ~ 1, data=dat); write_GLM_equation(modr,dv_name = "trump_votes", digits=4)`$$ 
with an associated $\text{SSE}(R) = `r sum(residuals(modr)^2)`$. The $F$ statistic is then computed as 
\begin{align}
F &= \frac{\frac{`r sum(residuals(modr)^2)` -  `r sum(residuals(modg)^2)`}{`r length(coef(modg))` - `r length(coef(modr))`}}{\frac{`r sum(residuals(modg)^2)`}{`r nrow(dat)` - `r length(coef(modg))`}} \\
&= \frac{`r  (sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr)))`}{`r sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg)))`} \\
&= `r  ((sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr))))/(sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg))))`
\end{align}
The critical value is different this time, because the degrees of freedom are now $\text{df}_1 = 3 - 1 = 2$ and $\text{df} = `r nrow(dat)` - 3 = `r nrow(dat) - 3`$, and equals `r qf(.95, 2, nrow(dat) - 3)`. Clearly, the computed $F$ value is well above the critical value, hence we reject the null hypothesis (i.e. reject MODEL R in favour of MODEL G).


## Partitioning and explaining variance

Let's have another look at the formula for the $F$ statistic:

$$F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar(G)}}}$$
The numerator (top part) consists of a difference between two SSE terms, divided by the difference in the number of estimated parameters. Because the comparison is between nested models, and MODEL G contains more estimated parameters than MODEL R, we can view this part as the average reduction of the SSE (per parameter) due to the additional parameters of MODEL G. The numerator of the $F$-statistic is also referred to as the Mean Square Reduced (MSR):
$$\text{MSR} = \frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}$$
The denominator (bottom part) in the formula of the $F$-statistic consists of the SSE of MODEL G divided by $n-\text{npar}(G)$. This is an unbiased estimate of the error variance of MODEL G, and also referred to as the Mean Squared Error (MSE):
$$\text{MSE} = \frac{\text{SSE}(G)}{n-\text{npar(G)}}$$
So an alternative definition of the $F$-statistic is
$$F = \frac{\text{MSR}}{\text{MSE}}$$

The SSE of _any_ linear model $M$ is related to an error variance, as the SSE is the sum of squared deviations between the observations and the model predictions ($\hat{Y} = \mu_{Y|X_1,\ldots,X_m}$):
$$\text{SSE}(M) = \sum_{i=1}^n (Y_i - \hat{Y}_{M,i})^2$$
Dividing the SSE by $n - \text{npar}(M)$ would provide us with an unbiased estimate of the error variance of that model. Although this is only done in the denominator of the $F$ statistic, which considers MODEL G, it should be clear there is a close relation between any SSE term and an error variance.

A useful perspective on unique effects in multiple regression models and GLMs in general is to consider them as __partitioning the variance of the dependent variable__ in parts that are uniquely attributable to the predictors. If you consider the SSE of a very simple MODEL R
$$Y_i = \beta_0 + \epsilon_i$$
with estimate $\hat{\beta}_0 = \overline{Y}$, then for this model
$$\text{SSE}(R) = \sum_{i=1}^n (Y_i - \hat{\beta}_0)^2 = \sum_{i=1}^n (Y_i - \overline{Y})^2 = n S_Y^2$$
So the SSE of a regression model without predictors is equal to $n$ times the sample variance of the dependent variable. When we add predictors to the model, we know the SSE can only decrease (or remain the same, but that is rather unlikely). The proportion of the SSE that is reduced by adding a predictor to the model can be seen as the proportion of the variance that is "explained" by the predictor. This __proportional reduction in error__ is a useful measure of the strength of the relation between a predictor and the dependent variable. As usual, this strength will have a true value for the Data Generating Process, and a value we can compute for a given limited dataset. We'll denote the true value as $\eta_p^2$. The sample estimate is: 
\begin{equation}
\hat{\eta}_p^2 = \frac{\text{SSE}(R) - \text{SSE}(G)}{\text{SSE}(R)}
(\#eq:definition-partial-eta-squared-glm)
\end{equation}
Because $0 \leq \text{SSE}(G) \leq \text{SSE}(R)$, the value of $\hat{\eta}_p^2$ is between 0 and 1 (as a proportion should be). Figure \@ref(fig:sse-partition) depicts such "proportions of variance explained" for a situation with two predictors ($X_1$ and $X_2$) of a dependent variable $Y$.

```{tikz sse-partition, fig.cap="Partitioning the variance in a multiple regression model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables. ", out.width="50%"}
\def\firstcircle{(1,1) circle (1.75cm)}
\def\secondcircle{(2,1) circle (1.75cm)}
\def\thirdcircle{(1.5,2) circle (1.75cm)}
\begin{tikzpicture}
	\pgfsetxvec{\pgfpoint{2cm}{0cm}}
	\pgfsetyvec{\pgfpoint{0cm}{2cm}}
	\tikzstyle{every node}=[minimum size=.7cm]

	\fill[white!50] (-1,0) -- (-1,3.5) -- (3.5,3.5) -- (3.5,0) -- (-1,0);

	\draw \firstcircle;
	\draw \secondcircle;
	\draw \thirdcircle;
	
	\draw (0.2,0.2) node {$X_1$};
	\draw (2.8,0.2) node {$X_2$};
	\draw (1.5,3) node {$Y$};
	\draw (1.5,2.3) node {$A$};
	\draw (1.1,1.6) node {$B$};
	\draw (1.5,1.4) node {$D$};
	\draw (1.9,1.6) node {$C$};

\end{tikzpicture}	
```

In this Figure, the variance of $Y$ is represented by the circle labelled $Y$, and the variance of the predictors is represented by the circles labelled $X_1$ and $X_2$ respectively. Overlap between the circles represents shared variance (e.g. covariance) between the variables. First, let's consider a simple regression model where $X_1$ is the sole predictor of $Y$. The proportion of variance of $Y$ that this model accounts for is the total shared area between $X_1$ and $Y$, which is the sum of the regions labelled as $B$ and $D$. Similarly, if we'd consider a simple regression model where $X_2$ is the sole predictor of $Y$, the proportion of variance of $Y$ that this model accounts for is the total shared area between $X_2$ and $Y$, which is the sum of the regions labelled as $C$ and $D$. If we consider a multiple regression model with both $X_1$ and $X_2$ as predictors of $Y$, the proportion of variance of $Y$ that this model accounts for is the total shared area between $X_1$, $X_2$ and $Y$, which is the sum of the regions labelled as $B$, $C$, and $D$. The area labelled as $A$ is variance of $Y$ that cannot be accounted for by $X_1$ or $X_2$, and is the SSE (the random, unexplainable part) of a multiple regression model which includes both $X_1$ and $X_2$ as predictors. When MODEL R is an intercept-only model (i.e. a model without any predictors) and MODEL G a model with predictors, the estimate of the variance accounted for is often denoted by $R^2$. In this case, $R^2$ reflects the proportion of the variance of the dependent variable $Y$ accounted for, or "explained", by the model as a whole. <!-- The $\eta^2$ is generally reserved to denote the proportion of variance explained by a single predictor. -->


Note that the area labelled as $D$ is "explained" by both $X_1$ and $X_2$ when each is used as a sole predictor of $Y$. In a multiple regression model however, the effects of predictors are _unique_ effects, meaning that we consider only that part of the variance of $Y$ which is solely explained by each predictor. For $X_1$, this unique proportion of the variance is the area labelled as $B$, and for $X_2$ it is the area labelled as $C$. The area labelled as $D$ is not uniquely attributable to either $X_1$ or $X_2$. Although it is not purely random variability of $Y$ -- in principle, it could be explained by _either_ $X_1$ or $X_2$ -- because it cannot be uniquely attributed to either $X_1$ or $X_2$, it will not contribute to a hypothesis test of the effect of $X_1$ or $X_2$. We will discuss this more when we consider the practical problem of _multicollinearity_ later on. 

## Effect size and the importance of predictors {#sec:04b-effect-size}

In a multiple regression model, you may be interested in which predictors are more important, in the sense that they have a stronger relation with the dependent variable and therefore help you to provide better predictions of the dependent variable than other variables. It is important to realise that this question can _not_ be answered by simply comparing the (absolute) value of the slopes of the predictors. Slopes reflect the change in the conditional mean of the dependent variable for every one-unit increase in the predictor. But what a one-unit increase is depends on the scale of the predictor. For example, I might predict people's weight by their height and daily calorie intake. Whether I measure their height in meters or centimetres should be arbitrary. If I know someone's height in centimetres, I also know their height in meters, and the choice of scale (whether centimetres or meters) does not change the relation between height and weight in any meaningful way. But the choice of scale clearly changes the (estimated) slope of the height predictor. If people's average weight increases by 1 gram for every increase in their height by 1 centimetre, the corresponding increase in weight for an increase in height by 1 meter is 100 grams. In other words, the slope of height in centimetres is 1, but the slope for height in meters is 100. But that obviously doesn't mean that height in meters is a better or more important predictor of weight than height in centimetres!

Even when predictors are measured on the same scale, their relative importance can _not_ be determined by comparing the (absolute) value of their slopes. For example, we could attempt to predict the impact of a negative life event (such as divorce or a bereavement) by the size of someone's support network, which could consist of the number of people in their immediate family, and the number of friends. Both these predictors (family and friends) are measured on the same scale (number of people), but they may have rather different ranges (e.g., between 0 and 5 for family, and between 0 and 30 for friends). If for example the slope of friends is smaller than the slope of family, that means that every additional family member may soften the negative impact more than every additional friend. However, because the range of friends is larger, the overall effect of friends on the impact of a negative life events may well be larger than that of family.

To resolve such issues of scale and range, we can attempt to standardize all the predictors. Generally, this is done by the $Z$-transformation (Equation \@ref(eq:standardized-Z-definition)), i.e. by subtracting the mean from all values and then dividing these by the standard deviation
$$Z_{X_j} = \frac{X_{j,i} - \overline{X}_j}{S_{X_j}}$$
Conventionally, the same $Z$-transformation is also applied to the dependent variable. After this standardization of all (predictor and dependent) variables, we obtain the __standardised slopes__:
\begin{equation}
\hat{\beta}^\text{std}_{X_j} = \hat{\beta}_j \frac{S_{X_j}}{S_Y}
(\#eq:standardized-slopes)
\end{equation}
These standardized slopes^[In some of the literature and output of statistical software, these are referred to as beta-coefficients. As we are using beta ($\beta$) to reflect the true value of a linear model parameter, this may be confusing.] reflect the increase or decrease of the dependent variable (in units of standard deviation $S_Y$) for every increase in the predictor by one standard deviation $S_{X_j}$. Although this makes the slopes more comparable, some issues with range may still remain. For one thing, as we are basing the standardisation on sample estimates of the standard deviations, the result will depend on the particulars of the dataset. If, by chance, a dataset includes only people with between 2 and 6 friends, the standardised slope of friends may be much higher than if the dataset included people with between 0 and 20 friends, even if the relation between friends and life-event impact is the same in both datasets.

Determination of the relative importance of predictors is tricky. There is no measure which resolves all issues. But if you take note of potential issues with artificially restricted ranges of predictors, measures of relative importance are not completely useless. They can inform you of the relative size of the effect a predictor has on the dependent variable. When considering whether there is evidence that a predictor has any effect on the dependent variable (i.e., when testing the null-hypothesis $H_0$: $\beta_j = 0$), it is often a good idea to also consider a measure of the __effect size__. For large datasets, the power of tests to detect _any effect_ of a predictor is generally large. That means that even if a predictor has a very small effect on the dependent variable (i.e. it only changes the conditional mean by a very small fraction), you would still be able to reliably detect such a small effect and reject the null hypothesis. But that would not imply that a predictor is "practically significant", in the sense that e.g. persuading someone to make one additional friend might help soften the blow of a negative life event in a subjectively perceivable manner.

### $R^2$ changes and the coefficient of (semi-)partial determination

The standardised slopes can be viewed as measures of effect size. But it is more common to consider effect size in terms of "proportion of variance explained". Personally, I like these measures because they have a clear and bounded scale between 0 (no effect) and 1 (a predictor is perfect in accounting for the dependent variable). Let's consider how we can use this measure to reflect the effect size for the different predictors in a multiple regression model. To frame our discussion, we will consider a situation where the dependent variable $Y$ can be modelled by two predictors, $X_1$ and $X_2$. In this situation, we can define four different models: 

$$\begin{aligned}
\text{MODEL R0}: && Y_i &= \beta_0 + \epsilon_i \\
\text{MODEL R1}: && Y_i &= \beta_0 + \beta_1 X_{1i} + \epsilon_i \\
\text{MODEL R2}: && Y_i &= \beta_0 + \beta_1 X_{2i} + \epsilon_i \\
\text{MODEL G}: && Y_i &= \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \epsilon_i
\end{aligned}$$

Referring back to Figure \@ref(fig:sse-partition), the Sums of Squared Error of each model can be defined as

$$\begin{aligned}
\text{SSE}(R0) &= A + B + C + D = S^2_Y \\
\text{SSE}(R1) &= A + C \\
\text{SSE}(R2) &= A + B \\
\text{SSE}(G) &= A 
\end{aligned}$$

Recall that the $R^2$ measure reflects the proportion of variance "explained" by all the predictors in a model, and is the $\hat{\eta}^2$ of a model compared to an intercept-only model (i.e. MODEL R0). For each of the models apart from R0, we can compute the $R^2$ measure as

$$\begin{aligned}
R^2_{R1} &= \frac{\text{SSE}(R0) - \text{SSE}(R1)}{\text{SSE(R0)}} =  \frac{B + D}{A + B + C + D} \\
R^2_{R2} &= \frac{\text{SSE}(R0) - \text{SSE}(R2)}{\text{SSE(R0)}} =  \frac{C + D}{A + B + C + D} \\
R^2_{G} &= \frac{\text{SSE}(R0) - \text{SSE}(G)}{\text{SSE(R0)}} =  \frac{B + C + D}{A + B + C + D}
\end{aligned}$$

We can define a measure of effect size by considering the following question: what additional proportion of the variance of $Y$ can be explained by including a predictor $X_j$ in a model. For example, what additional proportion of the variance can be explained by adding the predictor $X_2$ to MODEL R1? This is easily computed as the difference in $R^2$ values: $$R^2_{G} - R^2_{R1} = \frac{C}{A+B+C+D}$$ Similarly, we can define a measure of the effect of $X_1$ by considering what additional proportion of the variance is explained by adding the predictor $X_1$ to MODEL R2, providing $$R^2_{G} - R^2_{R2} = \frac{B}{A+B+C+D}$$ These $R^2$-change values are also called the __coefficient of semi-partial determination__. They reflect the unique proportion of variance of $Y$ that can be attributed to a predictor. Defining the Sum of Squared Reduced (SSR) -- the value of this for each predictor is usually given in the "SS" column of a multiple regression ANOVA table -- as follows:
$$\begin{aligned}
\text{SSR}(X_1) &= \text{SSE}(R2) - \text{SSE}(G) = A + B - A = B \\
\text{SSR}(X_2) &= \text{SSE}(R1) - \text{SSE}(G) = A + C - A = C
\end{aligned}$$
the coefficient of semi-partial determination can be computed as 
\begin{equation}
\text{Semi-partial determination of }X = \hat{\eta}^2 = \frac{\text{SSR}(X)}{\text{SSE}(R0)}
(\#eq:semi-partial-determination)
\end{equation}
The main thing to remember is that the coefficient of semi-partial determination reflects the proportion of the variance of $Y$ that can be _uniquely_ attributed to a predictor $X$.

A different measure of effect size is called the __coefficient of partial determination__. This reflects not the proportion of the total variance of $Y$ that can be uniquely attributed to a predictor $X$, but rather the proportion of the variance of $Y$ that can not be explained by the other predictors in a model, but which can (uniquely) be explained by predictor $X$. For instance, with reference to Figure \@ref(fig:sse-partition), MODEL R1 leaves $A + C$ "unexplained". Of this unexplained variance, predictor $X_2$ can explain part $C$ when added to model R1. So the proportion of previously unexplained variance now explained is $\frac{C}{A+C}$. This proportion can also be computed from $R^2$ values. As the proportion of unexplained variance for MODEL R1 is simply $1 - R^2_{R1}$, the proportion of this variance accounted for by $X_2$ is $\frac{R^2_{G} - R^2_{R1}}{1 - R^2_{R1}}$. Because the numerator is equal to the coefficient of semi-partial determination, and the denominator is between 0 and 1, the coefficient of partial determination is equal to or larger than the coefficient of semi-partial determination. It can also be computed directly from SSE and SSR terms as 
\begin{equation}
\text{Partial determination of }X = \hat{\eta}_p^2 = \frac{\text{SSR}(X)}{\text{SSE}(G) + \text{SSR}(X)}
(\#eq:partial-determination)
\end{equation}
The coefficient of partial determination is (in the context of ANOVA models, which we will discuss later) also called the _partial eta-squared_, or $\eta_p^2$. We have already come across this measure as the proportional reduction in error (Equation \@ref(eq:definition-partial-eta-squared-glm)).

<!--


## Controlling for education level and poverty

```{r}
#mod <- lm(percent_Trump_votes ~ hate_groups_per_million + #percent_bachelors_degree_or_higher + percent_in_poverty,data=dat)
```

```{r}
#opts <- options()
#options(knitr.kable.NA = "")
#tmp <- cbind(estimate = c(coefficients(mod),Error=NA),car::Anova(mod, type=3))
#knitr::kable(tmp)
#options(opts)
```
-->

## Assumptions {#sec:04b-assumptions}

The assumptions of the multiple regression model, and with that any of the versions of the General Linear Model we discuss, are all really about the errors, which are assumed to be independently and identically distributed:
$$\epsilon_i \sim \mathbf{Normal}(0, \sigma)$$

In other words, this implies the following assumptions for a multiple regression and/or GLM:

1. __Normality__: the errors $\epsilon_i$ are Normal-distributed
2. __Unbiasedness__: the mean of $\epsilon_i$ is 0. This means that the conditional means are indeed a linear function of the predictors, and that the model predictions are unbiased.
3. __Homoscedasticity__: the errors $\epsilon_i$ have a constant variance $\sigma^2$ and thus also a constant standard deviation $\sigma$.
4. __Independence__: any error term $\epsilon_i$ is independent of any other $\epsilon_j$ (for all $i \text{ and } j \neq i$). Independence here means that $p(\epsilon_j|\epsilon_i) = p(\epsilon_j)$ for all $i \text{ and } j \neq i$

It is important to realise that the assumption of Normal-distributed errors does not necessarily translate in assuming that the dependent variable $Y$ is itself Normal-distributed. This will often not be the case. Nor does the model require the predictors to be Normal-distributed. No assumption is made at all about the distribution of the predictors. 

In Section \@ref(sec:02-assumptions) we discussed some methods to assess the assumption of Normality (i.e. histograms, QQ-plots and statistical tests such as the Shapiro-Wilk test). The assumption of unbiasedness and homoscedasticity is generally assessed visually with a so-called predicted-by-residual plot. This plot (see Figure \@ref(fig:trump-predicted-residual)) depicts the residuals as a function of the model predictions (i.e. the estimated conditional means). The unbiasedness assumption implies that for each predicted value $\hat{Y}$ (i.e. each conditional mean), the residual or error terms are scattered around 0. For instance, it should _not_ be the case that for relatively low and high values of the predictions, the errors are generally above 0, while for medium values of the predictions, the errors are generally below 0. Such a pattern would be indicative of biased predictions and a likely _non-linear_ relation between predictors and the dependent variable. The homoscedasticity assumption implies that the spread of the residuals is equal for each predicted value $\hat{Y}$. Looking at the predicted vs residual plot in Figure \@ref(fig:trump-predicted-residual), there is no clear indication that either assumption is violated. In addition, the QQ-plot indicates that the expected and sample quantiles do not differ radically, at least for the middle quantiles. Note that at the extremes (low and high quantiles), you will generally find more variable results, as quantiles in the tails of a distribution are less reliable.

```{r trump-predicted-residual, fig.cap="Predicted vs residual plot, and a QQ plot, for MODEL G predicting Trump votes from hate groups and education level.", fig.show="hold", out.width="49%", fig.width=4,fig.height=4}
data.frame(predicted=predict(modg), residual=residuals(modg)) %>%
  ggplot(aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept = 0, lty=3)
data.frame(predicted=predict(modg), residual=residuals(modg)) %>%
  ggplot(aes(sample = residual)) + stat_qq() + stat_qq_line()
```

For comparison, Figure \@ref(fig:predicted-residual-for-violations-of-assumptions) shows two examples of predicted vs residual plots where the assumption of unbiasedness and homoscedasticity do not hold. 
```{r predicted-residual-for-violations-of-assumptions, fig.cap="Predicted vs residual plots for two cases where the assumptions of the GLM do not hold. In the plot of the left, the assumption of unbiasedness is violated. There is clearly a nonlinearity in the data, which result in the underprediction (positive residuals) for low and high predicted values, and an overprediction (negative residuals) for medium predicted values. In the plot on the right, the homoscedasticity assumption is violated. The variability of the residuals clearly increases as a function of the predicted values.", fig.show="hold", out.width="49%", fig.width=4,fig.height=4}
set.seed(234786)
tdat <- data.frame(x=runif(100,min=1,max=10))
tdat$y <- (tdat$x - 5)^2 - 10 + rnorm(100,sd=2)
tmod <- lm(y~x,data=tdat)
data.frame(predicted=predict(tmod), residual=residuals(tmod)) %>%
  ggplot(aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept = 0, lty=3)

tdat <- data.frame(x=runif(100,min=1,max=10))
tdat$y <- tdat$x + rnorm(100,sd=tdat$x^1)
# tdat$yt <- tdat$y + abs(min(tdat$y)) + .1
tmod <- lm(y ~ x,data=tdat)
data.frame(predicted=predict(tmod), residual=residuals(tmod)) %>%
  ggplot(aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept = 0, lty=3)

tmod <- lm(I(y/x) ~ x, data=tdat)

```


### Transforming variables

In certain cases where the assumptions of unbiasedness and/or homoscedasticity do not hold, it may be possible to transform the dependent variables and/or predictors and obtain a model with these transformed parameters where the assumptions do hold. 

```{block2, type = 'advancedtext', echo=TRUE}
For example, the true underlying relation in the left plot of Figure \@ref(fig:predicted-residual-for-violations-of-assumptions) was
$$Y_i = \beta_0 + \beta_1 \times X_{1,i}^2 + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
In this case, a linear model would be obtained by transforming the predictor $X_1$ with a __power-transformation__, raising it to the power of 2 (i.e., using $X_1^2$ rather than $X_1$ as a predictor). The true underlying relation in the right plot of Figure \@ref(fig:predicted-residual-for-violations-of-assumptions) was
$$\begin{aligned} Y_i &= \beta_0 + \beta_1 \times X_{1,i} + \epsilon_{i} \quad \quad \epsilon_i \sim \mathbf{Normal}(0,X_{1,i} \times \sigma_\epsilon) \end{aligned}$$
In this case, the standard deviation of the errors increases with the value of $X_1$. We could get a constant standard deviation by modelling $\frac{Y_i}{X_i}$ as the dependent variable, instead of $Y_i$.

You might -- rightly! -- think: but what if I don't know the true underlying relation between the predictors and the dependent variable? Well, in that case you might have to try a number of different transformations. As there are an infinite number of possible transformations, you would hopefully be lucky enough to arrive at one that works. Common transformations of the dependent variable, which aim to resolve issues with violations of the homoscedasticity assumption, are provided in Table \@ref(tab:common-data-transformations-ch4).

Table: (\#tab:common-data-transformations-ch4) Some common transformations for the dependent variable in linear models.

Transformation | Name | Usage
:--------------|:-----|:--------------
$Y^2$          | square | useful for negatively-skewed data, as it will increase the spread among higher scores compared to lower ones
$\sqrt{Y}$     | square-root | useful for positively skewed data, as it will decrease the spread among higher scores compared to lower one
$\log_e{(Y)}$  | natural logarithm | useful for positively skewed data, as it will decrease the spread among higher scores compared to lower one. If there are values equal to or smaller than 1, you can use an alternative form $\log_e{(Y + c)}$, with e.g. $c = \min(1,|\min(Y)|)$
$\log_{10}{(Y)}$ | base 10 logarithm | useful for highly positively skewed data, as it will decrease the spread among higher scores compared to lower one. If there are values equal to or smaller than 1, you can use a similar adjustment as above.

Of course, you don't have to try different transformations blindly. Inspecting the relations between the predictors and the dependent variable, and between the residuals and predicted values, can sometimes give reasonable clues to potentially useful transformations. The tricky thing is that non-linear transformations of the dependent or predictor variables can change many aspects of the model. For example, if the model
$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i$$
provides unbiased predictions, but violates the assumption of homoscedasticity, a transformed model
$$\log(Y_i) = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i$$
might resolve some of the issues with the homoscedasticity assumption, but lead to biased predictions. In this case, we might want to transform both the dependent variable _and_ the predictor
$$\log(Y_i) = \beta_0 + \beta_1 \times \log(X_{1.i}) + \epsilon_i$$
to (hopefully) resolve the issue with homoscedasticity and maintain unbiased estimates. And when we have more than one predictor, the situation becomes even more tricky to resolve. Transforming variables to make a model conform to the assumptions of the linear model is, in some sense, perhaps more of an art than a science. It is certainly not easy, and something that should be done with caution.
```

A general technique which aims to help you determine the appropriate transformation function is the so-called Box-Cox transformation [see e.g. @sakia1992box]. Describing this procedure in detail is beyond the scope of this book, but knowing of its existence might help you out in the future. And once you become comfortable with linear models, you will become more comfortable with the various ways in which to transform your data as well.

### Polynomial regression

If the assumption of unbiasedness is violated because the relation between a predictor and the dependent variable is not linear, you may also try a technique called __polynomial regression__. For example, if we go back to analysing the relation between hate groups and votes for Trump considered earlier, you might not expect the relation to be exactly linear. It might be more reasonable to assume that the effect of hate groups on votes for Trump diminishes for very large numbers of hate groups. In other words, you might expect the effect of a one-unit increase in hate groups to be _larger_ for moving from one to two hate groups per million citizens, compared to moving from eight to nine hate groups per million citizens. This means that the relation between hate groups and votes for Trump is nonlinear.

There are various techniques to deal with nonlinear relations. Polynomial regression can allow you to estimate nonlinear relations within the context of a linear model. That seems contradictory at first: how can a linear model provide a nonlinear relation? The key to doing so is to effectively transform the predictor in a variety of ways, and then average the functions relating each transformed predictor to the dependent variable. This "averaging" is done automatically, by estimating regression coefficients for each in a multiple regression model.

The transformations considered in a polynomial regression are all power-transformations of a predictor (i.e. $(X_i)^k$, with $k=1, 2, \ldots$), up to a certain maximum power, called the degree of the model. For example, a 2nd-degree polynomial model would be  

$$Y_i = \beta_0 + \beta_1 \times X_{i} + \beta_2 \times X_i^2 + \epsilon_i$$
which is sometimes also called a quadratic model. A 3rd-degree polynomial, also called a cubic model, would be
$$Y_i = \beta_0 + \beta_1 \times X_{i} + \beta_2 \times X_i^2 + \beta_3 \times X_i^3 + \epsilon_i$$
The power-transformed predictors (e.g., $X^2$ and $X^3$) are treated as any other predictor. But, because when we know the value of the original predictor $X$, we also exactly know the value of each transformed predictor, the conditional mean $\mu_{Y|X,X^2,X^3}$ is really just a function of $X$. Hence, we can unambiguously consider the predictions of the model for each possible value of $X$.^[In usual multiple regression models, the values of the other predictors $X_k$ can take a variety of values for any focal predictor $X_j$, $j \neq k$. Here there is a perfect--but crucially_nonlinear relation between the predictors.] These are shown, for a quadratic (second-order) and cubic (third-order) polynomial regression model in Figure \@ref(fig:trump-polynomial-regression-plot). As you can see, both the quadratic and cubic model predict a nonlinear relation between hate groups and Trump votes. The quadratic model indicates that the effect of hate groups diminishes with more hate groups. In the cubic model, the effect even appears to reverse, such that after an initial increase, Trump votes decrease with more hate groups. As such, the quadratic model seems more plausible.

```{r trump-polynomial-regression-plot, fig.cap="Polynomial regression models for the relation between hate groups and Trump votes."}
ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_smooth(method="lm", formula=y~x, se=FALSE, aes(colour='Linear')) + geom_smooth(method="lm", formula=y~poly(x,2,raw=TRUE), se=FALSE, aes(colour='Quadratic')) + geom_smooth(method="lm", formula=y~poly(x,3,raw=TRUE), se=FALSE, aes(colour='Cubic')) + scale_color_manual(name='Model',
                     breaks=c('Linear', 'Quadratic', 'Cubic'),
                     values=c('Cubic'=viridis::viridis_pal(alpha=.6)(3)[3], 'Quadratic'=viridis::viridis_pal(alpha=.6)(3)[2], 'Linear'=viridis::viridis_pal(alpha=.6)(3)[1])) + theme(legend.position = "bottom")
```

At this point in time, I would like you to just be aware of the possibility of polynomial regression. We will hopefully return to it at some later stage.

<!-- 

TODO: polynomial regression

One way to arbitrate between these models is to consider the more general  (third-order or cubic model) and determine whether the highest-order effect is significant. 

```{r trump-polynomial-regression-lm-summary-table, eval=FALSE}
mod <- lm(percent_Trump_votes ~ poly(hate_groups_per_million,3, raw=TRUE), data=dat)
knitr::kable(summary(mod)$coefficients, digits=c(3,3,3,3), booktabs=TRUE, linesep="")
```

-->

## Multicollinearity: Redundancy between predictors

Multicollinearity is a rather fancy word for interdependence between predictors in a linear model. Interdependence means that the values of a predictor are related to the values of other predictors, such that you can predict one from the others. In Figure \@ref(fig:sse-partition), this is represented by the partial overlap of the circles representing $X_1$ and $X_2$. A high covariation between predictors indicates __redundancy__ between them, in the sense that variation in one predictor can be accounted for by another. If this shared variation is also shared with the dependent variable (e.g. region $D$ in Figure \@ref(fig:sse-partition), than this can _not_ be attributed uniquely to any of those covarying predictors. While the model as a whole may be able to account for a large proportion in the variation of the dependent variable, the unique variance "explained" by individual predictors could be very small so that the hypothesis tests are not significant for any predictor, even though the "whole model test" (i.e. the comparison between the full model and an intercept-only model) is highly significant. This makes parameter inference tricky, to say the least.

Figure \@ref(fig:sse-partition-multicollinearity) illustrates the difference between a model without redundancy between between predictors and with (substantial) redundancy between predictors. In the case of no redundancy, each predictor uniquely accounts for a substantial proportion (regions labelled as $B$ and $C$) of the variance of the dependent variable. In the case of redundancy between the predictors, while the overlapping region between each predictor and the dependent variable is the same size as before (region $B + D$ for $X_1$, and region $C + D$ for $X_2$), because a large part of this (region $D$) is shared between $X_1$ and $X_2$, the unique parts ($B$ and $C$ respectively) are substantially smaller. In the most extreme case, the circles representing $X_1$ and $X_2$ would be completely overlapping, meaning that neither accounts for a unique proportion of the variation in $Y$. __Multicollinearity__ is an extension of this concept, referring to dependency between more than two predictors.

```{tikz sse-partition-multicollinearity, fig.cap="Partitioning the variance in a multiple regression model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables. ", out.width="60%"}
\def\firstcircleA{(1.1,1.3) circle (1.75cm)}
\def\secondcircleA{(2.9,1.3) circle (1.75cm)}
\def\thirdcircleA{(2,1.7) circle (1.75cm)}

\def\firstcircleB{(6.2,1) circle (1.75cm)}
\def\secondcircleB{(6.8,1) circle (1.75cm)}
\def\thirdcircleB{(6.5,2) circle (1.75cm)}

\begin{tikzpicture}
	\pgfsetxvec{\pgfpoint{2cm}{0cm}}
	\pgfsetyvec{\pgfpoint{0cm}{2cm}}
	\tikzstyle{every node}=[minimum size=.7cm]

	\draw \firstcircleA;
	\draw \secondcircleA;
	\draw \thirdcircleA;
	
	\draw (0.2,0.2) node {$X_1$};
	\draw (3.8,0.2) node {$X_2$};
	\draw (2,3) node {$Y$};
	\draw (2,2.3) node {$A$};
	\draw (1.5,1.5) node {$B$};
	\draw (2.5,1.5) node {$C$};
 
  \draw (1,1) node {$E$};
  \draw (3,1) node {$F$};

  \draw node[align=center] at (2,3.5) {\textbf{no redundancy}};

	\draw \firstcircleB;
	\draw \secondcircleB;
	\draw \thirdcircleB;
	
	\draw (5.2,0.2) node {$X_1$};
	\draw (7.8,0.2) node {$X_2$};
	\draw (6.5,3) node {$Y$};
	\draw (6.5,2.3) node {$A$};
	\draw (5.95,1.6) node {$B$};
	\draw (6.5,1.4) node {$D$};
	\draw (7.05,1.6) node {$C$};
  \draw (5.6,.8) node {$E$};
  \draw (7.4,.8) node {$F$};
  \draw (6.5,.8) node {$G$};

  \draw node[align=center] at (6.5,3.5) {\textbf{with redundancy}};

\end{tikzpicture}	
```

Another way to see the problem of multicollinearity is to consider the confidence interval of a slope in a multiple regression model. The confidence interval can be computed as
\begin{equation}
\hat{\beta}_j \pm \sqrt{\frac{F_{1,n - \text{npar}(G);\alpha} \text{MSE}(G)}{n S^2_{X_j} (1-R^2_{X_j})}}
(\#eq:confidence-interval-slope-multiple-regression)
\end{equation}
In this rather complicated looking equation, $F_{1,n - \text{npar}(G);\alpha}$ is the critical value for the $F$-distribution with $\text{df}_1 = 1$ and $\text{df}_2 =  n - \text{npar}(G)$ degrees of freedom and a significance level $\alpha$ (e.g. $\alpha = .05$ for a 95% confidence interval). $\text{MSE}(G) = \frac{\text{SSE}(G)}{n - \text{npar}(G)}$ is the Mean Squared Error of MODEL G, $S^2_{X_j}$ is the sample variance of $X$ (*not* the unbiased estimator^[you could also use that, but then you'd have to replace the preceding $n$ by $n-1$]), and $R^2_{X_j}$ is the $R^2$ of a model predicting $X_j$ from all other predictors
$$X_{j,i} = \beta_0 + \beta_1 X_{1,i} + \ldots + \beta_{j-1} X_{j-1,i} + \beta_{j+1} X_{j+1,i} + \ldots + \beta_m X_{m,i} + \epsilon_i$$
The value of $R^2_{X_j}$ is a measure of the redundancy of the predictor $X_j$. For $X_1$, this is represented in the right-hand plot of Figure \@ref(fig:sse-partition-multicollinearity) as the proportion $\frac{D+G}{B+D+E+G}$. For $X_2$, the corresponding proportion is $\frac{D+G}{C+D+F+G}$. 

If the value of $X_j$ could be perfectly predicted from the values of all other predictors, then $X_j$ would not be able to provide any information about the dependent variable $Y$ _over and above_ the other predictors. Remember that the slope in a multiple regression model represents an increase or decrease in the conditional mean of $Y$ for a one-unit increase in the predictor. If any increase in $X_j$ can be perfectly predicted by an increase or decrease in the other predictors, then those in- and decreases of the other predictors can also perfectly account for the associated in- or decrease in the conditional mean of $Y$. Very high values of $R^2_{X_j}$ indicate that a predictor $X_j$ will not be able to uniquely account for much variation in $Y$. The value of $1 - R^2_{X_j}$ then conversely represents the _uniqueness_ of predictor $X_j$, and this value is commonly referred to as the "tolerance":
\begin{equation}
\text{tolerance}(X_j) = 1 - R^2_{X_j}
(\#eq:tolerance)
\end{equation}

All else being equal, a decrease in the tolerance will increase the width of the confidence interval. In the extreme where $1 -  R^2_{X_j} = 0$, the denominator would be 0, and the confidence interval would be $$\hat{\beta}_j \pm \sqrt{\frac{F_{1,n - \text{npar}(G);\alpha} \text{MSE}(G)}{0}} = \hat{\beta}_j \pm \infty$$
An infinite confidence interval means that we would _not_ be able to reject _any_ null-hypothesis regarding the slope of $X_j$. In other words: anything goes.

A tolerance of 0 and complete redundancy is not common if you are working with real variables. It can certainly happen if you include the same predictor twice in a model, or if you for instance include a number of predictors and also their case-wise mean. For example, in the model
$$Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + \beta_3 \left( \frac{X_{1,i} + X_{2,i}}{2} \right) + \epsilon_i$$
the third predictor (the case-wise average of $X_1$ and $X_2$) is completely redundant. We can rewrite this model in an equivalent form as
$$\begin{aligned}
Y_i &= \beta_0 + (\beta_1 + \tfrac{1}{2} \beta_3) X_{1,i} + (\beta_2 + \tfrac{1}{2} \beta_3) X_{2,i} + \epsilon_i \\
Y_i &= \beta_0 + \beta_1' X_{1,i} + \beta_2' X_{2,i} + \epsilon_i 
\end{aligned}$$
which would give _exactly_ the same predictions of $Y_i$. Moreover, while we can estimate the slopes $\beta_1'$ and $\beta_2'$ as usual, we would _not_ be able to determine uniquely which part of $\hat{\beta}_1'$ should be assigned to $\beta_1$, and which part should be assigned to $\tfrac{1}{2}\beta_3$. Any such assignment would be arbitrary. For instance, if $\hat{\beta}_1' = 2$, then we could say that $\beta_1 = 1$ and $\beta_3 = 2$, but also that $\beta_1 = 2$ and $\beta_3 = 0$, without changing anything substantial about the relation between $Y$ and $X_1$ and $X_2$.

### Detecting and dealing with multicollinearity

Generally, tolerance values below $1- R^2_{X_j} = .20$ are considered potentially problematic. This is by no means a precise cut-off value, but it can guide you nonetheless. Some statistics programs also report the __variance inflation factor__ (VIF), which is defined as 
$$\text{VIF} = \frac{1}{1 - R^2_{X_j}}$$
As this is just a transformation of the tolerance, I don't really see the point. Although it might be more intuitive as a measure of collinearity in the sense that high values indicate stronger collinearity, the scale of this measure is between 1 and $\infty$, and when possible, I find a bounded scale more easily interpretable. But, as there is a one-to-one relation between the tolerance and VIF measures, both provide exactly the same information, so either is fine.

Multicollinearity is not a major issue if you are mainly concerned with predicting the dependent variable. The model as a whole will provide the best possible predictions, with or without collinearity. Problems arise mainly when your goal is to infer and test the true values of the parameters as reflecting relations in the Data Generating Process. When there is redundancy between predictors (high multicollinearity), the estimates of the slopes of each predictor become unreliable, and hypothesis tests will have low power. A "quick-and-dirty" solution is to simply remove some of the collinear predictors, keeping a set of predictors which are more independent of each other. The choice of which predictors to remove can be rather arbitrary however, and eliminating a predictor from consideration in an analysis does not mean that this predictor has no theoretical importance. Another option, if possible, is to increase the sample size $n$, which may reduce the width of the confidence intervals to more agreeable magnitudes. Alternatively, you could consider transforming all predictors into a set of orthogonal (uncorrelated) variables using a technique such as [Principal Components Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis), where each new variable is a linear combination of the original predictors. Those new predictors can be difficult to interpret, however. Another option is to use a technique called _ridge regression_ [@hoerl1970ridge], which allows inclusion of all predictors in the model with a reduced variance of the parameter estimates, but thereby introduces some bias in these estimates.

## Outliers {#sec:04b-outliers}

When we analysed the data regarding Trump votes and hate groups up to now, we excluded the data from the District of Columbia (Washington D.C.), because, while it is not part of any US state, it is itself not a state either. If we had included the data from this district, the results would have been quite different. Figure \@ref(fig:scatterplot-trump-votes-hate-groups-all-data) shows the Trump votes and hate groups with the Distric of Columbia included. You can see that the data point for the District of Columnbia is far removed from all other data points. The District of Columbia has both a very high number of hate groups (`r trump2016[trump2016$state == "District of Columbia","hate_groups_per_million"]` per million citizens), and a low percentage of votes for Trump (`r trump2016[trump2016$state == "District of Columbia","percent_Trump_votes"]`%). This makes the data District of Columbia rather unusual when compared to the 50 states. The plot also shows the estimated regression line of a simple regression model (with hate groups as a single predictor) for all the data, and the data excluding the District of Columbia. After inclusion of the District of Columbia, the estimated relation goes from positive to negative!

```{r scatterplot-trump-votes-hate-groups-all-data,fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the District of Columbia, as a function of the number of hate groups per 1 million citizens. The solid line is the estimated regression line for all data, and the dotted line the estimated regression line excluding the District of Columbia.", fig.align="center"}
dat <- trump2016
moda <- lm(percent_Trump_votes ~ hate_groups_per_million,data=dat)
modb <- lm(percent_Trump_votes ~ hate_groups_per_million,data=subset(dat,state != "District of Columbia"))
plt <- ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes,name=state)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_abline(intercept = coefficients(moda)[1], slope = coefficients(moda)[2], alpha=.6, colour=viridis::viridis_pal()(1)[1]) + geom_abline(intercept = coefficients(modb)[1], slope = coefficients(modb)[2], lty=3, alpha=.6, colour=viridis::viridis_pal()(1)[1])
if(knitr::is_html_output()) {
  plotly::ggplotly(plt, tooltip=c("name","x","y"))
} else {
  plt
}
```

When we estimate a multiple regression model for all the data, we obtain the results given in Table \@ref(tab:trump-multiple-regression-ANOVA-table-all-data). Instead of a significant positive relation between hate groups and Trump votes, we now find the slope is estimated close to 0, and not significant. Note that this estimated relation is not negative (as in Figure \@ref(fig:scatterplot-trump-votes-hate-groups-all-data)), which illustrates the fact that the unique relations determined in multiple regression models can differ substantially from the bivariate relations of simple regression models.

```{r trump-multiple-regression-ANOVA-table-all-data}
mod <- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat)
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(mod),Error=NA),car::Anova(mod, type=3))
rownames(tab) <- c("Intercept","Hate groups per million", "% bachelors degree or higher","Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Null-hypothesis significance tests by model comparisons and the F statistic.", escape = FALSE, booktabs=TRUE, linesep="")
options(opts)
```

The question now is: how reasonable are these new results? When looking at all 50 states (excluding the District of Columbia), there appears to be rather good evidence for a positive relation between hate groups and Trump votes, whether assessing this relation in a simple regression model or in a multiple regression model where we control for the possible confounding effect of education level. That these results change dramatically after inclusion of a single additional data point indicates that this additional data point might be an __outlier__. An outlier can be rather imprecisely defined as an _unusual data point that does not follow the general trend of the rest of the data_. Clearly, the District of Columbia does not appear to follow the general trend in terms of Trump votes and hate groups. 

```{r scatterplot-trump-votes-education-all-data,fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the District of Columbia, as a function of the percentage of citizens with a bachelors degree or higher. The solid line is the estimated regression line for all data, and the dotted line the estimated regression line excluding the District of Columbia."}
moda <- lm(percent_Trump_votes ~ percent_bachelors_degree_or_higher,data=dat)
modb <- lm(percent_Trump_votes ~ percent_bachelors_degree_or_higher,data=subset(dat,state != "District of Columbia"))
plt <- ggplot(dat,aes(x=percent_bachelors_degree_or_higher,y=percent_Trump_votes,name=state)) + geom_point() + xlab("% bachelors degree or higher") + ylab("% votes for Trump") + geom_abline(intercept = coefficients(moda)[1], slope = coefficients(moda)[2], alpha=.8, colour = viridis::viridis_pal()(1)[1]) + geom_abline(intercept = coefficients(modb)[1], slope = coefficients(modb)[2], lty=3, alpha=.8, colour=viridis::viridis_pal()(1)[1])
if(knitr::is_html_output()) {
  plotly::ggplotly(plt, tooltip=c("name","x","y"))
} else {
  plt
}
```

Figure \@ref(fig:scatterplot-trump-votes-education-all-data) shows the relation between Trump votes and education level for all the data. While the District of Columbia is again far removed from the other data points, it doesn't seem to "break" the negative relation between education level and Trump votes. Indeed, the estimated regression lines for a simple regression model predicting Trump votes from education level don't differ all that much whether we include the District of Columbia or not.

So, sometimes "unusual" data points can have a large influence on model estimates, and sometimes not. Outliers which do not affect the models estimates much are of relatively little concern. However, outliers which have an undue influence on the model estimates are better removed from the dataset. You don't generally want the results of an analysis to depend largely on a single observation (it just wouldn't  be fair to the other data points). For an observation to have a large effect on the model estimates, it generally has to have values on the predictor variables which are far from the average, and a value on the dependent variable which is far removed from the true regression plane (i.e. the true residual $\epsilon_i$ is large). If an observation has values on the predictor variables which are close to other observations, then those other observations generally reduce the impact of the outlier, because they will have values on the dependent variable which are not unusual (thus "pulling" the model estimates towards them and with that towards the true values). If an observation has a value on the dependent variable which is not far removed from the true regression plane, then it will not bias the solution either.

There are several measures which aim to determine the unusualness and undue influence of observations on the model estimates. Commonly used ones include the __leverage__ or __lever__ (which assess whether the predictor values are far from the average), __Studentized residuals__ (which aim to estimate whether an observation is far from the true regression line), and __Studentized deleted residuals__ and __Cook's distances__ (which aim to assess both). Some of these (e.g. Studentized deleted residuals) are effectively hypothesis tests applied to each individual observation. That has the benefit of providing them with a formal criterion to identify observations as outliers or not. On the other hand, by performing a total of $n$ hypothesis tests with a significance level of $\alpha$, on average $n \times \alpha$ of these tests will involve a Type 1 error (a false rejection of the null hypothesis that an observation is not an outlier). These measures are therefore likely to classify observations as outliers which are not truly outliers. Other measures such as Cook's distance use more heuristic cut-off values to determine when there is a needs to carefully consider whether an observation is an outlier or not. 

At this point, I don't want to go into the mathematical details of these outlier detection measures. For now, I just want you to be aware of the potential problems that arise from outliers. Often, a thorough visual exploration of the data will indicate potential outliers. If you suspect an outlier, it is useful to repeat an analysis with and without including the potential outlier. When the results differ substantially, you will then need to carefully consider whether there are good reasons to exclude the outlying observation from the analysis. In the case of the District of Columbia, there are several reasons to exclude the District of Columbia. Not only does this electoral district has an unusual high number of hate groups, and an unusual low number of Trump votes, the district is not a state but a single city (Washington D.C.), with consequently a much higher population density than the 50 states. As the political capitol of the United States, it is likely to be different in many aspects related to politics. As such, treating this as a separate entity from the 50 US states seems reasonable.


## In practice

Linear regression is the "bread and butter" of data analysis. Whilst the name is generally reserved for models with only metric predictors, we we will see later on, the model we have discussed here is the foundation of the General Linear Model. 

1. __Explore the data__. Check the distribution of the dependent variable and each (potential) predictor. Are there outlying or otherwise "strange" observations? Also explore pairwise relations between the (potential) predictors and the dependent variable, as well as between the predictors themselves. This may point to potential issues with the assumption of linearity (although pairwise linearity or nonlinearity is not necessarily reflective of linearity in a multiple regression model). 

2. Estimate the multiple regression model. Then check for potential issues in the assumptions with e.g. predicted vs residual plots. This may point to potential outliers, but you can also check for overly influential cases with a measure such as Cook's distance. A value of Cook's distance larger than 1 is generally considered as a cut-off. If there are clear outliers in the data, remove these, and then re-estimate the model. Also check for issues with multicollinearity.

3. Report the results. When reporting the results, make sure that you include all relevant statistics. For example, one way to write the results of the analysis reported in this chapter is as follows:

> We assessed the relation between votes for Donald Trump and the presence of hate groups in 50 states of the USA with a multiple linear regression model (we excluded data from the District of Columbia, as this was a clear outlier). For each state, we computed the number of hate groups per one million citizen, to correct for differences in population size in the states. As the level of education might be a possible confound, we controlled for education level (measured as the percentage of the population with a Bachelor's degree or higher) by including this as a second predictor. The model accounted for a large proportion of the variance in Trump votes, `r papaja::apa_print(mod)$full_result$modelfit$r2`. The analysis showed a significant positive effect of hate groups on votes for Donald Trump, `r papaja::apa_print(mod)$full_result$hate_groups_per_million`. In addition, we found a significant negative relation between education level and votes for Donald Trump, `r papaja::apa_print(mod)$full_result$percent_bachelors_degree_or_higher`.

<!--
## Summary
-->


<!--chapter:end:04b-GLM-multiple-regression.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Moderation and mediation {#ch-moderation-mediation}

In this chapter, we will focus on two ways in which one predictor variable may affect the relation between another predictor variable and the dependent variable. Moderation means the strength of the relation (in terms of the slope) of a predictor variable is determined by the value of another predictor variable. For instance, while physical attractiveness is generally positively related to mating success, for very rich people, physical attractiveness may not be so important. This is also called an interaction between the two predictor variables. Mediation is a different way in which two predictors affect a dependent variable. It is best thought of as a __causal chain__, where one predictor variable determines the value of another predictor variable, which then in turn determines the value of the dependent variable. the difference between moderation and mediation is illustrated in Figure \@ref(fig:moderation-mediation-difference-graph).

```{tikz moderation-mediation-difference-graph, fig.cap="Graphical depiction of the difference between moderation and mediation. Moderation means that the effect of a predictor ($X_1$) on the dependent variable ($Y$) depends on the value of another predictor ($X_2$). Mediation means that a predictor ($X_1$) affects the dependent variable ($Y$) indirectly, through its relation to another predictor ($X_2$) which is directly related to the dependent variable.", out.width="80%"}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\begin{tikzpicture}
	\begin{scope}{
		\pgfsetxvec{\pgfpoint{2cm}{0cm}}
		\pgfsetyvec{\pgfpoint{0cm}{1.5cm}}
		\tikzstyle{every node}=[minimum size=.95cm,circle]
		\draw node[draw] (x1) at (0,0) {$X_1$};
		\draw node[draw] (x2) at (1,1) {$X_2$};
		\draw node[draw] (y) at (2,0) {$Y$};
		\coordinate (c) at ($ (x1)!.5!(y)$);
		
		\draw node[align=center] at (1,2) {\textbf{moderation}};

		\draw[->, line width=.5mm] (x1) -- (y);
		\draw[->, line width=.5mm] (x2) -- (c);
	}
	\end{scope}

		\begin{scope}[shift={(7,0)}]{
			\pgfsetxvec{\pgfpoint{2cm}{0cm}}
			\pgfsetyvec{\pgfpoint{0cm}{1.5cm}}
			\tikzstyle{every node}=[minimum size=.95cm,circle]
			\draw node[draw] (x1) at (0,0) {$X_1$};
			\draw node[draw] (x2) at (1,1) {$X_2$};
			\draw node[draw] (y) at (2,0) {$Y$};
			
			\draw node[align=center] at (1,2) {\textbf{mediation}};

			\draw[->, line width=.5mm] (x1) -- (x2);
			\draw[->, line width=.5mm] (x2) -- (y);
		}
		\end{scope}
\end{tikzpicture}

```


## Moderation

```{r}
library(sdamr)
data("speeddate")
```

### Physical attractiveness and intelligence in speed dating

@fisman2006gender conducted a large scale experiment^[Here, we analyse only a subset of their data.] on dating behaviour. They placed their participants in a speed dating context, where they were randomly matched with a number of potential partners (between `r min(table(speeddate$iid))` and `r max(table(speeddate$iid))`) and could converse for four minutes. As part of the study, after each meeting, participants rated how much they liked their speed dating partners, as well as more specifically on their attractiveness, sincerity, intelligence, fun, and ambition. We will focus in particular on ratings of physical attractiveness, fun, and intelligence, and how these are related to the general liking of a person. Ratings were given on a 10-point scale, from 1 ("awful") to 10 ("great"). A multiple regression analysis predicting general liking from attractiveness, fun, and intelligence (Table \@ref(tab:multiple-regression-speed-dating-just-main-effects)) shows that all three predictors have a significant and positive relation with general liking.

```{r multiple-regression-speed-dating-just-main-effects}
modg <- lm(other_like ~ other_attr + other_intel + other_fun, data=speeddate)
tab <- summary(modg)$coefficients
rownames(tab) <- c("Intercept","Attractiveness","Intelligence","Fun")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SE}(\\hat{\\beta})$", "$t$", "$P(\\geq \\lvert t \\rvert)$")
knitr::kable(tab, caption="Multiple regression predicting liking from attractiveness, intelligence, and fun.", escape=FALSE, booktabs=TRUE, linesep="")
```

### Conditional slopes

If we were to model the relation between overall liking and physical attractiveness and intelligence, we might use a multiple regression model such as:^[Note that I'm using more descriptive labels here. If you prefer the more abstract version, then you can replace $Y_i = \texttt{like}_i$, $\beta_1 = \beta_{\texttt{attr}}$,  $X_{1,i} = \texttt{attr}_i$. $\beta_2 = \beta_{\texttt{intel}}$,  $X_{2,i} = \texttt{intel}_i$.]
$$\texttt{like}_i = \beta_0 + \beta_{\texttt{attr}} \times \texttt{attr}_i + \beta_\texttt{intel} \times \texttt{intel}_i + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
which is estimated as 
$$`r mod <- lm(other_like ~ other_attr + other_intel, data=speeddate); write_GLM_equation(mod, dv_name = "like", iv_names = c("attr","intel"), digits=3)`$$
The estimates indicate a positive relation to liking of both attractiveness and intelligence. Note that the values of the slopes are different from those in Table \@ref(tab:multiple-regression-speed-dating-just-main-effects). The reason for this is that the model in the Table also includes fun as a predictor. Because the slopes reflect _unique effects_, these depend on all predictors included in the model. When there is dependence between the predictors (i.e. there is multicollinearity) both the estimates of the slopes and the corresponding significance tests will vary when you add or remove predictors from the model. 

```{r}
coef <- coefficients(mod)
vals1 <- c(8,2)
like1 <- sum(coef*c(1,vals1))
vals2 <- c((like1 - coef[1] - coef[3]*8)/(coef[2]) , 8)
```

In the model above, a relative lack in physical attractiveness can be overcome by high intelligence, because in the end, the general liking of someone depends on the sum of both attractiveness and intelligence (each "scaled" by their corresponding slope). For example, someone with an attractiveness rating of $\texttt{attr}_i = `r vals1[1]`$ and an intelligence rating of $\texttt{intel}_i = `r vals1[2]`$ would be expected to be liked as much as a partner as someone with an attractiveness rating of $\texttt{attr}_i = `r vals2[1]`$ and an intelligence rating of $\texttt{intel}_i = `r vals2[2]`$:
$$\begin{aligned}
\texttt{like}_i &= `r coef[1]` + `r coef[2]` \times `r vals1[1]` + `r coef[3]` \times `r vals1[2]` = `r like1` \\
\texttt{like}_i &= `r coef[1]` + `r coef[2]` \times `r vals2[1]` + `r coef[3]` \times `r vals2[2]` = `r like1`
\end{aligned}$$

But what if for those lucky people who are very physically attractive, their intelligence doesn't matter _that much_, or even _at all_? And what if, for those lucky people who are very intelligent, their physical attractiveness doesn't really matter much or at all? In other words, what if the more attractive people are, the less intelligence determines how much other people like them as a potential partner, and conversely, the more intelligent people are, the less attractiveness determines how much others like them as a potential partner? This implies that the effect of attractiveness on liking depends on intelligence, and that the effect of intelligence on liking depends on attractiveness. Such dependence is not captured by the multiple regression model above. While a relative lack of intelligence might be overcome by a relative abundance of attractiveness, for any level of intelligence, the additional effect of attractiveness is the same (i.e., an increase in attractiveness by one unit will always result in an increase of the predicted liking of `r coef[2]`).

Let's define $\beta_{\texttt{attr}|\texttt{intel}_i}$ as the slope of $\texttt{attr}$ conditional on the value of $\texttt{intel}_i$. That is, we allow the slope of $\texttt{attr}$ to vary as a function of $\texttt{intel}$. Similarly, we can define $\beta_{\texttt{intel}|\texttt{attr}_i}$ as the slope of $\texttt{intel}$ conditional on the value of $\texttt{attr}$. Our regression model can then be written as:
\begin{equation}
\texttt{like}_i = \beta_0 + \beta_{\texttt{attr}|\texttt{intel}_i} \times \texttt{attr}_i + \beta_{\texttt{intel} | \texttt{attr}_i} \times \texttt{intel}_i + \epsilon_i
(\#eq:reg-like-attr-intel-interactions)
\end{equation}
That's a good start, but what would the value of $\beta_{\texttt{attr}|\texttt{intel}_i}$ be? Estimating the slope of $\texttt{attr}$ for each value of $\texttt{intel}$ by fitting regression models to each subset of data with a particular value of $\texttt{intel}$ is not really doable. We'd need lots and lots of data, and furthermore, we wouldn't also be able to simultaneously estimate the value of $\beta_{\texttt{intel} | \texttt{attr}_i}$. We need to supply some structure to $\beta_{\texttt{attr}|\texttt{intel}_i}$ to allow us to estimate its value without overcomplicating things. 

### Modeling slopes with linear models

One idea is to define $\beta_{\texttt{attr}|\texttt{intel}_i}$ with a linear model:
$$\beta_{\texttt{attr}|\texttt{intel}_i} = \beta_{\texttt{attr},0} + \beta_{\texttt{attr},1} \times \texttt{intel}_i$$
This is just like a simple linear regression model, but now the "dependent variable" is the slope of $\texttt{attr}$. Defined in this way, the slope of $\texttt{attr}$ is $\beta_{\texttt{attr},0}$ when $\texttt{intel}_i = 0$, and for every one-unit increase in $\texttt{intel}_i$, the slope of $\texttt{attr}$ increases (or decreases) by $\beta_{\texttt{attr},1}$. For example, let's assume $\beta_{\texttt{attr},0} = 1$ and $\beta_{\texttt{attr},1} = 0.5$. For someone with an intelligence rating of $\texttt{intel}_i = 0$, the slope of $\texttt{attr}$ is
$$\beta_{\texttt{attr}|\texttt{intel}_i} = 1 + 0.5 \times 0 = 1$$
For someone with an intelligence rating of $\texttt{intel}_i = 1$, the slope of $\texttt{attr}$ is
$$\beta_{\texttt{attr}|\texttt{intel}_i} = 1 + 0.5 \times 1 = 1.5$$
For someone with an intelligence rating of $\texttt{intel}_i = 2$, the slope of $\texttt{attr}$ is
$$\beta_{\texttt{attr}|\texttt{intel}_i} = 1 + 0.5 \times 2 = 2$$
As you can see, for every increase in intelligence rating by 1 point, the slope of $\texttt{attr}$ increases by 0.5. In such a model, there will be values of $\texttt{intel}$ which result in a negative slope of $\texttt{attr}$. For instance, for $\texttt{intel}_i = -4$, the slope of $\texttt{attr}$ is
$$\beta_{\texttt{attr}|\texttt{intel}_i} = 1 + 0.5 \times (-4) = - 1$$

We can define the slope of $\texttt{intel}$ in a similar manner as
$$\beta_{\texttt{intel}|\texttt{attr}_i} = \beta_{\texttt{intel},0} + \beta_{\texttt{intel},1} \times \texttt{attr}_i$$
When we plug these definitions into Equation \@ref(eq:reg-like-attr-intel-interactions), we get
$$\begin{aligned}
\texttt{like}_i &= \beta_0 + (\beta_{\texttt{attr},0} + \beta_{\texttt{attr},1} \times \texttt{intel}_i) \times \texttt{attr}_i + (\beta_{\texttt{intel},0} + \beta_{\texttt{intel},1} \times \texttt{attr}_i) \times \texttt{intel}_i + \epsilon_i \\
&= \beta_0 + \beta_{\texttt{attr},0} \times \texttt{attr}_i + \beta_{\texttt{intel},0} \times \texttt{intel}_i + (\beta_{\texttt{attr},1} + \beta_{\texttt{intel},1}) \times (\texttt{attr}_i \times \texttt{intel}_i) + \epsilon_i
\end{aligned}$$

Looking carefully at this formula, you can recognize a multiple regression model with three predictors: $\texttt{attr}$, $\texttt{intel}$, and a new predictor $\texttt{attr}_i \times \texttt{intel}_i$, which is computed as the product of these two variables. While it is thus related to both variables, we can treat this product as just another predictor in the model. The slope of this new predictor is the sum of two terms, $\beta_{\texttt{attr},1} + \beta_{\texttt{intel},1}$. Although we have defined these as different things (i.e. as the effect of $\texttt{intel}$ on the slope of $\texttt{attr}$, and the effect of $\texttt{attr}$ on the slope of $\texttt{intel}$, respectively), their value can not be estimated uniquely. We can only estimate their summed value. That means that moderation in regression is "symmetric", in the sense that each predictor determines the slope of the other one. We can not say that it is just intelligence that determines the effect of attraction on liking, nor can we say that it is just attraction that determines the effect of intelligence on liking. The two variables __interact__ and each determine the other's effect on the dependent variable.

With that in mind, we can simplify the notation of the resulting model somewhat, by renaming the slopes of the two predictors to $\beta_{\texttt{attr}} = \beta_{\texttt{attr},0}$ and $\beta_{\texttt{intel}} = \beta_{\texttt{intel},0}$, and using a single parameter for the sum $\beta_{\texttt{attr} \times \texttt{intel}} =  \beta_{\texttt{attr},1} + \beta_{\texttt{intel},1}$:

\begin{equation}
\texttt{like}_i = \beta_0 + \beta_{\texttt{attr}} \times \texttt{attr}_i + \beta_{\texttt{intel}} \times \texttt{intel}_i + \beta_{\texttt{attr} \times \texttt{intel}} \times (\texttt{attr} \times \texttt{intel})_i + \epsilon_i
\end{equation}

```{r}
mod0 <- lm(other_like ~ other_attr + other_intel, data=speeddate)
mod1 <- lm(other_like ~ other_attr*other_intel, data=speeddate)
```

Estimating this model gives
$$`r write_GLM_equation(mod1, dv_name = "like", iv_names = c("attr","intel","(attr}\\times\\texttt{intel)"), digits=3, include_sde = FALSE)`$$
The estimate of the slope of the interaction, $\hat{\beta}_{\texttt{attr} \times \texttt{intel}} = `r coefficients(mod1)[4]`$, is negative. That means that the _higher_ the value of $\texttt{intel}$, the _less_ steep the regression line relating $\texttt{attr}$ to $\texttt{like}$. At the same time, the _higher_ the value of $\texttt{attr}$, the _less_ steep the regression line relating $\texttt{intel}$ to $\texttt{like}$. You can interpret this as meaning that for more intelligent people, physical attractiveness is less of a defining factor in their liking by a potential partner. And for more attractive people, intelligence is less important.

A graphical view of this model, and the earlier one without moderation, is provided in Figure \@ref(fig:speeddate-multiple-regression-interaction-exaggerated). The plot on the left represents the model which does not allow for interaction. You can see that, for different values of intelligence, the model predicts parallel regression lines for the relation between attractiveness and liking. While intelligence affects the intercept of these regression lines, it does not affect the slope. In the plot on the right -- although subtle -- you can see that the regression lines are _not_ parallel. This is a model with an interaction between intelligence and attractiveness. For different values of intelligence, the model predicts a linear relation between attractiveness and liking, but crucially, intelligence determines both the intercept _and_ slope of these lines.

```{r speeddate-multiple-regression-interaction-exaggerated, fig.cap="Liking as a function of attractiveness (intelligence) for different levels of intelligence (attractiveness), either without moderation or with moderation of the slope of attraciveness by intelligence. Note that the actual values of liking, attractiveness, and intelligence, are whole numbers (ratings on a scale between 1 and 10). For visualization purposes, the values have been randomly jittered by adding a Normal-distributed displacement term.", fig.width=5, fig.height=5, out.width='49%', fig.show="hold"}
set.seed(23487610)
coefs <- coefficients(mod0)
adj <- 5
dat <- subset(speeddate, !(is.na(other_like) | is.na(other_attr) | is.na(other_intel))) 
dat$other_like <- dat$other_like + rnorm(nrow(dat),0,.2)
dat$other_attr <- dat$other_attr + rnorm(nrow(dat),0,.2)
dat$other_intel <- dat$other_intel + rnorm(nrow(dat),0,.2)
ggplot(dat,aes(x=other_attr, y=other_like, colour=other_intel)) + geom_point() + geom_abline(intercept = coefs[1] + (min(dat$other_intel)- adj)*coefs[3], slope = coefs[2], colour=viridis::viridis_pal()(3)[1]) + geom_abline(intercept = coefs[1] + (min(dat$other_intel) -adj + (max(dat$other_intel) - min(dat$other_intel) + 2*adj)/2)*coefs[3], slope = coefs[2], colour=viridis::viridis_pal()(3)[2]) + geom_abline(intercept = coefs[1] + (max(dat$other_intel) + adj)*coefs[3], slope = coefs[2], colour = viridis::viridis_pal()(3)[3]) + theme(legend.position = "bottom") + labs(colour = "intelligence") + xlab("attractiveness") + ylab("liking") + ggtitle("Without moderation") + expand_limits(colour = c(min(dat$other_intel)- adj, max(dat$other_intel) + adj))

coefs <- coefficients(mod1)
ggplot(dat,aes(x=other_attr, y=other_like, colour=other_intel)) + geom_point() + geom_abline(intercept = coefs[1] + (min(dat$other_intel) - adj)*coefs[3], slope = coefs[2] + (min(dat$other_intel) - adj)*coefs[4], colour=viridis::viridis_pal()(3)[1]) + geom_abline(intercept = coefs[1] + (min(dat$other_intel) - adj + ((max(dat$other_intel) - min(dat$other_intel)) + 2*adj)/2) *coefs[3], slope = coefs[2] + (min(dat$other_intel) - adj + (max(dat$other_intel) - min(dat$other_intel) + 2*adj)/2) * coefs[4], colour=viridis::viridis_pal()(3)[2]) + geom_abline(intercept = coefs[1] + (max(dat$other_intel) + adj)*coefs[3], slope = coefs[2] + (max(dat$other_intel) + adj)*coefs[4], colour = viridis::viridis_pal()(3)[3]) + theme(legend.position = "bottom") + labs(colour = "intelligence") + xlab("attractiveness") + ylab("liking") + ggtitle("With moderation")  + expand_limits(colour = c(min(dat$other_intel)- adj, max(dat$other_intel) + adj))

ggplot(dat,aes(x=other_intel, y=other_like, colour=other_attr)) + geom_point() + geom_abline(intercept = coefs[1] + (min(dat$other_attr)- adj)*coefs[3], slope = coefs[2], colour=viridis::viridis_pal()(3)[1]) + geom_abline(intercept = coefs[1] + (min(dat$other_attr) -adj + (max(dat$other_attr) - min(dat$other_attr) + 2*adj)/2)*coefs[3], slope = coefs[2], colour=viridis::viridis_pal()(3)[2]) + geom_abline(intercept = coefs[1] + (max(dat$other_attr) + adj)*coefs[3], slope = coefs[2], colour = viridis::viridis_pal()(3)[3]) + theme(legend.position = "bottom") + labs(colour = "attractiveness") + xlab("intelligence") + ylab("liking") + ggtitle("Without moderation") + expand_limits(colour = c(min(dat$other_attr)- adj, max(dat$other_attr) + adj))

coefs <- coefficients(mod1)
ggplot(dat,aes(x=other_intel, y=other_like, colour=other_attr)) + geom_point() + geom_abline(intercept = coefs[1] + (min(dat$other_attr) - adj)*coefs[3], slope = coefs[2] + (min(dat$other_attr) - adj)*coefs[4], colour=viridis::viridis_pal()(3)[1]) + geom_abline(intercept = coefs[1] + (min(dat$other_attr) - adj + ((max(dat$other_attr) - min(dat$other_attr)) + 2*adj)/2) *coefs[3], slope = coefs[2] + (min(dat$other_attr) - adj + (max(dat$other_attr) - min(dat$other_attr) + 2*adj)/2) * coefs[4], colour=viridis::viridis_pal()(3)[2]) + geom_abline(intercept = coefs[1] + (max(dat$other_attr) + adj)*coefs[3], slope = coefs[2] + (max(dat$other_attr) + adj)*coefs[4], colour = viridis::viridis_pal()(3)[3]) + theme(legend.position = "bottom") + labs(colour = "attractiveness") + xlab("intelligence") + ylab("liking") + ggtitle("With moderation")  + expand_limits(colour = c(min(dat$other_attr)- adj, max(dat$other_attr) + adj))

```

Note that we have constructed this model by simply including a new predictor in the model, which is computed by multiplying the values of $\texttt{attr}$ and $\texttt{intel}$. While including such an "interaction predictor" has important implications for the resulting relations between $\texttt{attr}$ and $\texttt{like}$ for different values of $\texttt{intel}$, as well as the relations between $\texttt{intel}$ and $\texttt{like}$ for different values of $\texttt{attr}$, the model itself is just like any other regression model. Thus, parameter estimation and inference are exactly the same as before. Table \@ref(tab:multiple-regression-with-interaction-ANOVA) shows the results of comparing the full MODEL G (with three predictors) to different versions of MODEL R, where in each we fix one of the parameters to 0. As you can see, these comparisons indicate that we can reject the null hypothesis $H_0$: $\beta_0 = 0$, as well as $H_0$: $\beta_{\texttt{attr}} = 0$ and $H_0$: $\beta_{\texttt{intel}} = 0$. However, as the p-value is above the conventional significance level of $\alpha=.05$, we would not reject the null hypothesis $H_0$: $\beta_{\texttt{attr} \times \texttt{intel}} = 0$. That implies that, in the context of this model, there is not sufficient evidence that there is an interaction. That may seem a little disappointing. We've done a lot of work to construct a model where we allow the effect of attractiveness to depend on intelligence, and vice versa. And now the hypothesis test indicates that there is no evidence that this moderation is present. As we will see later, there is evidence of this moderation when we also include $\texttt{fun}$ in the model. I have left this predictor out of the model for now to keep things as simple as possible.

```{r multiple-regression-with-interaction-ANOVA}
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(mod1),Error=NA),car::Anova(mod1, type=3))
rownames(tab) <- c("Intercept","$\\texttt{attr}$", "$\\texttt{intel}$","$\\texttt{intel} \\times \\texttt{attr}$","Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Multiple regression predicting liking from attractiveness, intelligence, and their interaction.", escape = FALSE, booktabs=TRUE, linesep="")
options(opts)

# tab <- summary(mod1)$coefficients
# rownames(tab) <- c("Intercept","Attractiveness","Intelligence","Attr $\\times$ Intel")
# colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SE}(\\hat{\\beta})$", "$t$", "$P(\\geq \\lvert t \\rvert)$")
# knitr::kable(tab, caption="Multiple regression predicting liking from attractiveness, intelligence, and their interaction.")
```

### Simple slopes and centering

It is very important to realise that in a model with interactions, there is no single slope for any of the predictors involved in an interaction, that is particularly meaningful in principle. An interaction means that the slope of one predictor varies as a function of another predictor. Depending on which value of that other predictor you focus on, the slope of the predictor can be positive, negative, or zero. Let's consider the model we estimated again:
$$`r write_GLM_equation(mod1, dv_name = "like", iv_names = c("attr","intel","(attr}\\times\\texttt{intel)"), digits=3, include_sde = FALSE)`$$
If we fill in a particular value for intelligence, say $\texttt{intel} = 1$, we can write this as 
```{r}
coef <- coefficients(mod1)
```
$$\begin{aligned}
\texttt{intel}_i &= `r coef[1]` + `r coef[2]` \times \texttt{attr}_i + `r coef[3]` \times 1 `r coef[4]` \times (\texttt{attr} \times 1)_i + \epsilon_i \\
&= (`r coef[1]` + `r coef[3]`) + (`r coef[2]` `r coef[4]`) \times \texttt{attr}_i + \epsilon_i \\
&= `r coef[1] + coef[3]` + `r coef[2] + coef[4]` \times \texttt{attr}_i + \epsilon_i
\end{aligned}$$

If we pick a different value, say $\texttt{intel} = 10$, the the model becomes 
$$\begin{aligned}
\texttt{intel}_i &= `r coef[1]` + `r coef[2]` \times \texttt{attr}_i + `r coef[3]` \times 10  `r coef[4]` \times (\texttt{attr} \times 10)_i + \epsilon_i \\
&= (`r coef[1]` + `r coef[3]` \times 10) + (`r coef[2]` `r coef[4]`\times 10) \times \texttt{attr}_i + \epsilon_i \\
&= `r coef[1] + 10*coef[3]` + `r coef[2] + 10*coef[4]` \times \texttt{attr}_i + \epsilon_i
\end{aligned}$$
This shows that the higher the value of intelligence, the lower the slope of $\texttt{attr}$ becomes. If you'd pick $\texttt{intel} = `r coef[2]/(-coef[4])`$, the slope would be exactly equal to 0.^[The value for which the slope is 0 is easily worked out as $\frac{\hat{\beta}_\texttt{attr}}{- \hat{\beta}_{\texttt{attr} \times \texttt{intel}}}$.] Because there is not just a single value of the slope, testing whether "the" slope of $\texttt{attr}$ is equal to 0 doesn't really make sense, because there is no single value to represent "the" slope. What, then, does $\hat{\beta}_\texttt{attr} =  `r coef[2]`$ represent? Well, it is the (estimated) slope of $\texttt{attr}$ when $\texttt{intel}_i = 0$. Similarly, $\hat{\beta}_\texttt{intel} =  `r coef[3]`$ is the estimated slope of $\texttt{intel}$ when $\texttt{attr}_i = 0$ 

A significance test of the null hypothesis $H_0$: $\beta_\texttt{attr} = 0$ is thus a test whether, when $\texttt{intel} = 0$, the slope of $\texttt{attr}$ is 0. This test is easy enough to perform, but is it interesting to know whether liking is related to attractiveness for people who's intelligence was rated as 0? Perhaps not. For one thing, the ratings were on a scale from 1 to 10, so no one could actually receive a rating of 0. Because the slope depends on $\texttt{intel}$ and we know that for some value of $\texttt{intel}$, the slope of $\texttt{attr}$ will equal 0, the hypothesis test will not be significant for some values of $\texttt{intel}$, and will be significant for others. At which value of $\texttt{intel}$ we might want to perform such a test is up to us, but the result seems somewhat arbitrary. 

That said, we might be interested in assessing whether there is an effect of $\texttt{attr}$ for particular values of $\texttt{intel}$. For instance, whether, for someone with an average intelligence rating, their physical attractiveness matters for how much someone likes them as a potential partner. We can obtain this test by __centering__ the predictors. Centering is basically just subtracting the sample mean of each value of a variable. So for example, we can center $\texttt{attr}$ as follows: $$\texttt{attr_cent}_i = \texttt{attr}_i - \overline{\texttt{attr}}$$
Centering does not affect the relation between variables. You can view it as a simple relabelling of the values, where the value which was the sample mean is now $\texttt{attr_cent}_i = \overline{\texttt{attr}} - \overline{\texttt{attr}} = 0$, all values below the mean are now negative, and values above the mean are now positive. The important part of this is that the centered predictor is 0 where the original predictor was at the sample mean. In a model with centered predictors
\begin{align}
\texttt{like}_i =& \beta_0 + \beta_{\texttt{attr_cent}} \times \texttt{attr_cent}_i + \beta_{\texttt{intel_cent}} \times \texttt{intel_cent}_i \\
&+ \beta_{\texttt{attr_cent} \times \texttt{intel_cent}} \times (\texttt{attr_cent} \times \texttt{intel_cent})_i + \epsilon_i
\end{align}
the slope $\beta_{\texttt{attr_cent}}$ is, as usual, the slope of $\texttt{attr_cent}$ whenever $\texttt{intel_cent}_i = 0$. We know that $\texttt{intel_cent}_i = 0$ when $\texttt{intel}_i = \overline{\texttt{intel}}$. Hence, $\beta_{\texttt{attr_cent}}$ is the slope of $\texttt{attr}$ when $\texttt{intel} = \overline{\texttt{intel}}$, i.e. it represents the effect of $\texttt{attr}$ for those with an average intelligence ratings.

Figure \@ref(fig:speeddate-multiple-regression-interaction-exaggerated-centered) shows the resulting model after centering both attractiveness and intelligence. When you compare this to the corresponding plot in Figure \@ref(fig:speeddate-multiple-regression-interaction-exaggerated), you can see that the only real difference is in the labels for the x-axis and the scale for intelligence. In all other respects, the uncentered and centered models predict the same relations between attractiveness and liking, and the models provide an equally good account, providing the same prediction errors. <!-- By changing the 0-point for both predictors, however, the centered and uncentered models can make different inferences about the "simple slopes" for attractiveness and intelligence, however. For example, in the centered model, the simple slope $\beta_\texttt{attr}$ reflects the   
-->

```{r speeddate-multiple-regression-interaction-exaggerated-centered, fig.cap="Liking as a function of centered attractiveness for different levels of (centered) intelligence in a model including an interaction between attractiveness and intelligence. Note that the actual values of liking, attractiveness, and intelligence, are whole numbers (ratings on a scale between 1 and 10). For visualization purposes, the values have been randomly jittered by adding a Normal-distributed displacement term.", fig.width=5, fig.height=5, out.width='49%', fig.show="hold"}
library(dplyr)
mod1c <- speeddate %>%
  mutate(attr_cent = scale(other_attr,scale=FALSE), intel_cent = scale(other_intel, scale=FALSE)) %>%
    lm(other_like ~ attr_cent*intel_cent, data = .)
coefs <- coefficients(mod1c)

set.seed(23487610)
adj <- 5
dat <- subset(speeddate, !(is.na(other_like) | is.na(other_attr) | is.na(other_intel))) 
dat$other_like <- dat$other_like + rnorm(nrow(dat),0,.2)
dat$other_attr <- dat$other_attr - mean(dat$other_attr) + rnorm(nrow(dat),0,.2)
dat$other_intel <- dat$other_intel - mean(dat$other_intel) + rnorm(nrow(dat),0,.2)
ggplot(dat,aes(x=other_attr, y=other_like, colour=other_intel)) + geom_point() + geom_abline(intercept = coefs[1] + (min(dat$other_intel) - adj)*coefs[3], slope = coefs[2] + (min(dat$other_intel) - adj)*coefs[4], colour=viridis::viridis_pal()(3)[1]) + geom_abline(intercept = coefs[1] + (min(dat$other_intel) - adj + ((max(dat$other_intel) - min(dat$other_intel)) + 2*adj)/2) *coefs[3], slope = coefs[2] + (min(dat$other_intel) - adj + (max(dat$other_intel) - min(dat$other_intel) + 2*adj)/2) * coefs[4], colour=viridis::viridis_pal()(3)[2]) + geom_abline(intercept = coefs[1] + (max(dat$other_intel) + adj)*coefs[3], slope = coefs[2] + (max(dat$other_intel) + adj)*coefs[4], colour = viridis::viridis_pal()(3)[3]) + theme(legend.position = "bottom") + labs(colour = "intelligence") + xlab("attractiveness") + ylab("liking")  + expand_limits(colour = c(min(dat$other_intel)- adj, max(dat$other_intel) + adj))
```


The results of all model comparisons after centering are given in Table \@ref(tab:multiple-regression-with-interaction-ANOVA-centered). A first important thing to notice is that __centering does not affect the estimate and test of the interaction term__. The slope of the interaction predictor reflects the increase in the slope relating $\texttt{attr}$ to $\texttt{like}$ for every one-unit increase in $\texttt{intel}$. Such changes to the steepness of the relation between $\texttt{attr}$ and $\texttt{like}$ should not -- and are not -- affected by changing the 0-point of the predictors through centering.  A second thing to notice is that __centering changes the estimates and test of the "simple slopes" and intercept__. In the centered model, the simple slope $\hat{\beta}_\texttt{attr_cent}$ reflects the effect of $\texttt{attr}$ on $\texttt{like}$ for cases with an average rating on $\texttt{intel}$. In Figure \@ref(fig:speeddate-multiple-regression-interaction-exaggerated-centered), this is (approximately) the regression line in the middle. In the uncentered model, the simple slope $\hat{\beta}_\texttt{attr}$ reflects the effect of $\texttt{attr}$ on $\texttt{like}$ for cases with $\texttt{intel} = 0$. In the top right plot in Figure \@ref(fig:speeddate-multiple-regression-interaction-exaggerated), this is (approximately) the lower regression line. This latter regression line is quite far removed from most of the data, because there are no cases with an intelligence rating of 0. The regression line for people with an average intelligence rating lies much more "within the cloud of data points", and reflects the model predictions for many more cases in the data. As a result, the reduction in the SSE that can be attributed to the simple slope is much higher in the centered model (Table \@ref(tab:multiple-regression-with-interaction-ANOVA-centered)) than the uncentered one (Table \@ref(tab:multiple-regression-with-interaction-ANOVA)). This results in a much higher $F$ statistic. You can also think of this as follows: because there are hardly any cases with an intelligence rating close to 0, estimating the effect of attractiveness on liking for these cases is rather difficult and unreliable. Estimating the effect of attractiveness on liking for cases with an average intelligence rating is much more reliable, because there are many more cases with a close-to-average intelligence rating.

```{r multiple-regression-with-interaction-ANOVA-centered}
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(mod1c),Error=NA),car::Anova(mod1c, type=3))
rownames(tab) <- c("Intercept","$\\texttt{attr_cent}$", "$\\texttt{intel_cent}$","$\\texttt{intel_cent} \\times \\texttt{attr_cent}$","Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Null-hypothesis significance tests after centering both predictors.", escape = FALSE, booktabs=TRUE, linesep="")
options(opts)
```

### Don't forget about fun! A model with multiple interactions {#sec:05-dont-forget-about-the-fun}

Up to now, we have looked at a model with two predictors, attractiveness and intelligence, and have allowed for an interaction between these. To simplify the discussion a little, we have not included $\texttt{fun}$ in the model. It is relatively straightforward to extend this idea to multiple predictors. For instance, it might also be the case that the effect of $\texttt{fun}$ is moderated by $\texttt{intel}$. To investigate this, we can estimate the following regression model:

$$\begin{aligned}
\texttt{like}_i =& \beta_0 + \beta_{\texttt{attr}} \times \texttt{attr}_i + \beta_{\texttt{intel}} \times \texttt{intel}_i + \beta_{\texttt{fun}} \times \texttt{fun}_i \\
&+ \beta_{\texttt{attr} \times \texttt{intel}} \times (\texttt{attr} \times \texttt{intel})_i + \beta_{\texttt{fun} \times \texttt{intel}} \times (\texttt{fun} \times \texttt{intel})_i + \epsilon_i
\end{aligned}$$

The results, having centered all predictors, are given in Table \@ref(tab:full-model-like). As you can see there, the simple slopes of $\texttt{attr}$, $\texttt{intel}$, and $\texttt{fun}$ are all positive. Each of these represents the effect of that predictor when the other predictors have the value 0. Because the predictors are centered, that means that e.g. the slope of $\texttt{attr}$ reflects the effect of attractiveness for people with an average rating on intelligence and fun. As before, the estimated interaction between $\texttt{attr}$ and $\texttt{intel}$ is negative, indicating that attractiveness has less of an effect on liking for those seen as more intelligent, and that intelligence has less of an effect for those seen as more attractive. The hypothesis test of this effect is now also significant, indicating that we have reliable evidence for this moderation. This shows that by including more predictors in a model, it is possible to increase the reliability of the estimates for other predictors. There is also a significant interaction between $\texttt{fun}$ and $\texttt{intel}.$ The estimated interaction is positive here. This indicates that fun has more of an effect on liking for those seen as more intelligent, and that intelligence has more of an effect for those seen as more fun. Perhaps you can think of a reason why intelligence appears to lessen the effect of attractiveness, but appears to strengthen the effect of fun... 

```{r full-model-like}
modg <- lm(other_like ~ scale(other_attr, scale=FALSE)*scale(other_intel, scale=FALSE) + scale(other_fun,scale=FALSE)*scale(other_intel,scale=FALSE), data=speeddate)

opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modg),Error=NA),car::Anova(modg, type=3))
rownames(tab) <- c("Intercept","$\\texttt{attr}$", "$\\texttt{intel}$","$\\texttt{fun}$","$\\texttt{attr} \\times \\texttt{intel}$", "$\\texttt{fun} \\times \\texttt{intel}$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "A model predicting liking from attractiveness, intelligence, and fun, and their interactions. All predictors are centered.", escape = FALSE, booktabs=TRUE, linesep="")
options(opts)

```


## Mediation

```{r}
data("legacy2015")
```


### Legacy motives and pro-environmental behaviours

@zaval_how_2015 investigated whether there is a relation between individuals' motivation to leave a positive legacy in the world, and their pro-environmental behaviours and intentions. The authors reasoned that long time horizons and social distance are key psychological barriers to pro-environmental action, particularly regarding climate change. But if people with a legacy motivation put more emphasis on future others than those without such motivation, they may also be motivated to behave more pro-environmentally in order to benefit those future others. In a pilot study, they recruited a diverse sample of 245 U.S. participants through Amazonâ€™s Mechanical Turk. Participants answered three sets of questions: one assessing individual differences in legacy motives, one assessing their beliefs about climate change, and one assessing their willingness to take pro-environmental action. Following these sets of questions, participants were told they would be entered into a lottery to win a $10 bonus. They were then given the option to donate part (between $0 and $10) of their bonus to an environmental cause (Trees for the Future). This last measure was meant to test whether people actually act on any intention to act pro-environmentally.

For ease of analysis, the three sets of questions measuring legacy motive, belief about the reality of climate change, and intention to take pro-environmental action, were transformed into three overall scores by computing the average over the items in each set. After eliminating participants who did not answer all questions, we have data from $n = `r nrow(legacy2015)`$ participants. Figure \@ref(fig:ggpairs-plot-legacy2015) depicts the pairwise relations between the four variables. As can be seen, all variables are significantly correlated. The relation is most obvious for $\texttt{belief}$ and $\texttt{intention}$. Looking at the histogram of $\texttt{donation}$, you can see that although all whole amounts between $0 and $10 have been chosen at least once, it looks like three values were particularly popular, namely $0, $5, and to a lesser extent $10. This results in what looks like a tri-modal distribution. This is not necessarily an issue when modelling $\texttt{donation}$ with a regression model, as the assumptions in a regression model concern the _prediction errors_, and not the dependent variable itself.  

```{r ggpairs-plot-legacy2015, fig.cap="Pairwise plots for legacy motives, climate change belief, intention for pro-environmental action, and donations."}
library(GGally)
ggpairs(legacy2015, columns = c("legacy", "belief", "intention", "donation"), diag=list(continuous=wrap(ggally_barDiag, bins=11, colour="black")),upper = list(continuous = wrap(ggally_cor, size = 5,family="sans")))
```

According to the [Theory of Planned Behavior](https://en.wikipedia.org/wiki/Theory_of_planned_behavior) [@ajzen1991theory], attitudes and norms shape a person's behavioural intentions, which in turn result in behaviour itself. In the context of the present example, that could mean that legacy motive and climate change beliefs do not directly determine whether someone behaves in a pro-environmental way. Rather, these factors shape a person's intentions towards pro-environmental behaviour, which in turn may actually lead to said pro-environmental behaviour. This is an example of an assumed __causal chain__, where legacy motive (partly) determines behavioural intention, and intention determines behaviour. __Mediation analysis__ is aimed at detecting an __indirect effect__ of a predictor (e.g. $\texttt{legacy}$) on the dependent variable (e.g. $\texttt{donation}$), via another variable called the __mediator__ (e.g. $\texttt{intention}$), which is the middle variable in the causal chain.


### Causal steps

A traditional method to assess mediation is the so-called __causal steps__ approach [@baron1986moderator]. The basic idea behind the causal steps approach is as follows: if there is a causal chain from predictor ($X$) to mediator ($M$) to dependent variable ($Y$), then, ignoring the mediator for the moment, we should be able to see a relation between the predictor and dependent variable. This relation reflects the indirect effect of the predictor on the dependent variable. We should also be able to detect an effect of the predictor on the mediator, as well as an effect of the mediator on the dependent variable. Crucially, if there is a true causal chain, then the predictor should not offer any additional predictive power over the mediator. Because the effect of the predictor is assumed to go only "through" the mediator, once we know the value of the mediator, this should be all we need to predict the dependent variable. In more fancy statistical terms, this means that conditional on the mediator, the dependent variable is independent of the predictor, i.e. $P(Y \mid M, X) = P(Y \mid M)$. In the context of a multiple regression model, we could say that in a model where we predict $Y$ from $M$, the predictor $X$ would not have a unique effect on $Y$ (i.e. its slope would equal $\beta_X = 0$).

The causal steps (Figure \@ref(fig:causal-steps-models)) approach involves assessing a pattern of significant relations in three different regression models. The first model is a simple regression model where we predict $Y$ from $X$. In this model, we should find evidence for a relation between $X$ and $Y$, meaning that we can reject the null hypothesis that the slope of $X$ on $Y$ (referred to here as $\beta_X = c$) equals 0. The second model is a simple regression model where we predict $M$ from $X$. In this model, we should find evidence for a relation between $X$ and $M$, meaning that we can reject the null hypothesis that the slope of $X$ on $M$ (referred to here as $\beta_X = a$ here) equals 0. The third model is a multiple regression model where we predict $Y$ from both $M$ and $X$. In this model, we should find evidence for a unique relation between $M$ and $Y$, meaning that we can reject the null hypothesis that the slope of $M$ on $Y$ (referred to here as $\beta_M = b$ here) equals 0. Controlling for the effect of $M$ on $Y$, in a true causal chain, there should no longer be evidence for a relation between $X$ and $Y$ (as any relation between $X$ and $Y$ is captured through $M$). Hence, we should not be able to reject the null hypothesis that the slope of $X$ on $Y$ in this model (referred to here as $\beta_X = c$', to distinguish it from the relation between $X$ and $Y$ in the first model, which was labelled as $c$) equals 0. If this is so, then we speak of __full mediation__. When there is still evidence of a unique relation between $X$ and $Y$ in the model that includes $M$, but the relation is reduced (i.e. $|c'| < |c|$), we speak of __partial mediation__.

```{tikz causal-steps-models, fig.cap='Assessing mediation with the causal steps approach involves testing parameters of three models. MODEL 1 is a simple regression model predicting $Y$ from $X$ and the slope of $X$ ($c$)  should be significant MODEL 2 is a simple regression model predicting $M$ from $X$ and the slope of $X$ ($a$) should be significant. MODEL 3 is a multiple regression model predicting $Y$ from both $X$ and $M$. The slope of $M$ ($b$) should be significant. The slope of $X$ ($c$\') should not be significant ("full" mediation) or be substantially smaller in absolute value ("partial" mediation).'}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\begin{tikzpicture}
	\begin{scope}{
		\pgfsetxvec{\pgfpoint{2cm}{0cm}}
		\pgfsetyvec{\pgfpoint{0cm}{1.5cm}}
		\tikzstyle{every node}=[minimum size=.95cm,circle]
		\draw node[draw] (x1) at (0,0) {$X$};
		\draw node[draw, opacity=0.4] (x2) at (1,1) {$M$};
		\draw node[draw] (y) at (2,0) {$Y$};

		\draw[->, line width=.5mm] (x1) -- node [sloped,below] {$c$} (y);
		
		\draw node[align=center] at (1,2) {\textbf{MODEL 1}};
	}
	\end{scope}
	\begin{scope}[shift={(7,0)}]{
		\pgfsetxvec{\pgfpoint{2cm}{0cm}}
		\pgfsetyvec{\pgfpoint{0cm}{1.5cm}}
		\tikzstyle{every node}=[minimum size=.95cm,circle]
		\draw node[draw] (x1) at (0,0) {$X$};
		\draw node[draw] (x2) at (1,1) {$M$};
		\draw node[draw, opacity=0.4] (y) at (2,0) {$Y$};		
		
		\draw[->, line width=.5mm] (x1) -- node [sloped,above] {$a$} (x2);
		
		\draw node[align=center] at (1,2) {\textbf{MODEL 2}};
	}
	\end{scope}
		
	\begin{scope}[shift={(14,0)}]{
		\pgfsetxvec{\pgfpoint{2cm}{0cm}}
		\pgfsetyvec{\pgfpoint{0cm}{1.5cm}}
		\tikzstyle{every node}=[minimum size=.95cm,circle]
		\draw node[draw] (x1) at (0,0) {$X$};
		\draw node[draw] (x2) at (1,1) {$M$};
		\draw node[draw] (y) at (2,0) {$Y$};		
		
		\draw[->, line width=.5mm] (x2) -- node [sloped,above] {$b$} (y);
		\draw[->, dashed, line width=.5mm] (x1) -- node [sloped,below] {$c$'} (y);
		
		\draw node[align=center] at (1,2) {\textbf{MODEL 3}};
		
	}
	\end{scope}
\end{tikzpicture}

```

#### Testing mediation of legacy motive by intention with the causal steps approach

Let's see how the causal steps approach works in practice by assessing whether the relation between $\texttt{legacy}$ on $\texttt{donation}$ is mediated by $\texttt{intention}$.

In MODEL 1 (Table \@ref(tab:causal-steps-mod1-results)), we assess the relation between $\texttt{legacy}$ and $\texttt{donation}$. In this model, we find a significant and positive relation between legacy motives and donations, such that people with stronger legacy motives donate more of their potential bonus to a pro-environmental cause. The question is now whether this is a direct effect of legacy motive, or an indirect effect "via" behavioural intent.

```{r causal-steps-mod1-results}
mod1 <- lm(donation ~ legacy, data=legacy2015)
tab <- summary(mod1)$coefficients
rownames(tab) <- c("Intercept","Legacy motive")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SE}(\\hat{\\beta})$", "$t$", "$P(\\geq \\lvert t \\rvert)$")
knitr::kable(tab, caption="Model 1: Simple regression model predicting donations from legacy motive", escape=FALSE, booktabs=TRUE, linesep="")
```

In MODEL 2 (Table \@ref(tab:causal-steps-mod2-results)), we assess the relation between $\texttt{legacy}$ and $\texttt{intention}$. In this model, we find a significant and positive relation between legacy motives and intention to act pro-environmentally, such that people with stronger legacy motives have a stronger intention to act pro-environmentally.

```{r causal-steps-mod2-results}
mod2 <- lm(intention ~ legacy, data=legacy2015)
tab <- summary(mod2)$coefficients
rownames(tab) <- c("Intercept","Legacy motive")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SE}(\\hat{\\beta})$", "$t$", "$P(\\geq \\lvert t \\rvert)$")
knitr::kable(tab, caption="Model 2: Simple regression model predicting pro-environmental intent from legacy motive", escape=FALSE, booktabs=TRUE, linesep="")
```

In MODEL 3 (Table \@ref(tab:causal-steps-mod3-results)), we assess the relation between $\texttt{legacy}$, $\texttt{intention}$, and $\texttt{donation}$. In this model, we find a significant and positive relation between intention to act pro-environmentally and donation to a pro-environmental cause, such that people with stronger intentions donate more. We also find evidence of a unique and positive effect of legacy motive on donation, such that people with stronger legacy motives donate more. Because there is still evidence of an effect of legacy motive on donations, after controlling for the effect of behavioural intent, we would not conclude that the effect of legacy motive is _fully mediated_ by intent. When you compare the slope of $\texttt{legacy}$ in MODEL 3 to that in MODEL 1, you can however see that the (absolute) value is smaller. Hence, when controlling for the effect of behavioural intent, a one-unit increase in $\texttt{legacy}$ is estimated to increase the amount of donation less then in a model where $\texttt{intention}$ is not taken into account.  


```{r causal-steps-mod3-results}
mod3 <- lm(donation ~ legacy + intention, data=legacy2015)
tab <- summary(mod3)$coefficients
rownames(tab) <- c("Intercept","Legacy motive", "Behavioral intent")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SE}(\\hat{\\beta})$", "$t$", "$P(\\geq \\lvert t \\rvert)$")
knitr::kable(tab, caption="Model 3: Multiple regression model predicting donations from legacy motive and pro-environmental intent.", escape=FALSE, booktabs=TRUE, linesep="")
```

In conclusion, the causal steps approach indicates that the effect of legacy motive of pro-environmental action (donations) is _partially mediated_ by pro-environmental behavioural intentions. There is a residual direct effect of legacy motive on donations that is not captured by behavioural intentions. 

### Estimating the mediated effect

One potential problem with the causal steps approach is that it is based on a pattern of significance in four hypothesis tests (one for each parameter $a$, $b$, $c$, and $c'$). This can result in a rather low power of the procedure  [@mackinnon2007mediation], which seems to be particularly related to the requirement of a significant $c$ (the direct effect of $X$ on $Y$ in the model without the mediator).

An alternative to the causal steps approach is to estimate the mediated (indirect) effect of the predictor on the dependent variable directly. Algebraically, this mediated effect can be worked out as [@mackinnon2007mediation]:

\begin{equation}
\text{mediated effect} = a \times b
\end{equation}

The rationale behind this is reasonably straightforward. The slope $a$ reflects the increase in the mediator $M$ for every one-unit increase in the predictor $X$. The slope $b$ reflects the increase in the dependent variable $Y$ for every one unit increase in the mediator. So a one-unit increase in $X$ implies an increase in $M$ by $a$ units, which in turn implies an increase in $Y$ of $a \times b$ units. Hence, the mediated effect can be expressed as $a \times b$.

In a single mediator model such as the one looked at here, the mediated effect $a \times b$ turns out to be equal to $c - c'$, i.e. the difference between the direct effect of $X$ on $Y$ in a model without the mediator, and the unique direct effect of $X$ on $Y$ in a model which includes the mediator. 

```{r}
set.seed(20201026)
med_boot <- mediation::mediate(mod2,mod3,treat="legacy",mediator="intention", boot=TRUE)
```

To test whether the mediated effect differs from 0, we can try to work out the sampling distribution of the estimated effect $\hat{a} \times \hat{b}$, under the null-hypothesis that in reality, $a \times b = 0$. Note that this null hypothesis can be true when $a = 0$, $b = 0$, or both $a = b = 0$. In the so-called Sobel-Aroian test, this sampling distribution is assumed to be Normal. However, it has been found that this assumption is often inaccurate. As there is no method to derive an accurate sampling distribution analytically, modern procedures rely on simulation. There are different ways to do this, but we'll focus on one, namely the __nonparametric bootstrap__ approach [@preacher2008asymptotic]. This involves generating a large number (e.g. $>1000$) of simulated datasets by randomly sampling $n$ cases with replacement from the original dataset. This means that any given case (i.e. a row in the dataset) can occur 0, 1, 2, \ldots times in a simulated dataset. For each simulated dataset, we can estimate $\hat{a} \times \hat{b}$ by fitting the two corresponding regression models. The variance in these estimates over the different datasets forms an estimate of the variance of the sampling distribution. A 95% confidence interval can then also be computed through by determining the 2.5 and 97.5 percentiles. Because just the original data is used, there is no direct assumption made about the distribution of the variables, apart from that the original data is a representative sample from the Data Generating Process. Applying this procedure (with 1000 simulated datasets) provides a 95% confidence interval for $a \times b$ of $[`r paste0(format(summary(med_boot)$d0.ci,digits=3), collapse=", ")`]$. As this interval does not contain the value 0, we reject the null hypothesis that the mediated effect of $\texttt{legacy}$ on $\texttt{donation}$ "via" $\texttt{intention}$ equals 0.

Note that in solely focusing on the mediated effect, we do not address the issue of total vs partial mediation. Using our simulated datasets, we can however also compute a bootstrap confidence interval for $c'$. For the present set of simulations, the 95% confidence interval for $c'$ is $[`r paste0(format(summary(med_boot)$z0.ci,digits=3), collapse=", ")`]$. As this interval does not contain the value 0, we reject the null hypothesis that the unique direct effect of $\texttt{legacy}$ on $\texttt{donation}$ equals 0. This thus provides a similar conclusion to the causal steps approach. 

<!-- In a single mediator model such as the one looked at here, this mediated effect 
To test whether this mediated effect equals 0 (in which case there is no mediation), we can attempt to work out the sampling -->


<!--chapter:end:05-GLM-moderation.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# A model of means (ANOVA) {#ch-ANOVA}

In this chapter, we will discuss how _nominal variables_, which often reflect different manipulations within an experiment, can be included in the General Linear Model. This is done by constructing so-called _contrast codes_, which provide a means to construct metric predictors which encode differences between the levels of the nominal variable (e.g. differences between conditions in an experiment). When a study concerns only a single nominal variable, the interest is generally in determining whether there are differences in the average of the dependent variable between the levels (groups). Traditionally, ANOVA has focused on so-called _omnibus tests_, which test whether the mean of at least one group mean is different to that of another. Contrast codes allow you to test more informative hypotheses, and we will discuss different ways of constructing these contrast codes, including so-called _orthogonal contrast codes_. Within the context of a single model, you can only include a limited number of contrast codes, and we end the chapter with considering how you can test more hypotheses, whether these were conceived beforehand (_planned comparisons_) or afterwards (_post-hoc tests_).  

```{r}
library(sdamr)
data("tetris2015")
```

## Can playing Tetris reduce intrusive memories?

After a traumatic experience, some people experience flashbacks, which are intrusive and involuntary memories that involve vivid imagery related to the traumatic event. These intrusive memories can be highly distressing and are a hallmark of acute stress disorder and posttraumatic stress disorder (PTSD). It has been suggested that recalling traumatic memories under certain conditions can reduce their negative impact. Memory consolidation refers to a collection of neural processes which stabilize a memory trace after it is acquired. According to reconsolidation theory, when a consolidated memory trace is reactivated (remembered), it again becomes malleable and will require restabilization for it to persist. Disruption of this reconsolidation after recall may then be a way to reduce the strength of traumatic memories, or even allow them to be forgotten. 

@james_computer_2015 conducted a study to investigate this idea. They reasoned that because intrusive memories of trauma are often visual in nature, performing a demanding visuospatial task (e.g. playing the computer game Tetris) after recall could interfere with the reconsolidation process and reduce subsequent intrusions of the traumatic memory. In their Experiment 2, they first created traumatic memories by showing their participants a 12-minute film with graphic scenes depicting death and serious injury (e.g. a van hitting a teenage boy while he was using his mobile phone crossing the road). Participants then went home and recorded the number of intrusive memories of the film during the subsequent 24-hour period (Day 0). The next day, they returned to the lab and were randomly assigned to one of four conditions:

1. No-task control: participants in this condition ($n=18$) completed a 10-minute music filler task, rating excerpts of classical music for pleasantness.
2. Tetris+Reactivation: participants in this condition ($n=18$) were shown a series of images from the scenes in the trauma film to reactivate the memories of the scenes. After this reactivation task, they completed the 10-minute music filler task, and then played the video game Tetris for 12 minutes.
3. Tetris-Only: participants in this condition ($n=18$) performed the music filler task and then played Tetris for 12 minutes, but did not complete the reactivation task.
4. Reactivation Only: participants in this condition ($n=18$) completed the reactivation and music filler task, but did not play Tetris.

All participants then went home and were asked to record the number of intrusive memories they experienced over the next seven days (Day 1 to 7). After this week passed, participants returned to the lab and completed an Intrusion-Provocation Task, in which they
were shown blurred images from the trauma film and asked to indicate whether each of these triggered an intrusive memory.

The sample means and standard deviations of the number of intrusions in each condition are:
```{r}
library(dplyr)
dat <- tetris2015
dat$Condition <- factor(dat$Condition, labels=c("Control","Tetris+Reactivation","Tetris Only", "Reactivation Only"))
dat$intrusions <- dat$Days_One_to_Seven_Number_of_Intrusions
tab <- dat %>% group_by(Condition) %>% summarise(mean = mean(intrusions), sd = sample_sd(intrusions)) %>% select(mean,sd) 
colnames(tab) <- c("$\\overline{Y}$", "$S_Y$")
rownames(tab) <- c("Control","Tetris+Reactivation","Tetris-Only","Reactivation-Only")
knitr::kable(tab, escape=FALSE, booktabs=TRUE, linesep="")

```

Boxplots for each condition are provided in Figure \@ref(fig:tetris-intrusive-memories).

```{r tetris-intrusive-memories, fig.cap="Number of intrusive memories during days 1 to 7 for each condition of Experiment 2 of James et al. (2015)."}
library(ggplot2)
ggplot(dat, aes(y = Days_One_to_Seven_Number_of_Intrusions, colour=Condition)) + geom_boxplot() + ylab("Number of intrusions") + theme(legend.position = "bottom")
```

## Comparing two groups {#sec:06-two-groups}

```{r}
dat <- subset(tetris2015, Condition %in% c("Tetris_Reactivation","Reactivation"))
dat$intrusions <- dat$Days_One_to_Seven_Number_of_Intrusions
dat$dummy <- 0
dat$dummy[dat$Condition == "Reactivation"] <- 1
```

To start with a relatively straightforward example, let's first focus only on the data from the Tetris+Reactivation and the Reactivation-Only conditions. We are interested in whether playing Tetris during reactivation reduces the number of memory intrusions on later days, in comparison to when the traumatic memory was reactivated only. To investigate this, we will extend the simple model of Chapter \@ref(ch-simple-GLM). In particular, we will assume that the number of intrusions is Normal-distributed with a mean that depends on the experimental condition participants were assigned to (Tetris+Reactivation or Reactivation-Only). We can write this model in two equivalent ways as:
$$Y_i \sim \mathbf{Normal}(\mu_{\text{con}},\sigma)$$
and
\begin{equation}
Y_i = \mu_\text{con} + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma)
(\#eq:two-group-model-mu)
\end{equation}
where $\mu_\text{con}$ is a placeholder for the mean of a particular condition. Specifying that mean explicitly, we can also write the model as 

$$Y_{i} = \begin{cases}  \mu_\text{t+r} + \epsilon_{i} \hspace{2em} \text{if condition = Tetris+Reactivation} \\ \mu_\text{react} + \epsilon_{i} \hspace{2em} \text{if condition = Reactivation-Only}  \end{cases} \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
Note that the model assumes that while the mean can differ between the conditions, deviations of the observations around the mean are assumed to have the same standard deviance $\sigma_\epsilon$. An example of what the model might look like is given in Figure \@ref(fig:two-group-Normal-densities).

```{r two-group-Normal-densities, fig.cap="Two Normal density functions with a different mean but identical standard deviation.", fig.height=4,out.width="80%"}
ggplot() + stat_function(fun=function(x) dnorm(x,mean=4,sd=1)) + stat_function(fun=function(x) dnorm(x,mean=6,sd=1)) + xlim(0,10) + ylab("p(y)") + xlab("y") + geom_line(data=data.frame(x=c(4,4),y=c(0,dnorm(4,4,1))),aes(x=x,y=y),lty=3) + geom_line(data=data.frame(x=c(6,6),y=c(0,dnorm(6,6,1))),aes(x=x,y=y),lty=3) + annotate("text",x=4,y=-.02,label=expression(mu[t+r])) + annotate("text",x=6,y=-.02,label=expression(mu[react])) + theme(axis.text.x = element_blank(),axis.ticks.x = element_blank(),axis.text.y = element_blank(),axis.ticks.y = element_blank())
```

Can we estimate such a model, using the tools we have already learned about? Yes! It is actually quite straightforward to construct a linear model to represent the model with means depending on condition. Condition is a nominal variable, which we can't simply include "as is" in a linear model. Linear models need _metric_ predictors. But we can construct a new predictor $X$, which has the value 0 for participants in the Tetris+Reactivation condition, and the value 1 for participants in the Reactivation-Only condition. This predictor $X$, with values 0 and 1 referring to different groups, is commonly referred to as a __dummy coding__ variable. With this dummy predictor, the model
$$Y_i = \beta_0 + \beta_1 \times X_i + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma)$$ 
is formally equivalent to the model of Equation \@ref(eq:two-group-model-mu). To see why, it is important to realize that linear regression concerns the conditional means of the dependent variable given the values of the predictor variable(s):
$$\hat{Y} = \mu_{Y|X_i}$$

If we give the model enough flexibility (and I will discuss more in detail what that means later), then the model will be able to represent those conditional means accurately. A main thing to focus on for now is that we assigned a different value of the predictor variable $X$ for the two conditions. This means that the conditional means can differ between the conditions.

Let's write out the structural part of the model (excluding the error term $\epsilon_i$) for participants from each condition. For _everyone_ in the Tetris+Reactivation condition, $X_i = 0$, so the model predictions for these cases are
$$\begin{aligned}
\hat{Y}_i &= \mu_{Y|X_i} \\
&=\beta_0 + \beta_1 \times 0 \\
&= \beta_0
\end{aligned}$$ 
For _everyone_ in the Reactivation-Only condition, $X_i = 1$, so the model predictions for these cases are 
$$\begin{aligned}
\hat{Y}_i &= \mu_{Y|X_i} \\
&= \beta_0 + \beta_1 \times 1 \\
&= \beta_0 + \beta_1
\end{aligned}$$ 

For cases in the Tetris+Reactivation condition, the model predictions are a constant: $\beta_0$. If the errors are Normal-distributed, this is effectively the same model as we used in Chapter \@ref(ch-simple-GLM). The only difference is in the label for the constant: in Chapter \@ref(ch-simple-GLM) we labelled this as $\mu$, and here we label it as $\beta_0$. Although the labels look different, they both refer to the same thing, namely the (conditional) mean of the dependent variable. If we "relabel" $\beta_0 = \mu_\text{t+r}$, then we have the the model we wanted for this condition. 

For cases in the Reactivation-Only condition, the prediction consists of the sum of two constants, $\beta_0 + \beta_1$. A sum of two constants is itself a constant. Moreover, because we have already "relabelled" $\beta_0 = \mu_\text{tetr}$, we can also write this sum of constants as $\mu_\text{react} = \mu_\text{t+r} + \beta_1$. If we then move $\mu_\text{t+r}$ to the left-hand side of this equation by subtracting $\mu_\text{t+r}$ from both sides of the equation, we can see that $\beta_1 = \mu_\text{react} - \mu_\text{t+r}$. In words, the slope of our predictor equals the difference between the mean of the the Reactivation-Only and the mean of Tetris+Reactivation condition.

Remember that the linear model represents the conditional mean of the dependent variable for each possible combination of values of the predictor variables. We have just constructed a predictor with a different value for cases in each condition, and we could write the resulting model predictions in terms of two parameters (the intercept and the slope) which we could relate to the means in each condition. In Chapter \@ref(ch-simple-GLM), we showed that the maximum likelihood estimate of the mean $\mu$ of a Normal-distributed variable is the sample mean $\overline{Y}$. Considering each group in isolation, the maximum likelihood estimates in each group are then simply the sample means in each group (condition). The model we just constructed is defined over all groups. However, by assigning a different value of the predictor variable $X$ to each group (and choosing these values wisely), this single model allows you to do the same (and more) than defining a different model for each group. This is the essence of contrast coding: constructing metric predictors which allow you to model different groups in a single linear (regression) model.

In the model just constructed, the maximum likelihood parameter estimates are a function of the sample means in each group. More precisely, the estimate of the intercept equals the mean in the Tetris+Reactivation condition:
$$\hat{\beta}_0 = \overline{Y}_\text{t+r}$$
The estimate of the slope equals the difference between the sample mean of the the Reactivation-Only and the sample mean of Tetris+Reactivation condition:
$$\hat{\beta}_1 = \overline{Y}_\text{react} - \overline{Y}_\text{t+r}$$

The average number of intrusive memories in the Tetris+Reactivation condition was $\overline{Y}_\text{t+r} = `r format(mean(subset(dat, Condition == "Tetris_Reactivation")$intrusions), digits=3)`$, and the corresponding mean in the Reactivation condition was $\overline{Y}_\text{react} = `r format(mean(subset(dat, Condition == "Reactivation")$intrusions), digits=3)`$. Estimating the regression model provides the following estimates
$$`r mod <- lm(intrusions ~ dummy, data=dat); write_GLM_equation(mod, dv_name = "intrusions", iv_names = c("dummy"), digits=3)`$$
As you can see, the estimated intercept equals 
$$\hat{\beta}_0 = \overline{Y}_\text{t+r}$$
and the slope of $\texttt{dummy}$ equals 
$$\hat{\beta}_1 = \overline{Y}_\text{react} - \overline{Y}_\text{t+r} = `r format(mean(subset(dat, Condition == "Reactivation")$intrusions), digits=3)` -  `r format(mean(subset(dat, Condition == "Tetris_Reactivation")$intrusions), digits=3)` = `r format(mean(subset(dat, Condition == "Reactivation")$intrusions) - mean(subset(dat, Condition == "Tetris_Reactivation")$intrusions), digits=3)`$$
The results of the hypothesis tests that the true intercept and slope equal 0 are given in Table \@ref(tab:tetris-intrusions-dummy-ANOVA). The test result for the intercept indicates that the average number of intrusions in the Tetris+Reactivation condition is different from zero. More interestingly, the test result for the slope of $\texttt{dummy}$ indicates that the difference between the Tetris+Reactivation and Reactivation-Only condition is not equal to 0. In other words, it is likely that there is a difference between the conditions in the average number of intrusions. Playing Tetris after memory reactivation seems to reduce the number of subsequent memory intrusions.

```{r tetris-intrusions-dummy-ANOVA}
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(mod),Error=NA),car::Anova(mod, type=3))
rownames(tab) <- c("Intercept","$\\texttt{dummy}$","Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting number of intrusions by a dummy predictor. Values under $\\hat{\\beta}$$ are parameter estimates. The value of SS in the Error row is the Sum of Squared Error (SSE) of the full model, and in the remaining rows reflect the Sum of Squares Reduced (SSR) between a MODEL R where that parameter is fixed to 0, and the full MODEL G where it is estimated. The values under df are $\\text{df}_2$ for the Error row, and $\\text{df}_1$ in the other rows. Values of $F$ are the $F$-statistic for the null-hypothesis tests where the null hypothesis is that the true value of the parameter is 0, i.e. $\\beta_j = 0$. Values of $P(\\geq F)$ are the corresponding $p$-values.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

A test comparing the means of two Normal-distributed variables is also known as an __independent samples t-test__. It involves an extension of the one-sample $t$ test discussed in Chapter \@ref(ch-simple-GLM) and is based on the sampling distribution of the difference between two sample means. As with any $\text{df}_1 = 1$ $f$-test, if you'd take the square-root of the $F$ statistic for this test, you obtain the value of the $t$ statistic of the independent samples t-test. As the mathematical details of the two-sample $t$-test don't provide any new insights, we omit them here. A main thing to realise is that this test will provide identical results to our test of the slope of $\texttt{dummy}$. By constructing a model where the slope of $\texttt{dummy}$ is identical to the difference between the means between the Tetris+Reactivation and Reactivation-Only conditions, we have made it possible to test this hypothesis in the context of a linear regression model. Whilst this model is basically a regression model, including new predictors for nominal variables makes the model flexible enough to encompass most of the commonly encountered analyses, which perhaps is why it was rebranded as the General Linear Model. 

The dummy coding procedure above can be generalized to the situation in which you want to compare more than two groups. In the Tetris study, there were four conditions. To allow a linear model to represent the means in all four conditions, you need to use more than one dummy-coding predictor. In fact, you would need 3 dummy-coding predictors. In such a dummy-coded model, the intercept represents the mean of one condition, which we can call the reference group. The slope of each predictor represents the difference between the mean of the remaining groups and the reference group. Whilst dummy coding is simple and provides interpretable parameters, there are alternative coding schemes that may provide more interesting tests.

<!-- 
$$t = \frac{\overline{Y}_1 - \overline{Y}_2}{\hat{\sigma}_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$

where $\hat{\sigma}_p$ is a so-called _pooled_ estimate of the standard deviation in each group
$$\hat{\sigma}_p = \sqrt{\frac{(n_1 - 1) S^2_1 + (n_2 - 1) S^2_2}{n_1 + n_2 - 2}}$$

-->

## The ANOVA model

Before going into such alternative coding schemes, we will take a slight detour and consider the traditional model for the case of Normal-distributed variables in multiple groups with (potentially) different means, but the same standard deviation. This model is also called the oneway ANOVA model, and can be stated as follows:
\begin{equation}
Y_{j,i} = \mu + \tau_j + \epsilon_{j,i} \quad \quad \epsilon_{j,i} \sim \textbf{Normal}(0, \sigma_\epsilon)
(\#eq:glm-ANOVA-model)
\end{equation}
Here, $Y_{j,i}$ denotes the value of the dependent variable for case $i = 1,\ldots, n_j$ in group $j = 1, \ldots, g$. So $g$ denotes the total number of groups (i.e. $g=4$ in the Tetris study), and $n_j$ the number of cases in group $j$ (i.e. $n_j = 18$ in the Tetris study for all $j=1,\ldots,4$). $\epsilon_{j,i}$ is the corresponding error term for participant $i$ in group $j$. The mean $\mu$ is the so-called "grand mean", which is the overall mean of the dependent variable, the mean of any observation that could be produced by the Data Generating Process, regardless of group. It is generally defined as the average of the means in all conditions, i.e. $\mu = \frac{\sum_{j=1}^g \mu_j}{g}$. This is identical to the mean over all observations if all the conditions have an equal number of observations. The term $\tau_j$ represents the so-called "treatment effect" of group $j$, defined as the difference between the mean of group $j$ and the grand mean: 
\begin{equation}
\tau_j = \mu_j - \mu
\end{equation}

The traditional goal of an ANOVA is to determine whether there is _any_ treatment effect. That is, to test the null-hypothesis $H_0: \tau_j = 0 \text{ for all } j = 1, \ldots, g$. Note that the hypothesis states that $\tau_j = 0$ for all groups. We could have also stated this hypothesis as $H_0: \tau_1 = \tau_2 = \ldots = \tau_g = 0$, if you find that clearer. As the hypothesis states that the difference between the group-specific mean $\mu_j$ and the grand mean $\mu$ is 0 for every group, the implication is that $\mu_j = \mu$ for all groups (i.e. all groups have an identical mean $\mu$).

<!--
. In the case of the oneway ANOVA model, the expected value of the $F$-statistic (the mean of the sampling distribution of the $F$-statistic) is
$$\mathrm{E}[F] = \frac{\sigma_\epsilon^2 + }{\sigma_\epsilon^2}$$
-->

The test statistic for this null-hypothesis is the $F$ statistic, calculated as a ratio of the (estimated) sample variation of the treatment effects and the (estimated) variance of residual error terms. As we will see, we can perform this hypothesis test with the General Linear Model through an overall model test (comparing a "full" MODEL G to an intercept-only MODEL R). In addition, we can also test more specific hypotheses regarding differences between the group means and the overall mean, or specific hypotheses regarding differences between particular (combinations of) groups. In contrast to traditional ANOVA, the GLM approach also deals naturally with situations in which the groups have unequal sizes (i.e. $n_j$ differs between the groups). This is not straightforward with the traditional ANOVA test.

## Contrast coding

<!-- The dummy coding procedure above can be generalized to the situation in which you want to compare more than two groups. In the Tetris study, there were four conditions. To allow a linear model to represent the means in all four conditions, you need to use more than one dummy-coding predictor. In fact, you would need 3 dummy-coding predictors. In such a model, the intercept represents the mean of one condition, which we can call the reference group. The slope of each predictor represents the difference between the mean of the remaining groups and the reference group. Whilst dummy coding is simple and provides interpretable parameters, there are alternative coding schemes that may provide more interesting tests. <!-- we will in the remainder of this book focus mostly on other forms of coding. --><!-- a different way of coding, which also provides interpretable parameters, but has the benefit that it (in the case of equally-sized groups) provides __independent predictors__. If the predictors in a linear model are all independent, there is no redundancy between them (i.e. no multicollinearity). This increases the reliability of parameter estimates and with that the power of the tests of those parameters.-->

As in the case of two groups, the approach to testing group differences in the GLM is to construct new predictor variables  that represent differences between groups, which we'll call __contrast-coded predictors__. 

To illustrate the general concept of contrast coding in an intuitive manner, let's consider a game in which you ask someone to pick a random number between 1 and 8 and your job is to determine the number they picked by asking questions which can be answered by "yes" or "no". There are different ways in which you can play this game, and some of these are more efficient than others. For instance, you can ask sequential questions about the identity of the numbers such as "Is the number 1?", "Is the number 2?", "Is the number 3?", etc. If they picked the number 1, then you would have needed just a single question, and if the number was 3, you would need three questions. If the number was 8, however, you would not need an additional question. After having asked "Is the number 7?", either the answer would be "Yes", in which case you would know the number to be 7, or "No" in which you would know the number to be 8. So the maximum number of questions is one less than the number of possibilities. As contrast coding is essentially asking such questions about group membership, you will always need one contrast-coding variable less than the number of groups.

Going back to the game, to determine any number between 1 and 8, an optimal playing strategy will always consist of 7 possible questions. You would not need to ask all of these questions in each game, but sometimes you opponent would have chosen 7 or 8, in which case you would need to ask all 7 questions. The strategy of asking whether the chosen number is equal to each of a sequence of number resembles dummy coding. In this view, the final number 8 is the reference group, and each question such as "Is the number 1?" is implicitly the same as "Is the number 1 _and not 8_?". 

A different way to play the game is by a strategy which guarantees you to always correctly "guess" the number in three questions. This strategy is depicted in Figure \@ref(fig:number-guessing-game-contrast-coding). In this strategy, each question halves the number of remaining options. If the first question "Is the number larger than 4?" is answered as "yes", the number can only be 5, 6, 7, or 8. The options 1, 2, 3, or 4, are ruled out. Subsequently asking "Is the number larger than 6?" would reduce the remaining options by half again. If the answer to this question was "yes", then the number would have to be either 7 or 8. If "no", the remaining options would be 5 or 6. If the answer was "yes", then subsequently asking the question "Is the number 7?" would allow you to finish the game (if the answer is "yes", the number is 7, if "no", it would have to be 8). Although more efficient in a single game, over all games, this strategy consists of a total of 7 questions: "Is the number greater than four?", "Is the number greater than two?", is the number greater than six?", "Is the number seven?", etc.  


```{tikz number-guessing-game-contrast-coding, out.width="60%", fig.cap="Questions to guess a random number between 1 and 8"}
\scriptsize
\tikzstyle{level 1}=[level distance=2cm, sibling distance=2.4cm]
\tikzstyle{level 2}=[level distance=2cm, sibling distance=1.2cm]
\tikzstyle{level 3}=[level distance=2cm, sibling distance=0.6cm]
\tikzstyle{level 4}=[level distance=2cm, sibling distance=0cm]
\tikzstyle{bag} = [circle, draw, fill=gray!20,text width=2.4em, text centered]
\tikzstyle{end} = [circle, minimum width=3pt,fill, inner sep=0pt]
\begin{tikzpicture}[grow=right, sloped]
\node[bag] {$>4$ ?}
    child {
        node[bag] {$>6$?}        
            child {
            	node[bag] {7?}
            		child {
            			node[end, label=right: {8}] {}
            			edge from parent
            			node[above] {no}
            		}
            		child {
            			node[end, label=right: {7}] {}
	                    edge from parent
						node[above] {yes}
            		}
            	edge from parent
            	node[above] {yes}
            }
                child {
            	node[bag] {5?}
            		child {
            			node[end, label=right: {6}] {}
            			edge from parent
            			node[above] {no}
            		}
            		child {
            			node[end, label=right: {5}] {}
	                    edge from parent
						node[above] {yes}
            		}
            	edge from parent
            	node[above] {no}
            }
            edge from parent
            node[above] {yes}
        }
    child {
        node[bag] {$>2$?}        
            child {
            	node[bag] {3?}
            		child {
            			node[end, label=right: {4}] {}
            			edge from parent
            			node[above] {no}
            		}
            		child {
            			node[end, label=right: {3}] {}
	                    edge from parent
						node[above] {yes}
            		}
            	edge from parent
            	node[above] {yes}
            }
                child {
            	node[bag] {1?}
            		child {
            			node[end, label=right: {2}] {}
            			edge from parent
            			node[above] {no}
            		}
            		child {
            			node[end, label=right: {1}] {}
	                    edge from parent
						node[above] {yes}
            		}
            	edge from parent
            	node[above] {no}
            }
            edge from parent
            node[above] {no}
	};

\end{tikzpicture}
```

Constructing contrast codes can be seen as a more complicated version of the game above. Suppose that instead of guessing a single number, the other person can assign numbers to groups $A, B, \ldots, H$, and your job is to determine what all these numbers are. You are given one hint, and then you can ask questions only about the _differences_ between the number(s) assigned to (combinations of) the groups. In the context of data analysis, the numbers are the average value of a dependent variable in those groups. For example, if I'm told the mean in group $H$ is $\mu_H = 2$, I can then ask what the difference in the means of group $H$ and group $G$ is. If I then learn that this difference is $\mu_H - \mu_G = 1$, I can infer that the mean in group $G$ must be $\mu_G = \mu_H + 1 = 1$. This game may be less fun to play than the earlier version, but its more interesting from a data analysis perspective.

```{r}
dat <- tetris2015
dat$intrusions <- dat$Days_One_to_Seven_Number_of_Intrusions
```

Let's get back to the Tetris study. There were four conditions: the no-task Control condition, Tetris+Reactivation, Tetris-Only, and Reactivation-Only. Remember, your job is not to determine which condition someone was in. This can be answered with a total of two yes-no questions ("Did the condition involve playing Tetris?" and "Did the condition involve memory reactivation?"). Your job is now to determine the average number of intrusions in each condition by asking questions about differences between the averages. To start, I will give you a hint, which is that the average over all conditions (i.e. the grand mean) is $\hat{\mu} = `r format(mean(dat$intrusions), digits=3)`$. 

```{r}
tab <- data.frame(c1 = c(1,0,0,-1),
                  c2 = c(0,1,0,-1),
                  c3 = c(0,0,1,-1))
colnames(tab) <- c("$c_1$","$c_2$","$c_3$")
#rownames(tab) <- c(paste("Control (mean = ",format(mean(subset(dat,Condition == "Control")$intrusions),digits=3),")"),
#paste("Tetris+Reactivation (mean =",format(mean(subset(dat,Condition == "Tetris_Reactivation")$intrusions),digits=3),")"),paste("Tetris-Only (mean =",format(mean(subset(dat,Condition == "Tetris")$intrusions),digits=3),")"),paste("Reactivation-Only (mean =",format(mean(subset(dat,Condition == "Reactivation")$intrusions),digits=3),")"))
rownames(tab) <- c("Control","Tetris+Reactivation", "Tetris-Only", "Reactivation-Only")
# knitr::kable(tab, escape=FALSE,align=rep('r', 3))
library(dplyr)
means <- dat %>% group_by(Condition) %>% summarise(mean = mean(intrusions)) %>% select(mean)
codes <- cbind(c(1,0,0,-1),
               c(0,1,0,-1),
               c(0,0,1,-1))
means <- means$mean
contrasts(dat$Condition) <- codes
modg <- lm(intrusions ~ Condition, data=dat)

```

One set of questions you could ask is the following:

1. What is the difference between the mean of the Control condition and the grand mean? 
2. What is the difference between the mean of the Tetris+Reactivation condition and the grand mean? 
3. What is the difference between the mean of the Tetris-Only condition and the grand mean? 

The answers to these questions are

1. $\hat{\mu}_\text{contr} - \hat{\mu} = `r format(means[1],digits=3)` - `r format(mean(means), digits=3)` = `r format(means[1] - mean(means),digits=3)`$.
2. $\hat{\mu}_\text{t+r} - \hat{\mu} = `r format(means[2],digits=3)` - `r format(mean(means), digits=3)` = `r format(means[2] - mean(means),digits=3)`$.
3. $\hat{\mu}_\text{tetr} - \hat{\mu} = `r format(means[3],digits=3)` - `r format(mean(means), digits=3)` = `r format(means[3] - mean(means),digits=3)`$.

Because I have told you the value of $\hat{\mu}$ already, you can simply add the value of this "hint" to the answer of each question to determine the mean of a condition. You do not need to ask a fourth question ("What is the difference between the mean of the reactivation-Only condition and the grand mean?"), because with the information provided, you would be able to determine this. Firstly, I should point out that $$\begin{aligned}
\hat{\mu} &= \frac{\hat{\mu}_\text{contr} + \hat{\mu}_\text{t+r} + \hat{\mu}_\text{tetr} + \hat{\mu}_\text{react}}{4} \\
&= \frac{\overline{Y}_\text{contr} + \overline{Y}_\text{t+r} + \overline{Y}_\text{tetr} + \overline{Y}_\text{react}}{4}
\end{aligned}$$
i.e., the estimate of the grand mean is a "mean of means". With a little algebra, we can then work out that
$$\hat{\mu} - \frac{\hat{\mu}_\text{contr} + \hat{\mu}_\text{t+r} + \hat{\mu}_\text{tetr}}{4} = \frac{\hat{\mu}_\text{react}}{4}$$
and hence
$$\hat{\mu}_\text{react} = 4 \times \mu - (\hat{\mu}_\text{contr} + \hat{\mu}_\text{t+r} + \hat{\mu}_\text{tetr})$$
The treatment effect can then also be determined as
$$\hat{\mu}_\text{react} - \hat{\mu} = -1 \times\left( (\hat{\mu}_\text{contr} - \hat{\mu}) + (\hat{\mu}_\text{t+r} - \hat{\mu}) + (\hat{\mu}_\text{tetr} - \hat{\mu}) \right)$$
i.e. as minus one times the sum of the treatment effect of the other conditions. It is precisely for dependencies like these that when constructing contrast codes for a nominal variable representing group membership, you need one less contrast code than the number of groups (i.e., you need $g-1$ contrast codes).

### Effect coding

Contrast codes are variables that assign numeric values to groups, such that when we use these values in a linear model, the resulting parameter estimates reflect differences in the means of the groups. They are, in a sense, a computational trick to allow us to model group differences with a linear model. We will denote contrast codes as $c_j$, where $j=1,\ldots,g$ is an indicator for groups. The questions about deviations between group means and the grand mean (the _treatment effects_) correspond to the following three contrast codes $c_j$:
```{r}
knitr::kable(tab, escape=FALSE,align=rep('r', 3), booktabs=TRUE, linesep="")
```
Just like for the dummy coding example discussed above, the idea is to construct a new predictor for each of these contrast codes. The contrast codes have values for each group or condition. The corresponding contrast-coded predictor has a value for each case $i$ in the data, where we give all cases in a condition the corresponding value of $c_j$ for that condition. For example, the first predictor $X_1$, which corresponds to the first contrast code $c_1$, would have the value $X_{1,i} = 1$ if case $i$ is in the Control condition, the value $X_{1,i} = 0$ if case $i$ is in the Tetris+Reactivation or Tetris-only condition, and the value $X_{1,i} = -1$ if case $i$ is in the Reactivation-Only condition. Similarly, the second predictor $X_2$, which corresponds to the second contrast code $c_2$, would have the value $X_{2,i} = 0$ if case $i$ is in the Control condition, the value $X_{2,i} = 1$ if case $i$ is in the Tetris+Reactivation condition, the value $X_{2,i} = 0$ if case $i$ is in the  Tetris-only condition, and the value $X_{2,i} = -1$ if case $i$ is in the Reactivation-Only condition. Having defined three contrast-coded predictors, $X_1$, $X_2$, and $X_3$ in this manner, we can then estimate the linear (regression) model
$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \beta_3 \times X_{3,i} + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
This model is treated as any other regression model. So after re-coding a nominal variable "condition" with four levels (Control, Tetris+Reactivation, Tetris-Only, and Reactivation-Only) with three contrast codes $c_1$, $c_2$, and $c_3$, each with a corresponding predictor $X_1$, $X_2$, and $X_3$, we end up with a linear model that is effectively like any other multiple regression model. Estimating the model gives:
$$`r write_GLM_equation(modg, dv_name = "intrusions", iv_names = c("}X_1\\texttt{","}X_2\\texttt{","}X_3\\texttt{"), digits=3, include_sde = TRUE)`$$
Figure \@ref(fig:tetris-effect-coding-parameter-plot) shows how the parameters of the model are related to the average number of intrusions in each condition.
```{r tetris-effect-coding-parameter-plot, fig.cap="Average number of instrusions in the four conditions in the Tetris study and how they are related to the parameters of the effect-coding model. The intercept $\\hat{\\beta}_0 = \\hat{\\mu}$ is the grand mean (dotted line). The slopes reflect treatment effects, which are deviations from the average intrusions in a condition and the grand mean. The averages of the first three conditions are equal to $\\beta_0 + \\beta_j$, the sum of the intercept and the slope of the effect-coding predictor $X_j$ representing the treatment effect of that condition. The mean of the last condition is the intercept minus the sum of the slopes of the effect-coding predictors."}
data.frame(Condition = factor(1:4,labels=c("Control","Tetris+Reactivation","Tetris-Only","Reactivation-Only")), mean=means) %>%
  ggplot(aes(x=Condition, y=mean, xend = Condition, yend=mean(mean))) + geom_point() + geom_hline(yintercept = mean(means), lty=3) + geom_segment(arrow = arrow(length=unit(0.20,"cm"), type="closed", ends = "first"), alpha=.6) + annotate("text",x=1.1,y=.5*mean(means) + .5*means[1], label=expression(beta[1])) + annotate("text",x=2.1,y=.5*mean(means) + .5*means[2], label=expression(beta[2])) + annotate("text",x=3.1,y=.5*mean(means) + .5*means[3], label=expression(beta[3])) + annotate("text",x=3.5,y=.5*mean(means) + .5*means[1], label=expression(beta[0] - (beta[1] + beta[2] + beta[3]))) + annotate("text",x=0.5,y=mean(means) - .15, label=expression(beta[0])) + ylab("Intrusions")
```


Test results for the parameters of the model are given in Table \@ref(tab:tetris-effect-coding-ANOVA). As can be seen there, the test of the intercept is significant. In this model, the intercept represents the "grand mean" $\mu$ (the average of the means in each condition). The test indicates that the true value of the grand mean is unlikely to be 0. In addition, the slope of $X_2$ is significant. This slope is equal to the _treatment effect_ of the Tetris+Reactivation condition (i.e. $\hat{\beta}_2 = \hat{\mu}_{t+r} - \hat{\mu}$). The test is a test of the null-hypothesis $H_0: \beta_2 = 0$, and this test involves a comparison of the models
$$\begin{aligned}
\text{MODEL G}: && Y_i &= \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \beta_3 \times X_{3,i} + \epsilon_i \\
\text{MODEL R}: && Y_i &= \beta_0 + \beta_1 \times X_{1,i} + \beta_3 \times X_{3,i} + \epsilon_i
\end{aligned}$$
The comparison indicates that fixing the slope $\beta_2 = 0$ in MODEL R results is a substantial increase in the Sum of Squared Error of MODEL R compared to MODEL G. As such, there is evidence that the true value of this slope does not equal 0, and with that, that the true treatment effect of the Tetris+Reactivation condition does not equal 0, i.e. that $\mu_{t+r} - \mu \neq 0$. Furthermore, the estimate of the treatment effect is $\hat{\beta}_2 = \hat{\mu}_{t+r} - \hat{\mu} = \overline{Y}_{t+r} - \frac{\overline{Y}_\text{contr} + \overline{Y}_\text{t+r} + \overline{Y}_\text{tetr} + \overline{Y}_\text{react}}{4}  =  `r coefficients(modg)[3]`$, which indicates that the number of intrusions in this condition is _lower_ than the grand mean. As such, we would conclude that playing Tetris after memory reactivation _reduces_ the subsequent memory intrusions. The tests of the slopes of $X_1$ and $X_3$ are not significant. We therefore do not have sufficient evidence that the treatment effect of the Control condition, or of the Tetris-only condition, are different to 0. 

```{r tetris-effect-coding-ANOVA}
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modg)[1],NA,coefficients(modg)[2:4],Error=NA),rbind(car::Anova(modg, type=3)[1:2,],expand_Anova(modg, type=3)[-1,]))
rownames(tab) <- c("Intercept","Condition","$\\quad X_1$","$\\quad X_2$", "$\\quad X_3$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting number of intrusions by three effect-coded predictors. Values under $\\hat{\\beta}$$ are parameter estimates. The value of SS in the Error row is the Sum of Squared Error (SSE) of the full model, and in the remaining rows reflect the Sum of Squares Reduced (SSR) between a MODEL R where that parameter is fixed to 0, and the full MODEL G where it is estimated. For the Condition row, MODEL R is a model which fixes all parameters (apart from the intercept) to 0. The values under df are $\\text{df}_2$ for the Error row, and $\\text{df}_1$ in the other rows. Values of $F$ are the $F$-statistic for the null-hypothesis tests where the null hypothesis is that the true value of the parameter is 0, i.e. $\\beta_j = 0$. Values of $P(\\geq F)$ are the corresponding $p$-values.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

Table \@ref(tab:tetris-effect-coding-ANOVA) also includes a row labelled "Condition". This is a test of the hypothesis that _all_ of the slopes of the contrast-coded predictors for Condition are equal to 0:
$$H_0\!: \beta_1 = \beta_2 = \beta_3 = 0$$
This hypothesis test is based on comparing the following two models:
$$\begin{aligned}
\text{MODEL G}: && Y_i &= \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \beta_3 \times X_{3,i} + \epsilon_i \\
\text{MODEL R}: && Y_i &= \beta_0 + \epsilon_i
\end{aligned}$$
i.e., it is a "whole model test". The result of this so-called __omnibus test__ (a simultaneous test of multiple parameters) is significant, indicating that it is unlikely that the true values of the slopes are all equal to 0. In MODEL R above, there is only a single parameter $\beta_0$. As such, this model predicts that all conditions have the same mean $\mu$. This test is thus a test of the null-hypothesis that _all_ treatment effects for Condition are equal to 0: 
$$H_0\!: (\mu_\text{contr} - \mu) = (\mu_\text{t+r} - \mu) = (\mu_\text{tetr} - \mu) = (\mu_\text{react} - \mu) = 0$$
which is also equivalent to the test that the means of all conditions are equal to each other:
$$H_0\!: \mu_\text{contr} = \mu_\text{t+r} = \mu_\text{tetr} = \mu_\text{react} $$

The result of the omnibus test indicates that there is at least one treatment effect which is different from 0. Omnibus tests for all slopes reflecting treatment effects are what is traditionally focused on in an ANOVA. But these omnibus tests are not always that informative. We'd generally like to know more than "there is at least one treatment that is likely to have an effect". It seems inherently more interesting to know _which_ conditions are associated with a treatment effect. This is where the tests of the individual slopes come in handy. Assessing the effect of the three effect-coding predictors, we can conclude that only the treatment effect of the Tetris+Reactivation condition is significant. As such, we only have sufficient evidence that a combination of memory reactivation and playing Tetris changes the number of subsequent memory intrusions. 

At this point, I'd like to make some important remarks. Firstly, the absence of sufficient evidence that any of the other treatment effects differs from 0 should _not_ be taken as direct evidence that the true treatment effects equal 0. A non-significant result indicates a lack of evidence _against_ the null-hypothesis, but not an abundance of evidence _for_ the null hypothesis. You can think of this as follows: that a suspect in a murder trial is not able to provide evidence that she is innocent, is in itself not sufficient evidence that she is guilty. You might also think of black swans. While the empirical statement "All swans are white" is impossible to prove conclusively without checking the colour of all swans that have and will ever grace this world, finding a single black swan disproves that statement immediately [@popper1959logic]. Although significance testing does not provide _conclusive evidence_ for or against the null-hypothesis,^[Remember that the significance level sets an allowable error rate if the null hypothesis is true, whilst the error rate when the null hypothesis is not true is unknown, but unlikely to be 0.] the analogy can be described as follows: a significant test is like spotting a swan that is "off-white" enough for you to decide it is not exactly white. But not having spotted such a swan could either mean that such a swan does not exist (the null hypothesis is true), or that you have not searched hard enough (the null hypothesis is false, but your test lacked power).

Secondly, the effect-coding predictors only reflect three out of a total of four treatment effects. The treatment effect of the reactivation-Only condition follows directly as minus the sum of these three treatment effects. As such, it is redundant. But we don't have a hypothesis test for this redundant treatment effect. If we had chosen a slightly different coding scheme, where we would have estimated the treatment effects of the Control, Tetris-Only, and Reactivation-Only condition, such that the treatment effect of the Tetris+Reactivation condition is redundant, then none of the significance tests of the slopes of this different model would have been significant. However, the omnibus test would give exactly the same results, thus indicating that at least one treatment effect does not equal 0. With the three treatment effects estimated in this different model, we couldn't have easily spotted which treatment effect(s) these were. Whilst the estimates and slopes of contrast-coded predictors are often more specific and informative than an omnibus test, because we can only use $g-1$ of such predictors (one less than the number of groups), we can't test for _all_ treatment effects in a single model, nor test all hypotheses that we might be interested in. We will come back to this when we discuss "multiple testing" approaches. For now, a main thing to realise is that a significant omnibus test indicates that at least two groups differ in their means. If none of the tests of the slopes of the contrast-coded predictors in a linear model are significant, but you have obtained a significant omnibus test, that indicates that none of the predictors encoded this particular difference between groups. 

Whilst some authors bemoan the use of omnibus tests, they have a role to play in the inference process, for instance in spotting whether you have missed a potentially important effect. Other authors put too much emphasis on omnibus tests, for instance requiring a significant omnibus tests before you might consider tests of the individual slopes that comprise this omnibus test. It is perfectly possible for an omnibus test to be non-significant, whilst a slope for one (or more) contrast-coded predictors is significant. For instance, if you'd conduct an experiment with 10 conditions, and only one has an actual treatment effect, the omnibus test might be non-significant because it effectively assigns that one treatment effect to nine parameters (slopes for nine contrast-coded predictors). Let's take an extreme example, where only $X_1$ (the first contrast-coded predictor) reduces the SSE with $\text{SSR}(X_1) > 0$, whilst the other predictors provide no reduction in the SSE whatsoever (i.e. $\text{SSR}(X_j) = 0$ for $j=2, \ldots, 9$). Then the $F$ statistic of the omnibus test might be
$$\begin{aligned}
F &= \frac{\frac{\text{SSE}(R) - \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar}(G)}} \\
&= \frac{\text{SSR}(X_1)/9}{\text{SSE}(G)/(n-10)}
\end{aligned}$$
whilst the $F$ statistic for the slope of $X_1$ would be
$$\begin{aligned}
F &= \frac{\frac{\text{SSE}(R) - \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar}(G)}} \\
&= \frac{\text{SSR}(X_1)/1}{\text{SSE}(G)/(n-10)}
\end{aligned}$$
which, with a higher value and smaller $\text{df}_1$, would be more likely to be significant.

Thirdly, when you look at the value of $\text{SSR}(\text{Condition}) = `r format(tab[2,2],digits=3)`$ in Table \@ref(tab:tetris-effect-coding-ANOVA), you can see that it is smaller than the sum of the SSR terms corresponding to the three predictors. This indicates that there is redundancy between the predictors $X_1$, $X_2$, and $X_3$ (i.e., some form of multicollinearity). Although Venn diagrams such as Figure \@ref(fig:sse-partition-multicollinearity) imply that the SSR of the full model would be larger than the sum of the unique SSR terms attributable to each predictor, the opposite can also be true! This situation is commonly referred to as __suppression__ and is rather difficult to explain here without a lengthy and detailed detour. For now, we will therefore leave it to the interested reader to consult other sources on this [e.g. @mackinnon2000equivalence], and simply note that if the sum of the SSR terms of the predictors does not equal the "whole model" SSR, this indicates redundancy between the predictors. 

### Orthogonal contrast codes

Whilst redundancy is not necessarily a problem, it is in some sense preferable if all the predictors in a linear model are independent, as we can then neatly divide the total SSR into parts that we can uniquely assign to each predictor. When the number of cases in each group ($n_j$) is equal for all groups, there are contrast coding schemes that ensure that the resulting contrast-coded predictors are non-redundant (i.e., independent). Such coding schemes are called __orthogonal contrast codes__. 

There are some benefits to employing orthogonal contrast codes, although these benefits are sometimes overstated. Firstly, using orthogonal contrast codes ensures that the model predictions will exactly equal to (sample) averages in the conditions --  as long as no additional predictors are included in the model; we will discuss such additional predictors in the context of Analysis of Covariance (ANCOVA). Whilst redundant coding schemes such as dummy coding or effect coding (amongst others) also ensure this, when you start defining your own contrast coding schemes, it might be difficult to check whether this is the case, and then resorting to orthogonal contrast codes may provide useful guidance. Secondly, as already mentioned, using independent predictors will ensure that the whole model SSR can be divided into SSR terms for the individual predictors. This makes it somewhat less likely that you will miss a difference between the groups in the tests of individual parameters that would be identified in the whole model test. Thirdly, using orthogonal contrast codes provides a general formula to state the estimate of the slopes of contrast-coded predictors in terms of the averages of the groups. If you use a set of orthogonal contrast codes, then the estimated slope of each predictor $X_j$ corresponding to contrast code $c_j$ can be expressed as the following function of the values of $c_{j,k}$ contrast code $c_j$ for group $k$ and the sample means $\overline{Y}_k$ of the dependent variable in group $k$ as: 
\begin{equation}
\hat{\beta}_j = \frac{\sum_{k=1}^{g} c_{j,k} \overline{Y}_k}{\sum_{k=1} c_{j,k}^2}
(\#eq:estimate-slope-orthogonal-contrast-codes)
\end{equation}
i.e. as the sum of the sample means $\overline{Y}_k$ multiplied by the group-wise values of contrast code $c_j$, divided by the sum of $c_{j,k}^2$, the squared group-wise values of contrast $c_j$. Note that this formula does _not_ hold for the effect-coding scheme discussed previously. For instance, if you'd fill in the values of $c_1$, you'd get
$$\begin{aligned}
\hat{\beta}_1 &= \frac{1 \times \overline{Y}_\text{contr} + 0 \times \overline{Y}_\text{t+r} + 0 \times \overline{Y}_\text{tetr} - 1 \times \overline{Y}_\text{react}}{(1)^2 + (0)^2 + (0)^2 + (-1)^2} \\
&= \frac{\overline{Y}_\text{contr} - \overline{Y}_\text{react}}{2}
\end{aligned}$$
which is _not_ the same as the treatment effect the estimated slope actually reflects:
$$\hat{\beta}_1 = \overline{Y}_\text{contr} - \frac{\overline{Y}_\text{contr} + \overline{Y}_\text{t+r} + \overline{Y}_\text{tetr} + \overline{Y}_\text{react}}{4}$$
Nevertheless, the effect-coding scheme did give us interpretable parameters which reflect treatment effects. Hence, using non-orthogonal contrast codes does not prohibit clearly interpretable parameters. With that said, if you can define the comparisons between groups that are of interest to you in terms of a set of orthogonal contrast codes, that would be preferable. If you can't, there are ways to deal with that too, and you shouldn't worry too much.

There are two rules you can follow to create orthogonal contrast codes:

1. For all contrast codes $c_j$, $j = 1, \ldots, g-1$, $\sum_{k=1}^g c_{j,k} = 0$. The sum of the values of each contrast code $c_j$ is zero.
2. For all pairs of contrast codes $c_j$ and $c_l$, $j \neq l$, $\sum_{k=1}^g c_{j,k} \times c_{l,k} = 0$. The sum of the cross-products of each pair of contrast codes $c_j$ and $c_l$ is equal to zero.

In the statements above, $c_{j,k}$ refers to the value of a contrast code $c_j$ for group $k$, and similarly $c_{l,k}$ refers to the value of a contrast code $c_l$ for group $k$, whilst $g$ refers to the total number of groups. Only the second rule is strictly necessary, but the first one is a useful one to follow as well, as --like in the effect coding scheme-- it ensures that the intercept represents the grand mean. If we check the two rules for the effect coding scheme, we can see that the first requirement is adhered to:

$$\begin{aligned}
\sum_{k=1}^g c_{1,k} &= 1 + 0 + 0 + (-1) = 0 \\
\sum_{k=1}^g c_{2,k} &= 0 + 1 + 0 + (-1) = 0 \\
\sum_{k=1}^g c_{3,k} &= 0 + 0 + 1 + (-1) = 0
\end{aligned}$$
However, the second is _not_:
$$\begin{aligned}
\sum_{k=1}^g c_{1,k} \times c_{2,k} &= 1 \times 0 + 0 \times 1 + 0 \times 0 + (-1) \times (-1) = 1 \\
\sum_{k=1}^g c_{1,k} \times c_{3,k} &= 1 \times 0 + 0 \times 0 + 0 \times 1 + (-1) \times (-1) = 1 \\
\sum_{k=1}^g c_{2,k} \times c_{3,k} &= 0 \times 0 + 0 \times 1 + 0 \times 1 + (-1) \times (-1) = 1 
\end{aligned}$$

One set of questions you could ask, which will result in orthogonal contrast codes, is the following:

1. What is the difference between doing nothing (the Control condition) and doing something (the three remaining conditions)?
2. What is the difference between playing Tetris (the Tetris-Only condition) and a procedure involving memory reactivation (the Tetris+Reactivation and reactivation-Only condition)?
3. What is the difference between memory reactivation without playing Tetris (the Reactivation-only condition) and the playing Tetris after reactivation (the Tetris+Reactivation condition)?

<!-- With answers to these three questions, and knowing the overall mean, you would be able to calculate the average in each condition with complete accuracy. How that works in practice we'll leave for the moment. Instead, let's focus on these three questions. -->
To define our contrast codes, we need to assign values to the conditions in each contrast code such that the slopes of the predictors based on them provide the answers to these questions. The first question refers to the difference between the Control condition and the three other conditions. It then makes sense to compare the mean of the Control condition to the average of the three remaining conditions combined. In this comparison, we would not distinguish between those three remaining conditions, and hence we should give each of them the same value. In addition, remember that a slope reflects an in- or decrease in the dependent variable for a one-unit increase in the predictor. It then makes sense to let the difference between the value we assign to the Control condition and the values we assign to the three other conditions equal 1 (i.e. a one-unit difference). Finally, we would like to use values such that their sum over the groups equals 0 (the first condition for orthogonal contrast codes). Combining these three ideas, to answer the first question, we could define a contrast code $c_1$ with value $\tfrac{3}{4}$ for the control condition, and the value $-\tfrac{1}{4}$ for the Tetris+Reactivation, Tetris-Only, and Reactivation-Only condition. These values sum to 0, and the difference between $\tfrac{3}{4}$ and $-\tfrac{1}{4}$ equals 1, meaning that a one-unit increase for this contrast code is equal to going from one of the three experimental conditions to the Control condition.

For the second question, we would like to compare the Tetris-Only condition to the Tetris+Reactivation and Reactivation-Only condition combined. In this comparison, we ignore the Control condition. By giving this condition a value of 0 in the contrast code, it will not be part of the comparison. Using similar reasoning as before, we can assign a value of $\tfrac{2}{3}$ to the Tetris-only condition, and a value $-\tfrac{1}{3}$ to the Tetris+Reactivation and Reactivation-Only condition. Using the same value for these latter two conditions means that we don't distinguish between them in the comparison, and compare their combined average to the mean of the Tetris-only condition. The suggested second contrast code $c_2$ then has values $0, -\tfrac{1}{3}, \tfrac{2}{3}, -\tfrac{1}{3}$ for the Control, Tetris+Reactivation, Tetris-Only, and Reactivation-Only conditions respectively. Again, the difference between $\tfrac{2}{3}$ and $-\tfrac{1}{3}$ equals 1, so that a one-unit increase in this contrast code is equal to going from one of the reactivation conditions to the Tetris-Only condition.

For the third question, finally, we would like to compare the Reactivation-Only condition to the Tetris+Reactivation condition, ignoring the other two conditions. The suggested contrast code $c_3$ then has values $0, -\tfrac{1}{2}, 0, \tfrac{1}{2}$ for the four conditions respectively. Here, a one-unit increase is equal to going from the Tetris+Reactivation condition to the Reactivation-Only condition. 

The values of the three contrast codes $c_1$, $c_2$, and $c_3$ (for questions 1 to 3 respectively) for each of the four conditions, are summarized given in the table below:

```{r tetris-orthogonal-contrast-codes}
tab <- data.frame(c1 = c("$\\tfrac{3}{4}$","$-\\tfrac{1}{4}$","$-\\tfrac{1}{4}$","$-\\tfrac{1}{4}$"),
                  c2 = c("0","$-\\tfrac{1}{3}$","$\\tfrac{2}{3}$","$-\\tfrac{1}{3}$"),
                  c3 = c("0","$-\\tfrac{1}{2}$","0","$\\tfrac{1}{2}$"))
colnames(tab) <- c("$c_1$","$c_2$","$c_3$")
rownames(tab) <- c("Control","Tetris+Reactivation", "Tetris-Only", "Reactivation-Only")
knitr::kable(tab, escape=FALSE,align=rep('r', 3), caption="A set of orthogonal contrast codes.", booktabs=TRUE, linesep="")
library(dplyr)
means <- dat %>% group_by(Condition) %>% summarise(mean = mean(intrusions)) %>% select(mean)
codes <- cbind(c(3/4,-1/4,-1/4,-1/4),
               c(0,-1/3,2/3,-1/3),
               c(0,-1/2,0,1/2))
means <- means$mean
```

Let's check whether these three contrast codes are orthogonal. The first requirements (values sum to 0) holds:
$$\begin{aligned}
\sum_{k=1}^g c_{1,k} &= \tfrac{3}{4} - \tfrac{1}{4} - \tfrac{1}{4} - \tfrac{1}{4} = 0 \\
\sum_{k=1}^g c_{2,k} &= 0 - \tfrac{1}{3} + \tfrac{2}{3} - \tfrac{1}{3} = 0 \\
\sum_{k=1}^g c_{3,k} &= 0 - \tfrac{1}{2} + 0 + \tfrac{1}{2} = 0
\end{aligned}$$
And indeed the second requirement (pairwise products sum to 0) also holds:
$$\begin{aligned}
\sum_{k=1}^g c_{1,k} \times c_{2,k} &= \tfrac{3}{4} \times 0 + (- \tfrac{1}{4}) \times (- \tfrac{1}{3}) + (-\tfrac{1}{4}) \times \tfrac{2}{3} + (- \tfrac{1}{4}) \times (- \tfrac{1}{3}) \\ &= \tfrac{1}{12} - \tfrac{2}{12} + \tfrac{1}{12} = 0 \\
\sum_{k=1}^g c_{1,k} \times c_{3,k} &= \tfrac{3}{4} \times 0 + (- \tfrac{1}{4}) \times (- \tfrac{1}{2}) + (-\tfrac{1}{4}) \times 0 + (- \tfrac{1}{4}) \times \tfrac{1}{2} \\ &= \tfrac{1}{8} - \tfrac{1}{8} = 0 \\
\sum_{k=1}^g c_{2,k} \times c_{3,k} &= 0 \times 0 + (-\tfrac{1}{3}) \times (- \tfrac{1}{2}) + \tfrac{2}{3} \times 0 + (-\tfrac{1}{3}) \times \tfrac{1}{2} \\ &=  \tfrac{1}{6} - \tfrac{1}{6} = 0
\end{aligned}$$

Because the contrast codes $c_1$, $c_2$, and $c_3$ are orthogonal, we can use Equation \@ref(eq:estimate-slope-orthogonal-contrast-codes) to determine the estimates of the slopes. The estimated slope of the contrast-coded predictor $X_1$ which takes its values from $c_1$ will be:
$$
\begin{aligned}
\hat{\beta}_1 &= \frac{ \tfrac{3}{4} \times \overline{Y}_\text{contr} - \tfrac{1}{4} \times \overline{Y}_\text{t+r} - \tfrac{1}{4} \times \overline{Y}_\text{tetr} - \frac{1}{4} \times  \overline{Y}_\text{react} }{ (\frac{3}{4})^2 + (-\frac{1}{4})^2 + (-\frac{1}{4})^2 + (-\frac{1}{4})^2 }  \\
&= \frac{ \tfrac{3}{4} \times \overline{Y}_\text{contr} - \tfrac{1}{4} \times \overline{Y}_\text{t+r} - \tfrac{1}{4} \times \overline{Y}_\text{tetr} - \frac{1}{4} \times  \overline{Y}_\text{react} }{ \tfrac{3^2}{4^2} + \tfrac{1^2}{4^2} + \tfrac{1^2}{4^2} + \tfrac{1^2}{4^2} }
\end{aligned}
$$
The denominator evaluates to $\tfrac{12}{16} = \tfrac{3}{4}$, and dividing both the numerator and denominator by $\tfrac{3}{4}$ gives^[you are allowed to multiply or divide the numerator and denominator by the same constant, as this does not change the fraction, i.e. $\tfrac{a}{b} = \frac{a/c}{b/c}$]
$$\hat{\beta}_1 = \overline{Y}_\text{contr} - \frac{\overline{Y}_\text{t+r} + \overline{Y}_\text{tetr} + \overline{Y}_\text{react}}{3}$$
i.e. the slope of the first predictor is equal to the difference between the average intrusions in the Control condition, and the average of the average number of intrusions in the three other conditions. When we fill in the actual sample averages, we get:
$$
\begin{aligned}
\hat{\beta}_1 &= \frac{ \tfrac{3}{4} \times `r format(means[1],digits=3)` - \tfrac{1}{4} \times `r  format(means[2],digits=3)` - \tfrac{1}{4} \times `r format(means[3],digits=3)` - \frac{1}{4} \times  `r format(means[4],digits=3)` }{ \tfrac{3}{4}}  \\
&= \frac{`r sum(codes[,1]*means)`}{`r sum(codes[,1]^2)`} = `r sum(codes[,1]*means)/sum(codes[,1]^2)`
\end{aligned}
$$

The estimated slope of the contrast-coded predictor $X_2$ which takes its values from $c_2$ will be:
$$
\begin{aligned}
\hat{\beta}_2 &= \frac{ 0 \times \overline{Y}_\text{contr} - \tfrac{1}{3} \times \overline{Y}_\text{t+r} + \tfrac{2}{3} \times \overline{Y}_\text{tetr} - \frac{1}{3} \times  \overline{Y}_\text{react} }{ (0)^2 + (-\frac{1}{3})^2 + (\frac{2}{3})^2 + (-\frac{1}{3})^2 }  \\
&= \frac{ \tfrac{2}{3} \times \overline{Y}_\text{tetr} - \tfrac{1}{3} \times \overline{Y}_\text{t+r}  - \frac{1}{3} \times  \overline{Y}_\text{react} }{ \tfrac{6}{9}} \\
&= \overline{Y}_\text{tetr} - \frac{\overline{Y}_\text{t+r} + \overline{Y}_\text{react}}{2}
\end{aligned}
$$
The slope of the second predictor is thus equal to the difference between the average number of intrusions in the Tetris-Only condition, and the average of the average number of intrusions in the Tetris+Reactivation condition and the Reactivation-Only condition. Filling in the actual sample averages then gives.
$$
\begin{aligned}
\hat{\beta}_2 &= \frac{ 0 \times `r format(means[1],digits=3)` - \tfrac{1}{3} \times `r  format(means[2],digits=3)` + \tfrac{2}{3} \times `r format(means[3],digits=3)` - \frac{1}{3} \times  `r format(means[4],digits=3)` }{ \tfrac{2}{3} }  \\
&= \frac{`r sum(codes[,2]*means)`}{`r sum(codes[,2]^2)`} = `r sum(codes[,2]*means)/sum(codes[,2]^2)`
\end{aligned}
$$
Finally, the estimated slope of the contrast-coded predictor $X_3$ which takes its values from $c_3$ will be:
$$
\begin{aligned}
\hat{\beta}_3 &= \frac{ 0 \times \overline{Y}_\text{contr}  - \tfrac{1}{2} \times \overline{Y}_\text{t+r} + 0 \times \overline{Y}_\text{tetr} + \frac{1}{2} \times  \overline{Y}_\text{react} }{ (0)^2 + (-\frac{1}{2})^2 + (0)^2 + (\frac{1}{2})^2 }  \\
&= \frac{\frac{1}{2} \times  \overline{Y}_\text{react} - \tfrac{1}{2} \times \overline{Y}_\text{t+r} }{ \tfrac{2}{4} } \\
&= \overline{Y}_\text{react} - \overline{Y}_\text{t+r}
\end{aligned}
$$
i.e. the difference between the average intrusions in the Reactivation-Only condition and the average intrusions in Tetris+Reactivation condition. Filling in the actual sample averages gives:
$$
\begin{aligned}
\hat{\beta}_3 &= \frac{ 0 \times `r format(means[1],digits=3)` - \tfrac{1}{2} \times `r  format(means[2],digits=3)` + 0 \times `r format(means[3],digits=3)` + \frac{1}{2} \times  `r format(means[4],digits=3)` }{\tfrac{1}{2}}  \\
&= \frac{`r sum(codes[,3]*means)`}{`r sum(codes[,3]^2)`} = `r sum(codes[,3]*means)/sum(codes[,3]^2)`
\end{aligned}
$$

While it is important to understand what each slope reflects in terms of differences between the group means, you would not normally use these equations to actually estimate the slopes. It is much easier to use statistical software for that. Estimating a linear regression model with the three predictors $X_1$, $X_2$, and $X_3$, which take values according to the contrast codes $c_1$, $c_2$, and $c_3$ respectively, gives us the following estimated model:

```{r}
contrasts(dat$Condition) <- codes
modg <- lm(intrusions ~ Condition, data=dat)
```
$$`r write_GLM_equation(modg, dv_name = "intrusions", iv_names = c("}X_1\\texttt{","}X_2\\texttt{","}X_3\\texttt{"), digits=3, include_sde = TRUE)`$$
As you can see, the parameter estimates are identical to those worked out above using Equation \@ref(eq:estimate-slope-orthogonal-contrast-codes) and the sample means. Figure \@ref(fig:tetris-contrast-coding-parameter-plot) shows how the parameters of the model are related to the average number of intrusions in each condition.
```{r tetris-contrast-coding-parameter-plot, fig.cap="Average number of instrusions in the four conditions in the Tetris study and how they are related to the parameters of the orthogonal contrast-coding model. The intercept $\\hat{\\beta}_0 = \\hat{\\mu}$ is the grand mean (dotted line). The slopes reflect deviations from the average intrusions in a condition and combinations of other conditions."}
data.frame(Condition = factor(1:4,labels=c("Control","Tetris+Reactivation","Tetris-Only","Reactivation-Only")), mean=means) %>%
  ggplot(aes(x=Condition, y=mean, xend = Condition)) + geom_point() + geom_hline(yintercept = mean(means), lty=3) + geom_line(data=data.frame(x=c(.8,1.2),y=rep(mean(means[2:4]),2)), aes(x=x,y=y,xend=x),lty=3) + annotate("text", x=1, y=mean(means[2:4] - .3), label=expression(over(bar(Y)[t+r] + bar(Y)[tetr] + bar(Y)[react],3))) +  geom_segment(data=data.frame(Condition="Control", mean = means[1], start = mean(means[2:4])), aes(x=Condition, y = mean, xend=Condition, yend=start), arrow = arrow(length=unit(0.20,"cm"), type="closed", ends = "first"), alpha=.6) + geom_line(data=data.frame(x=c(2.8,3.2),y=rep(mean(means[c(2,4)]),2)), aes(x=x,y=y,xend=x),lty=3) + annotate("text", x=3, y=mean(means[c(2,4)] - .3), label=expression(over(bar(Y)[t+r] + bar(Y)[react],2))) +  geom_segment(data=data.frame(Condition="Tetris-Only", mean = means[3], start = mean(means[c(2,4)])), aes(x=Condition, y = mean, xend=Condition, yend=start), arrow = arrow(length=unit(0.20,"cm"), type="closed", ends = "first"), alpha=.6) + geom_line(data=data.frame(x=c(3.8,4.2),y=rep(mean(means[c(2)]),2)), aes(x=x,y=y,xend=x),lty=3) + annotate("text", x=4, y=mean(means[c(2)] - .2), label=expression(bar(Y)[t+r])) + geom_segment(data=data.frame(Condition="Reactivation-Only", mean = means[4], start = mean(means[2])), aes(x=Condition, y = mean, xend=Condition, yend=start), arrow = arrow(length=unit(0.20,"cm"), type="closed", ends = "first"), alpha=.6) + annotate("text",x=0.5,y=mean(means) - .15, label=expression(hat(beta)[0])) + ylab("Intrusions") + annotate("text",x=1.1,y=.5*mean(means) + .5*means[1], label=expression(hat(beta)[1])) + annotate("text",x=3.1,y=.5*mean(means) + .5*mean(means[c(2,4)]), label=expression(hat(beta)[2])) + annotate("text",x=4.1,y=.5*mean(means) + .5*means[4], label=expression(hat(beta)[3]))
#+ annotate("text",x=3.5,y=.5*mean(means) + .5*means[1], label=expression(beta[0] - (beta[1] + beta[2] + beta[3]))) 
```


Hypothesis tests for all the parameters, as well as the omnibus test for Condition, are provided in Table \@ref(tab:tetris-contrast-coding-ANOVA). As before, we find a significant overall effect for Condition, which indicates that at least one of the group means differs from another one. The tests for the first two contrasts are not significant. As such, we have no evidence that the Control condition differs from the other three conditions combined, or that the Tetris-Only condition differs from the Tetris+Reactivation or Reactivation-Only condition combined. The test of the third contrast is significant, however, indicating a difference between the Tetris+Reactivation and Reactivation-Only condition. The slope of this comparison is positive, indicating that, as expected, there are more memory intrusions in the Reactivation-Only condition compared to the Tetris+Reactivation condition.

```{r tetris-contrast-coding-ANOVA}
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modg)[1],NA,coefficients(modg)[2:4],Error=NA),rbind(car::Anova(modg, type=3)[1:2,],expand_Anova(modg, type=3)[-1,]))
rownames(tab) <- c("Intercept","Condition","$\\quad X_1$","$\\quad X_2$", "$\\quad X_3$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting number of intrusions by three contrast-coded predictors.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

Comparing the results in Table \@ref(tab:tetris-contrast-coding-ANOVA) to those in Table \@ref(tab:tetris-effect-coding-ANOVA), there are a few things to note. Firstly, the result of the omnibus test is _exactly the same_, whether you use effect coding or orthogonal contrast coding. Because both ways of coding in the end make the same prediction $\hat{Y}_i$ (namely that $\hat{Y}_i$ equals the group mean of the condition that case $i$ belongs to), the model as a whole has the same SSE for both coding schemes. And as the omnibus test involves the same MODEL R (an intercept-only model), the SSR term of the omnibus Condition effect is the same for effect-coding and orthogonal coding. Indeed, for any form of contrast coding which results in model predictions that equal the group means, the omnibus test gives exactly the same results! The model with orthogonal contrast coding, however, separates the whole model SSR term neatly into into different SSR terms for the contrast-coded predictors. If you add up the SSR terms for the three predictors (in the column labelled SS), you get exactly the SSR term given for Condition. This shows that when you use orthogonal contrast coding, the variance explained by the model as a whole is separated exactly into three independent parts reflecting the unique part of the variance explained due to each predictor. 

### Defining your own (orthogonal) contrasts

Initially, coming up with orthogonal contrast codes (or for that matter, any coding scheme which allows the model predictions to equal the group means exactly) will not be easy, especially for nominal variables with more than 3 levels (i.e. more than 3 groups). With practice, you should become better at this. Ideally, the contrast codes reflect at least some of the theoretically important research questions you want to ask (e.g., relevant comparisons of the effects of experimental manipulations). In an experimental design with $g$ groups, you need to define $g-1$ contrast codes which correspond to such questions. 

When designing your own contrast codes, you should start with the most important question you want to ask. For instance, you may want to compare the conditions with memory reactivation to those without memory reactivation, because you expect more memory intrusions after memory reactivation than without this reactivation. Your first contrast code would focus on this comparison, and you could define it as $c_1 = (-\tfrac{1}{2}, \tfrac{1}{2}, -\tfrac{1}{2}, \tfrac{1}{2})$. Note that I'm giving the Control and Tetris-Only condition a negative value of $-\tfrac{1}{2}$ and the Tetris+Reactivation and Reactivation-Only condition a positive value of $\tfrac{1}{2}$. In this case, the resulting slope would be positive if the expectation of more memory intrusions after reactivation holds true. For expectations which involve not only a difference, but also a direction of that difference, I find it useful to assign values which are in line with those expectations. Negative slopes of contrast-coded predictors in the model then indicate that my expectations did not hold true. 

Having defined this initial contrast, we have two more to go. Perhaps I'm also interested in assessing the effect of Tetris within the conditions that involve memory reactivation, because I expect playing Tetris to reduce the number of memory intrusions. This is a comparison between the Reactivation-Only and Tetris+Reactivation conditions. Because we gave these conditions the same value in the first contrast $c_1$ (i.e. we did not differentiate between them in the first comparison), this new comparison will be independent of (orthogonal to) the first comparison. We can use the same contrast code for this as the one used earlier, so $c_2 = (0,-\tfrac{1}{2}, 0, \tfrac{1}{2})$. It is easy to check that this contrast is indeed orthogonal to the first one
$$\sum_{k=1}^g c_{1,k} \times c_{2,k} = -\tfrac{1}{2} \times 0 + \tfrac{1}{2} \times (-\tfrac{1}{2}) + (-\tfrac{1}{2}) \times 0 + \tfrac{1}{2} \times \tfrac{1}{2} = 0$$
We now have one contrast left to specify. Perhaps we don't really have any more important questions to ask, so this contrast might be arbitrary from a theoretical viewpoint. However, to have a model which is able to fit the group means exactly, we would need a contrast code that differentiates between the Control condition and the Tetris-Only condition, as these conditions received the same value on $c_1$ (both $-\tfrac{1}{2}$) _and_ $c_2$ (both 0). If our last contrast code would not differentiate between these two conditions, then there would be no way in which the model can make different predictions for those two conditions. The model then involves a quite strong assumption that means of the Control and Tetris-Only condition are exactly identical. I don't see a reason why such equivalence between the conditions would necessarily hold. So the final contrast code will be $c_3 = (\tfrac{1}{2}, 0, -\tfrac{1}{2}, 0)$. Note that if I had used different values for the Reactivation-Only and Tetris+Reactivation conditions, for instance a contrast code of $(\tfrac{1}{2}, -\tfrac{1}{2}, -\tfrac{1}{2}, \tfrac{1}{2})$, then the contrast code would correlate with $c_2$ (or with $c_1$ if I had used different values). So the final contrast code needs to give the same value (0) to the Reactivation-Only and Tetris+Reactivation conditions to give us a set of orthogonal contrast codes. So the final set of orthogonal contrast codes is

```{r}
tab <- data.frame(c1 = c("$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$","$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$"),
                  c2 = c("0","$-\\tfrac{1}{2}$","0","$\\tfrac{1}{2}$"),
                  c3 = c("$\\tfrac{1}{2}$","0","$-\\tfrac{1}{2}$","0"))
colnames(tab) <- c("$c_1$","$c_2$","$c_3$")
rownames(tab) <- c("Control","Tetris+Reactivation", "Tetris-Only", "Reactivation-Only")
knitr::kable(tab, escape=FALSE,align=rep('r', 3), booktabs=TRUE, linesep="")
```

I will leave checking the orthogonality, and estimating and testing the parameters of the resulting model, as an exercise to the reader. Note that this strategy of splitting conditions in halves to compare, and then splitting these halves in other halves, can generally be applied. And the question strategy in Figure \@ref(fig:number-guessing-game-contrast-coding) is an example of this.

## Default orthogonal coding schemes

While orthogonal contrasts provide benefits in terms of straightforward interpretation of the parameters, and, in the case of equally sized groups, independent predictors and and a neat division of the whole model $\text{SSR}$ into the separate $\text{SSR}$ terms for the predictors, orthogonality is not a strict requirement. For instance, effect coding and dummy coding also provide interpretable parameters, although the predictors are not independent. The most important thing is that the contrast codes can answer questions about the Data Generating Process which are of interest to you. 

In the absence of any comparisons of interest, you shouldn't really be conducting hypothesis tests in the first place. That said, there might be times where you would like to resort to a "default" way of orthogonal contrast coding. Such default coding schemes can also provide inspiration for defining your own contrast codes. Two default orthogonal contrast codes are Helmert coding, and polynomial contrast coding.

__Helmert coding__ involves a set of contrast codes in which each code compares a group mean to the average of all group means that come before it. You can find these contrast, for situations with a group size of $g=1, 2, \ldots, 6$, in Table \@ref(tab:Helmert-contrasts). Using Helmert contrasts will always provide a set of orthogonal contrasts. And if you reorder the conditions in Table \@ref(tab:tetris-orthogonal-contrast-codes) as Tetris+Reactivation, Reactivation-Only, tetris-Only, Control, you should recognize that this was actually an example of Helmert coding for $g=4$. 

```{r Helmert-contrasts}
options(knitr.kable.NA = "")
tmp <- data.frame(group = NA, H2=NA, H3 = NA, H4 = NA, H5 = NA, H6 = NA)
tab <- as.data.frame(codingMatrices::code_helmert(2))
tab$group <- 1:2
#mp <- data.frame(H2=NA, H3 = NA, H4 = NA, H5 = NA, H6 = NA)
tab <- dplyr::bind_rows(tmp, tab)
for(g in 3:6) {
  tmp <- data.frame(group = NA, H2=NA, H3 = NA, H4 = NA, H5 = NA, H6 = NA)
  tab <- dplyr::bind_rows(tab, tmp)
  tmp <- as.data.frame(codingMatrices::code_helmert(g))
  tmp$group <- 1:g
  tab <- dplyr::bind_rows(tab, tmp)
  #tmp <- data.frame(H2=NA, H3 = NA, H4 = NA, H5 = NA, H6 = NA)
  #tab <- dplyr::bind_rows(tab, tmp)
}
colnames(tab)[2:6] <- paste0("$c_",1:(6-1),"$")
knitr::kable(tab, escape = FALSE, caption = "Helmert contrast codes for situations with two to six groups.", row.names = FALSE, booktabs=TRUE, linesep="")
```

__Polynomial contrast codes__ tend to be used when the groups can be ordered in a meaningful way. For instance, a study might involve different age groups, e.g. "18-30 year old", "31-40 year old", and "41-50 year old". Using polynomial contrast coding, you can then determine a linear trend over these age groups with a _linear_ contrast $c_1 = (-1,0,1)$, as well as whether the average of the middle age group is lower (or higher) than would be expected from the linear contrast through a _quadratic_ contrast $c_2 = (1,-2,1)$. Examples of polynomial contrast codes for situations with between 2 and 6 groups are provided in Table \@ref(tab:polynomial-contrasts). 

```{r polynomial-contrasts}
options(knitr.kable.NA = "")
tmp <- data.frame(group = NA, ".I"=NA, ".II" = NA, ".III" = NA, ".IV" = NA, ".V" = NA)
tab <- as.data.frame(codingMatrices::code_poly(2))
tab$group <- 1:2
#mp <- data.frame(H2=NA, H3 = NA, H4 = NA, H5 = NA, H6 = NA)
tab <- dplyr::bind_rows(tmp, tab)
for(g in 3:6) {
  tmp <- data.frame(".I"=NA, ".II" = NA, ".III" = NA, ".IV" = NA, ".V" = NA)
  tab <- dplyr::bind_rows(tab, tmp)
  tmp <- as.data.frame(codingMatrices::code_poly(g))
  tmp$group <- 1:g
  tab <- dplyr::bind_rows(tab, tmp)
  #tmp <- data.frame(H2=NA, H3 = NA, H4 = NA, H5 = NA, H6 = NA)
  #tab <- dplyr::bind_rows(tab, tmp)
}
colnames(tab)[2:6] <- paste0("$c_",1:(6-1),"$")
knitr::kable(tab, escape = FALSE, caption = "Polynomial contrast codes for situations with two to six groups.", row.names = FALSE, booktabs=TRUE, linesep="")
```


## Effect-size in ANOVA

```{r}
tab <- cbind(estimate = c(coefficients(modg)[1],NA,coefficients(modg)[2:4],Error=NA),rbind(car::Anova(modg, type=3)[1:2,],expand_Anova(modg, type=3)[-1,]))
```

As contrast-coded predictors are essentially just predictors in a linear model, we can use similar measures of effect-size as in multiple regression (see Section \@ref(sec:04b-effect-size)). So we can measure the size of an effect as the "proportion of variance explained", or "eta-squared"
$$\hat{\eta}^2 = \frac{\text{SS}(\text{effect})}{\text{SS}(\text{total})}$$
Here, $\text{SS}(\text{effect})$ is the Sum of Squares Reduced (SSR) for that effect. This is the reduction in the error when estimating the effect rather than fixing it to an a priori value. $\text{SS}(\text{total})$ is the Sum of Squared Error of a model with only an intercept, which is a function of the sample variance of the dependent variable: $\text{SS}(\text{total}) = n S^2_Y$. In the context of multiple regression, this is also called the coefficient of semi-partial determination.

We may also consider the "partial eta-squared", or coefficient of partial determination:
$$\hat{\eta}_p^2 = \frac{\text{SS}(\text{effect})}{\text{SS}(\text{error}) + \text{SS}(\text{effect}) }$$

For both these measures, the effect can be either a single contrast-coded predictor, or a set of these. So we may also consider the effect of Condition. In Table \@ref(tab:tetris-contrast-coding-ANOVA), we can see that the Sum of Squares Reduced for Condition is $\text{SS}(\text{Condition}) = `r round(tab[2,2],2)`$. The value of $\text{SS}(\text{total})$ is `r round(sum(tab[c(2,6),2]),2)`, which is equal to the sum of $\text{SS}(\text{Condition})$ and $\text{SS}(\text{error})$. We can then compute the eta-squared as
$$\hat{\eta}^2(\text{Condition}) = \frac{`r round(tab[2,2],2)`}{`r round(sum(tab[c(2,6),2]),2)`} = `r round(tab[2,2]/sum(tab[c(2,6),2]),3)`$$
The value of $\text{SS}(\text{error})$ is `r round(tab[6,2],2)`, and hence the partial eta-squared is
$$\hat{\eta}_p^2(\text{Condition}) = \frac{`r round(tab[2,2],2)`}{`r round(tab[6,2],2)` + `r round(tab[2,2],2)`} = `r round(tab[2,2]/sum(tab[c(2,6),2]),3)`$$
In a oneway ANOVA model, the partial and non-partial eta-squared measures are the same for the overall omnibus effect of condition. That is because in that case, $\text{SS}(\text{error}) + \text{SS}(\text{Condition}) = \text{SS}(\text{total})$.   For the individual contrasts, there is a difference in the partial and non-partial eta-squared measures. For instance, computing the effect-sizes of $X_1$, which reflects the difference between the Control group and the other conditions, gives:
$$\hat{\eta}^2(X_1) = \frac{`r round(tab[3,2],2)`}{`r round(sum(tab[c(2,6),2]),2)`} = `r round(tab[3,2]/sum(tab[c(2,6),2]),3)`$$
and
$$\hat{\eta}_p^2(X_1) = \frac{`r round(tab[3,2],2)`}{`r round(tab[6,2],2)` + `r round(tab[3,2],2)`} = `r round(tab[3,2]/sum(tab[c(3,6),2]),3)`$$

## Assumptions

The assumptions of the ANOVA model are the same as those for a (multiple) regression model (see Section \@ref(sec:04b-assumptions)). Thus, we assume the errors $\epsilon$ are independent and Normal-distributed, with a mean of 0 and a homoscedastic standard deviation $\sigma_\epsilon$. As an ANOVA model provides the same prediction for each case in a group/condition, the errors reflect variation in the dependent variable within each group or condition. Another way to state the assumptions of the General Linear Model in the case of an ANOVA model is then that the variance of the dependent variable within each group or condition is the same (see Figure \@ref(fig:four-group-Normal-densities)).

```{r four-group-Normal-densities, fig.cap="An ANOVA model assumes that the variance within each condition or group is identical. The means of the conditions or groups can differ.", fig.height=4,out.width="80%"}
mus <- c(3.5,4,4.2,6)
ggplot() + stat_function(fun=function(x) dnorm(x,mean=mus[1],sd=1)) + stat_function(fun=function(x) dnorm(x,mean=mus[2],sd=1)) + stat_function(fun=function(x) dnorm(x,mean=mus[3],sd=1)) + stat_function(fun=function(x) dnorm(x,mean=mus[4],sd=1)) + xlim(0,10) + ylab("p(y)") + xlab("y") + geom_line(data=data.frame(x=c(mus[1],mus[1]),y=c(0,dnorm(mus[1],mus[1],1))),aes(x=x,y=y),lty=3) + geom_line(data=data.frame(x=c(mus[2],mus[2]),y=c(0,dnorm(mus[2],mus[2],1))),aes(x=x,y=y),lty=3) + geom_line(data=data.frame(x=c(mus[3],mus[3]),y=c(0,dnorm(mus[3],mus[3],1))),aes(x=x,y=y),lty=3) + geom_line(data=data.frame(x=c(mus[4],mus[4]),y=c(0,dnorm(mus[4],mus[4],1))),aes(x=x,y=y),lty=3) + annotate("text",x=mus[1],y=-.02,label=expression(mu[1])) + annotate("text",x=mus[2],y=-.02,label=expression(mu[2])) + annotate("text",x=mus[3],y=-.02,label=expression(mu[3]))  + annotate("text",x=mus[4],y=-.02,label=expression(mu[4])) + theme(axis.text.x = element_blank(),axis.ticks.x = element_blank(),axis.text.y = element_blank(),axis.ticks.y = element_blank())
```

We can use similar techniques to assess the assumptions as for multiple regression, such as QQ-plots and histograms of the errors. A predicted-vs-residual plot is less informative here, as there are only as many unique predictions as the number of conditions or groups. Because we have multiple observations within each group, we can test the homoscedasticity assumption. This assumption is also called the homogeneity of variance assumption. A widely-used test is known as __Levene's test__ [@levene1960robust]. This is a test of the null-hypothesis $H_0: \sigma^2_1 = \sigma^2_2 = \ldots = \sigma^2_g$, where $\sigma^2_j$ is the variance of $Y$ in a group $j$. The test effectively uses absolute deviations between observed values of $Y$ and the group average $\overline{Y}_g$ as the dependent variable in an ANOVA model. If one group has a larger variance, then the absolute deviations will also be larger, on average, than those in another group. A significant Levene test indicates that the null-hypothesis of equal variances is likely false. 

As usual, the assumptions of the model allow us to derive the sampling distribution of the test statistic under the null-hypothesis. If the assumptions hold, then we are assured the distribution of the $F$-statistic follows an $F$-distribution with the given degrees of freedom. Even if the errors are not strictly Normal-distributed, or there are some small differences in the variance between the groups, the $F$-statistic may still approximately follow the assumed $F$-distribution. The general consensus is that ANOVAs are reasonably robust against deviations from Normal-distributed errors, as long as the error distributions are approximately symmetric, or have the same shape in each group [@howell2012statistical]. When sample sizes are equal for each group, ANOVAs are also robust to violations of homoscedasticity [see @lix1996consequences; @maxwell2017designing; @howell2012statistical]. So even if the Levene test is significant, you may still be able to use an ANOVA model.

Regarding potential heteroscedasticity, @maxwell2017designing offer the following practical advice: in a one-way ANOVA model with equal sample sizes, you may stick to the standard $F$ test if the ratio of the largest sample variance $S^2_\text{max}$ to the smallest sample variance $S^2_\text{min}$ is smaller than 4, i.e. $\frac{S^2_\text{max}}{S^2_\text{min}} < 4$. When sample sizes are unequal, things become a little more complicated. The suggestion is to also take the ratio of the sample size in the largest group, $n_\text{max}$ to the sample size of the smallest group, $n_\text{min}$ into account, and stick to the standard $F$ test whenever
$$\frac{n_\text{max}}{n_\text{min}} \times \frac{S^2_\text{max}}{S^2_\text{min}} < 4$$
When the ratio is larger than 4, you could decide to use an alternative test, such as the Brown-Forsythe or the Welch test [see e.g. @tomarken1986comparison]. When the variance of a dependent variable increases with the mean, it may also be possible to transform the dependent variable using e.g. a logarithmic or square-root transformation.

@box1954some showed that in the case of heteroscedasticity (unequal variances), when there are equal sample sizes, the correct critical value for the $F$-test is somewhere between the critical value of the $F$-statistic with the usual degrees of freedom (i.e. $\text{df}_1 = \text{npar}(G) - \text{npar}(R)$ and $\text{df}_2 = n - \text{npar}(G)$), and a critical value of the $F$-statistic with smaller degrees of freedom: $\text{df}_1 = 1$ and $\text{df}_2 = \frac{n - \text{npar}(G)}{g}$. In the case of unequal variances, you may then also perform a conservative test, using the critical value for these smaller degrees of freedom. If this provides a significant result, that implies there are differences in group means regardless of whether the variances are equal or not.

## Multiple testing and post-hoc tests

Using contrast codes, you can test a total of $g-1$ comparisons within a single analysis. At times, this may suffice to test all hypotheses of interest. At other times, you would like to test more hypotheses. 

For example, you might want to test for differences in the means of all pairs of conditions. In a design with 4 conditions, there are a total of $(4-1)! = 6$ pairwise comparisons between the conditions. Obtaining all these pairwise tests is actually pretty simple: you can just perform multiple analyses with different contrast codes. If you are interested in pairwise differences, then dummy coding is the most obvious choice. In any model with dummy coding, you would obtain all pairwise tests comparing each condition to a reference condition. If you perform three analyses, and in each change the reference condition, you would obtain all pairwise tests. Case closed?

Well, maybe not entirely. Although this is a straightforward way to obtain the tests, you will end up performing quite a lot of these. The number of tests increases when you have more conditions. For instance, with 5 conditions, you would need to perform $(5-1)! = 24$ tests, and with 6 conditions you would perform $(6-1)! = 120$ tests. For each test, we allow a Type 1 error rate equal to the significance level $\alpha$, usually $\alpha = .05$. Say that we perform 120 tests. When there are no differences between the conditions, so all the null-hypotheses are true, then we would still expect $.05 \times 120 = 6$ significant test results. In a set of multiple tests, the probability of making at least one Type 1 error is higher than the significance level of each individual test.

The reason for focusing on the number of Type 1 Errors (also called false positives) in a set of tests, rather than for each individual test, is related to the distinction between __a priori comparisons__ and __post-hoc comparisons__. A priori comparisons are comparisons between conditions that reflect hypotheses which researchers planned to test before data collection or statistical analysis. Generally, there are only a relatively small number of such a priori comparisons. Post-hoc comparisons are comparisons which are planned after the data has been collected and an initial statistical analysis. For instance, the data might point to an unexpected difference between the Control and Tetris-only condition, and then the researcher might conduct a further analysis to test this specific difference. Or a significant omnibus test but a non-significant result for an expected difference might inspire you to conduct all pairwise comparisons to see what this significant omnibus test is due to. A danger with such post-hoc tests is that the result that inspired the subsequent tests might itself be a Type 1 error (i.e. a "fluke"). Post-hoc and data-driven hypotheses have a risk of confirming noise in the data. The means of two conditions might differ purely due to chance, and testing whether such an unexpected difference is significant may then just confirm chance patterns in the data, rather than reflecting a true aspect of the Data Generating Process. Although there is such risk in any statistical hypothesis test, whether planned a priori or post-hoc, the fact that post-hoc comparisons are generally tests of unexpected differences, there is reason to be more conservative in these latter tests. As a priori comparisons are, by definition, not inspired by the data, there is less of a chance of following a trail of spurious results.

The overall Type 1 error rate in a set of tests is conventionally called the __family-wise Type 1 error rate__, or $\alpha_\text{FW}$. This is equal to the probability of making at least one Type 1 error within a set of hypothesis tests. This probability is equal to one minus the probability of making no Type 1 error at all. If all tests are independent, then the probability of making no Type 1 error in a total of $q$ tests is $(1-\alpha)^q$. Hence, the family-wise error rate is
$$\alpha_\text{FW} = 1 - (1-\alpha)^q$$
If we use the conventional $\alpha = .05$ for each test, and $q = 120$, then the family-wise error rate would be
$$\alpha_\text{FW} = 1 - .95^{120} = `r round(1 - .95^120,3)`$$
which shows we are virtually guaranteed to make at least one Type 1 error. 

This __inflation of Type 1 error__ can be dealt with, though. One (perhaps too) simple solution is to apply the so-called __Bonferroni correction__ to the significance level of each test, such that the family-wise Type 1 error rate is kept at a desired level. The Bonferroni correction is simply to choose a desired value for $\alpha_\text{FW}$, e.g. $\alpha_\text{FW} = .05$, and then to set the significance level of the individual tests to $\alpha = \frac{\alpha_\text{FW}}{q}$. Doing so, the family-wise error rate is never above what we want it to be. For example:
$$\alpha_\text{FW} = 1 - \left(1 - \frac{.05}{120}\right)^{120} = `r round(1 - (1-.05/120)^{120},3)`$$
So, following the Bonferroni correction, we would use a significance level of $.05/120 = `r format(.05/120, digits=3)`$ for each test of a pairwise difference. While this correction ensures that the family-wise error rate is kept within bounds, it is also rather conservative. The difference between each pair of conditions would have to be rather large to pass such a high bar. Luckily, clever statisticians have devised alternatives which obtain the same objective (limiting the family-wise error rate), whilst being less conservative. There are a _lot_ of these, and I will only mention a few here.

The __Holm correction__ requires you to perform all tests first. You then need to order the resulting $p$-values, $p_j$, $j=1, \ldots,q$ for each test from smallest to largest. The smallest $p$-value is assigned $\text{rank}(p_j) = 1$, and the largest $\text{rank}(p_j) = q$. The Holm procedure is to compare each $p$-value to a significance level of
$$\alpha(p_j) = \frac{\alpha_\text{FW}}{(q - \text{rank}(p_j) + 1)}$$
For example, if you performed a $q=80$ tests and want to maintain a family-wise significance level of $\alpha_\text{FW} = .05$, you would compare the 40th lowest $p$-value to a significance level of $.05/(80 - 40 + 1) = `r format(.05/(80-40+1),digits=3)`$. Using the Bonferroni correction, the comparison level would be $.05/80 = `r format(.05/(80),digits=3)`$. As soon as you obtain a non-significant result (i.e. $p_j > \alpha(p_j)$), you stop testing. All subsequent tests are declared non-significant. Like the Bonferroni correction, this procedure ensures that the family-wise Type 1 error rate is kept within a desired bound $\alpha_\text{FW}$, but it is more powerful than the Bonferroni correction (i.e. the rate of Type 2 errors is lower). The Holm correction may be less intuitive and more involved to compute than the Bonferroni correction, but because it reaches the same goal with more power, there is no reason to use the Bonferroni correction instead of the Holm correction.

Another option, relevant within the context of ANOVA, is to use the __ScheffÃ© adjusted__ critical $F$-value:
$$F_\text{ScheffÃ©} = (g-1)F_{g-1,n-\text{npar}(G);\alpha}$$
where $F_{g-1,n-\text{npar}(G);\alpha_\text{FW}}$ is the critical value for an $F$ distribution with $\text{df}_1 = g-1$ and $\text{df}_2 = n - \text{npar}(G)$ degrees of freedom (note that $g$ refers to the number of groups/conditions in the design, not the number of tests you want to perform), and a desired family-wise error rate $\alpha_\text{FW}$. Just as for the Holm procedure, the ScheffÃ© testing procedure ensures that the family-wise error rate never exceeds the desired level. It can also be more powerful than the Bonferonni correction, depending on the number of hypotheses you want to test. The ScheffÃ© criterion is independent of the number of contrasts you want to test, and is effectively a correction for _any number_ of possible contrast you want to test, whether this be pairwise comparisons between conditions, or comparisons of combinations of the different conditions. The Holm procedure can also be applied in the case of these more general comparisons. If you have a limited number of hypotheses to test, the Holm procedure will generally be more powerful than the ScheffÃ© procedure. 

<!-- This is a good point to discuss the difference between __planned comparisons__ and __post-hoc tests__. The former refers to a limited set of hypotheses that are of interest to test, and that you planned to test _before_ looking at the data. The latter refers to hypotheses that are inspired by analyses you have already performed on the data. For instance, a significant omnibus test but a non-significant result for an expected difference might inspire you to conduct all pairwise comparisons to see what this significant omnibus test is due to. A danger with such post-hoc tests is that the result that inspired the subsequent tests might itself be a Type 1 error (i.e. a "fluke"). For this reason, more care should be taken with post-hoc tests, and a certain level of conservatism is warranted. Because planned comparisons are, by definition, not the result of prior test results, there is less of a chance of following a trail of spurious results.-->

Finally, if you are just interested in _all_ pairwise comparisons between conditions (whether post-hoc or not), I want to point out the __Tukey Honestly Significant Difference__ (HSD) test. It was specifically designed for this purpose, and will often be more powerful than the previously discussed correction methods. 

We now have to discuss a rather tricky issue. Suppose you are really just interested in testing a small number of interesting comparisons between conditions. Good! That means that you have thought about your study and the information you want to get out of it. The question is now whether you should apply any correction at all. You may have already wondered why, in a linear model with $m$ predictors, and tests of all $m+1$ parameters (the intercept and slopes for the $m$ predictors), we haven't worried before about an inflation of Type 1 error. Clearly, if we'd included 20 predictors in our model, we'd on average expect one of these tests to be significant, even if none of the predictors have any real effect. That is a very good question, and I don't have a definite answer to it. The answer really depends on what you think is an allowable error rate. And that is not tied to a single analysis. In your scientific career, you might perform a total of 100,000 hypothesis tests. How many of those would you like to be possible Type 1 errors? Should you apply something like a Bonferonni or Holm correction to all these tests? This is of course a rather silly question to ask, but it highlights that recommendations for multiple testing should be considered carefully. They are recommendations, and not strict rules that should be followed blindly. In my humble opinion, as long as you have a limited number of a priori comparisons, you don't necessarily have to apply any correction. And if you want to be careful, apply a Holm correction, as this is more powerful than a Bonferroni correction. When your tests are exploratory (post-hoc and inspired by previous test results), you should be careful and something like the ScheffÃ© procedure makes sense, because it corrects for all comparisons you might conduct, whether the result of "data-snooping" or otherwise. There is of course still no guarantee that a given test result is not a Type 1 error, but at least you have limited the total number of likely Type 1 errors within the set of possible comparisons.

## In practice

1. Explore the data. Check the distribution of the dependent variable in each condition/group. Are there outlying or otherwise "strange" observations? If so, you may consider removing these from the dataset. Do the distributions look roughly Normal or are the distributions at least similar over the groups? Calculate the sample variances for each group. Is the largest variance no more than 4 times larger than the smallest sample variance? If not, then you may consider an alternative analysis than an ANOVA. If you have doubts about the homogeneity of variance, perform a Levene test. If this test is significant, you may still perform the ANOVA analysis as usual if the largest variance is less than 4 times larger than the smallest sample variance and the sample sizes in the groups are equal.

2. Define a useful set of contrast codes for your study. Aim for these codes to represent the most important comparisons between the groups. Estimate the ANOVA model. Then check again for potential issues in the assumptions with e.g. histograms for the residuals and QQ-plots. If there are clear outliers in the data, remove these, and then re-estimate the model.

3. Consider both the results of the omnibus test (are there any differences between the groups?) and the tests of the individual contrasts (are there specific differences between the groups of interest?). If there are more comparisons of interest than those encoded in the contrast-coded predictors, perform additional follow-up tests. If there are many of these tests, consider correcting for this by using e.g. a Scheffe-adjusted critical value. 

4. Report the results. When reporting the results, make sure that you include all relevant statistics. For example, one way to write the results of the analysis with the contrast codes of Table \@ref(tab:tetris-orthogonal-contrast-codes), is as follows:

> We asessed the effect of memory reactivation and playing Tetris on subsequent memory intrusions with a oneway ANOVA. This showed a significant overall effect of Condition, `r papaja::apa_print(car::Anova(lm(intrusions~Condition, data=dat)), es="pes", mse=FALSE)$full_result$Condition`. A priori contrasts showed that the memory intrusions following a combination of memory reactivation and playing Tetris were significantly lower than those following memory reactivation alone, `r  papaja::apa_print(modg)$full_result$Condition3`. However, we found no significant difference between the Control condition and the remaining three conditions, `r  papaja::apa_print(modg)$full_result$Condition1`. Finally, memory reactivation with or without playing Tetris did not significantly increase or reduce the number of intrusive memories compared to just playing Tetris without memory reactivation, `r  papaja::apa_print(modg)$full_result$Condition2`. Post-hoc pairwise comparisons with Tukey's HSD test showed that the combined Tetris+reactivation condition differed from the Control condition and the Reactivation-only condition. There were no other significant pairwise differences. 

<!-- 
## Assumptions {#sec:06-assumptions}

The assumptions of the oneway ANOVA model are the same as the assumptions of any General Linear Model (see Section \@ref(sec:04b-assumptions)). Thus, we assume the errors (residuals) of the model to be Normal-distributed with a mean of 0, a constant standard deviation $\sigma_\epsilon$, and each error term $\epsilon_i$ to be independent from any other one. However, because a oneway ANOVA makes the same prediction within each group, $\hat{Y} = \overline{Y}_j$, where $\overline{Y}_j$ is the sample mean of the dependent variable in the group the case belongs to.  
-->

<!--chapter:end:06-ANOVA.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Factorial ANOVA {#ch-factorial-ANOVA}

In this chapter, we will extend our discussion of including nominal predictors in the General Linear Model to the case of multiple nominal predictors, where all the combinations of the levels of each are included in the design of a study. The analysis of such designs, with a factorial combination of all levels, is generally called factorial ANOVA. While the method is, as we will see, not fundamentally different than a oneway ANOVA, treated in the previous chapter, the factorial nature allows one to distinguish between _main effects_ of factors, and their _interactions_. After discussing how to interpret these effects, we turn to issues that arise when there are an unequal number of observations for the combinations of the levels. 

```{r}
library(sdamr)
data("expBelief")
```

## Experimenter beliefs and social priming

Research suggests that stimuli that prime social concepts can fundamentally alter people's behaviour. For instance, in one experiment people were primed with either a high-power or low-power social status. Not only did people differ in how powerful they felt, they also differed in their cognitive functioning, processing information more quickly when it was consistent with the induced status (low or high power). However, it is also known that experimenter expectations may alter participants' behaviour. As many studies on social priming have not been conducted as double-blind experiments (where neither experimenters nor participants are aware of the actual experimental conditions), it may be the case that some of the results were due to experimenter expectations. To investigate this, @gilder2018role conducted an experiment in which they systematically primed social status, as well as experimenters' beliefs about which prime was used for which participant.

A total of $n = 400$ students participated in their experiment. They first performed a priming task in which they were either assigned a high-power ("boss") or low-power ("employee") role. Independent of the actual condition, the experimenter (one of four research assistants) was made to believe that half of the participants in each condition were in the other condition (e.g., that people in the high-power condition were in the low-power condition). After the priming task, participants performed a lexical decision task, in which they as quickly as possible had to indicate whether a presented letter string was a word or non-word. Their response was made by pressing a key to move a stick figure closer (approach) or further away (avoid) from the word. Earlier research found that participants primed with high power were quicker to approach than avoid stimuli, while the reverse was true for those primed with low power. The dependent variable was therefore an "approach advantage", calculated $\texttt{ApproachAdvantage}_i = \overline{\texttt{RT}}_{\text{avoid},i} - \overline{\texttt{RT}}_{\text{approach},i}$).

According to the "social priming hypothesis", people will be faster to approach than avoid when they feel they have more power (and be faster to avoid than approach when they feel they have low power). If this hypothesis is true, then the $\texttt{ApproachAdvantage}$ measure would be positive on average in the high-power condition, and negative on average in the low-power condition. Alternatively, according to the "experimenter belief hypothesis", it is the experimenter belief about the condition, and not the actual condition, that drives any effects. If this hypothesis is true, then the $\texttt{ApproachAdvantage}$ measure would be positive whenever the experimenter believes a participant is in the high-power condition, and negative when the experimenter believes a participant is in the low-power condition. Figure \@ref(fig:expBelief-raincloud) shows the data for the four conditions (the four possible combinations of prime and experimenter belief). The plot looks like the results are more consistent with the experimenter-belief hypothesis: the averages of $\texttt{ApproachAdvantage}$ in the two "experimenter low" conditions are lower than in the two "experimenter high" conditions. 

```{r expBelief-raincloud, fig.cap="Approach advantage scores in the four conditions of Experiment 5 of @gilder2018role; PL = Low-power prime, PH = high-power prime, EL = experimenter believes condition was PL, EH = experimenter believes condition was PH"}
library(dplyr)
library(stringr)
dat <- expBelief %>% mutate(primeCond = str_replace_all(primeCond, c("LPP" = "PL", "HPP" = "PH"))) %>% mutate(experimenterBelief = str_replace_all(experimenterBelief, c("H" = "EH", "L" = "EL"))) %>% mutate(condition = interaction(primeCond, experimenterBelief, sep="-")) %>% mutate(condition = factor(condition, levels=c("PL-EL", "PL-EH", "PH-EL", "PH-EH")))
plot_raincloud(dat, ApproachAdvantage, groups=condition)
```

### A oneway ANOVA

Let's analyze the data with the tools we already have. There are in total four conditions in the experiment, so we can treat `condition` as a nominal independent variable with four levels. We then need to use three contrast codes. We are free to choose these in any way we like, as long as the model is able to predict `ApproachAdvantage` in each condition as the sample average in that condition. What would be the most interesting comparisons to make for this study? Two contrasts of interest follow directly from the two main hypotheses. To test the "social priming hypothesis", it is of interest to compare the conditions in which participants received a low-power prime to those in which they received a high-power prime. More specifically, if the social-priming hypothesis is false and the power-primes have _no effect_ on the speediness of approach and avoid responses, then the average of the low-power prime conditions would be equal to the average of the high-power prime conditions:
$$\frac{\mu_\text{PL,EL} + \mu_\text{PL,EH}}{2} = \frac{\mu_\text{PH,EL} + \mu_\text{PH,EH}}{2}$$
If the social-priming hypothesis is true, on the other hand, we would expect the approach advantage scores to be higher in the high-power prime conditions than in the low-power prime conditions, i.e.:
$$\frac{\mu_\text{PH,EL} + \mu_\text{PH,EH}}{2} - \frac{\mu_\text{PL,EL} + \mu_\text{PL,EH}}{2} > 0$$
The suggested contrast code is then $c_1 = (-\tfrac{1}{2}, -\tfrac{1}{2}, \tfrac{1}{2}, \tfrac{1}{2})$ for the PL-EL, PL-EH, PH-EL, and PH-EH conditions, respectively. 

To test the experimenter-belief hypothesis, a similar comparison would be made between the "experimenter believes low" and "experimenter believes high" conditions. More specifically, if the experimenter-belief hypothesis is false and the experimenter beliefs have _no effect_ on the speediness of approach and avoid responses, then the average of the experimenter-believes-low conditions would be equal to the average of the experimenter-believes-high conditions:
$$\frac{\mu_\text{PL,EL} + \mu_\text{PH,EL}}{2} = \frac{\mu_\text{PL,EH} + \mu_\text{PH,EH}}{2}$$
If the experimenter-belief hypothesis is true, on the other hand, we would expect the approach advantage scores to be higher in the experimenter-believes-high conditions than in the experimenter-believes-low conditions, i.e.:
$$\frac{\mu_\text{PL,EH} + \mu_\text{PH,EH}}{2} - \frac{\mu_\text{PL,EL} + \mu_\text{PH,EL}}{2} > 0$$
The suggested contrast code is then $c_2 = (-\tfrac{1}{2}, \tfrac{1}{2}, -\tfrac{1}{2}, \tfrac{1}{2})$ for the PL-EL, PL-EH, PH-EL, and PH-EH conditions, respectively. 

It is straightforward to check that these two contrasts are orthogonal.^[$\sum_{k=1}^g c_{1,k} = 0$, $\sum_{k=1}^g c_{2,k} = 0$, and $\sum_{k=1}^g c_{1,k} \times c_{2,k} = \tfrac{1}{4} - \tfrac{1}{4} - \tfrac{1}{4} + \tfrac{1}{4} = 0$.] If you don't have any burning other question to ask about differences between the conditions, it then makes sense to look for a third contrast which completes the set of orthogonal contrasts (i.e. we should look for a contrast $c_3$ that is orthogonal to $c_1$ and $c_2$). One such contrast is $c_3 = (\tfrac{1}{2}, -\tfrac{1}{2}, -\tfrac{1}{2}, \tfrac{1}{2})$. This contrast provides a test of the following equivalence:
$$\frac{\mu_\text{PL,EL} + \mu_\text{PH,EH}}{2} = \frac{\mu_\text{PL,EH} + \mu_\text{PH,EL}}{2}$$
Our full set of three (orthogonal) contrasts is thus the following:

```{r}
tab <- data.frame(c1 = c("$-\\tfrac{1}{2}$","$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$","$\\tfrac{1}{2}$"),
                  c2 = c("$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$","$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$"),
                  c3 = c("$\\tfrac{1}{2}$","$-\\tfrac{1}{2}$","$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$"))
colnames(tab) <- c("$c_1$","$c_2$","$c_3$")
rownames(tab) <- c("PL,EL","PL,EH", "PH,EL", "PH,EH")
knitr::kable(tab, escape=FALSE,align=rep('r', 3), booktabs=TRUE, linesep="")#, caption="A set of three orthogonal contrast codes for the Experimenter Belief study.")
means <- dat %>% group_by(condition) %>% summarise(mean = mean(ApproachAdvantage)) %>% select(mean)
codes <- cbind(c(-1/2,-1/2,1/2,1/2),
               c(-1/2,1/2,-1/2,1/2),
               c(1/2,-1/2,-1/2,1/2))
# means <- means$mean
```

Estimating a linear regression model with the corresponding contrast-coded predictors gives the results provided in Table \@ref(tab:expBelief-oneway-ANOVA-results).
```{r expBelief-oneway-ANOVA-results}
contrasts(dat$condition) <- codes
modg <- lm(ApproachAdvantage ~ condition, data=dat)
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modg)[1],NA,coefficients(modg)[2:4],Error=NA),rbind(car::Anova(modg, type=3)[1:2,],expand_Anova(modg, type=3)[-1,]))
rownames(tab) <- c("Intercept","Condition","$\\quad X_1$","$\\quad X_2$", "$\\quad X_3$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{ApproachAdvantage}$ by three orthogonal contrast-coded predictors.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```
The estimate of the intercept is positive and differs significantly from 0. Remember that in a model with sum-to-zero contrasts, the intercept reflects the grand mean (average of group means). Thus, we can conclude that, on average, participants were quicker in making approach than in making avoid responses. The scale of the dependent variable ($\texttt{ApproachAdvantage}$) is in milliseconds, so approach responses were on average `r format(coefficients(modg)[1],digits=3)` milliseconds faster than avoid responses. The omnibus test of Condition is significant, indicating that the mean of at least one condition differs from that of another. Each individual contrast represents the difference between one combination of two conditions and another combination of two conditions. Only the effect of $X_2$ (the predictor corresponding to contrast $c_2$) is significant. This indicates that when the experimenter believed a participant was in the high-power condition, the average `ApproachAdvantage` score was `r format(coefficients(modg)[3],digits=3)` milliseconds larger than when the experimenter believed a participant was in the low-power condition. Interestingly, the effect of $X_1$, the contrast-coded predictor corresponding to $c_1$ is _not_ significant. Hence, we can _not_ reject the null hypothesis that the difference in approach advantage between the high-power and low-power primes is 0. We have thus not found evidence for the social-priming hypothesis. The results are in line however with the experimenter-belief hypothesis.

## Factorial designs

In the above, we have pretty much conducted the analysis that answered the interesting questions for the study. In doing so, we treated the study consisting of four conditions, without any additional structure. But considering the design of the study, you might realise that the conditions consisted of two manipulations: the prime given to participants (a low-power or high-power prime) and the belief given to the experimenters (whether the participants was supposedly given the low-power or high-power prime). The conditions in the experiment consisted of all four ($2 \times 2$) possible combinations of these two manipulations (PL-EL, PL-EH, PH-EL, and PH-EH). An experimental design with all combinations of different manipulations is called a factorial design.

### Main effects and interactions {#sec:06-main-effects-and-interactions}

When a study has a factorial design, we can think of the _treatment effects_ in a slightly different manner than when treating the conditions as part of a oneway design. In our oneway contrast coding scheme above, we have actually already done this: in the first contrast ($c_1$), we chose to group the two "PL" conditions together, and the "PH" conditions together. Similarly, we chose to group the "EL" conditions together, and the "EH" conditions together, in the second contrast ($c_2$). By doing so, we have considered what are called the __main effects__ of the two factors (priming, and experimenter belief). The main effect of the priming manipulation reflects the effect of giving participants a low-power prime compared to a high-power prime, averaging over the levels of experimenter belief. The main effect of experimenter belief reflects the effect of making the experimenter believe participants were given a low-power prime compared to a high-power prime, averaging over the levels of the power prime. These means, where we average over all levels of other factors, are also called __marginal means__. For example, the marginal mean of the low-power prime conditions, which can be denoted as $\mu_{\text{PL},\cdot}$ (the $\cdot$ symbol reflects that we are averaging over the levels of the second factor), is simply $\mu_{\text{PL},\cdot} = \frac{\mu_{\text{PL}, \text{EL}} + \mu_{\text{PL},\text{EH}}}{2}$. The contrast $c_1$ reflects the _difference_ between these marginal means: $\mu_{\text{PH},\cdot} - \mu_{\text{PL},\cdot}$. Similarly, the second contrast, which encodes the main effect of Belief, is the difference between the marginal means $\mu_{\cdot,\text{EH}} - \mu_{\cdot,\text{EL}}$. 

Previously, we defined the remaining contrast code ($c_3$) as that code which would complete the set of orthogonal contrast codes. When treating the design as a factorial, rather than oneway design, we can take a different approach. The two contrast codes $c_1$ and $c_2$, and their corresponding predictors ($X_1$ and $X_2$), reflect independent effects of the power prime and experimenter belief manipulations, respectively. If we had a model with only these two predictors, we would assume the slope of each predictor is the same, no matter the value of the other predictor. That implies that the effect of the power prime is assumed to be the the same, no matter what the belief of the experimenter is. Similarly, the effect of the experimenter belief is assumed the same, no matter which power prime was presented. That is quite a strong assumption. What if the effect of the power prime depends on the belief of the experimenter, and what if the effect of the experimenter belief depends on the power prime? This implies that the effect of power prime is _moderated_ by experimenter belief (and vice versa). 

It is important to realise that, once we have constructed contrast-coding predictors, we can treat the model as any other multiple regression model. Hence, we can investigate whether effects are moderated by adding a product predictor to our model, i.e. $(X_1 \times X_2)_i$, just like we would in a multiple regression model. Such a product predictor is exactly the same as defining a contrast code $c_3' = c_1 \times c_2$! So, to investigate whether the effect of the power prime is moderated by experimenter belief, we should use a contrast code $c_3' = c_1 \times c_2 = (\tfrac{1}{4},- \tfrac{1}{4}, -\tfrac{1}{4}, \tfrac{1}{4})$. Note that this is, apart from scaling, the same contrast code as we used before, i.e. $c_3' = \tfrac{1}{2} \times c_{3}$. And, while scaling affects the value of the slope, it does not change the underlying relation between a predictor and the dependent variable. So the reduction in the Sum of Squared Error and the resulting null-hypothesis test, are exactly the same for a model based on $c_3$ and a model based on $c_3'$. 

Before showing you that this is actually the case, let's consider what we might expect from the slope of $X_3'$ (the predictor corresponding to $c_3'$). Because $c_3' = \tfrac{1}{2} \times c_{3}$, every one-unit increase in $c_3$ corresponds to a _half-unit_ increase in $c_3'$. Conversely, that implies that every one-unit increase in $c_3'$ corresponds to a _two-unit_ increase in $c_3$. So, what do you think the relation between the slope of $X_3$ and $X_{3}'$ would be?^[Remember that the slope of a predictor reflects the increase in the dependent variable for every one-unit increase in that predictor. Every one-unit increase in $X_3$ corresponds to a half-unit increase in $X_3'$. That implies that a one-unit increase in $X_3'$ is the same as a two-unit increase in $X_3$, so $\beta'_{3} = 2 \times \beta_3$.]

The results of estimating a model with this alternative version of the third contrast code (keeping the others the same) is given in Table \@ref(tab:expBelief-factorial-ANOVA-results). In the table, I have given the predictors more informative labels: $X_1$ becomes $\texttt{P}$ (for the main effect of Prime), $X_2$ becomes $\texttt{B}$ (for the main effect of experimenter Belief), and $X_3'$ becomes $\texttt{P} \times \texttt{B}$ (for the interaction between Prime and Belief). I have also omitted the omnibus test for Condition. In a factorial ANOVA, main effects and interaction effects can be omnibus tests themselves (we will see an example of this later). Besides these stylistic changes, the only real difference in these new results is the estimate of the slope of $\texttt{P} \times \texttt{B}$, which is twice the value of the corresponding slope of $X_3$ in Table \@ref(tab:expBelief-oneway-ANOVA-results). 
```{r expBelief-factorial-ANOVA-results}
codes <- cbind(c(-1/2,-1/2,1/2,1/2),
               c(-1/2,1/2,-1/2,1/2),
               c(1/4,-1/4,-1/4,1/4))
contrasts(dat$condition) <- codes
modg <- lm(ApproachAdvantage ~ condition, data=dat)
opts <- options()
options(knitr.kable.NA = "")
# tab <- cbind(estimate = c(coefficients(modg)[1],NA,coefficients(modg)[2:4],Error=NA),rbind(car::Anova(modg, type=3)[1:2,],expand_Anova(modg, type=3)[-1,]))
# rownames(tab) <- c("Intercept","Condition","$\\quad X_1$","$\\quad X_2$", "$\\quad X_3$", "Error")
# colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
tab <- cbind(estimate = c(coefficients(modg)[1],coefficients(modg)[2:4],Error=NA),rbind(car::Anova(modg, type=3)[1,],expand_Anova(modg, type=3)[-1,]))
rownames(tab) <- c("Intercept","$\\texttt{P}$","$\\texttt{B}$", "$\\texttt{P} \\times \\texttt{B}$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{ApproachAdvantage}$ by factorial contrast-coded predictors.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

At this point, you might wonder what is special about factorial designs and the way they are implemented in linear models. As the previous discussion indicates, in some sense, factorial designs are not special at all. If you have one experimental manipulation $A$ (e.g. power priming) with a total of $g_A$ levels $a_1, \ldots, a_{g_A}$, and another manipulation $B$ (e.g. experimenter belief) with a total of $g_B$ levels $b_1, \ldots, b_{L_B}$, and the experiment crosses all these levels as $(a_1 \text{ and } b_1), (a_1 \text{ and } b_2), \ldots, (a_2 \text{ and } b_1), \ldots, (a_{g_A} \text{ and } b_{g_B})$, then in the end, you will end up with an experiment that has a total of $g = g_A \times g_B$ conditions. It is up to you how you treat these conditions. There is nothing inherently wrong with ignoring the factorial nature of a design, and analysing it as a oneway design. Key is to come up with contrasts that test interesting hypotheses. Often, these contrasts will involve comparisons between levels of one manipulation, whilst averaging over the levels of another manipulation. This then naturally results in treating the design as factorial.

## The factorial ANOVA model

An alternative, more traditional way of specifying a factorial ANOVA is in terms of a grand mean and treatment effects. This is analogous to the oneway ANOVA model of Equation \@ref(eq:glm-ANOVA-model). Let's consider a case with two experimental factors, $A$ and $B$, and let $Y_{i,j,k}$ denote an observation for person $i$ at level $j$ of the first factor ($A$) and level $k$ of the second factor ($B$). We can state the factorial ANOVA model as
\begin{equation}
Y_{i,j,k} = \mu + \tau^{(A)}_j + \tau^{(B)}_k + \tau^{(A \times B)}_{j,k} + \epsilon_{i,j,k} \quad \quad \epsilon_{j,i} \sim \textbf{Normal}(0, \sigma_\epsilon)
(\#eq:glm-factorial-ANOVA-model)
\end{equation}
Here, as usual, $\mu$ denotes the grand mean, which is the average of all means. $\tau^{(A)}_j$ is the *treatment effect* of level $j$ of factor $A$:
$$\tau^{(A)}_j = \mu_{j,\cdot} - \mu$$
which is the difference between marginal mean $\mu_{j,\cdot}$ and the grand mean $\mu$. Similarly, $\tau^{(B)}_k$ the treatment effect of level $k$ of factor $B$:
$$\tau^{(B)}_k = \mu_{\cdot,k} - \mu$$
which is the difference between marginal mean $\mu_{\cdot,k}$ and the grand mean $\mu$. Finally, $\tau{(A \times B)}_{j,k}$ is the interaction effect:
$$\tau^{(A \times B)}_{j,k} = \mu_{j,k} - (\mu + \tau^{(A)}_j + \tau^{(B)}_k)$$
i.e. the difference between the true mean $\mu_{j,k}$ at level $j$ of factor $A$ and level $k$ of factor $B$, and the predicted mean by adding treatment effects $\tau^{(A)}_j$ and $\tau^{(B)}_k$, which is $\mu + \tau^{(A)}_j + \tau^{(B)}_k$.

## A threeway factorial ANOVA

One benefit of treating a factorial design as a factorial design, is that you only need to worry about defining contrast codes for the main effects of the different manipulations. If you choose orthogonal contrasts for these, then the remaining contrasts for the full factorial design are simple to work out. They are interactions and computed as pairwise products of contrasts.

To see how this works in a more complex situation, let's investigate whether the identity of the experimenter might also play a role in determining the approach advantage scores. In the study, there were four experimenters (the research assistants). Let's treat the identity of the experimenter as another factor in the design. A plot of the data, separated by experimenter, is provided in Figure \@ref(fig:expBelief-threeway-raincloud). As you can see there, while the first three experimenters show the same overall pattern as in Figure \@ref(fig:expBelief-raincloud), with higher scores for the "EH" conditions than the "EL" conditions, this is not so obvious for Experimenter 4. 

```{r expBelief-threeway-raincloud, fig.cap="Approach advantage scores separated by condition and experimenter", out.width="100%"}
library(ggplot2)
dat %>% mutate(exptrNum = factor(exptrNum, labels=paste("Experimenter",1:4))) %>%
plot_raincloud(ApproachAdvantage, groups=condition) + facet_wrap(~exptrNum)
```

Treating experimenter as another factor, we now have a 2 (Prime: low power vs high power) by 2 (experimenter Belief: low-power prime vs high-power prime) by 4 (Experimenter: 1, 2, 3, or 4) factorial design, with a total of $2 \times 2 \times 4 = 16$ conditions. We thus need a total of $g-1 = 16 - 1 = 15$ contrast codes. That is a lot! We will start by defining suitable contrast codes for the main effect of each of the three factors (manipulations). We have already done this for the first two factors. For prime-condition, we can assign a value of $-\frac{1}{2}$ to all conditions with a lower-power prime, and a value of $\tfrac{1}{2}$ to all conditions with a high-power prime. Thus, using a more informative label than $c_1$, the contrast code for Prime condition is $\texttt{P} = (-\tfrac{1}{2},\tfrac{1}{2})$. Similarly, to code for experimenter belief, we can assign a value of $-\tfrac{1}{2}$ to all conditions where the experimenter believed participants were in the low-power prime condition, and a value of $\tfrac{1}{2}$ for the conditions where the experimenter believed participants were assigned a high-power prime, i.e. $\texttt{B} = (-\tfrac{1}{2},\tfrac{1}{2})$. Experimenter is a factor with four levels, and hence we need three contrast codes. As I don't have a particular a priori hypothesis about which experimenters might differ from others, we can use one of the default orthogonal contrast codes. For instance, we can use a Helmert coding scheme, with contrasts $\texttt{E}_1 = (-\tfrac{1}{2}, \tfrac{1}{2}, 0, 0)$, $\texttt{E}_2 = (-\tfrac{1}{3}, -\tfrac{1}{3}, \tfrac{2}{3}, 0)$, and $\texttt{E}_3 = (-\tfrac{1}{4}, -\tfrac{1}{4}, -\tfrac{1}{4}, \tfrac{3}{4})$. We have now defined $1 + 1 + 3 = 5$ contrast codes. The remaining 10 contrasts are easy to determine. First, we will construct codes for the moderation of $\texttt{P}$ by $\texttt{B}$, by computing a product contrast (multiplying the values of $\texttt{P}$ and $\texttt{B}$ for all 16 conditions). We would do the same for the moderation of the priming effect by experimenter, which consists of the pairwise products between $\texttt{P}$ and $\texttt{E}_1$, $\texttt{P}$ and $\texttt{E}_2$, $\texttt{P}$ and $\texttt{E}_3$. The moderation of the effect of experimenter belief by experimenter consists of the pairwise products of $\texttt{B}$ and $\texttt{E}_1$, $\texttt{B}$ and $\texttt{E}_2$, and $\texttt{B}$ and $\texttt{E}_3$. We now have in total 7 contrast codes, each of which reflects a pairwise interaction. Great! The main effects and pairwise interactions provide us with $5+7 = 12$ of the required 15 contrast codes. The final three can be constructed as __threeway interactions__, i.e. as the products of three contrasts: $\texttt{P} \times \texttt{B} \times \texttt{E}_1$, $\texttt{P} \times \texttt{B} \times \texttt{E}_2$, and $\texttt{P} \times \texttt{B} \times \texttt{E}_3$. You can view these threeway interactions as a form of "moderated moderations". For example, does the moderation of the effect of power prime by experimenter belief depend on the identity of the experimenter? We will come back to the interpretation of this shortly.

The full set of 15 contrast codes, with values for all 16 conditions, is given in the (rather large) Table \@ref(tab:expBelief-threeway-ANOVA-codes).
```{r expBelief-threeway-ANOVA-codes}
tab <- data.frame(c1 = rep(c("$-\\tfrac{1}{2}$","$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$","$\\tfrac{1}{2}$"),each=4),
                  c2 = rep(c("$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$","$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$"),each=4),
                  c3 = c("$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$","$0$","$0$"),
                  c4 = c("$-\\tfrac{1}{3}$","$-\\tfrac{1}{3}$","$\\tfrac{2}{3}$","$0$"),
                  c5 = c("$-\\tfrac{1}{4}$","$-\\tfrac{1}{4}$","$-\\tfrac{1}{4}$","$\\tfrac{3}{4}$")
                  )
tab$c6 <- sapply(paste(tab$c1,tab$c2, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)
tab$c7 <- sapply(paste(tab$c1,tab$c3, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)
tab$c8 <- sapply(paste(tab$c1,tab$c4, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)
tab$c9 <- sapply(paste(tab$c1,tab$c5, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)
tab$c10 <- sapply(paste(tab$c2,tab$c3, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)
tab$c11 <- sapply(paste(tab$c2,tab$c4, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)
tab$c12 <- sapply(paste(tab$c2,tab$c5, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)
tab$c13 <- sapply(paste(tab$c1,tab$c10, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)
tab$c14 <- sapply(paste(tab$c1,tab$c11, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)
tab$c15 <- sapply(paste(tab$c1,tab$c12, sep="$\\times$"),gsub,pattern="$$\\times$$",replacement="\\times", fixed=TRUE)

colnames(tab) <- c("$\\texttt{P}$","$\\texttt{B}$","$\\texttt{E}_1$", "$\\texttt{E}_2$", "$\\texttt{E}_3$", "$\\texttt{P} \\times \\texttt{B}$", "$\\texttt{P} \\times \\texttt{E}_1$", "$\\texttt{P} \\times \\texttt{E}_2$", "$\\texttt{P} \\times \\texttt{E}_3$", "$\\texttt{B} \\times \\texttt{E}_1$", "$\\texttt{B} \\times \\texttt{E}_2$", "$\\texttt{B} \\times \\texttt{E}_3$", "$\\texttt{P} \\times \\texttt{B} \\times \\texttt{E}_1$", "$\\texttt{P} \\times \\texttt{B} \\times \\texttt{E}_2$", "$\\texttt{P} \\times \\texttt{B} \\times \\texttt{E}_3$")
rownames(tab) <- paste(rep(c("PL,EL","PL,EH", "PH,EL", "PH,EH"),each=4), c("E1","E2","E3","E4"), sep=",")
library(kableExtra)
knitr::kable(tab, escape=FALSE,align=rep('r', 3), caption="A set of 15 orthogonal contrast codes for the Experimenter Belief study.", booktabs=TRUE, longtable = TRUE, linesep="") %>% kable_styling() %>%
  scroll_box(width = "100%", box_css = "border: 0px;")

# means <- means$mean
```

Estimating the corresponding linear model with 15 predictors gives the results in Table \@ref(tab:expBelief-threeway-ANOVA-results). 

```{r expBelief-threeway-ANOVA-results}
dat$exptrNum <- factor(dat$exptrNum)
contrasts(dat$exptrNum) <- cbind(c(-.5,.5,0,0),c(-1/3, -1/3, 2/3, 0), c(-1/4,-1/4,-1/4,3/4))
dat$primeCond <- factor(dat$primeCond, levels=c("PL","PH"))
contrasts(dat$primeCond) <- c(-.5,.5)
dat$experimenterBelief <- factor(dat$experimenterBelief,levels=c("EL","EH"))
contrasts(dat$primeCond) <- c(-.5,.5)
modg <- lm(ApproachAdvantage ~ primeCond*experimenterBelief*exptrNum, data=dat)
opts <- options()
options(knitr.kable.NA = "")
# tab <- cbind(estimate = c(coefficients(modg)[1],NA,coefficients(modg)[2:4],Error=NA),rbind(car::Anova(modg, type=3)[1:2,],expand_Anova(modg, type=3)[-1,]))
# rownames(tab) <- c("Intercept","Condition","$\\quad X_1$","$\\quad X_2$", "$\\quad X_3$", "Error")
# colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
caov <- car::Anova(modg, type=3)
eaov <- expand_Anova(modg, type=3)
tab <- cbind(estimate = c(coefficients(modg)[1:3],NA,coefficients(modg)[4:7],NA, coefficients(modg)[8:10],NA,coefficients(modg)[11:13],NA,coefficients(modg)[14:16],Error=NA),rbind(caov[1:4,],eaov[4:6,],caov[5:6,],eaov[8:10,],caov[7,],eaov[11:13,],caov[8,],eaov[14:17,]))
rownames(tab) <- c("Intercept","Prime ($\\texttt{P}$)","Belief ($\\texttt{B}$)", "Experimenter", "$\\quad \\texttt{E}_1$", "$\\quad \\texttt{E}_2$", "$\\quad \\texttt{E}_3$", "Prime $\\times$ Belief", "Prime $\\times$ Experimenter", "$\\quad \\texttt{P} \\times \\texttt{E}_1$", "$\\quad \\texttt{P} \\times \\texttt{E}_2$", "$\\quad \\texttt{P} \\times \\texttt{E}_3$",  "Belief $\\times$ Experimenter", "$\\quad \\texttt{B} \\times \\texttt{E}_1$", "$\\quad \\texttt{B} \\times \\texttt{E}_2$", "$\\quad \\texttt{B} \\times \\texttt{E}_3$", "Prime $\\times$ Belief $\\times$ Experimenter", "$\\quad \\texttt{P} \\times \\texttt{B} \\times \\texttt{E}_1$", "$\\quad \\texttt{P} \\times \\texttt{B} \\times \\texttt{E}_2$", "$\\quad \\texttt{P} \\times \\texttt{B} \\times \\texttt{E}_3$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{ApproachAdvantage}$ by factorial contrast-coded predictors.", escape = FALSE, digits=c(3,8,3,1,3,3), booktabs=TRUE, linesep="")
options(opts)
```

I admit the number of tested effects is large and the results mind-boggling at first. First, note that the main effect of "Experimenter" is entered in the table as an omnibus test, as well as the test for the individual contrasts $(\texttt{E}_1$, $\texttt{E}_2$, and $\texttt{E}_3)$. In a conventional ANOVA table, you would only see the omnibus test. But as the individual contrasts are informative, I like to include the tests for each individual contrast as well. The omnibus test listed under "Experimenter" is a test that the slope of $\texttt{E}_1$, $\texttt{E}_2$, and $\texttt{E}_3$ are _all_ equal to 0. This is based on a model comparison between a MODEL G which includes all 16 parameters (intercept and the slopes of all 15 predictors) and a MODEL R which fixes the slopes of $\texttt{E}_1$, $\texttt{E}_2$, and $\texttt{E}_3$ to 0. Note that this MODEL R still includes the slopes of the various product-predictors such as $\texttt{P} \times \texttt{E}_1$. The omnibus test is significant, which indicates that, aggregating over the levels of Prime and Belief, at least one of the experimenters differs from one other one. The tests of the individual contrasts show a significant effect of $\texttt{E}_3$, which compares experimenter 4 to the average of experimenters 1, 2, and 3. The estimated slope is positive, which indicates that the approach advantage scores are generally higher for this experimenter than for the other ones. When you inspect Figure \@ref(fig:expBelief-threeway-raincloud), you can see this: whilst for all other experimenters, average scores in the "EL" conditions are negative and positive in the "EH" conditions, the averages for Experimenter 4 are always positive, regardless of the level of Belief or Prime. 

### Interpreting interactions

Aggregating over experimenters (i.e., looking at the main effect of Belief), we still obtain a significant effect of experimenter Belief, as we did before. However, the significant effect of the $\texttt{B} \times \texttt{E}_3$ contrast indicates that the effect of experimenter belief is different for Experimenter 4 compared to the other three experimenters. Let's consider this more carefully. The slope of $\texttt{B}$ is estimated as $\hat{\beta}_\texttt{B} = `r format(coefficients(modg)[3],digits=3)`$. This simple slope represents the difference between the "experimenter believes high prime" and "experimenter believes low prime" conditions, when all other predictors which moderate this effect equal 0. When using orthogonal "sum-to-zero" contrasts, the simple slope can be thought of as reflecting the average effect of belief over all levels of the other factors in the design (prime condition, and experimenter). So, aggregating over all other conditions, the approach advantage score is `r format(coefficients(modg)[3],digits=3)` milliseconds larger when the experimenter believed a participant was given a high-power prime compared to when the experimenter believed the participant received a low-power prime. 

As an example of how to interpret an interaction, we will focus on the significant $\texttt{B} \times \texttt{E}_3$ contrast. Using the slope of this interaction, we can work out the predicted slope of $\texttt{B}$ for Experimenter 4 as $$\hat{\beta}_{\texttt{B}|\text{Experimenter 4}} = `r format(coefficients(modg)[3],digits=3)` + \tfrac{3}{4} \times (`r format(coefficients(modg)[13],digits=3)`) = `r format(coefficients(modg)[3] + .75*coefficients(modg)[13],digits=3)`$$ As the scale of the dependent variable is in milliseconds, this seems a rather negligible effect. For the other three experimenters, on average, the predicted slope is $$\hat{\beta}_{\texttt{B}|\text{Experimenter 1, 2, or 3}} = `r format(coefficients(modg)[3],digits=3)` + (- \tfrac{1}{4})\times (`r format(coefficients(modg)[13],digits=3)`) = `r format(coefficients(modg)[3] + -.25*coefficients(modg)[13],digits=3)`$$ which is more substantial. Apparently, Experimenter 4 was "immune" to the experimenter belief manipulation, whilst the other three experimenters were not.

Note that the omnibus test of the interaction between Belief and Experimenter is not significant, while the test of the $\texttt{B} \times \texttt{E}_3$ is. It is not uncommon for an individual contrast to be significant while an omnibus test is not. When only a single contrast has a sizeable effect (i.e. it provides a substantial reduction in the Sum of Squared Error), the omnibus test effectively divides the RSS attributable to that contrast over all contrasts that are part of the omnibus test. The omnibus test then has less power than a test of the individual contrast. 

None of the other effects are significant. If the threeway interaction were significant, we'd have the tricky task to interpret this. Interpreting threeway interactions is not impossible, but it does require effort. For example, imagine that the $\texttt{P} \times \texttt{B} \times \texttt{E}_3$ interaction was in fact significant. One way to interpret this is as a moderation of a moderation. We have already interpreted the $\texttt{B} \times \texttt{E}_3$ interaction as indicating that the effect of Belief is reduced for Experimenter 4 compared to the other experimenters. The $\texttt{P} \times \texttt{B} \times \texttt{E}_3$ is estimated to be positive. That indicates that this reduction in the effect of Belief is smaller for those participants who received a high-power prime, as compared to a low-power prime. To see this, we can use the same method to determine conditional slopes we have used before, but now looking at the conditional slope of the $\texttt{B} \times \texttt{E}_3$ interaction, for the different levels of Prime. For the high-power prime conditions, we can determine this interaction slope as $$\hat{\beta}_{\texttt{B} \times \texttt{E}_3|\text{high-power prime}} = `r format(coefficients(modg)[13],digits=3)` + \tfrac{1}{2} \times (`r format(coefficients(modg)[16],digits=3)`) = `r format(coefficients(modg)[13] + .5*coefficients(modg)[16],digits=3)`$$ For the low-power prime conditions, the slope is $$\hat{\beta}_{\texttt{B} \times \texttt{E}_3|\text{low-power prime}} = `r format(coefficients(modg)[13],digits=3)` - \tfrac{1}{2} \times (`r format(coefficients(modg)[16],digits=3)`) = `r format(coefficients(modg)[13] - .5*coefficients(modg)[16],digits=3)`$$
We can thus conclude that the moderation of the effect of belief by experimenter (4 vs 1, 2, or 3) is larger for those participants who received a low-power prime, as compared to those who received a high-power prime. 

We can use these conditional interaction slopes in the same way as before to work out the effect of belief for Experimenter 4 and participants who received a high-power prime:
$$\hat{\beta}_{\texttt{B}|\text{Experimenter 4 and high-power prime}} = `r format(coefficients(modg)[3],digits=3)` + \tfrac{3}{4} \times (`r format(coefficients(modg)[13] + .5*coefficients(modg)[16],digits=3)`) = `r format(coefficients(modg)[3] + .75*(coefficients(modg)[13] + .5*coefficients(modg)[16]),digits=3)`$$
Similarly, the effect of belief for Experimenter 4 and participants who received a low-power prime is:
$$\hat{\beta}_{\texttt{B}|\text{Experimenter 4 and low-power prime}} = `r format(coefficients(modg)[3],digits=3)` + \tfrac{3}{4} \times (`r format(coefficients(modg)[13] - .5*coefficients(modg)[16],digits=3)`) = `r format(coefficients(modg)[3] + .75*(coefficients(modg)[13] - .5*coefficients(modg)[16]),digits=3)`$$

## Orthogonal contrast codes and unequal sample sizes

Using orthogonal contrast codes in factorial designs generally leads to interpretable parameters. While interaction effects can be difficult to interpret initially, with practice, you will become better at this. Another benefit is that, if the conditions have equal sample sizes, the contrast-coded predictors will be independent. But when the sample sizes are unequal, this independence between contrast-coded predictors does not hold, even if the contrast codes are orthogonal. 

To keep the following discussion relatively straightforward, let's go back to the two-way factorial ANOVA where we look at the effect of prime condition ($\texttt{P}$) and experimenter belief ($\texttt{B}$). To analyse this, we used a linear model
$$\text{MODEL G:} \quad \texttt{ApproachAdvantage}_i = \beta_0 + \beta_\texttt{P} \times \texttt{P}_i + \beta_\texttt{B} \times \texttt{B}_i + \beta_{\texttt{P} \times \texttt{B}} \times (\texttt{P} \times \texttt{B})_i + \epsilon_i$$
When all four conditions have equal sample sizes, i.e. $n_{\text{PL},\text{EL}} = n_{\text{PL},\text{EH}} = n_{\text{PH},\text{EL}} = n_{\text{PH},\text{EH}}$, then the three predictors $\texttt{P}$, $\texttt{B}$, and $(\texttt{P} \times \texttt{B})$ are independent. 
In a linear model with independent predictors, the estimated slope of one predictor, say $\texttt{P}$, does not depend on the whether the model includes a second predictor (e.g. $\texttt{B}$) or not. For example, the slope $\beta_\texttt{P}$ would be exactly the same in the model above and the model
$$\text{MODEL R:} \quad \texttt{ApproachAdvantage}_i = \beta_0 + \beta_\texttt{P} \times \texttt{P}_i + \epsilon_i$$
In both models, the estimated slope represents the difference
$$\hat{\beta}_\texttt{P} = \frac{\mu_\text{PH,EL} + \mu_\text{PH,EH}}{2} - \frac{\mu_\text{PL,EL} + \mu_\text{PL,EH}}{2}$$
When the sample sizes are _unequal_, however, this would only be the case for MODEL G above. Because of the resulting dependency between the predictors, the estimate of $\hat{\beta}_\texttt{P}$ in MODEL R will be different. It will still represent a difference between averages, but these would now be _weighted by sample size_.

To make the example dramatic, I have removed participants randomly from each condition, such that the conditions have rather unequal sizes:
```{r}
dat <- expBelief %>% mutate(primeCond = str_replace_all(primeCond, c("LPP" = "PL", "HPP" = "PH"))) %>% mutate(experimenterBelief = str_replace_all(experimenterBelief, c("H" = "EH", "L" = "EL"))) %>% mutate(condition = interaction(primeCond, experimenterBelief, sep="-")) %>% mutate(condition = factor(condition, levels=c("PL-EL", "PL-EH", "PH-EL", "PH-EH")))
set.seed(876234)
ns <- c(20,40,60,80)
tid <- c()
for(i in 1:4) {
  tid <- c(tid,sample(subset(dat,condition == levels(dat$condition)[i])$pid,size=ns[i]))
}
dat <- subset(dat,pid %in% tid)
contrasts(dat$condition) <- cbind(c(-.5,-.5,.5,.5),c(-.5,.5,-.5,.5),c(.5,-.5,-.5,.5))
tab <- aggregate(ApproachAdvantage ~ condition,data=dat,mean)
tab$n <- table(dat$condition)
colnames(tab)[2] <- "mean"
tab$mean <- round(tab$mean,2)
tab <- t(tab[,c(1,3,2)])
rownames(tab) <- c("","$n$","mean")
knitr::kable(tab, escape=FALSE, booktabs=TRUE, linesep="")
modg <- lm(ApproachAdvantage ~ condition, data = dat)
modr <- lm(dat$ApproachAdvantage ~ model.matrix(modg)[,2])
means <- aggregate(ApproachAdvantage ~ condition,data=dat,mean)
```
Estimating MODEL G gives the following estimates
$$`r write_GLM_equation(modg, dv_name = "ApproachAdvantage", iv_names = c("P","B", "}(\\texttt{P}\\times \\texttt{B}){"), digits=4, include_sde = FALSE)`$$
This shows that the estimated slope of $\texttt{P}$ indeed equals
$$\hat{\beta}_\texttt{P} = \frac{`r format(means[3,2],digits=4)` + `r format(means[4,2],digits=4)`}{2} - \frac{`r format(means[1,2],digits=4)` + `r format(means[2,2],digits=4)`}{2} = `r format(sum(means[3:4,2])/2 - sum(means[1:2,2])/2, digits=4)`$$
For MODEL R, the estimate is
$$`r write_GLM_equation(modr, dv_name = "ApproachAdvantage", iv_names = c("P"), digits=4, include_sde = FALSE)`$$
Obviously, this is different from the estimate in MODEL G. While the slope still reflects a difference between the high-power and low-power prime conditions, the averages of these conditions are weighted by the sample size as follows:
$$\begin{aligned}
\hat{\beta}_\texttt{P} &= \frac{n_{\text{PH,EL}} \times \mu_\text{PH,EL} + n_\text{PH,EH} \times \mu_\text{PH,EH}}{n_\text{PH,EL} + n_\text{PH,EH}} - \frac{n_\text{PL,EL} \times \mu_\text{PL,EL} + n_\text{PL,EH} \times \mu_\text{PL-EH}}{n_\text{PL,EL} + n_\text{PL,EH}} \\
&= \frac{60 \times `r format(means[3,2],digits=4)` + 80 \times `r format(means[4,2],digits=4)`}{140} - \frac{20 \times `r format(means[1,2],digits=4)` + 40 \times `r format(means[2,2],digits=4)`}{60} \\
&= `r format(sum(c(60,80)*means[3:4,2])/140 - sum(c(20,40)*means[1:2,2])/60, digits=4)`
\end{aligned}$$

Why is this of importance? We're generally interested in the estimates from MODEL G, and use a restricted MODEL R mainly for the purpose of conducting hypothesis tests. Well, the estimates of MODEL R become important when we consider different ways of conducting model comparisons to perform hypothesis tests.

### Comparison schemes and SS types

When predictors are dependent, they are partially redundant. Going back to our discussion of _multicollinearity_, that means that the predictors in a model together can account for more of the variance of the dependent variable than the sum of their unique contributions. In Figure \@ref(fig:sse-partition-type-SS), as a whole, the model can account for a proportion $B + C + D$, but the sum of the unique contributions is $B + C$. This can result in a relative lack of power for the tests of main effects and interactions.

```{tikz sse-partition-type-SS, fig.cap="Partitioning the variance in a General Linear Model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables. ", out.width="50%"}
\def\firstcircle{(1,1) circle (1.75cm)}
\def\secondcircle{(2,1) circle (1.75cm)}
\def\thirdcircle{(1.5,2) circle (1.75cm)}
\begin{tikzpicture}
	\pgfsetxvec{\pgfpoint{2cm}{0cm}}
	\pgfsetyvec{\pgfpoint{0cm}{2cm}}
	\tikzstyle{every node}=[minimum size=.7cm]

	\fill[white!50] (-1,0) -- (-1,3.5) -- (3.5,3.5) -- (3.5,0) -- (-1,0);

	\draw \firstcircle;
	\draw \secondcircle;
	\draw \thirdcircle;
	
	\draw (0.2,0.2) node {$X_1$};
	\draw (2.8,0.2) node {$X_2$};
	\draw (1.5,3) node {$Y$};
	\draw (1.5,2.3) node {$A$};
	\draw (1.1,1.6) node {$B$};
	\draw (1.5,1.4) node {$D$};
	\draw (1.9,1.6) node {$C$};

\end{tikzpicture}	
```

Up to now, we have performed hypothesis tests by comparing a full MODEL G to a restricted MODEL R where some of the effects in MODEL G are fixed to particular values (generally 0). This procedure is -- in the context of the General Linear Model -- called a __Type 3 Sums of Squares__ procedure. The Sum of Squares attributed to each predictor or set of predictors reflect their _unique_ contributions (i.e., regressions $B$ and $C$ in Figure \@ref(fig:sse-partition-type-SS)). When there is redundancy, these unique contributions do not add up to the total SS that can be attributed to all predictors in the model (i.e. the reduction in the SSE comparing the full model to an intercept-only model). There are two alternative schemes which guarantee that the SS terms do add up to this total SS. These are, as you might have guessed, Type 1 and Type 2 SS procedures.

In the __Type 1 SS__ procedure, also called _sequential SS_, you build up the model sequentially. You start by comparing a model which includes just the contrast-coded predictors for the main effect of one of the factors in the design (for instance $\texttt{P}$ in the simple factorial design) to an intercept only model. The reduction in the SSE (e.g. $B + D$ in Figure \@ref(fig:sse-partition-type-SS)) is the SS assigned to that main effect. You then add all contrast-coded predictors for the main effect of the second factor in the design (e.g. $\texttt{B}$), and compare this more general model to the one of the previous step (e.g. the model with only the predictors for $\texttt{P}$). The reduction in SSE (e.g. $C$ in Figure \@ref(fig:sse-partition-type-SS)) is the SSR assigned to that second main effect. If there are more factors in the design, you would then add the contrast-coded predictors for another factor, and compute an SSR for this factor as the reduction in the SSE compared to the model defined in the step before, etc. Once you have done this for all the main effects, you would then continue this procedure for all interactions, until you arrive at the full model. Note that the model comparisons performed on the way are solely to compute SSR terms. You would not perform hypothesis tests at each step. Rather, each SSR term computed is entered in the usual formula to compute the $F$ statistic, where you'd use the SSE of the full MODEL G:
$$F = \frac{\text{SSR}/(\text{npar}(G) - \text{npar}(R))}{\text{SSE}(G)/(n-\text{npar}(G))}$$
In this computation, all elements are the same as for the usual Type 3 procedure, apart from SSR, which is the one computed with the sequential procedure.

While this procedure has the benefit of ensuring that all the SSR terms add up to the total SSR, it is important to note that the hypotheses tested are not necessarily those that you expect to test. As was shown earlier, the slope of contrast-coded predictor $\texttt{P}$, when the only predictor in the model, reflects the _sample-size weighted_ difference between the means. The omnibus test for the main effect that is entered first is thus a test that all these _sample-size weighted_ means are equal to each other, it is not a test that the _unweighted_ means are all equal to each other. For the second main effect, the test is also one of sample-size weighted comparisons, although the precise weights are a more complex function of the sample sizes. 

If the sample-sizes are reflective of the actual proportions in which you might find the various factor levels in the Data Generating Process, then it might make sense to test such sample-size weighted equality between means. However, unequal sample sizes often do not reflect such meaningful differences in the DGP. In that case, the hypotheses tested with a Type 1 procedure might not be meaningful. Also, the results of a Type 1 SS procedure depend on the _order_ in which you enter the main effects. If you start with the main effect of $\texttt{B}$ rather than $\texttt{P}$, then the SSR of $\texttt{B}$ would have been $C+D$ rather than just $C$ in Figure \@ref(fig:sse-partition-type-SS). There often isn't a clear reason to prefer one order over another. Hence, because of these issues, I generally don't recommend using a Type 1 SS procedure. The reason for mentioning it here is that some statistical software (e.g. base R) use Type 1 SS procedures by default, and it is important to be aware of this.

The __Type 2 SS__ procedure is (even) more complicated, in terms of the hypotheses that are tested. The comparisons involved in computing the SSR terms for the different effects can be described reasonably clearly though. The idea is to determine the SSR associated to an effect whilst controlling for any effects that do not fully contain that effect. For example, let's consider the threeway Prime by Belief by Experimenter ANOVA we conducted earlier. To compute the SSR of main effect of Prime, you would construct a model with the contrast codes corresponding to the main effects of Prime, Belief, and Experimenter, as well as the Belief $\times$ Experimenter interaction, as the latter does not contain Prime. Because the Prime $\times$ Experimenter and Prime $\times$ Belief $\times$ Experimenter interactions do contain Prime, they would not be included. You would then compute the SSR for the main effect of Prime by computing the difference in the SSE of this model and a model that excludes the Prime main effect. Similarly, to compute the SSR for Belief, you would compare a model with the main effects of Prime, Belief, and Experimenter, and Prime $\times$ Experimenter interaction, to a model which excludes the Belief main effect. To compute the SSR for e.g. the Prime $\times$ Belief interaction, you would compare a model which includes the main effects of Prime, Belief, and Experimenter, and the Prime $\times$ Belief, Prime $\times$ Experimenter, and Belief $\times$ Experimenter interactions. All effects included, apart from the Prime $\times$ Belief interaction itself, do not fully contain this interaction (i.e. do not include both Prime and Belief). The Prime $\times$ Belief $\times$ Experimenter interaction is excluded, because it involves both Prime and Belief. As for the Type 1 procedure, these comparisons are solely used to compute the SSR terms for the main effects and interactions. Once these have been obtained, they are used in the usual formula for the $F$ statistic, where everything else is the same as in the Type 3 procedure. 

As for the Type 1 procedure, the hypotheses tested by the Type 2 procedure reflect a (complex) weighting of means by sample size. But, unlike the Type 1 procedure, the results are not dependent on the order in which you include effects. Some authors argue that a Type 2 procedure is preferable to a Type 3 procedure when you don't expect interactions, as the tests of the main effects are more powerful. That may be so, but interactions are often difficult to rule out on theoretical grounds. Moreover, because the hypothesis tests are generally more straightforward to interpret in the Type 3 procedure, as they involve comparisons between unweighted population means, I would generally advice the use of Type 3 tests, unless you have a strong conviction that there are no interactions, and a good grasp of the hypotheses tested by the Type 2 procedure. 

The issue of different testing schemes for _unbalanced_ designs (unequal sample sizes) is a complex one, and opinions differ on which scheme is preferred. I mainly want you to be aware of these different approaches, and you can determine your own preference once you have gained enough experience with the General Linear Model to do so. A more comprehensive treatment of the different schemes is given in Chapter 7 of @maxwell2017designing. 

## In practice

In practice, a factorial ANOVA is not that different from a oneway ANOVA. So the steps are again

1. Explore the data. Check the distribution of the dependent variable in each condition/group (i.e. each combination of experimental manipulations). Are there outlying or otherwise "strange" observations? If so, you may consider removing these from the dataset. Do the distributions look roughly Normal or are the distributions at least similar over the groups? Calculate the sample variances for each group. Is the largest variance no more than 4 times larger than the smallest sample variance? If not, then you may consider an alternative analysis than an ANOVA. If you have doubts about the homogeneity of variance, perform a Levene test. If this test is significant, you may still perform the ANOVA analysis as usual if the largest variance is less than 4 times larger than the smallest sample variance and the sample sizes in the groups are equal.

2. Define a useful set of contrast codes for the main effects of the manipulations in your study. Aim for these codes to represent the most important comparisons between the levels of the experimental factors. Then compute the interaction contrasts as (pairwise, threeway, fourway, ...) products of the main-effects contrasts. Estimate the factorial ANOVA model. Then check again for potential issues in the assumptions with e.g. histograms for the residuals and QQ-plots. If there are clear outliers in the data, remove these, and then re-estimate the model.

3. Consider both the results of the omnibus tests for the main effects and interactions, and the tests of the individual contrasts. Some people try to interpret significant interactions by conducting follow-up ANOVAs separately for different levels of one of the factors involved in the interaction. For example, an interaction between prime and belief might be investigated by testing the effect of belief twice, once for participants who received a high-power prime, and once for participants who received a low-power prime. If the effect of belief is significant for one subset of the data, but not another, this is then taken to "explain" the interaction. This is not a good procedure, for a number of reasons. Firstly, the power in these separate analyses will be reduced (each subset has less observations than the full dataset). Moreover, there will be many situations where the test is significant for multiple subsets. An interaction indicates that the size of an effect of one experimental manipulation is different for the different levels of another experimental manipulation, not that there is an effect for one level but not for another. So you should compare the size of an effect between different levels of another manipulation. This is exactly what the product contrasts do. Hence, to interpret an interaction, you should compute the (predicted) effect for each level of the other factor. If there are more comparisons of interest than those encoded in the contrast-coded predictors, perform additional follow-up tests. If there are many of these tests, consider correcting for this by using e.g. a Scheffe-adjusted critical value. 

4. Report the results. When reporting the results, make sure that you include all relevant statistics. For example, one way to write the results of the analysis of Table \@ref(tab:expBelief-factorial-ANOVA-results), is as follows:
```{r}
library(sdamr)
data("expBelief")
dat <- expBelief
dat$prime <- factor(dat$primeCond)
dat$belief <- factor(dat$experimenterBelief)
contrasts(dat$prime) <- c(.5,-.5)
contrasts(dat$belief) <- c(.5,-.5)
moda <- car::Anova(lm(ApproachAdvantage ~ prime*belief, data=dat), type=3)
modg <- lm(ApproachAdvantage ~  prime*belief, data=dat)
```

> For each participant, we calculated an approach advantage score as the average difference in reaction time (in milliseconds) between avoid and appeoach trials. We then asessed the effect of power prime and experimenter belief on approach advantage in a 2 (Prime: high-power, low-power) by 2 (Belief: high-power, low-power) factorial ANOVA. We found no significant effect of Prime, `r papaja::apa_print(moda, mse=FALSE, es="pes")$full_result$prime`. However, the effect of experimenter Belief was significant, `r papaja::apa_print(moda, mse=FALSE, es="pes")$full_result$belief`. The approach advantage score was positive when the experimenter believed participants received a high-power prime, `r papaja::apa_print(emmeans::emmeans(modg, ~ belief))$estimate$H`, and negative when the experimenter believed they received a low-power prime, `r papaja::apa_print(emmeans::emmeans(modg, ~ belief))$estimate$L`. The interaction between Prime and Belief was not significant, `r papaja::apa_print(moda, mse=FALSE, es="pes")$full_result$prime_belief`. As such, we found clear evidence for the experimenter-belief hypothesis: whether the experimenter believes participants were given a high- or low-power prime affected participants' readiness to avoid or approach targets. The prime participants actually received appeared to have no effect. As such, we found no evidence for the social priming hypothesis.

<!--
some useful online resources:
http://dwoll.de/rexrepos/posts/anovaSStypes.html 
http://md.psych.bio.uni-goettingen.de/mv/unit/lm_cat/lm_cat_unbal_ss_explained.html
!-->


<!--chapter:end:06b-Factorial-ANOVA.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Mixing categorical and metric predictors (ANCOVA)

In this chapter, we will consider combining categorical and metric independent variables to predict a dependent variable. Such an analysis is generally called an Analysis of Covariance (ANCOVA). There are a number of reasons why you might want to include metric predictors (or "covariates") in an ANOVA-type analysis: to statistically control for pre-existing differences between conditions of potential confounders, to reduce prediction error and increase the power of statistical tests, and because they are theoretically interesting as for instance potential mediators of the effect of experimental manipulations. In the course of this chapter, we will see examples of each of these. We will also discuss the interpretation of contrasts in ANCOVA models as reflecting differences between group means which are _adjusted_ for the effect of the covariate.

## Subjective feelings of power and priming

To illustrate the models, we will consider the data from @gilder2018role again. As part of the experiment, the researchers measured participants' subjective feeling of power, both before and after providing them with the power priming task. In the priming task, participants were presented with randomly scrambled sentences, and they were asked to reorder the words into grammatically correct sentences. In the high-power prime condition, half of the sentences included words associated with high power (e.g., "dominates," â€œcommandsâ€) and the other half were neutral words, while in the low-power prime condition, half the sentences contained words associated with low power (e.g., "subordinate," "obeys"). If the priming task had the desired effect, then participants in the high-power condition should report feeling more power after the priming task than participants in the low-power condition. Before the administration of the power priming task, we would expect no difference in subjective feeling of power. Figure \@ref(fig:expBelief-power-test-interaction-plots) shows the means of the subjective feeling of power before and after the power priming manipulation. As can be seen there, the task indeed appears to have the desired effect. Before the priming task, differences between the groups are relatively small and there appears to be no real difference between the priming conditions. After the priming task, the averages in the low-power prime conditions are lower and those in the high-power prime conditions higher than before the priming task. 

```{r expBelief-power-test-interaction-plots, fig.cap="Means and standard errors of subjective feeling of power before (pre-test) and after (post-test) power prime.", fig.width=6, fig.height=3.5, out.width="80%"}
library(sdamr)
data("expBelief")
dat <- expBelief
library(dplyr)
library(stringr)
library(ggplot2)
dat <- expBelief %>% mutate(primeCond = str_replace_all(primeCond, c("LPP" = "PL", "HPP" = "PH"))) %>% mutate(experimenterBelief = str_replace_all(experimenterBelief, c("H" = "EH", "L" = "EL"))) %>% mutate(condition = interaction(primeCond, experimenterBelief, sep="-"))

dat$primeCond <- factor(dat$primeCond, levels=c("PL","PH"))
contrasts(dat$primeCond) <- c(-.5,.5)
dat$experimenterBelief <- factor(dat$experimenterBelief, levels=c("EL","EH"))
contrasts(dat$experimenterBelief) <- c(-.5,.5)

library(ggplot2)

modpre <- lm(powerPRE ~ primeCond*experimenterBelief, data=dat)
modpost <- lm(powerPOST ~ primeCond*experimenterBelief, data=dat)
rbind(cbind(test="pre-test",summary(emmeans::emmeans(modpre, ~ primeCond:experimenterBelief))),
      cbind(test="post-test",summary(emmeans::emmeans(modpost, ~ primeCond:experimenterBelief)))) %>% mutate(ymax=emmean+SE, ymin=emmean-SE) %>% mutate(test = factor(test, levels=c("pre-test","post-test"))) %>%
  ggplot(aes(x=primeCond,y=emmean,colour=experimenterBelief, ymin=ymin, ymax=ymax)) +geom_point(position=position_dodge(.2)) + geom_line() + geom_errorbar(width=.1,position=position_dodge(.2)) + ylab("power") + xlab("Priming condition") + facet_wrap(~test) + theme(legend.position = "bottom") + labs(colour = "Experimenter belief")
```

To determine whether the differences between the conditions after the power prime manipulation are significant, we can use the by now hopefully familiar procedure of constructing appropriate contrast codes for priming condition and experimenter belief. To code for prime condition, it makes sense to assign a value of $\tfrac{1}{2}$ to the high-power prime condition and a value of $-\tfrac{1}{2}$ to the lower-power prime condition. As one would expect the high-power prime to increase the subjective feeling of power, one would then expect a positive slope for the associated contrast-coded predictor. Similarly, it makes sense to assign a value of $\tfrac{1}{2}$ to the "experimenter believes high-power" condition, and a value of $-\tfrac{1}{2}$ to the "experimenter believes low-power" condition. If experimenter belief has an effect on feelings of power, one would expect participants' subjective feeling of power to be higher when the experimenter believes the participant was given a high-power prime. This form of contrast coding turns that into an expectation of a positive slope of the corresponding contrast-coded predictor. The contrast code for the interaction is constructed as usual by multiplying the values of these two contrast codes. So our full set of (orthogonal) contrast codes is:
```{r}
tab <- data.frame(c1 = c("$-\\tfrac{1}{2}$","$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$","$\\tfrac{1}{2}$"),
                  c2 = c("$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$","$-\\tfrac{1}{2}$","$\\tfrac{1}{2}$"),
                  c3 = c("$\\tfrac{1}{4}$","$-\\tfrac{1}{4}$","$-\\tfrac{1}{4}$","$\\tfrac{1}{4}$"))
colnames(tab) <- c("$\\texttt{P}$","$\\texttt{B}$","$\\texttt{P}\\times\\texttt{B}$")
rownames(tab) <- c("PL,EL","PL,EH", "PH,EL", "PH,EH")
knitr::kable(tab, escape=FALSE,align=rep('r', 3), booktabs=TRUE, linesep="")#, caption="A set of three orthogonal contrast codes for the Experimenter Belief study.")
```

We can then estimate a linear model predicting $\texttt{post-power}$, the subjective feeling of power after the priming manipulation, as:
\begin{equation}
\texttt{post-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \beta_{\text{P}\times \text{B}} \times (\texttt{P} \times \texttt{B})_i + \epsilon_i
(\#eq:post-power-ANOVA)
\end{equation}
The estimates and hypothesis tests of the parameters of this model are given in Table \@ref(tab:expBelief-power-post-ANOVA).

```{r expBelief-power-post-ANOVA}
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = round(c(coefficients(modpost)[1],coefficients(modpost)[2:4],Error=NA),2),rbind(car::Anova(modpost, type=3)[1,],expand_Anova(modpost, type=3)[-1,]))
rownames(tab) <- c("Intercept","$\\texttt{P}$","$\\texttt{B}$", "$\\texttt{P} \\times \\texttt{B}$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{post-power}$ by factorial contrast-coded predictors.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

The results show a significant effect of power prime ($\texttt{P}$). The slope is estimated as $\hat{\beta}_\text{P} = `r round(coefficients(modpost)[2],digits=2)`$. Remember that the slope of a contrast-coded predictor represents a difference between marginal means; in this case, we are comparing the marginal mean of conditions with a high-power prime to conditions with a low-power prime (averaging over the levels of experimenter Belief):
$$
\begin{aligned}
\hat{\beta}_\text{P} &= \hat{\mu}_{\text{PH},\cdot} - \hat{\mu}_{\text{PL},\cdot} \\
&= \frac{\overline{Y}_\text{PH,EL} + \overline{Y}_\text{PH,EH}}{2} - \frac{\overline{Y}_\text{PL,EL} + \overline{Y}_\text{PL,EH}}{2} \\
&= `r format(coefficients(modpost)[2],digits=3)`
\end{aligned}
$$
Hence, the power prime manipulation appears to result in a difference of `r format(coefficients(modpost)[2],digits=3)` points in subjective feeling of power. Neither the main effect of experimenter belief, nor the interaction between Power Prime and Experimenter Belief are significant. We may thus conclude that experimenter belief has no effect on participants' subjective feeling of power, neither directly nor by moderating the effect of power prime. 

While the results are indicative of a successful manipulation of participants' subjective feeling of power, it could be the case that these differences between the conditions were already present before the priming manipulation. While random assignment of participants to the conditions makes such pre-existing differences unlikely, they cannot be ruled out _a priori_. As we also have a measure of participants' feeling of power before the priming task, we can test whether there is evidence for such pre-existing differences. This can be done with a similar model as before, but now using $\texttt{pre-power}$ as the dependent variable. The results are provided in Table \@ref(tab:expBelief-power-pre-ANOVA). As only the intercept is significant (which tells us, rather uninterestingly, that the grand mean of subjectively felt power differs from 0), we have no evidence for pre-existing differences, which strengthens our belief that the power manipulation had indeed a causal effect on feelings of power.

```{r expBelief-power-pre-ANOVA}
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modpre)[1],coefficients(modpre)[2:4],Error=NA),rbind(car::Anova(modpre, type=3)[1,],expand_Anova(modpre, type=3)[-1,]))
rownames(tab) <- c("Intercept","$\\texttt{P}$","$\\texttt{B}$", "$\\texttt{P} \\times \\texttt{B}$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{pre-power}$ by factorial contrast-coded predictors.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

## Acounting for pre-existing differences

We have just used two separate analyses to (1) assess differences between the priming conditions in feelings of power after the priming task, and (2) rule out that these were due to pre-existing differences. Relying on patterns of significance over multiple analyses is not ideal, particularly when one of these involves an expectation of a null effect. The problem is that a non-significant test result is not direct evidence for the absence of an effect. Non-significant results can always be due to a lack of power. So again, to help you remember: a non-significant test result is not direct evidence for the absence of an effect, and furthermore a non-significant test result is not direct evidence for the absence of an effect. 

There is a better way to reach both objectives: by including $\texttt{pre-power}$ as a predictor in our model of $\texttt{post-power}$, we can determine the effect of prime and experimenter belief, whilst _controlling_ for the effect of $\texttt{pre-power}$. This is the general idea of ANCOVA: to test for group differences whilst controlling for the effect of __covariates__ (metric predictors). Because in a linear model, effects (i.e slopes) of predictors represent _unique_ effects, we always control for the effects of the other predictors in the model when assessing the effect of a focal predictor.

When we add $\texttt{pre-power}$ to the model, the model can be written as:
\begin{equation}
\texttt{post-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \beta_{\text{P}\times \text{B}} \times (\texttt{P} \times \texttt{B})_i + \beta_\text{pre} \times \texttt{pre-power}_i + \epsilon_i
(\#eq:post-power-ANCOVA)
\end{equation}
Parameter estimates and tests are provided in Table \@ref(tab:expBelief-power-post-ANCOVA). The results show a significant effect of prime ($\texttt{P}$), as well as the pre-test power score ($\texttt{pre-power}$). The latter effect is expected, and can be interpreted as usual: the positive slope indicates that participants who scored relatively high in the pre-test also score relatively high in the post-test. Comparing the effect of power prime to that obtained earlier (Table \@ref(tab:expBelief-power-post-ANOVA)), we can notice two things: the slope is somewhat higher, and the value of the $F$ statistic is substantially higher. We will consider the interpretation of the slope in the next section, and the reason for the higher $F$ statistic in Section \@ref(sec:ANCOVA-power). 

```{r expBelief-power-post-ANCOVA}
modpostpre <- update(modpost, .~. + powerPRE)
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modpostpre)[1],coefficients(modpostpre)[2:5],Error=NA),rbind(car::Anova(modpostpre, type=3)[1,],expand_Anova(modpostpre, type=3)[-1,]))
rownames(tab) <- c("Intercept","$\\texttt{P}$","$\\texttt{B}$", "$\\texttt{pre-power}$", "$\\texttt{P} \\times \\texttt{B}$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{post-power}$ by $\\texttt{pre-power}$ and factorial contrast-coded predictors.", escape = FALSE, digits=c(3,0,0,2,3), booktabs=TRUE, linesep="")
options(opts)
```


## Slopes of contrast-coded predictors in ANCOVA models

Although a metric predictor in an ANCOVA model is really just like any other predictor in the GLM, it is useful for clarity to (momentarily) denote such a __covariate__ with a different symbol, e.g. $Z$, instead of the usual $X$ we used for predictors. We can then write the model of Equation \@ref(eq:post-power-ANCOVA) more abstractly as:
\begin{equation}
Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \beta_3 \times X_{3,i} + \beta_\text{z} \times Z_i + \epsilon_i
(\#eq:ANCOVA-one-covariate)
\end{equation}
Here, the $X$ variables are the contrast-coded predictors (e.g. $\texttt{P}$, $\texttt{B}$, and $\texttt{P}\times\texttt{B}$), and $Z$ is a single covariate (e.g. $\texttt{pre-power}$). 

As in any General Linear Model, inclusion of additional predictors will likely change the slopes of already included predictors. The slopes of the latter predictors will remain the same only when the additional predictors are _completely independent_ from the already-included predictors. Such complete non-redundancy is in practice extremely unlikely. If there is non-redundancy (i.e. multicollinearity) in a model like that of Equation \@ref(eq:ANCOVA-one-covariate), we can no longer use Equation \@ref(eq:estimate-slope-orthogonal-contrast-codes) to compute the estimated slopes of contrast-coded predictors which are based on a set of orthogonal contrast codes. Fortunately, that does not mean those slopes no longer have a useful interpretation. In fact, the estimated slopes can be expressed as an adjusted version of Equation \@ref(eq:estimate-slope-orthogonal-contrast-codes):
\begin{equation}
\hat{\beta}_j = \frac{\sum_{k=1}^{g} c_{j,k} \overline{Y}_k}{\sum_{k=1} c_{j,k}^2} - \hat{\beta}_z \frac{\sum_{k=1}^{g} c_{j,k} \overline{Z}_k}{\sum_{k=1} c_{j,k}^2}
(\#eq:estimate-slope-orthogonal-contrast-codes-ANCOVA-one-covariate)
\end{equation}
where $\hat{\beta}_z$ is the slope of the covariate in the full model (Equation \@ref(eq:ANCOVA-one-covariate)). You can think of this as follows: An orthogonal contrast code reflects differences between marginal means. In the presence of a covariate, these differences are adjusted for the same difference in the marginal means of the covariate, weighted by its effect on the dependent variable. If there are differences in the covariate between the groups, and the covariate has an effect on the dependent variable, then at least some part of these differences can be explained by differences in the covariate between the groups. The adjustment removes the effect of differences in covariate values between the conditions from the values of the dependent variable. After the adjustment, the slope of the contrast code then reflects differences between the groups that are not explained by the covariate.

Let's illustrate how this works. The means of $\texttt{pre-power}$ and $\texttt{post-power}$ are:
```{r expBelief-power-means}
tab <- dat %>%
  group_by(primeCond, experimenterBelief) %>%
    summarise(powerPRE = mean(powerPRE), powerPOST = mean(powerPOST))
colnames(tab) <- c("Prime","Belief","pre-power","post-power")
knitr::kable(tab, digits = c(0,0,2,2), escape=FALSE, row.names = FALSE, booktabs=TRUE, linesep="")
```
The estimated slope of $\texttt{pre-power}$ was $\hat{\beta}_\text{pre} = `r format(coefficients(modpostpre)[4], digits=4)`$. Using the contrast code for Prime as we used before (i.e. $-\tfrac{1}{2}$ for the low-power prime conditions and $-\tfrac{1}{2}$ for the high-power prime conditions), we can determine the estimated slope of Prime as on `post-power`

$$\begin{aligned}
\hat{\beta}_\texttt{P} &=&& \frac{\tfrac{1}{2} \times `r format(format(as.numeric(tab[3,4]),digits=4))` + \tfrac{1}{2} \times `r format(format(as.numeric(tab[4,4]),digits=4))` - \tfrac{1}{2} \times `r format(format(as.numeric(tab[1,4]),digits=4))` - \tfrac{1}{2} \times `r format(format(as.numeric(tab[2,4]),digits=4))`}{ (\tfrac{1}{2})^2 + (\tfrac{1}{2})^2 + (-\tfrac{1}{2})^2 + (-\tfrac{1}{2})^2} - \\
&&& `r format(coefficients(modpostpre)[4], digits=4)` \times \frac{\tfrac{1}{2} \times `r format(format(as.numeric(tab[3,3]),digits=4))` + \tfrac{1}{2} \times `r format(format(as.numeric(tab[4,3]),digits=4))` - \tfrac{1}{2} \times `r format(format(as.numeric(tab[1,3]),digits=4))` - \tfrac{1}{2} \times `r format(format(as.numeric(tab[2,3]),digits=4))`}{ (\tfrac{1}{2})^2 + (\tfrac{1}{2})^2 + (-\tfrac{1}{2})^2 + (-\tfrac{1}{2})^2} \\
& = && \frac{`r sum(c(-.5,-.5,.5,.5)*tab[,4])`}{1} - `r format(coefficients(modpostpre)[4], digits=4)` \times \frac{`r sum(c(-.5,-.5,.5,.5)*tab[,3])`}{1} \\
& = && `r sum(c(-.5,-.5,.5,.5)*tab[,4]) - coefficients(modpostpre)[4] * sum(c(-.5,-.5,.5,.5)*tab[,3])`
\end{aligned}$$

An alternative way to define the slope of Equation \@ref(eq:estimate-slope-orthogonal-contrast-codes-ANCOVA-one-covariate) is to first adjust the means of $Y$ (i.e. `post-power` here) and then enter the adjusted group means in the usual formula for the slope of a contrast-coded predictor. These adjusted means are computed using a _centered_ version of the covariate (i.e. $Z'_i = Z_i - \overline{Z}$):
$$\overline{Y}_k' = \overline{Y}_k - \hat{\beta}_z \times (\overline{Z}_k - \overline{Z})$$
where $\overline{Z}$ is the average of the covariate over all observations (i.e. it is not an "average of averages"). The slope can then be expressed as a function of these adjusted means:
$$\hat{\beta}_j = \frac{\sum_{k=1}^{g} c_{j,k} \overline{Y}_k'}{\sum_{k=1} c_{j,k}^2}$$
The fact that these two formulations are equivalent again illustrates the main idea of an ANCOVA model, which is to assess group differences that can _not_ be attributed to the covariate. 

## Homogeneity of slopes

The model of Equation \@ref(eq:post-power-ANOVA) does not include any interactions between the covariate and the contrast-coding differences. As such, the model assumes the effect of the covariate on the dependent variable is the same in each condition. This assumption is usuallty referred to as the assumption of __homogeneity of regression slopes__. The resulting regression lines for the relation between $\texttt{power-pre}$ and $\texttt{power-post}$ in the different conditions are depicted in Figure \@ref(fig:expBelief-modpostpre-regression-lines). The assumption that the slopes are identical means that the the regression lines are parallel to each other. Note that the height of each regression line reflects the combined effect of power priming and experimenter belief. Within each condition, these effects are a constant, and hence the combined effect of these factors effectively is equal to the intercept of each regression line. 

As discussed above, you can think of an ANCOVA model as statistically correcting the conditions for any differences in the covariate. After this correction, the conditions effectively have the same value on the covariate. In the context of Figure \@ref(fig:expBelief-modpostpre-regression-lines), you can pick any value of the covariate you like, and then think of the contrasts between the conditions as comparisons between the model predictions at that value of the covariate. Because the regression lines for both high-power prime conditions, and for both low-power prime conditions, are almost the same, it is clear that experimenter belief has little effect on feelings of power. The high-power and low-power conditions have more clearly separated regression lines, which is corroborated by the significant effect of power prime in the model.

```{r expBelief-modpostpre-regression-lines, fig.cap="Regression lines reflecting the relation between $\texttt{power-pre}$ and $\texttt{power-post}$ in the four conditions of the experimenter belief and power priming experiment."}
newdata <- data.frame(primeCond=rep(unique(dat$primeCond),each=2),experimenterBelief = unique(dat$experimenterBelief),powerPRE = c(rep(min(dat$powerPRE),4),rep(max(dat$powerPRE),4))) %>% mutate(condition = interaction(primeCond,experimenterBelief,sep="-"))
dat %>% 
  ggplot(aes(x=powerPRE,y=powerPOST,colour=condition)) + geom_point(alpha=.5) + geom_line(data=mutate(newdata, powerPOST = predict(modpostpre, newdata = newdata))) + xlab("pre-power") + ylab("post-power") + theme(legend.position = "bottom")
```

While the assumption of parallel regression lines (i.e. homogeneity of regression slopes) allows for a straightforward interpretation of the other effects as reflecting differences in adjusted means, as for any assumption we make, there is always uncertainty about whether it can reasonably be assumed to hold. Due to the symmetry of moderation, not only does the lack of interactions between the covariate and contrast-coded predictors imply that the effect of the covariate on the dependent variable is identical in each condition, it also means that the effect of the conditions is the same regardless of the value on the covariate. There may be good reasons to suspect that the effect of the experimental manipulations differs for people who score differently on the covariate. For instance, you might think that a high-power prime is effective in lifting the subjective feeling of power for those who have a relatively low feeling of power to start off with, while it doesn't do much for those who feel powerful anyway. In a similar vain, the low-power prime might lower the subjective feeling of power for those who have a relatively low feeling of power to start off with, while those with a relatively high feeling of power might be immune to it. If this were true, than the effects of experimental manipulations are moderated by the the covariate. This indicates we should include interactions between the covariate and the contrast-coded predictors. Allowing for such interactions is straightforward: we just construct new product-predictors and include them in the model.

Allowing for moderation of the effect of conditions by the covariate in the model of Equation \@ref(eq:post-power-ANOVA), we would need add three product predictors: $(\texttt{P} \times \texttt{power-pre})_i$, $(\texttt{B} \times \texttt{power-pre})_i$, and $(\texttt{P} \times \texttt{B} \times \texttt{power-pre})_i$. The results of the expanded model are given in Table \@ref(tab:expBelief-power-post-ANCOVA-non-homgeneous).

```{r expBelief-power-post-ANCOVA-non-homgeneous}
modpostpre_int <- lm(powerPOST ~ primeCond*experimenterBelief*powerPRE, data=dat)
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modpostpre_int)[1],coefficients(modpostpre_int)[2:8],Error=NA),rbind(car::Anova(modpostpre_int, type=3)[1,],expand_Anova(modpostpre_int, type=3)[-1,]))
rownames(tab) <- c("Intercept","$\\texttt{P}$","$\\texttt{B}$", "$\\texttt{pre-power}$", "$\\texttt{P} \\times \\texttt{B}$", "$\\texttt{P} \\times \\texttt{pre-power}$", "$\\texttt{B} \\times \\texttt{pre-power}$", "$\\texttt{P} \\times \\texttt{B} \\times \\texttt{pre-power}$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{post-power}$ by  $\\texttt{pre-power}$ and factorial contrast-coded predictors, as well as interactions between those.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

We can see that none of the interactions between $\texttt{pre-power}$ and the contrast-coded predictors are significant. As such, there is no strong evidence of an interaction between the covariate and the experimental manipulations. Rather than inspecting each interaction separately, it makes sense to perform an omnibus test, comparing the expanded model to the one of Equation \@ref(eq:post-power-ANOVA). This comparison tests the hypothesis that _all_ the slopes of the additional product-predictors equal 0. If there is no overly strong reason to suspect a moderation for particular conditions, the omnibus test is a reasonable test that the covariate interacts with any of the experimental manipulations.^[Although my reasoning why the power-prime manipulation might have a different effect for those participants with an initial high or low feeling of power might suggest otherwise, this was to illustrate why you _might_ suspect an interaction, it is not a hypothesis I would stand by personally. I'm by no means an expert in the scientific study of social power.] This omnibus test is a straightforward application of the model comparison approach. We need the Sum of Squared Errors of each model, which are $\text{SSE}(G) = `r format(sum(residuals(modpostpre_int)^2), digits=2)`$ and $\text{SSE}(R) = `r format(sum(residuals(modpostpre)^2), digits=2)`$ for the expanded model and the one of Equation \@ref(eq:post-power-ANOVA), respectively. The $F$ statistic is then computed as usual as 

$$\begin{aligned}
F &= \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar}(G)}} \\
&= \frac{\frac{`r format(sum(residuals(modpostpre)^2), digits=2)` -  `r format(sum(residuals(modpostpre_int)^2), digits=2)`}{`r length(coefficients(modpostpre_int))` - `r length(coefficients(modpostpre))`}}{\frac{`r format(sum(residuals(modpostpre_int)^2), digits=2)`}{`r nrow(dat)`-`r length(coefficients(modpostpre_int))`}} \\
&= `r format(((sum(residuals(modpostpre)^2) -  sum(residuals(modpostpre_int)^2))/(length(coefficients(modpostpre_int)) - length(coefficients(modpostpre))))/( sum(residuals(modpostpre_int)^2)/(nrow(dat)- length(coefficients(modpostpre_int)))),digits=3)`
\end{aligned}$$
With $\text{df}_1 = `r length(coefficients(modpostpre_int))- length(coefficients(modpostpre))`$ and $\text{df}_2 = `r nrow(dat) - length(coefficients(modpostpre_int))`$, the critical value is $F_{`r length(coefficients(modpostpre_int))- length(coefficients(modpostpre))`,`r nrow(dat) - length(coefficients(modpostpre_int))`, .05} = `r format(qf(.95, length(coefficients(modpostpre_int))- length(coefficients(modpostpre)), nrow(dat) - length(coefficients(modpostpre_int))),digits=3)`$. Hence, the result is non-significant, and we do not reject the null hypothesis that there are no interactions between the covariate and the contrast-coded predictors. In other words, we have no evidence that the assumption of homogeneous regression slopes is violated.

If the assumption of homogeneous regression slopes is violated, then the model should really include the appropriate interaction terms. In that case, it is important to remember that the "simple slopes" of the contrast-coded predictors then reflect tests of group differences at particular values of the covariate. Group differences at one value of the covariate may be substantial, but absent or even reversed at another. It might make sense to center the covariate, so that you would test for group differences for an average value of the covariate. You could also consider not only centering at the mean, but at a few informative other values. For instance, you could consider testing the slopes at three values of the covariate: the minimum value, mean, and maximum value in the data. You could do this by creating three different "centered" covariates: $Z'_i = Z_i - \overline{Z}$, $Z''_i = Z_i - \text{min}(Z)$, and $Z'''_i = Z_i - \text{max}(Z)$, and entering each in a different version of the same model. If the estimates of the group differences have the same direction and the tests significant in each, then you would conclude that there is evidence of group differences in the whole range of the covariate observed in the data. Other strategies are possible, of course. The main thing to realise is that in a model with interactions between the covariate and contrast-codes, group differences depend on the value of the covariate, and statements about group differences should be qualified by for which value of the covariate they hold. 

## Power considerations in ANCOVA {#sec:ANCOVA-power}

A primary reason for including covariates in models with categorical independent variables is to assess group differences in the dependent variable whilst controlling for any possible differences in the covariates. But there is another reason why the inclusion of covariates can be a good idea: it may increase the power of the tests of group differences. We saw evidence of this when we included $\texttt{pre-power}$ in the model for $\texttt{post-power}$. Comparing the results in Table \@ref(tab:expBelief-power-post-ANCOVA) to those in Table \@ref(tab:expBelief-power-post-ANOVA), you can see that the test results for the main effect of priming are stronger after inclusion of the covariate. The reason for this is that the covariate can explain differences in $\texttt{post-power}$ within each condition. Accounting for this within-condition variance reduces the overall error variance of the model, which can subsequently increase the power of the tests. This is particularly the case when the covariate is strongly related to the dependent variable, but independent of the contrast-coded predictors. Such independence implies that the groups do not differ in the average value of the covariate. 

We can illustrate this with the Venn diagrams in Figure \@ref(fig:sse-partition-ANCOVA). Let's consider first the situation of no redundancy between the covariate $Z$ and the contrast-coded predictors $X$, which is depicted in the left-hand plot. The SSR associated to the contrast-coded predictors is region $B$, and the SSR associated to the covariate is region $C$. The test of group differences (i.e. the test of the contrast-coded predictors) would involve a comparison of the SSR of region $B$ against the unexplained error of region $A$. In a model without the covariate, the unexplained error would be the sum of region $A$ and $B$. As this error is larger, the $F$ statistic will be smaller, even though the SSR term for $X$ is the same. As such, inclusion of the covariate increases the power of the test of $X$, and the increase in power is larger the stronger the relation between the covariate $Z$ and the dependent variable $Y$ (i.e. the larger region $B$). The situation is more complicated when there is redundancy between the covariate and the contrast-coded predictors. This is the situation depicted in the right-hand plot. Whilst inclusion of the covariate again reduces the error -- which would be the sum $A+B$ for a model without the covariate, but only $A$ for a model with the covariate -- it also reduces the unique SSR that can be associated to the contrast-coded predictors. In a model without the covariate, $\text{SSR}(X) = B + D$, and in a model with the covariate, $\text{SSR}(X) = B$. The ANCOVA model is now no longer guaranteed to increase the power of the test of $X$, as this depends on how the reduction in error compares to the reduction of the SSR.

In conclusion, for purposes of increasing power, the covariate is ideally strongly related to the dependent variable, but unrelated to the other predictors in the model.

```{tikz sse-partition-ANCOVA, fig.cap="Partitioning the variance in an ANCOVA model. the circle labelled as $Y$ represents the variance of the dependent variable, circle $X$ represents a (set of) contrast-coded predictor(s), and circle $Z$ the covariate. Overlapping regions represent shared variability (e.g. covariance) between variables.", out.width="60%"}
\def\firstcircleA{(1.1,1.3) circle (1.75cm)}
\def\secondcircleA{(2.9,1.3) circle (1.75cm)}
\def\thirdcircleA{(2,1.7) circle (1.75cm)}

\def\firstcircleB{(6.2,1) circle (1.75cm)}
\def\secondcircleB{(6.8,1) circle (1.75cm)}
\def\thirdcircleB{(6.5,2) circle (1.75cm)}

\begin{tikzpicture}
	\pgfsetxvec{\pgfpoint{2cm}{0cm}}
	\pgfsetyvec{\pgfpoint{0cm}{2cm}}
	\tikzstyle{every node}=[minimum size=.7cm]

	\draw \firstcircleA;
	\draw \secondcircleA;
	\draw \thirdcircleA;
	
	\draw (0.2,0.2) node {$X$};
	\draw (3.8,0.2) node {$Z$};
	\draw (2,3) node {$Y$};
	\draw (2,2.3) node {$A$};
	\draw (1.5,1.5) node {$B$};
	\draw (2.5,1.5) node {$C$};
 
  %\draw (1,1) node {$E$};
  %\draw (3,1) node {$F$};

  \draw node[align=center] at (2,3.5) {\textbf{no redundancy}};

	\draw \firstcircleB;
	\draw \secondcircleB;
	\draw \thirdcircleB;
	
	\draw (5.2,0.2) node {$X$};
	\draw (7.8,0.2) node {$Z$};
	\draw (6.5,3) node {$Y$};
	\draw (6.5,2.3) node {$A$};
	\draw (5.95,1.6) node {$B$};
	\draw (6.5,1.4) node {$D$};
	\draw (7.05,1.6) node {$C$};
  %\draw (5.6,.8) node {$E$};
  %\draw (7.4,.8) node {$F$};
  %\draw (6.5,.8) node {$G$};

  \draw node[align=center] at (6.5,3.5) {\textbf{with redundancy}};

\end{tikzpicture}	
```

## Models with multiple covariates

The analyses reported above indicate that the power priming task had the desired result of changing participants' subjective feeling of power. The significant effect of prime on $\texttt{post-power}$ reflects differences between the means of the priming conditions. It is likely that within those conditions, there is variability in the subjective feeling of power. According to the social priming hypothesis, we might expect the approach advantage scores to be higher for those participants with a relatively strong subjective feeling of power, and lower for those with a relatively weak subjective feeling of power.

Given the significant differences in $\texttt{post-power}$ between the priming conditions, but the lack of a significant effect of priming condition on approach advantage scores, a relation between $\texttt{post-power}$ and $\texttt{ApproachAdvantage}$ seems unlikely, but we can include $\texttt{post-power}$ as a predictor of $\texttt{ApproachAdvantage}$ and see. Rather than the final subjective feeling of power, it might also be the case that the approach advantage is related to how much the priming manipulation increased or decreased the feeling of power. To investigate this, we might include the _difference_ $\texttt{post-power} - \texttt{pre-power}$ as a predictor. But actually, we can obtain a similar effect by just including $\texttt{pre-power}$ as a predictor in the model; when the slope of $\texttt{pre-power}$ is negative and the slope of $\texttt{post-power}$ positive, the combined effect on $\texttt{ApproachAdvantage}$ would be similar.
The model thus becomes:
$$\begin{aligned}
\texttt{ApproachAdvantage}_i =& \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \beta_{\text{P}\times \text{B}} \times (\texttt{P} \times \texttt{B})_i + \\ & \beta_\text{pre} \times \texttt{pre-power}_i + \beta_\text{post} \times \texttt{post-power}_i + \epsilon_i
\end{aligned}$$
Parameter estimates and tests are provided in Table \@ref(tab:expBelief-ApproachAdvantage-ANCOVA). As in the earlier ANOVA (Table \@ref(tab:expBelief-oneway-ANOVA-results)), we obtain a significant effect of experimenter belief. None of the other effects are significant. Neither the subjective feeling of power before or after the priming manipulation appears to affect the approach advantage scores. Interestingly, whilst not significant, the slope of $\texttt{post-power}$ is positive, and the slope of $\texttt{pre-power}$ negative, which is what one would expect when the difference between the two is related to the dependent variable.

```{r expBelief-ApproachAdvantage-ANCOVA}
modg <- lm(ApproachAdvantage ~ primeCond*experimenterBelief + powerPRE + powerPOST, data=dat)
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modg)[1],coefficients(modg)[2:6],Error=NA),rbind(car::Anova(modg, type=3)[1,],expand_Anova(modg, type=3)[-1,]))
rownames(tab) <- c("Intercept","$\\texttt{P}$","$\\texttt{B}$", "$\\texttt{pre-power}$","$\\texttt{post-power}$", "$\\texttt{P} \\times \\texttt{B}$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{ApproachAdvantage}$ by $\\texttt{pre-power}$, $\\texttt{post-power}$,  and factorial contrast-coded predictors.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

Note that the slopes of the contrast-coded predictors are somewhat different from those in Table \@ref(tab:expBelief-oneway-ANOVA-results). As before, this is because the slopes in the ANCOVA model represent differences between adjusted means. Analogous to Equation \@ref(eq:estimate-slope-orthogonal-contrast-codes-ANCOVA-one-covariate), when there are a total of $L$ covariates included in the model, which we can denote as $Z_1, Z_2, \ldots, Z_L$, the estimate of the slope of the contrast-coded predictors can be written as:
\begin{equation}
\hat{\beta}_j = \frac{\sum_{k=1}^{g} c_{j,k} \overline{Y}_k}{\sum_{k=1} c_{j,k}^2} - \sum_{l=1}^L \hat{\beta}_{z_l} \frac{\sum_{k=1}^{g} c_{j,k} \overline{Z}_{l,k}}{\sum_{k=1} c_{j,k}^2}
(\#eq:estimate-slope-orthogonal-contrast-codes-ANCOVA-multiple-covariates)
\end{equation}
i.e. the total adjustment of the slope consists of the sum of adjustments for each covariate. 

## Mediation with categorical independent variables

The finding that experimenter belief has an effect on participants' approach advantage indicates that experimenter expectations affect participants behaviour, even though the experimenters in the study of @gilder2018role asserted that their knowledge of the condition had not affected their behaviour towards the participants. The question is then how experimenter belief changed their interaction with the participants to change their approach advantage scores. To attempt to answer this question, we assess whether experimenter belief changed participants' perception of the experimenters, and whether such changes resulted in the difference in approach advantage. Thus, as another example of mixing categorical and metric independent variables, we can consider assessing whether the effect of experimenter belief on approach advantage is mediated by participants' perceptions of the experimenters. 

At the end of the experiment, participants rated how attractive, competent, friendly, and trustworthy they found their experimenter. Preliminary analysis (not shown here for brevity) indicates that when experimenters believed participants were assigned to the high-power condition, they were rated as more attractive, friendly, and trustworthy. If the effect of experimenter belief is mediated by these changes in perception, then after including the ratings of attractiveness, friendliness, and trustworthiness, we would expect the effect of experimenter belief to be reduced. However, the results of the analysis (see Table \@ref(tab:expBelief-ApproachAdvantage-mediation-ANCOVA)) are only slighly suggestive of this. Controlling for attractiveness, friendliness, and trustworthiness, we still obtain a highly significant effect of experimenter belief, with a slope which is a little lower than in a model without these covariates. But none of the covariates appears to be related to the approach advantage. 

```{r expBelief-ApproachAdvantage-mediation-ANCOVA}
modg2 <- lm(ApproachAdvantage ~ primeCond*experimenterBelief + attractive + friendly + trustworthy, data=dat)
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modg2)[1],coefficients(modg2)[2:7],Error=NA),rbind(car::Anova(modg2, type=3)[1,],expand_Anova(modg2, type=3)[-1,]))
rownames(tab) <- c("Intercept","$\\texttt{P}$","$\\texttt{B}$", "$\\texttt{attractive}$","$\\texttt{friendly}$", "$\\texttt{trustworthy}$", "$\\texttt{P} \\times \\texttt{B}$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{ApproachAdvantage}$ by $\\texttt{pre-power}$, $\\texttt{post-power}$,  and factorial contrast-coded predictors.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

## ANCOVA vs difference scores

To end the chapter, I want to discuss an alternative method to assess whether the power prime manipulation was effective. Note that the main effect of power prime reported in Table \@ref(tab:expBelief-power-post-ANOVA) just reflects a difference between high-power and low-power priming conditions. That there is a difference between the conditions does not really tell us exactly what the priming task did to participants' feeling of power. It could be that unscrambling high-power sentences increases feelings of power, and unscrambling low-power sentences decreases feelings of power. But it could also be that the task increased feelings of power in both conditions, but more so in the high-power conditions. Or it could be that the task decreased feelings of power in both conditions, but less so in the high-power conditions. In all these cases, the mean would be higher in the high-power conditions than in the low-power conditions, but the effect of the priming task is rather different. To more directly assess whether the high-power prime _increased_, and the low-power prime _decreased_ participants' subjective feeling of power, we can consider using the difference $\texttt{diff-power}_i =  \texttt{post-power} - \texttt{pre-power}$ as dependent variable in the model
$$\texttt{diff-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \beta_{\text{P}\times \text{B}} \times (\texttt{P} \times \texttt{B})_i + \epsilon_i$$
The model results are given in Table \@ref(tab:expBelief-diff-power-ANOVA). We again find a significant and positive effect of power prime. We also find that the intercept is close to 0 and non-significant. That indicates that averaged over all conditions, the subjective feeling of power seems to neither increase or decrease. However, the significant positive slope of power prime then shows that the high-power prime increased, and the low-power prime decreased, subjective feeling of power. 

```{r expBelief-diff-power-ANOVA}
dat$powerDIFF <- dat$powerPOST - dat$powerPRE
modgdiff <- lm(powerDIFF ~ primeCond*experimenterBelief, data=dat)
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modgdiff)[1],coefficients(modgdiff)[2:4],Error=NA),rbind(car::Anova(modgdiff, type=3)[1,],expand_Anova(modgdiff, type=3)[-1,]))
rownames(tab) <- c("Intercept","$\\texttt{P}$","$\\texttt{B}$", "$\\texttt{P} \\times \\texttt{B}$", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting $\\texttt{diff-power}$ by factorial contrast-coded predictors.", escape = FALSE, digits=3, booktabs=TRUE, linesep="")
options(opts)
```

Although a model with a difference score as dependent variable is straightforward to interpret when we are mainly interested in determining group differences (i.e. the effects of the contrast-coded predictors), there are reasons to prefer an ANCOVA model (e.g. Equation \@ref(eq:post-power-ANCOVA)) to this analysis. If we write out the difference score model as
$$\texttt{post-power}_i - \texttt{pre-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \beta_{\text{P}\times \text{B}} \times (\texttt{P} \times \texttt{B})_i + \epsilon_i$$
we can add $\texttt{pre-power}$ to both sides of the equation to obtain 
$$\texttt{post-power}_i = \beta_0 + \beta_\text{P} \times \texttt{P}_i + \beta_\text{B} \times \texttt{B}_i + \beta_{\text{P}\times \text{B}} \times (\texttt{P} \times \texttt{B})_i +  \texttt{pre-power}_i + \epsilon_i$$
Note that this model is the same model as in Equation \@ref(eq:post-power-ANCOVA) if we fix $\beta_\text{pre} = 1$. In other words, the difference model is a special case of the ANCOVA model, where we assume we know the slope of the covariate equals 1. If this assumption is (approximately) true, then the difference model can be preferred, because it has one parameter less to estimate. If, however, the true slope of the covariate is different from 1, then the difference model is less accurate than the ANCOVA model. If the slope of the covariate is sufficiently different from 1, then the ANCOVA model will provide more powerful tests of the contrasts than the difference model [@judd2011data]. In addition, if one is interested in modelling changes within participants, then approaches such as repeated-measures ANOVA and linear mixed-effects models are a better choice.

<!--chapter:end:07-ANCOVA.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Repeated-measures ANOVA {#ch-RM-ANOVA}

In this chapter, we will discuss how to deal with the non-independence of errors that can result from repeated measures of the same individual or from other groupings in the data, within the context of ANOVA-type analyses (i.e. a GLM with only categorical predictors). The resulting class of models is known as repeated-measures ANOVA. We will consider one way to construct this class of models, by constructing new dependent variables by applying orthogonal contrasts to create difference scores between the observed dependent variables. For each of these new dependent variables, we can use a GLM as usual. The main conceptual leap will be in interpreting these new dependent variables, which we'll call _within-subjects composite scores_. The alternative approach is to apply contrast codes to the different "units of observation" (e.g. participants), and then treat this as a factorial design involving a mix of fixed and random experimental factors. As random and fixed effects are more straightforwardly dealt with in mixed-effects regression models -- the topic of the next chapter -- we will leave such considerations until later. The way we'll will discuss repeated-measures ANOVA is close to how most modern computational approaches actually conduct these analyses.

## Non-independence in linear models

The General Linear Model we have considered thus far can be stated as follows:

$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
The assumptions of this model concern the errors, or residuals, $\epsilon_i$. These are assumed to be _independent and identically distributed_ (iid), following a Normal distribution with a mean of 0 and a standard deviation $\sigma_\epsilon$.

You can expect violations of the _iid_ assumption if data are collected from "units of observation" that are clustered in groups. A common example of this in psychology is when you repeatedly observe behaviour from the same person.^[Other examples of such clustering in data are when data is collected in group settings, such as students within classrooms, or patients within hospitals. In such situations one could expect again that observations within each cluster (i.e., a specific group, classroom, or hospital) are more similar to each other than observations across clusters.] For example, suppose participants in an experiment perform two tests to measure their working memory capacity. A person with a high working memory capacity would likely score high on both tests, whilst a person with a low working memory capacity would likely score low on both. In that case, the scores in the two tests are correlated and hence _not_ independent. If the model does not adequately account for this, then the errors (residuals) would also not be independent. Violating the independence or errors assumption implies that the General Linear Model is misspecified, and does not account for all the structure in the data. This has consequences for the tests of parameters of the model. In addition to the test statistics no following the assumed distributions, the tests often have less power than when properly accounting for the dependencies. Following @judd2011data, the approach we take here is to remove the dependencies by transforming the data. In a nutshell, using orthogonal contrast codes, we transform a set of correlated dependent variables (e.g. two tests of working memory) into a set of orthogonal (uncorrelated) dependent variables. We then apply General Linear Models to each of these transformed dependent variables. 

<!-- As another example of dependency between model residuals, let's go back to the simplest version of the GLM, the model with just an intercept that was the focus of Chapter \@ref(ch-simple-GLM):
\begin{equation}
Y_i = \beta_0 + \epsilon_i
(\#eq:simple-glm-ch08)
\end{equation}
Suppose the dependent variable $Y$ again reflects judgements of the height of Mount Everest, but that one of the labs in the ManyLabs study that collected the data, was based in Nepal. We might expect participants in Nepal to have a more accurate idea of the height of Mount Everest than participants in Poland (the country the participants were from in the subset of the data we considered in Chapter \@ref(ch-simple-GLM)). As a result, participants in Nepal would likely be less influenced by the low anchor than those with less knowledge of Mount Everest. If $\beta_0$ represents the mean judgement over all participants, then participants in Nepal would generally provide judgements which are higher than $\beta_0$ (i.e positive errors), whilst participants in Poland would generally provide judgements that are lower than $\beta_0$ (i.e. negative errors). If our model does not account for such grouping in the data, then the errors for cases from Nepal would be correlated, because all would tend to be positive. Similarly, the errors for cases from Poland would also be correlated, as all would tend to be negative. 

What this example points to is that the simple model does not account for the fact that participants from different countries might have a different average judgement. As such, the model is misspecified. Here, that leads to dependent errors. Unfortunately, compared to violations of other assumptions, such as the normality assumption or the assumption of variance homogeneity, standard statistical procedures are usually _not_ robust to violations of the independence assumption [@kenny_consequences_1986;@judd_treating_2012]. They often lead to considerably increased Type I errors (i.e., false positives) and more generally can produce overconfident results (e.g., too narrow standard errors).

In the example above, we could alleviate the problem of dependent errors by including an effect for country, using a contrast-coding predictor $X_1$ to reflect the difference between participants from Nepal ($X_1 = \tfrac{1}{2}$) and Poland ($X_1 = -\tfrac{1}{2}$) in an expanded model
$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i$$
This model would allow for a different mean judgement for cases from Nepal ($\beta_0 + \tfrac{1}{2} \times \beta_1$) and cases from Poland ($\beta_0 - \tfrac{1}{2} \times \beta_1$). Effectively, this is the strategy employed in repeated-measures ANOVA: include an additional grouping factor in the model.

<!-- The assumption that the errors are independent means that knowing the value of the error for one case $i$ in the data does not give you any information to determine the value of the error for another case $j$. More formally, statistical independence is defined in terms of conditional probabilities. Remember when we discussed the rules of probability (Section \@ref(sec:02-rules-of-probability))? In particular, we used independence to state rule 7, which is a specific case of the multiplication rule (rule 6). We can define the distribution of the error for case $i$ as a conditional probability $p(\epsilon_i | \epsilon_i, X_{1,i}, \ldots, X_{m,i})$, where we conditionalise on all the predictor values, as well as another error term $\epsilon_j$. The errors are independent when 
$$p(\epsilon_i | \epsilon_j, X_{1,i}, \ldots, X_{m,i}) = p(\epsilon_i |  X_{1,i}, \ldots, X_{m,i})$$
i.e. the distribution of $\epsilon_i$, conditional upon knowing the value of the predictors and the error $\epsilon_j$ of another case $j$, is the same as the distribution conditional upon just knowing the value of the predictors. This means that error $\epsilon_j$ provides no information about the error $\epsilon_i$.
-->

## The cheerleader effect

The attractiveness of a person's face is traditionally considered to be related to physical features, such as how symmetrical the features are, how close a face is to the "average" face over many people, and sexual dimorphism (whether a face looks exclusively male or female). However, there is also evidence that perceived facial attractiveness can vary due to factors outside of the face. In what has become known as the "cheerleader effect", the same face is perceived to be more attractive when seen in a group, as compared to when it is seen alone. @walker2014hierarchical proposed that the cheerleader effect arises due to people encoding faces in a hierarchical manner. When presented with a group of faces, people encode the display by first calculating an average face for the group, and then encoding individual faces as deviations from the group average. Because faces which are closer to the average face tend to be perceived as more attractive than individual faces, the encoded group average lifts the attractiveness of each face in the group, resulting in each face in the group being perceived as more attractive than if it were presented by itself. 

@carragher2019limited set out to test this explanation. In one part of their study (Experiment 1), they let participants rate the attractiveness of a face when presented by itself (the Alone condition), as part of a group of different faces (the Different condition), or as part of a group of similar faces (the Similar condition). There were two variants of the latter condition, and participants encountered only one of them in the experiment. In the Identical condition, the group consisted of three copies of exactly the same photo. In the Variant condition, the group consisted of three different photos of the same face.^[@carragher2019limited use different names for the conditions. They call the Different condition the Control condition, the Similar condition the Distractor condition, the Identical condition the Identical-distractors condition, and the Variant condition the Self-distractors condition.] The authors argued that if the hierarchical-encoding explanation for the cheerleader effect is true, then the cheerleader effect should not be observed in the Identical condition. This is because the average of three identical photos is just the photo itself, so there should be no difference between an encoded average face and the face itself. In the Variant condition however, variability between the different photos of the same face might still lead to an average face which is deemed more attractive than each individual face. 

The design of the study is an example of a 2 (Version: Identical, Variant) by 3 (Presentation: Alone, Different, or Similar) design. The first factor (Version) varied between people, and the second (Presentation) within people (each participant rated a face in each of the three conditions). The rated attractiveness of the faces in the different conditions are provided in Figure \@ref(fig:cheerleader-raincloud-plot).

```{r cheerleader-raincloud-plot, fig.cap="Attractiveness ratings for photo's of faces when presented alone, as part of a group of dissimilar faces (Different), or as part of a group of similar faces (Similar), which are either identical (Identical) or different photos (Variant) of the same face.", fig.width=6, fig.height=3}
library(dplyr)
library(ggplot2)
library(sdamr)
data("cheerleader")
dat <- subset(cheerleader, Excluded == 0) %>%
  mutate(Item = factor(Item)) %>%
  mutate(Item = forcats::fct_recode(Item, Different = "Control_Group", Similar = "Distractor_Manipulation"),
         Task = forcats::fct_recode(Task, Identical = "Identical-Distractors", Variant = "Self-Distractors"))
dat %>%
  # group_by(Task,Item) %>%
  sdamr::plot_raincloud(y=Response, groups = Item) + facet_wrap(~Task)
```

## As a oneway ANOVA

```{r}
dat <- dat %>%
  filter(Task == "Variant") %>%
  mutate(Item = factor(Item))
```

To keep matters relatively simple, we will for now just consider the $n=`r length(unique(dat$Participant))`$ participants in the Variant conditions. We can now treat the study as a oneway design, with three levels (Presentation: Alone, Different, or Similar) that all vary within-participants. As the cheerleader effect predicts that faces presented as part of a group will be rated as more attractive, and that this effect will be larger if there is more variety in the faces within the group, a reasonable set of contrast codes is:
```{r helmert-contrast-cheerleader}
tab <- data.frame(c1 = c("$-\\tfrac{2}{3}$","$\\tfrac{1}{3}$","$\\tfrac{1}{3}$"),
                  c2 = c("$0$","$\\tfrac{1}{2}$","$-\\tfrac{1}{2}$"))
colnames(tab) <- c("$c_1$","$c_2$")
rownames(tab) <- c("Alone","Different", "Similar")
knitr::kable(tab, booktabs=TRUE, linesep="", align="r")
```
The first contrast code reflects the expectation that a face in a group (Different or Similar) will be rated as more attractive than faces in the Alone condition. The second contrast reflects the expectation that a face surrounded by different faces will be rated as more attractive than when surrounded by more similar faces.

If we were to (wrongly!) treat all ratings as independent, and analyse the data with a regular oneway ANOVA, we would obtain the results in Table \@ref(tab:cheerleader-oneway-ANOVA-results). As you can see, this analysis indicates there is no effect of presenting a face alone or in a group.

```{r cheerleader-oneway-ANOVA-results}
codes <- cbind(c(-2/3,1/3,1/3),
               c(0,1/2,-1/2))
contrasts(dat$Item) <- codes
modg <- lm(Response ~ Item, data=dat)
opts <- options()
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modg)[1],NA,coefficients(modg)[2:3],Error=NA),rbind(car::Anova(modg, type=3)[1:2,],expand_Anova(modg, type=3)[-1,]))
rownames(tab) <- c("Intercept","Presentation","$\\quad X_1$ (D+S vs A)","$\\quad X_2$ (D vs S)", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting attractiveness ratings in the three conditions, wrongly assuming independence.", escape = FALSE, digits=c(3,0,0,2,3), booktabs=TRUE, linesep="")
options(opts)
```

<!-- If we let $Y_{i,j}$ denote a response by person $i$ in condition $j$, the errors in this model are $\hat{\epsilon}_{i,j} = Y_{i,j} - \overline{Y}_{\cdot,j}$. But if you consider Table \@ref(tab:cheerleader-self-attractiveness-table), you might realise that we can much more specific in considering what the effect of displaying a face amongst in a group is. -->

The problem with this analysis is that it ignores the (on occasion rather large) differences between participants in how attractive they find a face on average (i.e. over the three conditions). Table \@ref(tab:cheerleader-self-attractiveness-table) shows the attractiveness ratings for 10 participants in the Variant condition. You can see there that when a participant rates a face as relatively attractive when presented alone, (s)he also tends to rate the face as relatively attractive when shown in a group of faces. Some participants (e.g. participant 37) rate the face as relatively unattractive in all conditions. This indicates individual differences in how attractive people find a face. Simply put: people's tastes differ. Whilst that may be interesting in its own right, for the purposes of the experiment, we do not care about such individual differences. What we want to know is whether the attractiveness of a face increases when presented as part of a group vs when presented in isolation. To answer this question, we can't completely ignore individual differences. 

```{r cheerleader-self-attractiveness-table}

dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = Participant, names_from = Item, values_from = Response) %>%
  ungroup() %>%
  filter(row_number() <= 10) %>%
  mutate("Average" = (Alone + Different + Similar)/3, 
         "D-A" = Different - Alone,
         "S-A" = Similar - Alone) %>%
  mutate(Alone = format(Alone, digits=2, nsmall=2),
         Different = format(Different, digits=2, nsmall=2),
         Similar = format(Similar, digits=2, nsmall=2)) %>%
  knitr::kable(row.names = FALSE, col.names = c("Participant","Alone", "Different", "Similar", "Average", "D-A", "S-A"), digits=2, caption=c("Attractiveness ratings in the Alone, Different, and Similar condition for 10 participants in the Variant condition. Also shown is the average over the three ratings for each participant, as well as differences between the rating in the Different and Alone condition (D-A) and between the Similar and Alone condition (S-A)."))  

#  summarize(Response = mean(Response)) %>%
#  ggplot(aes(x=Response)) + geom_histogram(bins=10)

```

<!-- This individual variability makes the ratings within each condition also highly variable. By ignoring that we have repeated measures for each participant, the model errors are the differences between each rating in a condition and the average in that condition. --> 


## Oneway repeated-measures ANOVA

For each participant, we could consider whether the rating in e.g. the Different condition is higher than the rating in the Alone condition. These difference scores are provided in the D-A and S-A columns in Table \@ref(tab:cheerleader-self-attractiveness-table). For participant 36, the rating in the Different condition is a little lower than their rating in the Alone condition, and for participant 37, it is a little higher. These difference scores remove variability in how attracted people are to a face in general. The difference scores just reflect whether people find a face more or less attractive when presented in the context of a group of faces, compared to when presented alone. We can then simply ask whether these differences are, on average, positive (indicating increased attractiveness) or negative (indicating decreased attractiveness). We can answer this question by using a one-sample t-test on the difference scores, comparing a general intercept-only MODEL G to an even simpler MODEL R where we fix the intercept to 0. If we can reject the null hypothesis that the mean of the difference is equal to 0, that is evidence of an effect of the experimental manipulation. 

For example, we can compare attractiveness ratings between the Different and Alone conditions by, for the D-A difference scores, comparing a MODEL G 
$$\begin{aligned} (\text{D - A})_i &= Y_{i,\text{D}} - Y_{i,\text{A}} \\ &= \beta_0 + \epsilon_{\text{D-A},i} \end{aligned}$$
to a MODEL R:
$$\begin{aligned} (\text{D - A})_i &= Y_{i,\text{D}} - Y_{i,\text{A}} \\ &= 0 + \epsilon_{\text{D-A},i} \end{aligned}$$
```{r}
wdat <- dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = Participant, names_from = Item, values_from = Response) %>%
  ungroup() %>%
  mutate("Average" = (Alone + Different + Similar)/3, 
         "D_A" = Different - Alone,
         "S_A" = Similar - Alone)
aov1 <- anova(lm(`D_A`~0, data=wdat), lm(`D_A`~1, data=wdat))
aov2 <- anova(lm(`S_A`~0, data=wdat), lm(`S_A`~1, data=wdat))
```
This comparison is a test whether the mean of the D-A difference score is equal to 0. The result of this test is `r write_anova_results(aov1)`. We can therefore conclude there is evidence for a difference between the Different and Alone presentions, as would be expected from the "cheerleader effect".

Using a similar model comparison
approach for the S-A difference scores, we obtain a test result of `r write_anova_results(aov2)`. So, if we focus on differences between the conditions "within persons" (by computing differences between the conditions for each person), we find evidence that both the Different and Similar condition differ from the Alone condition. On average, participants' rating of the attractiveness of a face was `r  round(coefficients(lm(D_A~1, data=wdat))[1],2)` points higher when presented amongst a group of different faces, and `r  round(coefficients(lm(S_A~1, data=wdat))[1],2)` points higher when presented amongst a group of similar faces, compared to when the face was presented by itself. Given that attractiveness was rated on a scale between 1-100, these are not large differences, but they are statistically significant.

### Within-subjects composite scores

The idea of computing difference scores and then using these in linear models is essentially how we will test for effects of manipulations that vary within persons. To do this more generally, we will apply orthogonal contrast codes to compute such difference scores. We will refer to the resulting within-person-difference-scores as __within-subjects composite scores__. 

We will use the same contrast codes as before, but denote them as $d_j$ to separate them from between-subjects contrasts ($c_j$):
```{r}
tab <- data.frame(c1 = c("$-\\tfrac{2}{3}$","$\\tfrac{1}{3}$","$\\tfrac{1}{3}$"),
                  c2 = c("$0$","$\\tfrac{1}{2}$","$-\\tfrac{1}{2}$"))
colnames(tab) <- c("$d_1$","$d_2$")
rownames(tab) <- c("Alone","Different", "Similar")
knitr::kable(tab, booktabs=TRUE, linesep="", align="r")
```
<!-- The first contrast code reflects the expectation that a face in a group (Different or Similar) will be rated as more attractive than faces in the Alone condition. The second contrast reflects the expectation that a face surrounded by different faces will be rated as more attractive than when surrounded by more similar faces.-->

For each within-subjects contrast $d_j$, we compute a within-subjects composite score as:
\begin{equation}
W_{j,i} = \frac{\sum_{k=1}^g d_{j,k} Y_{i,k}}{\sqrt{\sum_{k=1}^g d_{j,k}^2}}
(\#eq:within-subjects-composite-scores-ch08)
\end{equation}
The top part of this equation (the numerator) is just the sum of a participant $i$'s score in each condition $k$ multiplied by the corresponding value of contrast code $d_j$. The bottom part (the denominator) is a scaling factor, computed as the square-root of the sum of the squared contrast values. The reason for applying this scaling factor is to make the sums of squares of the resulting analyses add up to the total sum of squares (i.e. the Sum of Squared Error of an intercept-only model). Otherwise, it is not of theoretical importance. 

As an example, let's compute the within-subjects composite scores for participant 36 in Table \@ref(tab:cheerleader-self-attractiveness-table). For contrast $d_1$, we compute
$$\begin{aligned}
W_{1,36} &= \frac{-\tfrac{2}{3} \times 56.32 + \tfrac{1}{3} \times 55.92 + \tfrac{1}{3} \times 54.30}{\sqrt{ \left(-\tfrac{2}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2)}} \\
&= \frac{-0.81}{\sqrt{\tfrac{6}{9}}} = `r round((-(2/3)*56.32 + (1/3)*55.92 + (1/3)*54.3)/sqrt(6/9),2)`
\end{aligned}$$
For contrast $d_2$, the within-subjects composite is computed as:
$$\begin{aligned}
W_{2,36} &= \frac{\tfrac{1}{2} \times 55.92 + (-\tfrac{1}{2}) \times 54.30}{\sqrt{\left(\tfrac{1}{2}\right)^2 + \left(-\tfrac{1}{2}\right)^2)}} \\
&= \frac{-0.81}{\sqrt{\tfrac{2}{4}}} = `r round(((1/2)*55.92 + (-1/2)*54.3)/sqrt(2/4),2)`
\end{aligned}$$
Table \@ref(tab:cheerleader-self-composite-table) shows the resulting values for other participants as well.
```{r cheerleader-self-composite-table}
#options(pillar.sigfig = 2)
dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = Participant, names_from = Item, values_from = Response) %>%
  ungroup() %>%
  filter(row_number() <= 10) %>%
  mutate("W0" = (Alone + Different + Similar)/sqrt(3), 
         "W1" = (-(2/3)*Alone + (1/3)*Different + (1/3)*Similar)/sqrt((2/3)^2 + (1/3)^2 + (1/3)^2),
         "W2" = (-(1/2)*Similar + (1/2)*Different)/sqrt(1/2)) %>%
  mutate(Alone = format(Alone, digits=2, nsmall=2),
         Different = format(Different, digits=2, nsmall=2),
         Similar = format(Similar, digits=2, nsmall=2)) %>%
  knitr::kable(row.names = FALSE, col.names = c("Participant","Alone", "Different", "Similar", "$W_0$", "$W_1$", "$W_2$"), digits=c(0,2,2,2,2,2,2), caption=c("Attractiveness ratings in the Alone, Different, and Similar condition for 10 participants in the Variant condition. Also shown are three within-subjects composite scores."))  

```

```{r}
wdat <- dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = Participant, names_from = Item, values_from = Response) %>%
  ungroup() %>%
  mutate(#"W0" = ((1/3)*Alone + (1/3)*Different + (1/3)*Similar)/sqrt(1/3), 
         "W0" = (1*Alone + 1*Different + 1*Similar)/sqrt(3), 
         "W1" = (-(2/3)*Alone + (1/3)*Different + (1/3)*Similar)/sqrt((2/3)^2 + (1/3)^2 + (1/3)^2),
         "W2" = (-(1/2)*Similar + (1/2)*Different)/sqrt(1/2))
aov0 <- anova(lm(W0~0, data=wdat), lm(W0~1, data=wdat))
aov1 <- anova(lm(W1~0, data=wdat), lm(W1~1, data=wdat))
aov2 <- anova(lm(W2~0, data=wdat), lm(W2~1, data=wdat))
```

The first within-subjects composite variable ($W_1$) reflects the difference between the average of the Different and Similar conditions and the Alone condition. If the mean of this composite variable is positive, that indicates that faces in the Different and Similar condition are on average rated as more attractive than in the Alone condition. If the mean of this composite variable is negative, that indicates that faces in the Different and Similar condition are on average rated as less attractive than in the Alone condition. If the mean is equal to 0, that indicates there is no difference in attractiveness ratings between the marginal mean of the Different and Similar conditions, compared to the Alone condition. To test whether this latter option is truem we can compare a MODEL G
$$W_{1,i} = \beta_0 + \epsilon_i$$
to a MODEL R:
$$W_{1,i} = 0 + \epsilon_i$$
The Sum of Squared Error of MODEL R is $\text{SSE}(R) = `r round(sum(residuals(lm(W1~0, data=wdat))^2),2)`$, and for MODEL G this is $\text{SSE}(G) = `r round(sum(residuals(lm(W1~1, data=wdat))^2),2)`$. There are $n=`r nrow(wdat)`$ participants, and $\text{npar}(R) = 0$ and $\text{npar}(G) = 1$. The test result of the model comparison is therefore `r write_anova_results(aov1)`. We can thus reject the null hypothesis that there is no difference between the Alone condition and the marginal mean of the other two conditions. The estimated intercept of MODEL G is $\hat{\beta}_0 = `r round(coefficients(lm(W1~1, data=wdat))[1],2)`$. Due to the scaling applied in the within-subjects composite, this intercept does _not_ equal the average of $\frac{Y_{i,D} + Y_{i,S}}{2} - Y_{i,A}$. To get this value, we need to __rescale__ the within-subjects composite to the scale of the dependent variable. We do this by dividing the estimated parameter by the scaling factor. The scaling factor equals $\sqrt{ \left(-\tfrac{2}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2} = \sqrt{\tfrac{6}{9}}$. So
$$\frac{\overline{Y}_{D} + \overline{Y}_{S}}{2} - \overline{Y}_{A} = \frac{`r round(coefficients(lm(W1~1, data=wdat))[1],2)`}{\sqrt{\tfrac{6}{9}}} = `r round(coefficients(lm(W1~1, data=wdat))[1]/sqrt(6/9),2)`$$
We can conduct a similar model comparison for the second within-subjects composite, $W_2$. The Sum of Squared Error of MODEL R is $\text{SSE}(R) = `r round(sum(residuals(lm(W2~0, data=wdat))^2),2)`$, and for MODEL G this is $\text{SSE}(G) = `r round(sum(residuals(lm(W2~1, data=wdat))^2),2)`$. The results of this comparison are then `r write_anova_results(aov2)`. Hence, we can not reject the null hypothesis that there is no difference between the Different and Alone condition. The estimated intercept of MODEL G is $\hat{\beta}_0 = `r round(coefficients(lm(W2~1, data=wdat))[1],2)`$. Rescaling this to the scale of the dependent variable indicates that
$$\overline{Y}_{D} - \overline{Y}_{S} = \frac{`r round(coefficients(lm(W2~1, data=wdat))[1],2)`}{\sqrt{\tfrac{2}{4}}} = `r round(coefficients(lm(W2~1, data=wdat))[1]/sqrt(2/4),2)`$$

Comparing the results to the oneway ANOVA of Table \@ref(tab:cheerleader-oneway-ANOVA-results) shows that we get the same parameter estimates (after rescaling), but different test results. That is because the analyses using within-subjects composite scores consider the effects of the within-subjects effects regardless of whether someone is generally attracted to that face or not. In the oneway ANOVA, a substantial part of the variation within the conditions is due to individual differences in how attracted people are to a particular face. Some participants gave higher attractiveness ratings (regardless of condition) than other participants (see Table \@ref(tab:cheerleader-self-attractiveness-table)). This relatively high variability within the conditions can't be "explained" in the oneway ANOVA. This leads to relatively large errors, and with that relatively low power for the tests of condition. If we know that one participant is relatively more attracted to a face than other participants, we could use that knowledge to make better predictions of their attractiveness ratings. This is, roughly, what repeated-measures ANOVA is about: separating individual differences in average scores (over all within-subjects conditions) from effects of the within-subjects manipulations. 

### A composite for between-subjects effects

In the analyses above, we did not consider individual differences in how attractive people found a face in general. For each participant, we can compute an average attractiveness rating over all conditions as:

$$\overline{Y}_{i,\cdot} = \frac{Y_{i,A} + Y_{i,D} + Y_{i,S}}{3}$$
(see Table \@ref(tab:cheerleader-self-attractiveness-table)). Variation in these averages reflects variation between participants. 

To analyze this variation between participants, we can use another composite score by applying a special "contrast" $d_0 = (1, 1, 1)$. This is not really a contrast in the usual sense, as it does not compare conditions. Also, you don't have freedom in choosing the values: you have to use a 1 for each condition to make this work in the same way as the within-subjects composite scores.^[You might think that it would make sense to use the values $\left(\tfrac{1}{3}, \tfrac{1}{3}, \tfrac{1}{3}\right)$. Whilst this would give you exactly the same computed values of $W_{0,i}$, rescaling back to $Y$ from $\sqrt{\left(\tfrac{1}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2 + \left(\tfrac{1}{3}\right)^2}$ does not work then. You have to rescale from $\sqrt{3}$.] Plugging these values into Equation \@ref(eq:within-subjects-composite-scores-ch08) provides us with the $W_0$ composite score. For example, the computation of this score for participant 36 is:
$$\begin{aligned}
W_{0,36} &= \frac{56.32 + 55.92 + 54.30}{\sqrt{ 1^2 + 1^2 + 1^2 }} \\
&= \frac{166.54}{\sqrt{3}} = `r round((56.32 + 55.92 + 54.3)/sqrt(3),2)`
\end{aligned}$$
Values for the other participants are provided in Table \@ref(tab:cheerleader-self-composite-table). Like the values of the $W_1$ and $W_2$ composite scores, these are _scaled_ values to ensure that the Sums of Squares add up appropriately. But you can think of them conceptually as averages, for each participant, over the three conditions.

We can apply a similar analysis to these $W_0$ scores as for $W_1$ and $W_2$, comparing a MODEL G
$$W_{0,i} = \beta_0 + \epsilon_i$$
to a MODEL R:
$$W_{0,i} = 0 + \epsilon_i$$
The Sum of Squared Error of MODEL R is $\text{SSE}(R) = `r format(sum(residuals(lm(W0~0, data=wdat))^2),scientific=FALSE)`$, and for MODEL G this is $\text{SSE}(G) = `r round(sum(residuals(lm(W0~1, data=wdat))^2),2)`$. The results of this comparison are then `r write_anova_results(aov0)`. Hence, we can reject the null hypothesis that the average rating of attractiveness is equal to 0. The estimated intercept of MODEL G is $\hat{\beta}_0 = `r round(coefficients(lm(W0~1, data=wdat))[1],2)`$. Rescaling this to the scale of the dependent variable indicates that
$$\overline{Y} = \frac{`r round(coefficients(lm(W0~1, data=wdat))[1],2)`}{\sqrt{3}} = `r round(coefficients(lm(W0~1, data=wdat))[1]/sqrt(3),2)`$$
Note that this is equal to the intercept of the oneway ANOVA in Table \@ref(tab:cheerleader-oneway-ANOVA-results). As in that model, a test of the hypothesis that the intercept equals 0 is not overly interesting. But when we consider between-subjects effects, the composite score $W_0$ is crucial. 

Presently, a main realisation is that through the models for the three composite scores ($W_0$, $W_1$, and $W_2$), we can compute SSR terms which together add up to the "total SS". The "total SS" is the Sum of Squared Error of the intercept-only model:
$$Y_{i,j} = \beta_0 + \epsilon_{i,j}$$
(i.e. a model which makes a single prediction all the observations, both over participants and over conditions).

### Collecting all results and omnibus tests

Table \@ref(tab:rm-anova-model-summary) summarizes the results we have obtained thus far. This is admittedly not an easy table to read. But it does highlight some important aspects of repeated-measures ANOVA. So let's give it a go. The first thing to remember is that we have just performed three different model comparisons, one for $W_0$, one for $W_1$, and one for $W_2$. Each of these model comparisons used a different dependent variable, and therefore each had a different $\text{SSE}(G)$. The Sum of Squares attributable to an effect is simply the difference between these SSE terms: $\text{SSR} = \text{SSE}(R) - \text{SSE}(G)$. The degrees of freedom associated to this Sum of Squares Reduced is $\text{df}_1 = \text{npar}(G) - \text{npar}(R)$, i.e. the number of additional parameters in MODEL G used to reduce the SSE. The degrees of freedom associated to $\text{SSE}(G)$ is $\text{df}_2 = n - \text{npar}(G)$. For each model comparison, the $F$-statistic is, as usual
$$F = \frac{\text{SSR}/\text{df}_1}{\text{SSE}(G)/\text{df}_2}$$
The resulting statistics are provided in Table \@ref(tab:rm-anova-model-summary). For easy comparison, the results of the oneway ANOVA are repeated in Table \@ref(tab:cheerleader-oneway-ANOVA-results-2).

```{r rm-anova-model-summary}
opts <- options()
options(knitr.kable.NA = "")
tab <- data.frame(beta = c(coefficients(lm(W0~1, data=wdat))[1]/sqrt(3),
                           coefficients(lm(W1~1, data=wdat))[1]/sqrt(6/9),
                           coefficients(lm(W2~1, data=wdat))[1]/sqrt(1/2)),
           SSER = c(sum(residuals(lm(W0~0, data=wdat))^2),
                   sum(residuals(lm(W1~0, data=wdat))^2),
                   sum(residuals(lm(W2~0, data=wdat))^2)),
           SSEG = c(sum(residuals(lm(W0~1, data=wdat))^2),
                   sum(residuals(lm(W1~1, data=wdat))^2),
                   sum(residuals(lm(W2~1, data=wdat))^2))) %>%
  mutate(SSR = SSER-SSEG)
tab$df1 <- c(1,1,1)
tab$df2 <- rep(nrow(wdat) - 1,3)
tab <- tab %>%
  mutate(F = SSR/(SSEG/df2))
tab <- rbind(tab,c(NA, colSums(tab[1,2:6]), NA), c(NA, colSums(tab[2:3,2:6]), NA), c(NA, colSums(tab[2:6]), NA))
rownames(tab) <- c("$W_0$","$W_1$","$W_2$", "Sum Between ($W_0$)", "Sum Within ($W_1 + W_2$)", "Sum Total ($W_0 + W_1 + W_2$)")
tab %>%
  knitr::kable(col.names=c("$\\hat{\\beta}$","$\\text{SSE}(R)$", "$\\text{SSE}(G)$", "$\\text{SSR}$", "$\\text{df}_1$", "$\\text{df}_2$", "$F$"), escape=FALSE, booktabs=TRUE, digits=c(2,0,0,0,0,0,2), caption="(Rescaled) parameter estimates, model comparison SS terms, degrees of freedom, and $F$ values, for the three composite scores. The last three rows show different summations over these, where relevant.")
options(opts)
```

```{r cheerleader-oneway-ANOVA-results-2}
modg <- lm(Response ~ Item, data=dat)
opts <- options()
options(knitr.kable.NA = "")
tab <- cbind(estimate = c(coefficients(modg)[1],NA,coefficients(modg)[2:3],Error=NA),rbind(car::Anova(modg, type=3)[1:2,],expand_Anova(modg, type=3)[-1,]))
rownames(tab) <- c("Intercept","Presentation","$\\quad X_1$ (D+S vs A)","$\\quad X_2$ (D vs S)", "Error")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Linear model predicting attractiveness ratings in the three conditions, wrongly assuming independence.", escape = FALSE, digits=c(3,0,0,2,3), booktabs=TRUE, linesep="")
options(opts)
```

Comparing the results between the oneway ANOVA which wrongly assumed independence, and the results of the repeated-measured ANOVA, you can see that the parameter estimates and the SSR terms for the effects are identical between the two. However, the values of the $F$-statistic are different. The main reason for this is that the oneway ANOVA compares different MODEL Rs to the _same_ MODEL G. In other words, the $\text{SSE}(G)$ term is the same for each test. In the repeated-measures ANOVA with composite scores, each test uses a different MODEL G, and hence a different $\text{SSE}(G)$ term. The sum of these three SSE terms is equal to the SS Error term in the oneway ANOVA (which equals 9127). But each test uses a different part of this overall error. As the SSR terms are the same between the two analyses, that implies that the relative value of the SSR terms compared to the SSE term(s) is _larger_ in the repeated-measures ANOVA. This will increase $F$-values and the power of the tests. However, by splitting the analysis into different models for the between-subjects effects and within-subjects contrasts, the $\text{df}_2$ values are smaller in the repeated-measures ANOVA. This in turn reduces the $F$ values and decreases the power of the tests. Although it would be wrong to say that you can choose whichever analysis provides you the highest $F$ values (the assumptions of a standard ANOVA are generally violated with repeated-measures data), understanding how the Sums of Squares Reduced (SSR), Sums of Squared Errors (SSE), and degrees of freedom (df) terms relate between the two analyses is important in understanding the nuances of these different approaches.

```{r}
tab <- data.frame(beta = c(coefficients(lm(W0~1, data=wdat))[1]/sqrt(3),
                           coefficients(lm(W1~1, data=wdat))[1]/sqrt(6/9),
                           coefficients(lm(W2~1, data=wdat))[1]/sqrt(1/2)),
           SSER = c(sum(residuals(lm(W0~0, data=wdat))^2),
                   sum(residuals(lm(W1~0, data=wdat))^2),
                   sum(residuals(lm(W2~0, data=wdat))^2)),
           SSEG = c(sum(residuals(lm(W0~1, data=wdat))^2),
                   sum(residuals(lm(W1~1, data=wdat))^2),
                   sum(residuals(lm(W2~1, data=wdat))^2))) %>%
  mutate(SSR = SSER-SSEG)
tab$df1 <- c(1,1,1)
tab$df2 <- rep(nrow(wdat) - 1,3)
tab <- tab %>%
  mutate(F = SSR/(SSEG/df2))
```

There is one test in the oneway ANOVA that we haven't yet considered an alternative for: the omnibus test for Presentation. It is possible to conduct such a test, by aggregating results over the within-subjects composites $W_1$ and $W_2$. Like the contrasts in the oneway ANOVA ($c_1$ and $c_2$), the set of within-subjects contrasts ($d_1$ and $d_2$) together allow you to reconstruct all differences between the (marginal) means of conditions. By fixing both $\beta_0$ in MODEL G for $W_1$ and MODEL G for $W_2$ to $\beta_0 = 0$), we obtain two models which together imply that there are no differences between these (marginal) means. A complication of such aggregation over models is that we need an additional assumption to the standard assumptions of the GLM. This assumption is called sphericity, and we will discuss it later. For the moment, let's focus on how you would compute this omnibus test. Computationally, it is actually straightforward: we compute the omnibus SSR, SSE, $\text{df}_1$, and $\text{df}_2$ terms, simply by summing for each of these the respective terms for $W_1$ and $W_2$. So the $F$-statistic for the omnibus test of Presentation can be computed as

$$\begin{aligned} 
F &= \frac{\text{SSR}/\text{df}_1}{\text{SSE}(G)/\text{df}_2} \\
&= \frac{(`r round(tab[2,4],0)` + `r round(tab[3,4],0)`)/(1+1)}{(`r round(tab[2,3],0)` + `r round(tab[3,3],0)`)/(30 + 30)} \\
&= `r round(((tab[2,4] + tab[3,4])/(2))/((tab[2,3] + tab[3,3])/(30 + 30)),2)`
\end{aligned}$$
To determine the $p$-value, we can compute the exceedance probability of this value in an $F$-distribution with $\text{df}_1 = 2$ and $\text{df}_2 = 60$, and find that $p < .001$. Hence, the omnibus test is significant. But, as I said before, this omnibus test rests on a new assumption of sphericity. We will discuss this after generalizing a repeated-measures ANOVA to a so-called "mixed design", with some experimental manipulations that vary within, and other manipulations that vary between participants. Before we take this step, let's summarize what we have done so far. Table \@ref(tab:collated-results-oneway-rm-anova-cheerleader) collates the results of all previous analyses. 
```{r collated-results-oneway-rm-anova-cheerleader}
wdat <- dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = Participant, names_from = Item, values_from = Response) %>%
  ungroup() %>%
  mutate(#"W0" = ((1/3)*Alone + (1/3)*Different + (1/3)*Similar)/sqrt(1/3), 
         "W0" = (1*Alone + 1*Different + 1*Similar)/sqrt(3), 
         "W1" = (-(2/3)*Alone + (1/3)*Different + (1/3)*Similar)/sqrt((2/3)^2 + (1/3)^2 + (1/3)^2),
         "W2" = (-(1/2)*Similar + (1/2)*Different)/sqrt(1/2))

mod0 <- lm(W0 ~ 1, data=wdat)
mod1 <- lm(W1 ~ 1, data=wdat)
mod2 <- lm(W2 ~ 1, data=wdat)

tab0 <- cbind(estimate = c(coefficients(mod0)/sqrt(3),Error=NA),car::Anova(mod0, type=3))
tab1 <- cbind(estimate = c(coefficients(mod1)/sqrt(1/3),Error=NA),car::Anova(mod1, type=3))
tab2 <- cbind(estimate = c(coefficients(mod2)/sqrt(1/2),Error=NA),car::Anova(mod2, type=3))

atab <- rbind(
  c(NA, sum(tab0[,2]), sum(tab0[,3]), NA, NA),
  tab0,
  c(NA, sum(c(tab1[,2],tab2[,2])), sum(c(tab1[,3],tab2[,3])), NA, NA),
  tab1,
  tab2,
  c(NA, sum(c(tab0[,2], tab1[,2], tab2[,2])), sum(c(tab0[,3], tab1[,3], tab2[,3])), NA, NA)
)
rownames(atab) <- c("*Between-subjects*", "&nbsp;&nbsp;Intercept", "&nbsp;&nbsp;Error between", "*Within-subjects*", "&nbsp;&nbsp;$d_1$ (D + S vs A)", "&nbsp;&nbsp;Error ($d_1$)", "&nbsp;&nbsp;$d_2$ (D vs S)", "&nbsp;&nbsp;Error ($d_2$)", "*Total*")
colnames(atab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$",  "$P(\\geq F)$")
opts <- options()
options(knitr.kable.NA = "")
knitr::kable(atab, caption="Complete results of the oneway (Presentation: Alone, Different, Similar) repeated-measures ANOVA for the data in the Variant condition.", digits=c(3,0,0,2,3), escape = FALSE, booktabs = TRUE, linesep="")
options(opts)
```


## Partitioning the variance

The crux of repeated-measures ANOVA is that we can separate between-subjects effects (e.g. individual differences) from within-subjects effects. This means we can separate the total variance in $Y_{j,i}$ scores into parts that are due to between-subjects effects (e.g. individual differences) and within-subjects effects (e.g. differences due to within-subjects manipulations). This often provides more powerful tests, at least for the within-subjects effects. A graphical representation of the partitioning of the total variation into the between and within parts is provided in Figure \@ref(fig:partitioning-variance-rm-anova).

<!-- TODO: why is svg not working well with fonts? -->

```{tikz partitioning-variance-rm-anova, fig.cap="Partitioning the total Sum of Squares in a oneway repeated-measures ANOVA.", dev="png", dev.args=list(pointsize=12)}
  \usetikzlibrary{arrows}
	\begin{tikzpicture}
	\pgfsetxvec{\pgfpoint{2cm}{0cm}}
	\pgfsetyvec{\pgfpoint{0cm}{2cm}}
  \tikzstyle{every node}=[minimum size=2cm, text width=8em, text badly centered, minimum height=4em]

  \draw node at (0,-.5) (SST) {SS Total};
  \draw node at (-2,-2) (SSB) {SS Between};
  \draw node at (2,-2) (SSW) {SS Within};
  \draw node at (1,-3.5) (SSt) {SS Presentation};
  \draw node at (0.5,-4.5) (SSd1) {SS $d_1$};
  \draw node at (1.5,-4.5) (SSd2) {SS $d_2$};
  \draw node at (3, -3.5) (SSE) {SSE Within};
  \draw node at (2.5,-4.5) (SSEd1) {SSE $d_1$};
  \draw node at (3.5,-4.5) (SSEd2) {SSE $d_2$};

	\draw[->] (SST) -- (SSB);
  \draw[->] (SST) -- (SSW);
  \draw[->] (SSW) -- (SSt);
  \draw[->] (SSW) -- (SSE);
  \draw[->] (SSt) -- (SSd1);
  \draw[->] (SSt) -- (SSd2);
  \draw[->] (SSE) -- (SSEd1);
  \draw[->] (SSE) -- (SSEd2);

	\end{tikzpicture}
```

## A mixed ANOVA with between- and within-subjects effects

```{r}
dat <- subset(cheerleader, Excluded == 0) %>%
  mutate(Item = factor(Item)) %>%
  mutate(Item = forcats::fct_recode(Item, Different = "Control_Group", Similar = "Distractor_Manipulation"),
         Task = forcats::fct_recode(Task, Identical = "Identical-Distractors", Variant = "Self-Distractors"))
```

Up to now, we just considered the data from the Variant conditions. Having worked out how to perform effective comparisons of the within-subjects conditions, we are now in a position to consider the data from the whole experiment. Whether the other faces in the Similar condition where exact copies of a single photograph, or different photographs of the same person, was a manipulation that varied _between participants_ (i.e., a participant was only assigned to one of these manipulations, not both). The full design of the study is thus a 2 (Version: Identical, Variant) by 3 (Presentation: Alone, Different, Similar) design, where the first factor (Version) varied between, and the last factor (Presentation) varied within participants.

Just like in a "normal" factorial ANOVA, we would like to consider main effects of, and the interaction between, these two experimental manipulations. And just like in a "normal" factorial ANOVA, we will focus on defining contrasts for the main effects, and let the interactions follow from these. We have already defined our $d_j$ contrasts for the within-subjects effects. There is no need to change these when considering the full experiment, as we still would expect the attractiveness ratings to be different for the Alone conditions compared to the Different and Similar conditions. And while the theory would predict no difference between the Alone condition and the Similar condition when the Version is Identical, we could expect a higher attractiveness rating when the Version is Variant. So, aggregating over the levels of Version, we could still expect a higher attractiveness rating for the Similar conditions. So we just have to define a suitable contrast for the Identical and Variant manipulation. This manipulation only affected the nature of the Similar conditions, and as already indicated, according to the theory, we should expect a higher attractiveness rating when the Version is Variant compared to Identical. So a reasonable contrast code for our single between-subjects manipulation is $c_1 = (-\tfrac{1}{2},\tfrac{1}{2})$ for the Identical and Variant levels respectively.

Having defined the contrast codes for the main effects, we would normally proceed by defining product-contrasts to reflect the interactions. But in this mixed design, the within-subjects contrasts ($d$) are used to transform a set of correlated dependent variables ($Y$) in a set of orthogonal dependent variables ($W$), whilst the between-subjects contrast ($c$) are used to compare different subsets for each of these dependent variables (i.e. because some of the $W$ values are obtained for the Version: Identical manipulation, and the remainder for the Version: Variant manipulation). Rather than computing product-contrasts, what we will do now is to consider the effect of a contrast-coded predictor $X_1$ (which takes its values from $c_1$) on our three composite variables. As $W_0$ effectively encodes the marginal mean over all within-subjects conditions, the effect of this contrast-coded predictor on $W_0$ is equal to a main effect of the between-subjects manipulation. As the within-subjects composite scores encode differences between the within-subjects conditions, an effect of the between-subjects manipulation on such differences is identical to an interaction: The effect of within-subjects manipulations is moderated by the between-subjects manipulation.

To make this less abstract, let's apply this idea now. Table \@ref(tab:cheerleader-all-composite-table) shows the values of the composite variables (computed in the same way as before), as well as the contrast-coded predictor ($X$) which codes for Version.
```{r cheerleader-all-composite-table}
#options(pillar.sigfig = 2)
dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = c("Participant", "Task"), names_from = Item, values_from = Response) %>%
  ungroup() %>%
  group_by(Task) %>%
  filter(row_number() <= 5) %>%
  mutate(X = ifelse(Task == "Identical", -0.5, 0.5)) %>%
  mutate("W0" = (Alone + Different + Similar)/sqrt(3), 
         "W1" = (-(2/3)*Alone + (1/3)*Different + (1/3)*Similar)/sqrt((2/3)^2 + (1/3)^2 + (1/3)^2),
         "W2" = (-(1/2)*Similar + (1/2)*Different)/sqrt(1/2)) %>%
  mutate(Alone = format(Alone, digits=2, nsmall=2),
         Different = format(Different, digits=2, nsmall=2),
         Similar = format(Similar, digits=2, nsmall=2)) %>%
  knitr::kable(row.names = FALSE, col.names = c("Participant","Version","Alone", "Different", "Similar", "$X_1$", "$W_0$", "$W_1$", "$W_2$"), digits=c(0,2,2,2,2,2,2,2,2), caption=c("Attractiveness ratings in the Alone, Different, and Similar condition for 5 participants in the Identical and 5 participants in the Variant condition. Also shown are three within-subjects composite scores."))  

```

Let's start with the tests for $W_0$, the composite variable for between-subjects effects. We formulate a MODEL G as:
$$W_{0,i} = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_{i}$$
The main effect of between-subjects contrast $c_1$ is tested by comparing this model to a MODEL R:
$$W_{0,i} = \beta_0 + \epsilon_{i}$$
A test of the hypothesis that $\beta_0 = 0$ is, as usual, obtained by comparing MODEL G to an alternative MODEL R:
$$W_{0,i} = \beta_1 \times X_{1,i} + \epsilon_{i}$$
The results of these two model comparisons are provided in Table \@ref(tab:cheerleader-W0-ANOVA-table). We can see that the test of the intercept is significant, tells us that the grand mean of the attractiveness ratings is not likely to equal 0. More interesting is the test of $X_1$, which compares the marginal means of the attractiveness ratings between the Variant and Identical versions. This test is not significant. Hence, the main effect of Version is not significant: aggregating over the levels of Presentation, there are no differences between the two levels of Version. This is perhaps not overly surprising, as the Version manipulation only concerned the identity of the Similar presentation (whether identical photos or different photos of the same face). The Alone and Different presentations were the same between the Variant and Identical presentation conditions. Therefore, any effect of Version should only affect the attractiveness ratings in the Similar presentation. If the effect on Similar is large enough, this might also show as a difference in the average over Alone, Different, and Similar. But we see here that this is not the case.

```{r}
wdat <- dat %>%
  group_by(Participant) %>%
  tidyr::pivot_wider(id_cols = c("Participant", "Task"), names_from = Item, values_from = Response) %>%
  ungroup() %>%
  mutate(X = ifelse(Task == "Identical", -0.5, 0.5)) %>%
  mutate("W0" = (Alone + Different + Similar)/sqrt(3), 
         "W1" = (-(2/3)*Alone + (1/3)*Different + (1/3)*Similar)/sqrt((2/3)^2 + (1/3)^2 + (1/3)^2),
         "W2" = (-(1/2)*Similar + (1/2)*Different)/sqrt(1/2))
```

```{r cheerleader-W0-ANOVA-table}
mod <- lm(W0 ~ X, data=wdat)
opts <- options()
options(knitr.kable.NA = "")
tab0 <- cbind(estimate = c(coefficients(mod)/sqrt(3),Error=NA),car::Anova(mod, type=3))
rownames(tab0) <- c("Intercept","$X_1$ (Variant vs Identical)","Error")
colnames(tab0) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$",  "$P(\\geq F)$")
knitr::kable(tab0, caption = "Results of the model for $W_0$, testing for between-subjects effects.", digits=c(3,0,0,2,3), escape = FALSE, booktabs = TRUE, linesep="")
options(opts)
```

We now turn to the analysis of the first within-subjects composite variable, $W_1$. Remember that this variable encodes the first within-subjects contrast $d_1$, which compares the  Different and Similar conditions to the Alone condition. We formulate a MODEL G as:
$$W_{1,i} = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_{i}$$
As when we conducted a oneway repeated-measures ANOVA, a difference between the Alone vs Different and Similar conditions would show itself through a non-zero intercept. Hence, the main effect of the within-subjects contrast $d_1$ is tested by comparing this model to a MODEL R where we fix the intercept to 0:
$$W_{1,i} = \beta_1 \times X_{1,i} + \epsilon_{i}$$
Note that this model allows the value of $W_1$ to be non-zero through the effect of $X_1$. Crucially, however, as we are using a sum-to-zero contrast in the construction of $X_1$ (i.e. the values in $c_1 = (-\tfrac{1}{2}, \tfrac{1}{2})$ sum to 0), the intercept represents the marginal mean of $W_1$ over the Identical and Variant versions. In other words, it represents the midpoint of the Identical and Variant versions, or the average effect of the within-subjects contrast $d_1$ over the levels of the between-subjects factor.

To test whether the effect of $d_1$ is moderated by $c_1$, we compare MODEL G to an alternative MODEL R:
$$W_{1,i} = \beta_0 + \epsilon_{i}$$
If the D + S vs A difference varies over the Identical and Variant groups, then the value of $W_1$ would be different over these groups. If the D + S vs A difference is not affected by Version, then this last MODEL R would be just as good as MODEL G.

The results of these two model comparisons are provided in Table \@ref(tab:cheerleader-W1-ANOVA-table). We see a significant and positive intercept. In this model, the intercept is highly relevant, as it reflects a main effect of the $d_1$ contrast. We thus find evidence that the attractiveness ratings are on average higher in the Different and Similar conditions, compared to the Alone condition. In other words, when presented in the context of a group of faces, a face is rated as more attractive then when presented alone. The test for the slope of $X_1$, which reflects an interaction between $d_1$ and $c_1$, is not significant. Hence, there is no evidence that the D + S vs A contrast is moderated by Version.

```{r cheerleader-W1-ANOVA-table}
mod <- lm(W1 ~ X, data=wdat)
opts <- options()
options(knitr.kable.NA = "")
tab1 <- cbind(estimate = c(coefficients(mod)/sqrt(6/9),Error=NA),car::Anova(mod, type=3))
rownames(tab1) <- c("Intercept","$X_1$ (Variant vs Identical)","Error")
colnames(tab1) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$",  "$P(\\geq F)$")
knitr::kable(tab1, caption = "Results of the model for $W_1$, testing for the within-subjects difference between the Different and Similar vs Alone presentations, and the interaction of this effect with Version.", digits=c(3,2,0,2,3), escape = FALSE, booktabs = TRUE, linesep="")
options(opts)
```

The procedure for $W_2$ is the same as for $W_1$. The results of the two model comparisons are provided in Table \@ref(tab:cheerleader-W2-ANOVA-table). Here, we see that the intercept is not significant. Hence, there is no evidence of a main effect of $d_2$, which compares the Different presentation to the Similar presentation. But the slope of $X_1$, which reflects the $d_2$ by $c_1$ interaction, is significant and estimated as negative. There is thus evidence that the difference between the Different and Similar presentation varies over the two versions of the Similar presentation.
```{r cheerleader-W2-ANOVA-table}
mod <- lm(W2 ~ X, data=wdat)
opts <- options()
options(knitr.kable.NA = "")
tab2 <- cbind(estimate = c(coefficients(mod)/sqrt(1/2),Error=NA),car::Anova(mod, type=3))
rownames(tab2) <- c("Intercept","$X_1$ (Variant vs Identical)","Error")
colnames(tab2) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$",  "$P(\\geq F)$")
knitr::kable(tab2, caption = "Results of the model for $W_2$, testing for the within-subjects difference between the Different and Similar vs Alone presentations, and the interaction of this effect with Version.", digits=c(3,2,0,2,3), escape = FALSE, booktabs = TRUE, linesep="")
options(opts)
```
To interpret the $d_2$ by $c_1$ interaction, we can The predicted D - S difference for the Variant condition is:
$$\hat{W}_{2,V} = \hat{\beta}_0 + \hat{\beta}_1 \times X_{1,i} = `r round(coefficients(mod)[1],3)` + `r round(coefficients(mod)[2],3)` \times \tfrac{1}{2} = `r round(coefficients(mod)[1] + coefficients(mod)[2]*.5,3)`$$
For the Identical condition, it is 
$$\hat{W}_{2,I} = `r round(coefficients(mod)[1],3)` + `r round(coefficients(mod)[2],3)` \times (-\tfrac{1}{2}) = `r round(coefficients(mod)[1] - coefficients(mod)[2]*.5,3)`$$
Thus, there appears to be a relatively small difference between presenting a face in the context of a group if different faces, or a group of different images of the same face (Variant version). However, compared to presenting a face within a group of identical photos of the same face (Identical version), a face presented within a group of dissimilar faces is rated as more attractive. This is consistent with the hierarchical encoding hypothesis.

We have considered the effect of Presentation, and the Version by Presentation interaction, through two separate contrasts ($d_1$ and $d_2$). We can also perform omnibus tests of these effects. This is done by summing the relevant SSR, SSE, and df terms. For the main effect of Presentation, we sum the SS terms for the intercept in Table \@ref(tab:cheerleader-W1-ANOVA-table) and \@ref(tab:cheerleader-W2-ANOVA-table) to obtain an omnibus SSR for Presentation: 
$$\text{SSR}(\text{Presentation}) = `r round(tab1[1,2],2)` + `r round(tab2[1,2],2)` = `r round(tab1[1,2] + tab2[1,2],2)`$$
The error term for this test is computed by summing the SS Error values in these tables:
$$\text{SSE}(\text{Presentation}) = `r round(tab1[3,2],2)` + `r round(tab2[3,2],2)` = `r round(tab1[3,2] + tab2[3,2],2)`$$
The value of $\text{df}_1$ is the sum of the df terms for the intercept ($\text{df}_1 = 1 + 1 = 2$), and the value of $\text{df}_2$ is the sum of the df terms for the Error ($\text{df}_2 = 57 + 57 = 114)$. So the resulting $F$-statistic is
$$F = \frac{`r round(tab1[1,2] + tab2[1,2],2)`/2}{`r round(tab1[3,2] + tab2[3,2],2)`/114} = `r round( ((tab1[1,2] + tab2[1,2])/2)/( (tab1[3,2] + tab2[3,2])/114),2)`$$
The exceedence probability of this value in an $F$-distribution with $\text{df}_1 = 2$ and $\text{df}_2 = 114$ degrees of freedom is $P(F_{2,114} \geq `r round( ((tab1[1,2] + tab2[1,2])/2)/( (tab1[3,2] + tab2[3,2])/114),2)`) < .001$, and hence this omnibus test is significant.

Similarly, for the Version by Presentation interaction, the omnibus test statistic is computed as
$$F = \frac{(`r round(tab1[2,2],2)` + `r round(tab2[2,2],2)`)/2}{(`r round(tab1[3,2],2)` + `r round(tab2[3,2],2)`)/114} = `r round( ((tab1[2,2] + tab2[2,2])/2)/( (tab1[3,2] + tab2[3,2])/114),2)`$$
The exceedence probability of this value is $P(F_{2,114} \geq `r round( ((tab1[2,2] + tab2[2,2])/2)/( (tab1[3,2] + tab2[3,2])/114),2)`) = `r pvalue(round(1-pf(((tab1[2,2] + tab2[2,2])/2)/( (tab1[3,2] + tab2[3,2])/114), 2, 114),3))`$, and hence this omnibus test is not significant. However, we know from our earlier analysis of $W_2$ that there is evidence for an interaction between $d_2$ and $c_1$. Looking at specific contrasts can provide more powerful and interesting tests than an omnibus test whether there is _any_ moderation.

The collated results of all analyses are provided in Table \@ref(tab:cheerleader-rm-anova-collated-results-table). This table is separated in a between-subjects part, and a within-subjects part. Relevant omnibus tests and tests for specific contrasts are provided for each. The final row shows the Total Sum of Squares, which is the sum over the between-subjects and within-subjects Sum of Squares. This shows how the Total Sum of Squares is partitioned in the different (between- and within-subjects) effects. 
```{r cheerleader-rm-anova-collated-results-table}
atab <- rbind(
  c(NA, sum(tab0[,2]), sum(tab0[,3]), NA, NA),
  tab0,
  c(NA, sum(c(tab1[,2],tab2[,2])), sum(c(tab1[,3],tab2[,3])), NA, NA),
  c(NA, tab1[1,2] + tab2[1,2], 2, ((tab1[1,2] + tab2[1,2])/2)/( (tab1[3,2] + tab2[3,2])/114), 1-pf( ((tab1[1,2] + tab2[1,2])/2)/( (tab1[3,2] + tab2[3,2])/114), 2, 114)),
  #c(NA, (tab1[3,2] + tab2[3,2]), 114, NA, NA),
  tab1[c(1,3),],
  tab2[c(2,3),],
  c(NA, tab1[2,2] + tab2[2,2], 2, ((tab1[2,2] + tab2[2,2])/2)/( (tab1[3,2] + tab2[3,2])/114), 1-pf( ((tab1[2,2] + tab2[2,2])/2)/( (tab1[3,2] + tab2[3,2])/114), 2, 114)),
  # c(NA, (tab1[3,2] + tab2[3,2]), 114, NA, NA),
  tab1[c(2,3),],
  tab2[c(2,3),],
  c(NA, (tab1[3,2] + tab2[3,2]), 114, NA, NA),
  c(NA, sum(c(tab0[,2], tab1[,2], tab2[,2])), sum(c(tab0[,3], tab1[,3], tab2[,3])), NA, NA)
)
rownames(atab) <- c("*Between-subjects*", "&nbsp;&nbsp;Intercept", "&nbsp;&nbsp;$c_1$ (Variant vs Identical)", "&nbsp;&nbsp;Error between", "*Within-subjects*", "&nbsp;&nbsp;Presentation", "&nbsp;&nbsp;&nbsp;&nbsp;$d_1$ (D + S vs A)", "&nbsp;&nbsp;&nbsp;&nbsp;Error ($d_1$)", "&nbsp;&nbsp;&nbsp;&nbsp;$d_2$ (D vs S)", "&nbsp;&nbsp;&nbsp;&nbsp;Error ($d_2$)", "&nbsp;&nbsp;Version $\\times$ Presentation", "&nbsp;&nbsp;&nbsp;&nbsp;$c_1 \\times d_1$", "&nbsp;&nbsp;&nbsp;&nbsp;Error ($c_1 \\times d_1$)", "&nbsp;&nbsp;&nbsp;&nbsp;$c_1 \\times d_2$", "&nbsp;&nbsp;&nbsp;&nbsp;Error ($c_1 \\times d_2$)", "&nbsp;&nbsp;Error within", "*Total*")
colnames(atab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$\\text{df}$", "$F$",  "$P(\\geq F)$")
opts <- options()
options(knitr.kable.NA = "")
knitr::kable(atab, caption = "Complete results of the 2 (Version: Identical, Variant) by 3 (Presentation: Alone, Different, Similar) ANOVA with repeated-measures on the last factor.", digits=c(3,2,0,2,3), escape = FALSE, booktabs = TRUE, linesep="")
options(opts)
```

A graphical overview of how the variation in $Y$ is partitioned in this analysis is provided in Figure \@ref(fig:partitioning-variance-rm-anova-full).

```{tikz partitioning-variance-rm-anova-full, fig.cap="Partitioning the total Sum of Squares in a 2 by 3 mixed repeated-measures ANOVA.", dev="png", dev.args=list(pointsize=12)}
  \usetikzlibrary{arrows}
	\begin{tikzpicture}
	\pgfsetxvec{\pgfpoint{2cm}{0cm}}
	\pgfsetyvec{\pgfpoint{0cm}{2cm}}
  \tikzstyle{every node}=[minimum size=2cm, text width=8em, text badly centered, minimum height=4em]

  \draw node at (-1,-.5) (SST) {SS Total};
  \draw node at (-3,-2) (SSB) {SS Between};
  \draw node at (-4,-3.5) (SSV) {SS Version (V)};
  \draw node at (-2,-3.5) (SSEB) {SSE Between};
  \draw node at (2,-2) (SSW) {SS Within};
  \draw node at (0,-3.5) (SSP) {SS Presentation (P)};
  \draw node at (-0.5,-4.5) (SSd1) {SS $d_1$};
  \draw node at (0.5,-4.5) (SSd2) {SS $d_2$};
  \draw node at (2,-3.5) (SSPV) {SS P $\times$ V};
  \draw node at (1.5,-4.5) (SSc1d1) {SS $c_1 \times d_1$};
  \draw node at (2.5,-4.5) (SSc1d2) {SS $c_1 \times d_2$};
  \draw node at (4,-3.5) (SSE) {SSE Within};
  \draw node at (3.5,-4.5) (SSEd1) {SSE $d_1$};
  \draw node at (4.5,-4.5) (SSEd2) {SSE $d_2$};

	\draw[->] (SST) -- (SSB);
  \draw[->] (SSB) -- (SSV);
  \draw[->] (SSB) -- (SSEB);
  \draw[->] (SST) -- (SSW);
  \draw[->] (SSW) -- (SSP);
  \draw[->] (SSW) -- (SSPV);
  \draw[->] (SSW) -- (SSE);
  \draw[->] (SSP) -- (SSd1);
  \draw[->] (SSP) -- (SSd2);
  \draw[->] (SSPV) -- (SSc1d1);
  \draw[->] (SSPV) -- (SSc1d2);
  \draw[->] (SSE) -- (SSEd1);
  \draw[->] (SSE) -- (SSEd2);

	\end{tikzpicture}
```

<!--
We could do this for any pair of conditions, but that might result in a large number of tests. A repeated-measures ANOVA allows us to to essentially this within a single analysis.

### Including effects for participants

In a repeated-measures ANOVA, we include Person as another factor in the design, in addition to the Condition factor. As the design includes each combination of Person and Condition, this can be treated as a Person by Item factorial design. We can state an ANOVA model as
<!--
\begin{equation}
Y_{i,j} = \mu + \tau^{(P)}_i + \tau^{(A)}_j + \epsilon_{i,j}
(\#eq:glm-oneway-RM-ANOVA-model)
\end{equation}
But this assumes that the effect of a treatment $j$ is the same for all
individuals $i$. If we would allow the effect of a treatment to be different
for participants, that would amount to including an interaction term (the 
effect of treatment $j$ is moderated by individual $i$). This could be stated as

\begin{equation}
Y_{i,j} = \mu + \pi_i + \tau_j + (\pi \tau)_{i,j} + \epsilon_{i,j}
(\#eq:one-RM-ANOVA-model-ch08)
\end{equation}
Here, I'm using a somewhat different notation than in Equation \@ref(eq:glm-factorial-ANOVA-model). Here, $\pi_i$ stands for the person-effect of participant $i$:
$$\pi_i = \mu_{i,\cdot} - \mu$$
and as usual, $\tau_j$ represents the treatment-effect of level $j$ of the experimental factor (Condition):
$$\tau_j = \mu_{\cdot, j} - \mu$$
$(\pi \tau)_{i,j}$ reflects the person-by-treatment interaction effect:
$$(\pi \tau)_{i,j} = \mu_{i,j} - (\mu+\pi_i + \tau_j)$$
There is one complication, however. We only have a single observation for each combination of person and condition. That means that the average for each combination of person and condition, $\overline{Y}_{i,j} = \hat{\mu}_{i,j}$, which is what the model aims to predict, equals the observation $Y_{i,j}$. As such, the errors of the estimated model would all be equal to $\hat{\epsilon}_{i,j} = 0$. In other words, the model fits the data perfectly!

Although you might think that a perfectly fitting model is ideal (why wouldn't you be happy when a model predicts the data perfectly?), that is far from the case here. Let's consider the number of parameters in the model. If we have $P$ people, and $A$ levels for condition, our model estimates $1 + (P-1) + (A-1) + (P-1)\times(A-1) = P \times A$ parameters, which is equal to the total number of observations: $n = P \times A$. A model with as many parameters as observations is bound to _overfit_ the data.

The model of Equation \@ref(eq:one-RM-ANOVA-model-ch08) makes intuitive sense. In practice, however, because there is just one observation for each participant/condition combination, we can not distinguish between the interaction term $\pi\tau_{i,j}$ and the error term $\epsilon_{i,j}$. To make this explicit, we could rewrite the model as
\begin{equation}
Y_{i,j} = \mu + \pi_i + \tau_j + (\tau\pi + \epsilon)_{i,j}
\end{equation}
This is just notational, and does not resolve anything. A more practical version is the following MODEL G:
\begin{equation}
Y_{i,j} = \mu + \pi_i + \tau_j + \epsilon_{i,j}
\end{equation}
which is a model that can be estimated by assigning appropriate contrasts to the Item and Participant factors. 
<!--
both $\tau_{i,j}$ and $\epsilon_{i,j}$ target the same difference between observation $Y_{i,j}$ and $\mu + \tau^{(P)}_i + \tau^{(A)}_j$. Whilst it is straightforward to compute this difference
$$Y_{i,j} - (\mu + \tau^{(P)}_i + \tau^{(A)}_j)$$
we cannot determine which part of this belongs to $\tau^{(P\times A)}_{i,j}$, and which part to $\epsilon_{i,j}$.
-->

<!-- It's like a pizza,  where some part is yours, and some part belongs to a random stranger. Halves may seem fair, but what if you haven't eaten for days, and perhaps the stranger is starving. Without knowing anything else, it could be just as fair to give on person everything and the other nothing, as splitting in equal halves. The right way to split is, in that case, indeterminate. When two terms in an equation "fight" for the same slice of the pie, that is similar. There is no way to  determine what belongs to what. -->

<!--
To test for the main effect of Condition, we can compare MODEL G above to a reduced model where we set $\tau_j = 0$ for all $j$. This provides MODEL R:
\begin{equation}
Y_{i,j} = \mu + \pi_i + \epsilon_{i,j}
\end{equation}

```{r}
dat$Participant <- factor(dat$Participant)
contrasts(dat$Participant) <- contr.sum(length(unique(dat$Participant)))
anova(lm(Response ~ Participant, data=dat), lm(Response ~ Participant + Item, data=dat))
```

Comparing MODEL G to MODEL R provides an appropriate (omnibus) test for the main effect of Condition. [INCLUDE RESULTS] You might wonder if you can or should also test for the main effect of Person. The short answer is: no. The longer answer is that the inability to distinguish between the interaction term $(\pi\tau)_{i,j}$ and the error term $\epsilon_{i,j}$ affects the sampling distribution of the estimates of the person effects $\hat{\pi}_i$ and treatment effects $\hat{\tau}_j$ differently. That is because Person should usually be treated as a __random effect__, whilst an experimental manipulation such as Condition should be treated as a __fixed effect__.

### Random vs fixed effects

Unlike Condition, Person is not really an experimental manipulation. Whilst the levels of Condition (i.e. whether participants viewed a photo by itself, in the context of a group of different faces, or in the context of a group of similar faces) were carefully designed to test a hypothesis of interest, the persons participating in a study are generally sampled from a wider population. We don't want to make claims just about those persons who happened to be available as participants in a study. We want to generalize the results to the wider population. For the levels of Condition, however, we are just interested in claims about these carefully designed levels. We wouldn't pretend that the study would also speak to what would have happened if the face was presented surrounded by photos of a participant's own face, for example. That implies that if we were to replicate the experiment a second or third time, the true effects of Condition would be the same, as we would use exactly the same condition. However, as we would include different participants, the effects of Person would vary from replication to replication. As the Data Generating Process concerns how data varies over repetitions of an experiment or scientific procedure, the status of fixed and random effects is different. Obtaining more observations means we should have better estimates of fixed effects. But by adding other people to an experiment, we cannot obtain a better estimate of how one particular participant differs from the average person. 

Insofar as we want to make any claims about random effects, these would concern the variability of those effects, rather than each specific effect for each person. To make such leaps from sample to population, we need a DGP for person effects. Commonly, we might assume that each person effect is drawn from a Normal distribution:
$$\pi_i \sim \mathbf{Normal}(0,\sigma_\pi)$$
Similarly, for the interaction effect, which is also random (as the persons are sample from the population), we might make a similar assumption:
$$(\pi\tau)_{i,j} \sim \mathbf{Normal}(0,\sigma_{\pi\tau})$$
For fixed effects, we would not need to make any assumption about their distribution, because we don't need to generalize beyond the specific effects included in the design.

Whilst the variance of the sampling distribution of the estimated treatment effects $\hat{\tau}_j$ depends on both the variance in the interaction term $(\pi\tau)_{i,j}$ and the error term $\epsilon_{i,j}$, whilst the variance of the sampling distribution of the person effects depends only on the variance of the error term $\epsilon_{i,j}$. 
<!--
$$\hat{\tau}_j \sim \mathbf{Normal}(\tau_j,\sqrt{\sigma^2_{\pi\tau} + \sigma^2_\epsilon})$$
$$\hat{\pi}_i \sim \mathbf{Normal}(\pi_i,\sigma_\epsilon)$$
-->

<!-- 
Intuitively, you might think of this as follows: whilst the treatment effects may differ between persons (as reflected in the interaction term $(\pi\tau)_{i,j}$), these deviations from the average treatment effects (as reflected in the treatment term $\tau_{j}$) are random, and will cancel each other out.  

<!--
A set of participants in a study is generally a sample from a (much) larger population of potential participants. Insofar as we are interested in differences between participants (i.e., an effect of P), we are not so much interested in whether there are differences between the actual participants included in the study, but whether there are differences between the people in the whole population. In other words, we would like to infer from variability between participants in a study whether there are differences between people in the population of interest. That means we want to generalize person effects in a particular sample of participants to person effects in the general population. To make such a leap from sample to population, we need a DGP for person effects, and commonly we might assume that each person effect is drawn from a Normal distribution:
$$\tau_i^{(P)} \sim \mathbf{Normal}(0,\sigma_\text{P})$$
Our objective here is to estimate the variability between persons, i.e. $\sigma_\text{P}$, and _not_ to estimate each $\tau^{P}_i$ as precisely as possible. 

The generalization problem for fixed effects (experimental manipulations) is different: our objective is to infer from a sample what effect precisely these experimental manipulations will have in the whole population. The objective here is to get the most precise estimation of each $\tau_j^{(A)}$. 
-->

## Assumptions

The assumptions for the analyses for each composite variable are the same as for any General Linear Model: the errors are assumed to be independent and Normal-distributed, with a mean of 0 and a constant variance. If the model includes between-subjects groups, then this translates in the assumption that within these groups, each composite score is Normal distributed with the same variance, but a possibly different mean. These assumptions are not guaranteed to hold. However, by focusing on separate analyses of composite scores which, by construction, are orthogonal to each other, there is no immediate reason to suspect a violation of the independence assumption.

When performing omnibus tests of main or interaction effects with a within-subjects component, an additional assumption is required: the assumption of sphericity. 

### Omnibus tests and sphericity

Omnibus tests involving within-subjects components were performed by aggregating SSR and SSE terms over different models. This is sensible only insofar as the SSE terms are comparable between the models. If the true variances of the errors are substantially different between the models of the within-subjects composite scores, then aggregating them to perform an omnibus test is like treating apples and oranges as the same fruit. As a result, the $F$-statistic will not follow the assumed $F$-distribution. The omnibus tests are valid when the Data Generating Process is fulfills the requirement of __sphericity__. 

Sphericity means that the variances of all pairwise differences between within-subjects measurements are equal. In our example, there were three within-subjects measurements: the Alone, Different, and Similar attractiveness ratings. As these are ratings by the same person, they are likely to be correlated (the reason for going through the effort in performing a repeated-measures ANOVA). For a more precise definition of sphericity, let's consider the true variance-covariance matrix of these three measures:

$$\Sigma = \left[ \begin{matrix} \sigma_A^2 & \sigma_{A,D} & \sigma_{A,S} \\ \sigma_{D,A} & \sigma_D^2 & \sigma_{D,S} \\ \sigma_{S,A} & \sigma_{S,D} & \sigma_S^2 \end{matrix} \right]$$ 
Here, $\sigma^2_A$ represents the true variance of the Alone measurement in the DGP, and $\sigma_{A,D}$ the true covariance between the Alone and Different measurement. Note that this matrix is symmetric, as the covariance between the Alone and Different measurement is the same as the covariance between the Different and Alone measurement, i.e. $\sigma_{A,D} = \sigma_{D,A}$. <!--Homogeneity of variance means that
$$\sigma_A^2 = \sigma_D^2 = \sigma_S^2$$
For omnibus tests in repeated-measures analyses, an additional assumption is necessary for the covariances, namely that of __sphericity__. This assumption means that the _variances of all pairwise differences are equal_. -->
The variance of a pairwise difference between e.g. the Alone and Different measures is $\sigma_A^2 + \sigma_D^2 - 2 \sigma_{A,D}$, i.e. the sum of the variances of the two variables, minus twice the covariance. Hence, the assumption of sphericity can be stated as:
$$\sigma_j^2 + \sigma_k^2 - 2\sigma_{jk} = \sigma_{l}^2 + \sigma_{m}^2 - 2 \sigma_{lm} \quad \quad \text{for all } j,k,l,m$$
For example, for our three variables, there are 3 pairwise differences, and hence the assumption is 

$$\sigma_A^2 + \sigma_D^2 - 2\sigma_{A,D} = \sigma_{A}^2 + \sigma_{S}^2 - 2 \sigma_{A,S} = \sigma_{D}^2 + \sigma_{S}^2 - 2 \sigma_{D,S}$$
If that seems like a complicated and stringent assumption: it is! And it is not that easy to check. Moreover, if there are between-subjects groups, then the variance-covariance matrix should be equal for each of those groups as well. Sphericity holds when a more stringent condition, called __compound symmetry__ holds. Compound symmetry means that all variances are identical to each other (i.e. $\sigma_A^2 = \sigma_D^2 = \sigma^2_S = \sigma^2$), and all covariances are identical to each other (i.e. $\sigma_{A,D} = \sigma_{A,S} = \sigma_{D,S} = \sigma_{\cdot,\cdot}$). The variance-covariance can then be stated as
$$\Sigma = \left[ \begin{matrix} \sigma^2 & \sigma_{\cdot,\cdot} & \sigma_{\cdot,\cdot} \\ \sigma_{\cdot,\cdot} & \sigma^2 & \sigma_{\cdot,\cdot} \\ \sigma_{\cdot,\cdot} & \sigma_{\cdot,\cdot} & \sigma^2 \end{matrix} \right]$$ 

### Correcting for non-sphericity

When the assumption of sphericity does _not_ hold (the assumption is violated) the $F$-statistic still (approximately) follows an $F$ distribution, but with a smaller value for $\text{df}_1$ and $\text{df}_2$ than usual. @greenhouse1959methods showed that the correct degrees of freedom can be stated as $\zeta \times \text{df}_1$ and $\zeta \times \text{df}_2$, where $0 \geq \zeta \geq 1$ is a correction fraction.^[The correction factor is usually denoted by $\epsilon$, but I'm using $\zeta$ ("zeta") as we are already using $\epsilon$ for the error terms.] Whilst the value of $\zeta$ depends on the true (co)variances underlying the data, it's value can be estimated. The estimator proposed by @greenhouse1959methods is known as the Greenhouse-Geisser estimate, and the corrected degrees of freedom using this estimate as the Greenhouse-Geisser correction. @huynh1976estimation showed that, if the true value is close to or higher than $\zeta = 0.75$, the Greenhouse-Geisser correction tends to be too conservative. They suggested a correction which provides an upward-adjusted estimate of $\zeta$, which will increase the power of the tests. The suggestion is thus to use the Huynh-Feldt correction when the Greenhouse-Geisser estimate of $\zeta$ is close to or higher than $\hat{\zeta} = 0.75$.

A statistical test for the assumption of sphericity was developed by @mauchly1940significance and is known as Mauchly's sphericity test. Whilst routinely provided by statistical software, it is not an ideal test, as it rests strongly on the assumption of normality and it commonly has low power. Rather than only correcting the degrees of freedom after a significant Mauchly test, @howell2012statistical suggests to _always_ adjust the degrees of freedom according to the either the Greenhouse-Geisser or Huynh-Feldt correction (whichever is more appropriate given the estimated $\hat{\zeta}$). As sphericity is only required for omnibus tests, another consideration is to avoid these omnibus tests, and only focus on tests for individual within-subjects contrasts [@judd2011data].

```{r sphericity}
dat <- subset(cheerleader, Excluded == 0) %>%
  mutate(Item = factor(Item)) %>%
  mutate(Item = forcats::fct_recode(Item, Different = "Control_Group", Similar = "Distractor_Manipulation"),
         Task = forcats::fct_recode(Task, Identical = "Identical-Distractors", Variant = "Self-Distractors"))
cmod <- afex::aov_car(Response ~ Task*Item + Error(Participant/Item), data=dat)
scmod <- summary(cmod$Anova, multivariate=FALSE)
```

For the present analysis, the Greenhouse-Geisser estimate is $\hat{\zeta} = `r round(scmod$pval.adjustments[1,1],3)`$. This is very close to 1, and hence there is no strong evidence for a violation of sphericity. In this case, that is supported by a non-significant Mauchly test for sphericity, $W = `r scmod$sphericity.tests[1,1]`$, $p = `r pvalue(round(scmod$sphericity.tests[1,2],3))`$. Whilst there is little need to do this in this case, if we were to apply the Greenhouse-Geisser correction for the omnibus test of presentation, we would compare the value of the $F$-statistic to an $F$-distribution with $\text{df}_1' = \hat{\zeta} \times \text{df}_1 = `r round(scmod$pval.adjustments[1,1],3)` \times 2 = `r round(scmod$pval.adjustments[1,1]*2,3)`$ and $\text{df}_2' = \hat{\zeta} \times \text{df}_2 = `r round(scmod$pval.adjustments[1,1],3)` \times 114 = `r round(scmod$pval.adjustments[1,1]*114,3)`$ degrees of freedom. The exceedence probability is then $P(F_{`r round(scmod$pval.adjustments[1,1]*2,3)`,`r round(scmod$pval.adjustments[1,1]*114,3)`} \geq `r round( ((tab1[1,2] + tab2[1,2])/2)/( (tab1[3,2] + tab2[3,2])/114),2)`) < .001$. The Greenhouse-Geisser corrected test of the Version by Presentation interaction is $P(F_{`r round(scmod$pval.adjustments[1,1]*2,3)`,`r round(scmod$pval.adjustments[1,1]*114,3)`} \geq `r round( ((tab1[2,2] + tab2[2,2])/2)/( (tab1[3,2] + tab2[3,2])/114),2)`) = `r pvalue(round(1-pf(((tab1[2,2] + tab2[2,2])/2)/( (tab1[3,2] + tab2[3,2])/114), scmod$pval.adjustments[1,1]*2, scmod$pval.adjustments[1,1]*114),3))`$. As the correction is only minor, neither test result is changed much.

<!-- ## Effect size -->
<!-- 
## Mixed ANOVA with between and within factors

\begin{equation}
Y_{i,j,k} = \mu + \tau^{(P)}_i + \tau^{(A)}_j + \tau^{(B)}_k + \tau^{(P \times A)}_{i,j} + \tau^{(P \times B)}_{i,k} + \tau^{(A \times B)}_{j,k} + \epsilon_{i,j,k}
(\#eq:glm-mixed-RM-ANOVA-model)
\end{equation}
Note that we are excluding the three-way interaction term $\tau^{(P \times A \times B)}_{i,j,k}$ as again, this can not be distinguished from the error term $\epsilon_{i,j,k}$.
-->

## In practice

Performing a repeated-measures ANOVA is, as you may have noticed, a somewhat laborious affair. It is therefore usually left to statistical software to conduct the various model comparisons. The steps are mostly similar to that of a factorial ANOVA:

1. Explore the data. For each repeated measure, check the distribution of the scores within each between-subjects condition. Are there outlying or otherwise "strange" observations? If so, you may consider removing these from the dataset. Note that as repeated-measures ANOVA requires complete data for each participant, this implies that you would remove all data from a participant with outlying data.

2. Define a useful set of contrast codes for the main effects of the within-subjects factors, and any between-subjects factors. Aim for these codes to represent the most important comparisons between the levels of the experimental factors. Then, separately for the within- and between-subjects factors, compute the interaction contrasts as (pairwise, threeway, fourway, ...) products of the main-effects contrasts. Compute within-subjects composite scores for all within-subjects effects. Then, for each composite score, estimate a linear model with the relevant contrast-coded predictors for the between-subjects effects. For each of these models, check again for potential issues in the assumptions with e.g. histograms for the residuals and QQ-plots. If there are clear outliers in the data, remove these, and then re-estimate the models.

3. If you want to compute omnibus tests, check whether the assumption of sphericity is likely to hold. This is best assessed through the Greenhouse-Geisser estimate of the correction factor (which was denoted as $\hat{\zeta}$ here, but most software will refer to this as $\hat{\epsilon}$). If the estimate is far from 1, then the sphericity assumption is likely violated. If the estimate is $\hat{\zeta} \geq .75$, consider using a Huynh-Feldt correction, rather than Greenhouse-Geisser correction. 

4. If the contrasts do not encode all the comparisons you wish to make, perform follow-up tests with other contrasts. If there are many of these tests, consider correcting for this by using e.g. a Scheffe-adjusted critical value. 

5. Interpret and report the results. When reporting the results, make sure that you include all relevant statistics. For example, one way to write the results of the analysis of Table \@ref(tab:cheerleader-rm-anova-collated-results-table), is as follows:

> Attractiveness ratings were analysed with a 2 (Version: Identical, Variant) by 3 (Presentation: Alone, Different, Similar) ANOVA, with repeated-measures on the last factor. A Greenhouse-Geisser correction was applied to the degrees of freedom, to correct for any potential problems of non-sphericity. The analysis showed a significant effect of Presentation, `r papaja::apa_print(scmod, mse=FALSE, es="pes")$full_result$Item`. Contrast analysis showed that attractiveness ratings were higher in the Different and Similar conditions compared to the Alone conditions, `r papaja::apa_print(lm(W1 ~ X, data=wdat))$full_result$Intercept`. The contrast between the Different and Similar conditions was not significant, `r papaja::apa_print(lm(W2 ~ X, data=wdat))$full_result$Intercept`. Whilst the omnibus test for the interaction between Version and Presentation was not significant, `r papaja::apa_print(scmod, mse=FALSE, es="pes")$full_result$Task_Item`, contrast analysis indicates that the difference between the Different and Similar conditions varied between the two Versions, `r papaja::apa_print(lm(W2 ~ X, data=wdat))$full_result$X`. When the Similar condition corresponded to a group of identical photos, the attractiveness ratings were `r round(sum(coefficients(lm(W2 ~ X, data=wdat))*c(1,-.5)),3)` points higher in the Different compared to the Similar condition. This difference was only `r round(sum(coefficients(lm(W2 ~ X, data=wdat))*c(1,.5)),3)` when the Similar condition corresponded to a group of different photos of the same individual. The analysis showed no further significant results. 

<!-- 
### Defining and estimating the model

### Assessing the assumptions

### Reporting the results

## Summary

-->




<!--chapter:end:08-RM-ANOVA.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Linear mixed-effects models {#ch-linear-mixed-effects-models}

In this chapter, we will discuss an alternative approach to dealing with the non-independence of errors that can result from repeated-measures of the same individual, or otherwise multiple observations which come from different groupings in the data. Traditionally, such data has been analysed with repeated-measures ANOVA models. Linear mixed-effects models offer an alternative to repeated-measures ANOVA with certain benefits, particularly in dealing with missing values. Once you get your head around the idea of random effects, linear mixed-effects models are a natural extension of the General Linear Model, which can then make them easier to understand than repeated-measures ANOVA models, which rely on complicated model comparison schemes and strong assumptions. Linear mixed-effects models offer a more flexible univariate modelling technique. This flexibility does mean, as we will see, that sometimes difficult choices need to be made regarding the random-effects structure of the model. As always, there are no set rules in defining statistical models, and this should be led by substantive concerns, in addition to issues of reliability and maintaining appropriate error rates. With practice and experience, you should become more confident in making such decisions. 

In writing this chapter, I have adapted some sections of @Singmann2019-mixed. This is a very clear introduction to linear mixed-effects models, that you may wish to consult in addition to this Chapter (you can download it [here](http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf)).

## Non-independence in linear models

The General Linear Model we have considered thus far can be stated as follows:

$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
The assumptions of this model concern the residuals or errors $\epsilon_i$. They are assumed to be _independent and identically distributed_ (iid), following a Normal distribution with a mean of 0 and a standard deviation $\sigma_\epsilon$.

The assumption that the errors are independent means that knowing the value of the error for one case $i$ in the data does not give you any information to determine the value of the error for another case $j$. More formally, statistical independence is defined in terms of conditional probabilities. Remember when we discussed the rules of probability (Section \@ref(sec:02-rules-of-probability))? In particular, we used independence to state rule 7, which is a specific case of the multiplication rule (rule 6). We can define the distribution of the error for case $i$ as a conditional probability $p(\epsilon_i | \epsilon_i, X_{1,i}, \ldots, X_{m,i})$, where we conditionalise on all the predictor values, as well as another error term $\epsilon_j$. The errors are independent when 
$$p(\epsilon_i | \epsilon_j, X_{1,i}, \ldots, X_{m,i}) = p(\epsilon_i |  X_{1,i}, \ldots, X_{m,i})$$
i.e. the distribution of $\epsilon_i$, conditional upon knowing the value of the predictors and the error $\epsilon_j$ of another case $j$, is the same as the distribution conditional upon just knowing the value of the predictors. This means that error $\epsilon_j$ provides no information about the error $\epsilon_i$.

When might such independence _not_ hold? Let's for the moment go back to the simplest version of the GLM, the model with just an intercept that was the focus of Chapter \@ref(ch-simple-GLM):
\begin{equation}
Y_i = \beta_0 + \epsilon_i
(\#eq:simple-glm-ch09)
\end{equation}
Suppose $Y$ are again judgements of the height of Mount Everest, but that one of the labs in the ManyLabs study that collected the data, was based in Nepal. We might expect participants in Nepal to have a more accurate idea of the height of Mount Everest than participants in Poland (the country the participants were from in the subset of the data we considered in Chapter \@ref(ch-simple-GLM)). As a result, participants in Nepal would likely be less influenced by the low anchor than those with less knowledge of Mount Everest. If $\beta_0$ represents the mean judgement over all participants, then participants in Nepal would generally provide judgements which are higher than $\beta_0$ (i.e positive errors), whilst participants in Poland would generally provide judgements that are lower than $\beta_0$ (i.e. negative errors). If our model does not account for such grouping in the data, then the errors for cases from Nepal would be correlated, because all would tend to be positive. Similarly, the errors for cases from Poland would also be correlated, as all would tend to be negative. What this example points to is that the simple model does not account for the fact that participants from different countries might have a different average judgement. As such, the model is misspecified. We could of course alleviate this problem by including an effect for country, using a contrast-coded predictor $X_1$ to reflect the difference between participants from Nepal ($X_1 = \tfrac{1}{2}$) and Poland ($X_1 = -\tfrac{1}{2}$) in an expanded model
$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i$$
This model would allow for a different mean judgement for cases from Nepal ($\beta_0 + \tfrac{1}{2} \times \beta_1$) and cases from Poland ($\beta_0 - \tfrac{1}{2} \times \beta_1$). Doing so for all labs in the data quickly becomes cumbersome, however. In this study, there were `r library(sdamr); data("anchoring"); length(unique(anchoring$referrer))` labs involved, meaning we would need `r length(unique(anchoring$referrer)) - 1` contrast-coded predictors to reflect possible differences between them. Moreover, differences between the labs are not of primary interest here. The question we posed in Chapter \@ref(ch-simple-GLM) was whether people in general would, on average, give an accurate judgement of the hight of Mount Everest, not whether the country they reside in influences this.  Thus, country, if it has an effect, can be seen as a _nuisance effect_, something we have to consider for a valid model, but we would otherwise rather ignore. __Linear mixed-effects models__ allow you to solve this conundrum gracefully, providing a way for effects to differ between groupings in the data, without dramatically increasing the number of to-be-estimated parameters. They do so by treating such effects as random, rather than fixed, effects. The model then focuses on estimating the distribution of these random effects, rather than estimating each separately.

You can expect violations of the _iid_ assumption if data are collected from units of observations that are clustered in groups. A common example of this in psychology is when you repeatedly observe behaviour from the same person. Other examples of this are data from experiments collected in group settings, students within classrooms, or patients within hospitals. In such situations one would expect that observations within each cluster (i.e., a specific group, classroom, or hospital) are more similar to each other than observations across clusters. Unfortunately, compared to violations of other assumptions, such as the normality assumption or the assumption of variance homogeneity, standard statistical procedures are usually not robust to violations of the independence assumption [@kenny_consequences_1986;@judd_treating_2012]. They often lead to considerably increased Type I errors (i.e., false positives) and more generally can produce overconfident results (e.g., too narrow standard errors).

## Random intercept models

We will now define a model which allows for each grouping (e.g., lab) in the data to have a different mean. To help define the model, let $Y_{i,j}$ denote the judgement of the height of Mount Everest for participant $i = 1,\ldots,n_j$ in grouping (lab) $j = 1, \ldots, n_g$. The double subscript is solely to distinguish between participants and groupings. We still consider a single dependent variable $Y$. We will allow each group $j$ to have a different mean judgement. The first step to do so is to rewrite the simple model of Equation \@ref(eq:simple-glm-ch09) as
\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
(\#eq:random-intercept-model-level-1)
\end{equation}
For each observation $Y_{i,j}$, we thus assume the intercept $\beta_{0,j}$ which is different for each group $j$. In other respects, the model is exactly the same as the one of Equation \@ref(eq:simple-glm-ch09), and observations can deviate from the intercept (which represents the mean in group $j$) through the error term $\epsilon_{i,j}$, which is assumed Normal-distributed with a mean of 0 and constant (homogeneous) standard deviation $\sigma_\epsilon$. Whilst the model effectively states that the structural part $\beta_{0,j}$ is different for each grouping level $j$, the groupings are linked because they share the same distribution for the error term. 

The second step is to define a model for the grouping-dependent intercepts:
\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
(\#eq:random-intercept-model-level-2)
\end{equation}
Let's pause for a moment and consider the Equation \@ref(eq:random-intercept-model-level-2) above in detail. You can think of $\beta_{0,j}$ as the dependent variable here. On the right hand side, there are two terms, an intercept $\beta_0$, and a residual term $\gamma_{0,j}$ which is assumed Normal-distributed with a mean of 0 and a constant (homogeneous) standard deviation $\sigma_{\gamma_0}$. Note that this model is structurally identical to that of Equation \@ref(eq:simple-glm-ch09). If you'd replace $\beta_{0,j}$ by $Y_i$, $\gamma_{0,j}$ by $\epsilon_i$, and $\sigma_{\gamma_0}$ by $\sigma_\epsilon$, you'd reproduce Equation \@ref(eq:simple-glm-ch09) exactly. We thus have a simple model for the parameter $\beta_{0,j}$ as an outcome. In such a __parameters as outcomes__ formulation of linear mixed-effects models, it is customary to call Equation \@ref(eq:random-intercept-model-level-1) a __level 1__ model. It is a model on the level of the observed values $Y_{i,j}$. Equation \@ref(eq:random-intercept-model-level-2) is then a __level 2__ model, a higher-order model which is on the level of the parameters of the model on the level below it (level 2). As we will see later on, further higher-order models can be specified, leading to so-called multilevel models. For present purposes, a level 2 model suffices.

The random-intercepts model defined by equations \@ref(eq:random-intercept-model-level-1) and \@ref(eq:random-intercept-model-level-2) has random elements ($\gamma_{0,j}$ and $\epsilon_{i,j}$), and non-random elements ($\beta_0$). Parameter $\beta_0$ is an (unknown) constant, and in the context of mixed-effects models is called a __fixed effect__. The $\gamma_{0,j}$ terms are called __random effects__. The reflect deviations between the means in the groupings from the overall mean $\beta_0$:
$$\gamma_{j,0} = \beta_{0,j} - \beta_0$$
This is similar to how the residuals $\epsilon_{i,j}$ reflect deviations of observations from the means $\beta_{0,i}$. In the study, differences between the labs (grouping levels) are not of direct interest, and this is generally the case for random effects. As indicated earlier, they are usually nuisance factors, which we include in a model to make the model valid and precise, but we would otherwise like to ignore them. What is of interest in e.g. determining whether people's judgements on average are equal to the true height of Mount Everest is the fixed effect $\beta_0$. In our model with random intercepts $\beta_{0,j}$, this parameter equals the mean of these intercepts, because we can rewrite Equation \@ref(eq:random-intercept-model-level-2) in an equivalent form as
$$\beta_{0,j} \sim \mathbf{Normal}(\beta_0,\sigma_{\gamma_0})$$
A useful view of the model is then as a hierarchical model, as we have already done in our level-1 and level-2 formulation. This is depicted in Figure \@ref(fig:random-intercepts-plot-hierarchy). 

```{r random-intercepts-plot-hierarchy, fig.cap="Graphical depiction of the random-intercepts model. At the top you see the distribution of $\\beta_{0,j}$, which is a Normal distribution with mean $\\beta_0$ and standard deviation $\\sigma_{\\gamma_0}$. Each intercept $\\beta_{0,j}$ in the middle layer is a random sample from this distribution. The $\\beta_{0,j}$ terms are the means of the group-wise distributions from which the data points (lowest layer) are randomly sampled. Each data point $Y_{i,j}$ is drawn from the group-wise distributions depicted on the middle layer, which are all Normal distributions with mean $\\beta_{0,j}$ and standard deviation $\\sigma_{\\gamma_0}$. The random effects $\\gamma_{0,j}$ (only one shown) are the deviations between the group-wise mean $\\beta_{0,j}$ and the overall mean $\\beta_0$. The errors $\\epsilon_{i,j}$ (only one shown) are the deviations between the observations $Y_{i,j}$ and the group-wise means $\\beta_{0,j}$."}
set.seed(872346)
beta0 <- 0
sigma_gamma <- 1
sigma_epsilon <- .4
beta0j <- c(-1.5,1,2)
nobs <- 5
y <- NULL
for(i in 1:length(beta0j)) {
  y <- c(y, rnorm(nobs,beta0j[i],sigma_epsilon))
}
p <- ggplot() + stat_function(fun=function(x) 2 + dnorm(x,mean=beta0,sd=sigma_gamma)) + geom_point(data=data.frame(x=beta0,y=rep(2,length(beta0))),aes(x=x,y=y))
p <- p +  stat_function(fun=function(x) 1 + .3*dnorm(x,mean=beta0j[1],sd=sigma_epsilon)) +  stat_function(fun=function(x) 1 + .3*dnorm(x,mean=beta0j[2],sd=sigma_epsilon)) +  stat_function(fun=function(x) 1 + .3*dnorm(x,mean=beta0j[3],sd=sigma_epsilon))
p <- p + geom_point(data=data.frame(x=beta0j,y=rep(1,length(beta0j))),aes(x=x,y=y)) + geom_point(data=data.frame(x=y,y=rep(0,length(y))),aes(x=x,y=y)) + xlim(-4,4) + ylab("") + xlab("") + annotate("text",x=0,y=1.9,label=expression(beta[0])) + theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
for(i in 1:length(beta0j)) {
  p <- p + annotate("text",x=beta0j[i],y=0.9,label=paste0("beta[list(0,",i,")]"),parse=TRUE)
}
p <- p + geom_segment(data = data.frame(x = beta0j + .5*(beta0 - beta0j), y = rep(1.9,length(beta0j)), xend = beta0j, yend = rep(1.1,length(beta0j))),aes(x=x,y=y,xend=xend,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)
p <- p + geom_segment(data = data.frame(x = rep(beta0j,each=nobs) - .5*(rep(beta0j,each=nobs) - y), y = rep(0.8,length(beta0j)*nobs), xend = y, yend = rep(0.1,length(beta0j)*nobs)),aes(x=x,y=y,xend=xend,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)
p <- p + geom_segment(data = data.frame(x = beta0, xend = beta0j[1], y = 1.05, yend=1.05), aes(x=x,xend=xend,y=y,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)  + annotate("text",x=beta0j[1] + .5*(beta0 - beta0j[1]),y=1.1,label=paste0("gamma[list(0,",1,")]"),parse=TRUE)
p <- p + geom_segment(data = data.frame(x = beta0j[2], xend = y[6], y = -0.05, yend=-0.05), aes(x=x,xend=xend,y=y,yend=yend), arrow = arrow(length = unit(0.3, "cm")), alpha=.5)  + annotate("text",x=y[6] + .5*(beta0j[2] - y[6]),y=-0.1,label=paste0("epsilon[list(2,",2,")]"),parse=TRUE)
p

```


## Parameter estimation

If we could observe the parameters $\beta_{0,j}$ directly, model estimation would be easy. We could just use the methods of Chapter \@ref(ch-simple-GLM). But parameters are not observed, and need to be inferred. This is where the "magic" of linear mixed-effects models happens. In estimating a mixed-effects model, the particular values of the random effects $\gamma_{0,j}$ are not directly the focus. Rather, the objective is to estimate the variance of the random effects, $\sigma^2_{\gamma_0}$. Once we have estimates of the fixed effects, as well as the random-effect variance $\sigma^2_\gamma$ and residual error variance $\sigma^2_\epsilon$, we can obtain predictions of the random effects $\hat{\gamma}$, which are called __best linear unbiased predictions__ (BLUPs). These can be considered a side-product of the estimation, rather than an integral part.

As we have discussed in earlier chapters, maximum likelihood provides biased estimates of variances. As you may recall, the sample variance $S^2 = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}$ is the maximum likelihood estimate of the variance, whilst the unbiased estimator is $\hat{\sigma^2} = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}$.  In the context of linear mixed-effects models, there is a similar issue that maximum likelihood estimation provides biased estimates of the variances of random effects. Unbiased estimates are obtained with a procedure called __restricted maximum likelihood__ (REML). It is beyond the scope of this book to go into the details of this procedure, but it roughly corresponds by first removing any fixed effects from the data, and then estimating the variance components. For most purposes, relying on restricted maximum likelihood estimation is generally recommended. A main exception is when we need to obtain the maximised likelihood to perform a likelihood ratio test (to e.g. test the significance of random effect variances). For these limited cases, you can re-estimate the model with maximum likelihood (ML), solely to obtain the maximised (log) likelihood.

Parameter estimation in mixed-effects models is more complicated than for linear models with only fixed effects. This is especially the case for more complex models with random intercepts and slopes and correlations between these. Care must therefore be taken in defining a model which is complex enough to allow you to capture the dependence between observations, but not so complex to render the estimation of parameters so unreliable, or even impossible, to make the inferences effectively meaningless. We will discuss the specification of the appropriate random effects structure in more detail later on. 

## Parameter inference

After estimating the parameters of the linear mixed-effects model, the same procedures of model comparisons can be applied to obtain null hypothesis significance tests for the fixed effects. In the same way as for the General Linear Model, this can consist of computing $F$ statistics from comparisons of the Sum of Squared Errors of a general MODEL G and a restricted MODEL R which sets some of the fixed effects parameters to a priori values, often $\beta = 0$. 

A complication with linear mixed-effects models concerns the appropriate degrees of freedom. In models with only fixed effects, we could use $\text{df}_1 = \text{npar}(G) - \text{npar}(R)$ and $\text{df}_2 = n - \text{npar}(G)$. Unfortunately, this is not so for mixed-effects models. The problem is the inclusion of the random effects $\gamma$. These are not parameters in the sense that the fixed effects $\beta$ and the variances $\sigma$ are parameters, they nevertheless are factors which influence the model errors $\epsilon$. You might think of them as "partial" parameters, which you can assign a weight between 0 and 1, in terms of how they influence the model errors $\epsilon$. Assigning each random effect $\gamma_{0,j}$ a weight of 1 would add, in our example above, a total of $n_g$ parameters to the model (and hence reduce $\text{df}_2$ by $n_g$). But this generally assigns too much importance to the random effects, and the appropriate correction is somewhere between 0 and $n_g$. Precisely what value the correction to the degrees of freedom should have is unfortunately not known for most models. Different approximations have been proposed. Two main ones are the Satterthwaite [@satterthwaite_synthesis_1941] and the Kenward-Roger [@kenward_small_1997] approximation. As the latter is known to provide the best control of Type I errors with the limited sample sizes that are common in psychology studies, this one is generally recommended [@Singmann2019-mixed]. @luke2017evaluating provides a further comparison of the approximation methods.

You may also be interested in testing evidence for the existence of random effects. If there are no random effects (i.e. all $\gamma_{0,j} = 0$), that is equivalent to setting $\sigma_{\gamma_0} = 0$. The $F$ statistic is not suitable for comparing a model where we set the standard deviation of a random effect $\gamma$ to $\sigma_{\gamma} = 0$ to a model where we allow it to take any value $\sigma_{\gamma} \geq 0$ (and hence need to estimate it). For such model comparisons, we can revert back to the general idea of comparing models via the likelihood ratio (e.g. Section \@ref(sec:02-likelihood-ratio)):

$$\begin{aligned} 
\text{likelihood-ratio} &= \frac{p(Y_1,\ldots,Y_n|\text{MODEL R})}{p(Y_1,\ldots,Y_n|\text{MODEL G})} \\
&= \frac{p(\text{DATA}|\text{MODEL R})}{p(\text{DATA}|\text{MODEL G})}
\end{aligned}$$
where $\text{DATA}$ in the second line of this equation is just a short-hand for all observations (e.g. $\text{DATA} = Y_1,\ldots,Y_n$). This will make it easier to refer to more complex data structures later on.

If the number of total observations $n$ is sufficiently large, we can rely on an important and general statistical theorem by @wilks1938large that shows that as $n \rightarrow \infty$ (i.e., as the number of observations approaches infinity), then under the null-hypothesis that MODEL R is true:
\begin{equation}
-2 \log \left(\text{likelihood-ratio}\right) \sim \mathbf{chi-squared}(\text{npar}(G) - \text{npar}(R))
\end{equation}
where the "minus two times log likelihood ratio" is computed as
$$\begin{aligned}
-2 \log \left(\text{likelihood-ratio}\right) &= -2 \log p\text{DATA}|\text{MODEL R}) - (-2 \log p(\text{DATA}|\text{MODEL G}))
\end{aligned}$$
In words, under the null-hypothesis that the restricted model is true, the sampling distribution of minus twice the natural logarithm of the likelihood ratio is distributed as a Chi-squared ($\chi^2$) distribution. A Chi-squared distribution has one parameter, the degrees of freedom, which here equals the difference in the number of parameters of the two models. In this test, if there are any unknown parameters in the models, they need to be estimated by maximum likelihood (not by restricted maximum likelihood). This is because the ratio needs to be the ratio between the maximum value of each likelihood. The parameters to count are the fixed effects $\beta$ and the variances of the random effects $\sigma^2_\gamma$, as well as any covariances between them (we will discuss this later), and the residual error variance $\sigma^2_\epsilon$. In this test, you don't have to worry about adjusting the degrees of freedom for the influence of the random effects $\gamma$. An important caveat is however that Wilks' theorem assumes that the restriction of the parameters in MODEL R are in the _interior of the parameter space_. That means that if parameters have upper or lower bounds, the fixed values can not be set at exactly these bounds in MODEL R. For example. a variance can not be negative, so a variance parameter has a lower bound of $\sigma^2 \geq 0$. Unfortunately, in the model comparison to test for random effects, we need to set the variance at exactly this lower bound in MODEL R. As a result, the Chi-squared distribution is not an accurate approximation to the true sampling distribution. Usually, the tests are too conservative, providing too large $p$-values (suggestions are that when testing a single variance of a random effect, they are approximately twice as large as they should be, see e.g. @pinheiro2006mixed).^[When parameters are set on the bounds of the parameter space, the sampling distribution of the statistic is a _mixture_ of different Chi-squared distributions, which effectively is a weighted sum of chi-squared distributions. Although such mixture distributions are well-defined in principle, it is not straightforward to determine the component weights accurately in practice.] The general conservatism of the likelihood ratio tests means that if you obtain a significant result, you can be reassured that you have sufficient evidence for an effect. A conservative test procedure however means that the true rate of Type 1 errors will be lower than set by the significance level $\alpha$. When it is crucial to maintain the rate of Type 1 errors sought for by the significance level, a better approach is to use a __parametric bootstrap__ procedure where you simulate a large number of data sets according to the estimated MODEL R, and for each compute the value of the $-2 \log\left(\text{likelihood-ratio}\right)$ value, which provides an "empirical distribution" of the sampling distribution of this statistic. You can then determine how "unusual" the actual value computed for the true data is in the context of this simulated distribution.   

## Application of the random-intercepts model

Having spent a bit of time discussing the estimation and testing of effects in linear mixed-effects models in abstract terms, let's see how we can use a model like this in practice. For this first example, we will consider the data from the anchoring study again, focussing on those (non US or UK based) labs that were able to obtain estimates of the height of Mount Everest in meters, after having provided the participants with a low anchor. The judgements of participants acquired by these different labs, as well as the mean judgements and standard errors of the mean (remember, these standard errors are the standard deviation of the sampling distribution of the mean) are shown in Figure \@ref(fig:anchoring-low-boxplot-by-referrer). 

```{r anchoring-low-boxplot-by-referrer, fig.cap="Judged height of Mount Everest after a low anchor for all international labs where participants provided their answer in meters. Boxes represent the mean in each group plus or minus the standard error of the mean."}
library(dplyr)
library(sdamr)
data("anchoring")
dat <- subset(anchoring, anchor == "low" & us_or_international == "International" & referrer != "lse") %>%
  mutate(referrer = factor(referrer))
contrasts(dat$referrer) <- contr.sum(length(levels(dat$referrer)))
dat %>%
  ggplot(aes(x=referrer,y=everest_meters,colour = referrer)) + stat_summary(fun.data="mean_se",geom="crossbar") + geom_jitter(alpha=.4) + theme(legend.position = "none") + xlab("Lab") + ylab("Height of Mount Everest in meters") 
```

As this figure shows, the mean judgements seem to vary quite a bit between the different labs. As a result, predicting judgements in each group by the mean over all groups will likely introduce dependence between the errors, with errors in groups with a relatively high mean tending to be positive, and errors in groups with a relatively low mean will tend to be negative. We can eliminate such dependence by introducing random intercepts for each group, because the errors will then be deviations of the observations from the group means:
$$\epsilon_{i,j} = Y_{i,j} - \beta_{0,j}$$
We are still interested in the overall mean judgement, reflected by the fixed intercept $\beta_0$, which can be interpreted as the average of all group-wise intercepts $\beta_{0,j}$.

```{r estimate-random-effects-model, message = FALSE, results='hide'}
library(afex)
dat$true_meters <- 8848
modg <- mixed(everest_meters ~ 1 + (1|referrer), data=dat, test_intercept = TRUE, method="KR")
modr <- lm(everest_meters ~ 1, data=dat)
modr2 <- dat %>% mutate(dev_everest_meters = everest_meters - 8848) %>% 
mixed(dev_everest_meters ~ 1 + (1|referrer), data=., test_intercept = TRUE, method="KR")
modr3 <- dat %>% mutate(dev_everest_meters = everest_meters - 8848) %>% 
lm(dev_everest_meters ~ 1, data=.) %>% car::Anova(type=3)

```

The estimated model (by REML) can be written as
$$\begin{aligned}
Y_{i,j} &= \beta_0 + \gamma_{0,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model),digits=3, nsmall=2)` + \gamma_{0,j} + \epsilon_{i,j} \\
\gamma_{0,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model)$referrer,"stddev"), digits=3, nsmall=2)`) \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model),"sc"), digits=3, nsmall=2)`)
\end{aligned}$$

We can compare this to an estimated model without random intercepts:
$$\begin{aligned}
Y_{i,j} &= \beta_0 + \epsilon_{i,j} \\
&= `r format(coefficients(modr)[1],digits=3,nsmall=2)` + \epsilon_{i,j} \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(sqrt(sum(residuals(modr)^2)/modr$df.residual), digits=3,nsmall=2)`)
\end{aligned}$$

In the latter model, which is similar to the simple model we focused on in Chapter \@ref(ch-simple-GLM), the estimate of the intercept is the sample mean over all the observations. You can see that this differs from the estimated fixed intercept of the random intercepts model. This fixed effect represents the average of the random intercepts $\beta_{0,j}$. Contrary to what you might have thought, the predicted^[In the context of mixed-effects models, it is common to state that random effects are _predicted_, rather than _estimated_.] random intercepts are not equal to the group-wise sample averages. Table \@ref(tab:anchoring-means-random-intercepts-table) shows the sample averages, and the predicted random intercepts, as well as the estimated random effects.

```{r anchoring-means-random-intercepts-table}
tab <- data.frame(mean = aggregate(everest_meters ~ referrer, data=dat, mean)$everest_meters, n = as.numeric(table(dat$referrer)), fixed_intercept = fixef(modg$full_model), intercept = (fixef(modg$full_model) + ranef(modg$full_model)$referrer)[,1], gamma = ranef(modg$full_model)$referrer, sample_minus_fixed_intercept = aggregate(everest_meters ~ referrer, data=dat, mean)$everest_meters - fixef(modg$full_model))
knitr::kable(tab, col.names = c("$\\overline{Y}_j$","$n_j$","$\\hat{\\beta}_{0}$", "$\\hat{\\beta}_{0,j}$", "$\\hat{\\gamma}_{0,j}$", "$(\\overline{Y}_j - \\hat{\\beta}_{0})$"), caption = "Group-wise averages and sample sizes, estimated fixed intercept ($\\hat{\\beta}_0$) and predicted random intercepts ($\\hat{\\beta}_{0,j}$), random effects ($\\hat{\\gamma}_{0,j}$). The latter are the deviations between the random intercepts and teh fixed intercept. For comparison, we also show the deviations between the sample means and the fixed intercept $(\\overline{Y}_j - \\hat{\\beta}_0)$.", digits=2, escape=FALSE, booktabs=TRUE, linesep="")
```

As you can see, the random intercepts $\hat{\beta}_{0,j}$ are closer to the fixed intercept $\hat{\beta}_0 = `r format(fixef(modg$full_model),digits=3, nsmall=2)`$ than the sample means $\overline{Y}_j$. In statistical terms, this is usually called __shrinkage__. Due to the assumption that the random effects are Normal-distributed, the random intercepts are pulled towards the average of that distribution, which is the fixed intercept $\beta_0$. This is because in a Normal distribution, large deviations from the mean are unlikely. Hence, if the average in a grouping is far from the overall average (the average over all the groupings), that is perhaps due to a random fluke in this particular dataset. Adjusting the estimated mean of the grouping to be closer to those of the other groupings then makes sense: although each grouping can differ, there will be some similarities between the groupings. The relative adjustment depends not only on the difference between the sample mean from the overall mean, but also on the sample size ($n_j$) of each grouping level. The smaller the sample size, the more the estimate will be adjusted towards the overall mean, because there is less information in smaller samples to precisely estimate a parameter. Hence, because estimates are more noisy, the true mean can be expected to be further away from the sample mean.

Because the groupings are not completely dissimilar, when estimating the mean for one grouping, we can partly rely on the data from other groupings. This is also called __partial pooling__. It can be contrasted with __complete pooling__, where all the data is treated as if from one single group (i.e. as in the model of Equation \@ref(eq:simple-glm-ch09)). Complete pooling is inappropriate here, because when there are effective groupings in the data, this will violate the _iid_ assumption which the complete pooling model relies on. The opposite of complete pooling is __no pooling__. This means that you would estimate a model for each grouping separately. If the sample sizes of the groupings are sufficiently large to allow for reliable parameter estimation, this approach is not unreasonable and would not violate the _iid_ assumption. However, compared to partial pooling, there are several disadvantages, most notably that there is no easy way to combine the results of the separate analyses to obtain inferences over the whole set of groupings. 

Testing whether the fixed intercept $\beta_0$ equals an a priori value, such as $\beta_0 = 8848$, can be essentially done in the same way is in Chapter \@ref(ch-simple-GLM). We can estimate a model in which we fix the intercept to that value, and then conducting a model comparison. The results of this comparison, using the Kenward-Roger approximation for the degrees of freedom is $F(`r modr2$anova_table[1,1]`, `r modr2$anova_table[1,2]`) = `r modr2$anova_table[1,3]`$, $p `r pvalue(modr2$anova_table[1,4], limit=TRUE)`$. As such, we reject the null hypothesis that the average judgement is equal to the true height of Mount Everest. Performing the same analysis but with models which don't include random effects would have given $F(`r modr3[1,2]`, `r modr3[2,2]`) = `r modr3[1,3]`$, $p `r pvalue(modr3[1,4], limit=TRUE)`$. The latter test is not a good reflection of the data, because the _iid_ assumption on which it rests is violated here. This leads to a biased and _overconfident_ test result (i.e., a too high $F$ value). You can also see the Kenward-Roger approximation of the degrees of freedom at work in the test for the mixed-effects model. The value for $\text{df}_1$ is 1 because the model comparison involves one less parameter in MODEL R compared to MODEL G. The difference is in the value of $\text{df}_2$: Compared to the (wrong) full pooling model, the value of $\text{df}_2$ is substantially lower (`r modr2$anova_table[1,2]` compared to `r modr3[2,2]`). Therefore, the test with the mixed model has effectively less power, but as the complete pooling model is not valid, the mixed-model analysis is preferred to the latter.

## Models with random intercepts and slopes

The model we dealt with so far is the simplest example of a linear mixed-effects model. Let's now consider a slightly more complex model, where we will now assess the effect of the anchor that was given to participants. The data from the international labs providing judgements in meters are shown in Figure \@ref(fig:anchoring-both-boxplot-by-referrer). As for the low anchor condition, average judgements in the high anchor condition seem to differ between the labs, albeit to a lesser extent.

```{r anchoring-both-boxplot-by-referrer, fig.cap="Judged height of Mount Everest after a low or high anchor for all international labs where participants provided their answer in meters. Boxes represent the mean in each group plus or minus the standard error of the mean.", fig.width=8, fig.height=4}
library(dplyr)
library(sdamr)
data("anchoring")
dat <- subset(anchoring, us_or_international == "International" & referrer != "lse") %>% mutate(anchor=factor(anchor,levels=c("low","high")), referrer = factor(referrer))
dat %>% 
  ggplot(aes(x=referrer,y=everest_meters,colour = referrer)) + stat_summary(fun.data="mean_se",geom="crossbar") + geom_jitter(alpha=.4) + theme(legend.position = "none") + xlab("Lab") + ylab("Height of Mount Everest in meters") + facet_wrap(~anchor)
```

Accounting for the effect of anchor is done by including a contrast-coded predictor in the model, e.g. a predictor $X_1$ which has the value $\tfrac{1}{2}$ for cases in the high anchor condition, and a value $-\tfrac{1}{2}$ for cases in the low anchor condition. Just adding this predictor to the random intercepts model of Equations \@ref(eq:random-intercept-model-level-1) and \@ref(eq:random-intercept-model-level-2) would mean that we would change Equation \@ref(eq:random-intercept-model-level-1) to
$$Y_{i,j} = \beta_{0,j} + \beta_{1} \times X_{1,i,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})$$
For this model, we don't need a further random-effects specification to Equation \@ref(eq:random-intercept-model-level-2). In this new model, as for ANOVA models, the intercepts $\beta_{0,j}$ reflect a grand mean (average of the averages of the two anchor conditions), whilst the fixed slope $\beta_1$ reflects the difference between the mean of the high and low anchor condition. As there is only a fixed slope $\beta_1$, this model assumes that the effect of the anchors is exactly the same for each grouping (lab). That implies that for each grouping, the distance between the mean judgement in the high and low anchor is assumed to be identical. The random intercept would allow the midpoint between the two means to vary over the groupings, such that if one grouping has relatively high judgements in the low anchor condition, it would also have relatively high judgements in the high anchor condition. Looking at Figure \@ref(fig:anchoring-both-boxplot-by-referrer), this may be a too strong assumption. For example, referrer $\texttt{swpson}$ has a relatively high mean in the low anchor condition, but a relatively low mean in the high anchor condition (and both are suspiciously close to the true height of Mount Everest). Because we have multiple cases in each lab-condition combination, we could allow the effect of anchor to differ between the groupings. This can be achieved by adding a random effect for anchor. Conceptually, that means that we allow the effect of anchor to be moderated by lab. If we allow a different slope $\beta_{1,j}$ for each grouping $j$, we can write the model as
\begin{equation}
Y_{i,j} = \beta_{0,j} + \beta_{1,j} \times X_{1,i,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
(\#eq:random-intercept-and-slope-model-level-1)
\end{equation}
The level-2 models for the intercept and slope are:
\begin{align}
\beta_{0,j} &= \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0}) \\
\beta_{1,j} &= \beta_1 + \gamma_{1,j} \quad \quad \quad \gamma_{1,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_1})
(\#eq:random-intercept-and-slope-model-level-2)
\end{align}
As for the random intercepts, the random slope $\beta_{1,j}$ consists of a fixed part $\beta_1$, which is the average slope over all groupings, and a random part $\gamma_{1,j}$, which is the deviation of the grouping-wise slope ($\beta_{1,j}$) from the average ($\beta_1$). 

```{r mixed-effects-random-slopes, cache=TRUE, message=FALSE, results='hide'}
library(sdamr)
library(afex)
contrasts(dat$anchor) <- c(-1/2,1/2)
dat$referrer <- as.factor(dat$referrer)
contrasts(dat$referrer) <- contr.sum(length(levels(dat$referrer)))
modg <- mixed(everest_meters ~ anchor + (anchor||referrer), data=dat, check_contrasts=FALSE, expand_re=TRUE, test_intercept = TRUE, method="KR")
```

The estimated model (obtained with REML) can be stated as
$$\begin{aligned}
Y_{i,j} &= \beta_0 + \gamma_{0,j} + (\beta_1 + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model)[1],digits=3, nsmall=2)` + \gamma_{0,j} + (`r format(fixef(modg$full_model)[2],digits=3, nsmall=2)` + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
\gamma_{0,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model)$referrer,"stddev"), digits=3, nsmall=2)`) \\
\gamma_{1,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model)$referrer.1,"stddev"), digits=3, nsmall=2)`) \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model),"sc"), digits=3, nsmall=2)`)
\end{aligned}$$
In terms of inference, we are mainly interested in the fixed effect of anchor. A test of the hypothesis that the slope of anchor is $\beta_1 = 0$, again using the Kenward-Roger approximation for the degrees of freedom, gives us $F(`r modg$anova_table[2,1]`, `r modg$anova_table[2,2]`) = `r modg$anova_table[2,3]`$, $p `r pvalue(modg$anova_table[2,4], limit=TRUE)`$. Hence, we reject the null hypothesis that the true effect of the anchor equals 0. There clearly is a difference between the low and high anchor condition, such that the judgements in the former are lower on average than in the latter conditions.

```{r est-ml-models}
dat$anchor_n <- -1/2
dat$anchor_n[dat$anchor == "high"] <- 1/2
modg_ml <- lme4::lmer(everest_meters ~ anchor_n + (1 + anchor_n||referrer), data=dat, REML=FALSE)
modr_ml <- lme4::lmer(everest_meters ~ anchor_n + (1|referrer), data=dat, REML=FALSE)
```

At this point, let's also consider testing whether there is evidence that the effect of the anchor varies over the groupings. In order to obtain this test, we need to use a likelihood ratio test, or a parametric bootstrap. Here, we'll focus on the former. To test the null-hypothesis $H_0: \sigma_{\gamma_1} = 0$ with a likelihood-ratio test, we first re-estimate the full MODEL G (which includes random intercepts and slopes) by maximum likelihood. This provides us a value of 
$$-2 \log p(Y_{i,j}|\text{MODEL G}) = `r format(anova(modg_ml,modr_ml)$deviance[2],scientific=FALSE,nsmall=2)`$$ In MODEL R, we set $\sigma_{\gamma_1} = 0$, effectively omitting the random slopes $\gamma_{1,j}$. Otherwise, we keep everything else the same. Estimating this MODEL R gives
$$-2 \log p(Y_{i,j}|\text{MODEL R}) = `r format(anova(modg_ml,modr_ml)$deviance[1],scientific=FALSE,nsmall=2)`$$
The likelihood-ratio test statistic is simply the difference between these two values:
$$-2 \log (\text{likelihood-ratio}) = `r format(anova(modg_ml,modr_ml)$deviance[1],scientific=FALSE,nsmall=2)` - `r format(anova(modg_ml,modr_ml)$deviance[2],scientific=FALSE,nsmall=2)` = `r anova(modg_ml,modr_ml)$Chisq[2]`$$
As MODEL G contains just one more estimated parameter, the degrees of freedom for the test equals $\text{npar}(G) - \text{npar}(R) = `r anova(modg_ml,modr_ml)$npar[2]` - `r anova(modg_ml,modr_ml)$npar[1]` = 1$. The critical value for a Chi-squared distribution with one degree of freedom and significance level $\alpha = .05$ is $\chi^2(1; .05) = `r qchisq(.95,1)`$. Clearly, the value computed for the data is well above the critical value. Hence, we reject the null hypothesis that $\sigma^2_{\gamma_1} = 0$. So, we have sufficient evidence that the effect of anchor varies over the groupings.

### Correlation between random effects

In the model above, we assumed the random effects $\gamma_{0,j}$ and $\gamma_{1,j}$ are independent and drawn from separate Normal distributions (each with a different standard deviation, $\sigma_{\gamma_0}$ and $\sigma_{\gamma_1}$. There are situations where we would however like to allow the random effects to be correlated. In the example above, it might be the case that the effect of the anchor differs between groups who on average give relatively high judgements, compared to groups who tend to give lower judgements. Such a dependence indicates a moderation of the effect of anchor by the average judgement in each group. In linear mixed-effects models, such a moderation can be implemented by allowing for a correlation between random intercepts (e.g. the average values within each lab) and slopes (e.g. the effects of anchor). <!-- Such a dependence indicates a moderation of the effect of anchor by the average (grand mean) judgement. In models without random effects, such a moderation would not make sense, because there is a single intercept, and a single slope for anchor. However, we now have different intercepts and slopes for the groups, such that we could, in principle, estimate a relation (a dependence) between the intercepts and slopes. -->

In general, linear mixed-effects models assume that the random effects are samples from a __multivariate Normal distribution__. A multivariate Normal distribution is a distribution over _vectors_ of values. A multivariate Normal distribution is parametrized by a mean vector $\boldsymbol{{\mu}}$ and a covariance matrix $\boldsymbol{{\Sigma}}$. For the two random effects in our model, we can specify the joint distribution as follows:
\begin{equation}
\begin{aligned}
\left( \begin{matrix} \gamma_{0,j} \\ \gamma_{1,j} \end{matrix} \right)
&\sim \mathbf{Normal}\left(\boldsymbol{\mu}, \boldsymbol{\Sigma} \right) \\
&\sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} \sigma^2_{\gamma_0}&\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1} \\ \rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}&\sigma^2_{\gamma_1} \end{matrix} \right] \right)
\end{aligned}
\end{equation}
So
$$\boldsymbol{\mu} = \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right]$$
and
$$\boldsymbol{\Sigma} = \left[ \begin{matrix} \sigma^2_{\gamma_0}&\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1} \\ \rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}&\sigma^2_{\gamma_1} \end{matrix} \right]$$
In the covariance matrix $\boldsymbol{\Sigma}$, the $\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}$ component in the off-diagonal elements are the covariance between $\gamma_{0,j}$ and $\gamma_{1,j}$. A covariance can be written as a product of the correlation $\rho_{\gamma_0,\gamma_1}$ between $\gamma_{0,j}$ and $\gamma_{1,j}$, and the standard deviations $\sigma_{\gamma_0}$ and $\sigma_{\gamma_1}$.

Unfortunately, the added complexity of estimating the correlation between the random effects is sometimes too much, resulting in estimation failures. That was the case for the subset of data for the international labs analysed above. By including all the labs in the full study, the model is estimable. In this case, we focus on the estimation in feet, rather than meters. In other respects, the model is the same as before. 

```{r mixed-effects-random-slopes-correlated, cache=TRUE, message=FALSE, results='hide'}
library(sdamr)
library(afex)
data("anchoring")
anchoring$anchor <- factor(anchoring$anchor, levels=c("low","high"))
contrasts(anchoring$anchor) <- c(-1/2,1/2)
anchoring$referrer <- factor(anchoring$referrer)
contrasts(dat$referrer) <- contr.sum(length(levels(dat$referrer)))
modg <- mixed(everest_feet ~ anchor + (1 + anchor|referrer), data=anchoring, check_contrasts = FALSE, expand_re = TRUE, method="KR")
modg2 <- lmer_alt
modr <- mixed(everest_feet ~ anchor + (1 + anchor||referrer), data=anchoring, check_contrasts = FALSE, expand_re = TRUE, method="KR")
aov_rg <- anova(modr, modg)
```

The estimated model can be written as
$$\begin{aligned}
Y_{i,j} &= \beta_0 + \gamma_{0,j} + (\beta_1 + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model)[1],digits=3, nsmall=2)` + \gamma_{0,j} + (`r format(fixef(modg$full_model)[2],digits=3, nsmall=2)` + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
\left( \begin{matrix} \gamma_{0,j} \\ \gamma_{1,j} \end{matrix} \right)
&\sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} \sigma^2_{\gamma_0}&\rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1} \\ \rho_{\gamma_0,\gamma_1}\sigma_{\gamma_0}\sigma_{\gamma_1}&\sigma^2_{\gamma_1} \end{matrix} \right] \right) \\
& \sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix}  (`r attr(VarCorr(modg$full_model)$referrer,"stddev")[1]`)^2 & `r attr(VarCorr(modg$full_model)$referrer,"correlation")[2,1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[2]` \\ `r attr(VarCorr(modg$full_model)$referrer,"correlation")[2,1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[1]` \times `r attr(VarCorr(modg$full_model)$referrer,"stddev")[2]` & (`r attr(VarCorr(modg$full_model)$referrer,"stddev")[2]`)^2 \end{matrix} \right] \right) \\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(modg$full_model),"sc"), digits=3, nsmall=2)`)
\end{aligned}$$
This shows a negative correlation between the random intercepts and the random slopes: $\hat{rho}_{\gamma_0,\gamma_1} = `r round(attr(VarCorr(modg$full_model)$referrer,"correlation")[2,1],3)`. This can be interpreted as indicating that the higher the average judgements in a grouping (the intercept), the smaller the difference is between the low and high anchor conditions within that grouping (the slope). 

To assess whether there is sufficient evidence to include this correlation between the random intercepts and slopes, we can perform a model comparison, comparing the MODEL G just estimated, to a MODEL R where we fix the correlation to 0. As we are testing for random effects, we will use the likelihood-ratio test. The result of this test is $\chi^2(1) = `r round(aov_rg$Chisq[2],3)`$, $p `r apa_pvalue(aov_rg$"Pr(>Chisq)"[2])`$. Hence, we would reject the null-hypothesis that in the Data Generating Process, the correlation between the random effects is 0. 

## Crossed random effects: dating partners in the speed dating experiment

As a final example of a mixed-effects analysis, we will reconsider the data from the speed-dating experiment of @fisman2006gender we analysed in Chapter \@ref(ch-moderation-mediation). In this experiment, groups of participants encountered each other in a number of speed-dates. They then rated each other on a number of attributes (e.g. physical attractiveness, intelligence, etc). Each participant thus rated a number of different other participants, but was also the object of ratings by multiple other participants. In a sense, the design is then a "doubly repeated" design, where the same participant provides multiple ratings, but the same "Item" (i.e. dating partner) is also rated multiple times. For example, in one group, female participants 132, 133, 134, 135, and 136, each went on a four-minute date with male participants 137, 138, 139, 140, and 141. So participant 132 provided ratings for five other participants (the "items"), but was herself also an "item" for five other participants (137, 138, 139, 140, and 141). If, as we did in Chapter \@ref(ch-moderation-mediation), we focus on how much participants liked their date, there may be both _person effects_ and _item effects_. By person effects we mean individual differences between raters. For instance, some participants might generally like all their dating partners more than expected because they take more of an interest in their partners. By item effects we mean differences between the persons who were rated. For instance, some people may generally receive higher ratings than expected because they have a positive quality that is not captured by the predictors. Although calling a dating partner an item is not very nice, the name "item effect" is pretty standard in the literature, and hence we will use "item" in the remainder for the person being rated, and "person" for the person who provides the rating. Both person and item effects can lead to dependencies in the model errors. Unless you have a full factorial design where each person rates each item, this is difficult to deal with in a repeated-measures ANOVA. But it is pretty straightforward with linear mixed-effects models.

Let's consider modelling the dependent variable $\texttt{like}_{i,j}$, which is a liking rating by person $i$ for item $j$, as a function of attractiveness, $\texttt{attr}_{i,j}$, intelligence, $\texttt{intel}_{i,j}$, and the product-predictor for their interaction, $(\texttt{attr} \times \texttt{intel})_{i,j}$. We will consider the level-1 model
$$\texttt{like}_{i,j} = \beta_{0,i,j} + \beta_{1, i, j} \times \texttt{attr}_{i,j} + \beta_{2, i, j} \times \texttt{intel}_{i,j} + \beta_{3, i, j} \times (\texttt{attr} \times \texttt{intel})_{i,j} + \epsilon_{i,j} \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})$$
Here, we assume that the intercept $\beta_{0,i,j}$ as well as the slopes $\beta_{1, i, j}$, $\beta_{2, i, j}$, and $\beta_{3, i, j}$, may be different for each combination of person $i$ and item $j$. As we have only a single rating of the liking for each person-item combination, this is not a model we would be able to estimate separately for each person-item combination. However, because persons and items are __crossed__, we do have multiple observations for each person and item, which allows us to use partial pooling for both. As level two models, we can use
$$\begin{aligned}
\beta_{0,i,j} &= \beta_0 + \gamma_{P,0,i} + \gamma_{I,0,j} \\
\beta_{1,i,j} &= \beta_1 + \gamma_{P,1,i} + \gamma_{I,1,j} \\
\beta_{2,i,j} &= \beta_2 + \gamma_{P,2,i} + \gamma_{I,2,j} \\
\beta_{3,i,j} &= \beta_3 + \gamma_{P,3,i} + \gamma_{I,3,j}
\end{aligned}$$
So we are using one fixed part for each parameter (e.g. $\beta_0$), but now _two_ random parts: one dependent on the person $i$ who provides the rating (e.g. $\gamma_{P,0,i}$), and one dependent on the item $j$ (e.g. $\gamma_{I,0,j}$). Whilst the random effects for Person may be correlated, and the random effects for Item may be correlated as well, the random effects for Person cannot be correlated with effects for Item. This results in a so-called __block-diagonal__ structure for the random effects:
\begin{equation}
\left( \begin{matrix} \gamma_{0,i} \\ \gamma_{P,1,i} \\ \gamma_{P,2,i} \\ \gamma_{P,3,i}   \\ \gamma_{I,0,j} \\ \gamma_{I,1,j} \\ \gamma_{I,2,j} \\ \gamma_{I,3,j} \end{matrix} \right)
\sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} 
\sigma^2_{{P0}} & \sigma_{{P01}} & \sigma_{P02} & \sigma_{P03} & 0 & 0 & 0 & 0 \\
\sigma_{P01} & \sigma^2_{P1} & \sigma_{P12} & \sigma_{P13} & 0 & 0 & 0 & 0\\
\sigma_{P02} & \sigma_{P12} &  \sigma^2_{P2} & \sigma_{P23} & 0 & 0 & 0 & 0 \\
\sigma_{{0,3,P}} & \sigma_{P13} & \sigma_{P23} & \sigma^2_{P3} & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \sigma^2_{I0} & \sigma_{I01} & \sigma_{I02} & \sigma_{I03} \\
0 & 0 & 0 & 0 & \sigma_{I01} & \sigma^2_{I1} & \sigma_{I12} & \sigma_{I13} \\
0 & 0 & 0 & 0 & \sigma_{I02} & \sigma_{I12} & \sigma^2_{I2} & \sigma_{I23} \\
0 & 0 & 0 & 0 & \sigma_{I03} & \sigma_{I13} & \sigma_{I23} & \sigma^2_{I3} 
\end{matrix} \right] \right)
\end{equation}
Here, $\sigma^2_{P0}$ refers to the variance of the random intercept for persons, i.e. the variance of $\gamma_{P,0,i}$, and $\sigma^2_{I3}$ to the variance of the random slopes for $(\texttt{attr} \times \texttt{intel})_{ij}$ for items, i.e. the variance of $\gamma_{I,3,j}$. A term such as $\sigma_{{P01}}$ stands for the covariance between the random intercept and random slope for $\texttt{attr}_{ij}$, i.e. the covariance between $\gamma_{P,0,i}$ and $\gamma_{P,1,i}$, which can be computed from the correlation and standard deviations as usual:
$$\sigma_{P01} = \rho_{P01} \sigma_{P0} \sigma_{P1}$$
where $\rho_{P01}$ refers to the correlation between $\gamma_{P,0,i}$ and $\gamma_{P,1,i}$.

As the name suggests, a block-diagonal structure implies there are blocks (groups) of parameters which are interrelated, but not related to other parameters outside the group. 


<!--
single predictor $X_1$ (e.g. physical attractiveness). Denoting the value of the dependent variable for rater $i$ of partner $j$, we can consider the level-1 model



$$Y_{i,j} = \beta_{0,i,j} + \beta_{1, i, j} \times X_{1,i,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})$$
Here, we assume that both the intercept $\beta_{0,i,j}$ and the slope $\beta_{1,i,j}$ may be different for each combination of rater $i$ and ratee $j$. As we have only a single rating of the liking for each rater-ratee combination, this is not a model we would be able to estimate separately for each rater-ratee combination. However, by __crossing__ raters and ratees, we do have multiple observations for each rater and ratee, which allows us to use partial pooling for both. As level two models, we can use
$$\beta_{0,i,j} = \beta_0 + \gamma_{0,i} + \gamma_{0,j}$$
and
$$\beta_{1,i,j} = \beta_1 + \gamma_{1,i} + \gamma_{1,j}$$
So we are using as usual one fixed part for each parameter (e.g. $\beta_0$), but now _two_ random parts: one dependent on the rater $i$ (e.g. $\gamma_{0,i}$), and one dependent on the ratee $j$ (e.g. $\gamma_{0,j}$). Whilst the random effects for rater may be correlated, and the random effects for ratee may be correlated as well, the random effects for rater cannot be correlated with effects for ratee. This results in a so-called __block-diagonal__ structure for the random effects:
\begin{equation}
\left( \begin{matrix} \gamma_{0,i} \\ \gamma_{1,i} \\ \gamma_{0,j} \\ \gamma_{1,j} \\ \end{matrix} \right)
\sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \\ 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} 
\sigma^2_{\gamma_{0,r}} & \rho_{\gamma_{0,r},\gamma_{1,r}} \sigma_{\gamma_{0,r}}\sigma_{\gamma_{1,r}} & 0 & 0 \\
\rho_{\gamma_{0,r},\gamma_{1,r}} \sigma_{\gamma_{0,r}} \sigma_{\gamma_{1,r}} & \sigma^2_{\gamma_{1,r}} & 0 & 0 \\
0 & 0 & \sigma^2_{\gamma_{0,o}} & \rho_{\gamma_{0,o},\gamma_{1,o}} \sigma_{\gamma_{0,o}} \sigma_{\gamma_{1,o}} \\ 
0 & 0 & \rho_{\gamma_{0,o},\gamma_{1,o}} \sigma_{\gamma_{0,o}} \sigma_{\gamma_{1,o}} & \sigma^2_{\gamma_{1,o}}
\end{matrix} \right] \right)
\end{equation}
As the name suggests, a block-diagonal structure implies there are blocks (groups) of parameters which are interrelated, but not related to other parameters outside the group. 
-->

```{r cross-mixed-analysis, cache= TRUE, results='hide'}
library(sdamr)
data("speeddate")

dat <- speeddate
dat$other_attr_c <- center(dat$other_attr)
dat$other_intel_c <- center(dat$other_intel)
dat$iid <- factor(dat$iid)
dat$pid <- factor(dat$pid)

#modl <- lme4::lmer(other_like ~ other_attr_c*other_intel_c + (1 + other_attr_c*other_intel_c|iid) + (1 + other_attr_c*other_intel_c|pid), data=dat)

#ndat <- dat %>%
#  group_by(iid) %>%
#  mutate(npid = factor(row_number()))
#
#mod <- lme4::lmer(other_like ~ other_attr_c*other_intel_c + (1 + other_attr_c*other_intel_c|iid/npid), data=ndat)

#moda <- afex::mixed(other_like ~ other_attr_c + (1 + other_attr_c|iid) + (1 + other_attr_c|pid), data=dat, test_intercept = TRUE, method="KR")

moda <- modai <- afex::mixed(other_like ~ other_attr_c*other_intel_c + (1 + other_attr_c*other_intel_c|iid) + (1 + other_attr_c*other_intel_c|pid), data=dat, test_intercept = TRUE, method="KR")

```

The results for the fixed effects of the model, where $\texttt{attr}$ and $\texttt{intel}$ were mean-centered, are provided in Table \@ref(tab:cross-mixed-fixed-results-table). Compared to Table \@ref(tab:multiple-regression-with-interaction-ANOVA-centered), which effectively considered the same model without random effects, we generally obtain similar results, although the estimates and $F$-values do differ due to the partial pooling and shrinkage of the mixed-effects model. As dependency between ratings from the same rater, and for the same ratee ("item") is rather likely, this new analysis should be preferred to the one in Table \@ref(tab:multiple-regression-with-interaction-ANOVA-centered).


```{r cross-mixed-fixed-results-table, cache= TRUE, message=FALSE}

tab_fix <- cbind(summary(modai$full_model)$coefficients[,1],modai$anova_table)
rownames(tab_fix) <- c("Intercept","$\\texttt{attr}$", "$\\texttt{intel}$", "$\\texttt{attr}\\times\\texttt{intel}$")
knitr::kable(tab_fix, col.names = c("$\\hat{\\beta}$", "$\\text{df}_1$", "$\\text{df}_2$", "$F$", "$P(\\geq F)$"), booktabs=TRUE, digits=c(3,0,2,2,3), caption="Estimates and tests (Kenward-Roger approximation) for the fixed-effects of a model with crossed random Person and Item effects.")

#data("speeddate")
#dat <- speeddate
#dat$iid <- as.factor(dat$iid)
#dat$pid <- as.factor(dat$pid)
#summary(afex::mixed(other_like ~ other_attr + other_sinc + other_intel + other_fun + other_amb + (1|iid) + (1|pid), data=dat))
```

The estimated covariance matrix for the random Person effects is provided in Table \@ref(tab:random-P-effects-covariance-speeddating).

```{r random-P-effects-covariance-speeddating}
tab <- VarCorr(modai$full_model)$iid
rownames(tab) <- c("$\\gamma_{P,0}$", "$\\gamma_{P,1}$", "$\\gamma_{P,2}$","$\\gamma_{P,3}$")
tab %>%
  knitr::kable(col.names = rownames(tab), caption="Variance-covariance matrix for the Person random effects for the linear mixed-effects model predicting liking from attractiveness, intelligence, and their interaction.", digits=3)
```

The estimated covariance matrix for the random Item effects is provided in Table \@ref(tab:random-I-effects-covariance-speeddating).

```{r random-I-effects-covariance-speeddating}
tab <- VarCorr(modai$full_model)$pid
rownames(tab) <- c("$\\gamma_{I,0}$", "$\\gamma_{I,1}$", "$\\gamma_{I,2}$","$\\gamma_{PI,3}$")
tab %>%
  knitr::kable(col.names = rownames(tab), caption="Variance-covariance matrix for the Item random effects for the linear mixed-effects model predicting liking from attractiveness, intelligence, and their interaction.", digits=3)
```

As you can see, the item variability is generally estimated to be higher than the person variability. This, to a certain extent, indicates that variability between items (the dating partners here) weighs more heavily than differences between the persons (the raters here). As such, whether someone is liked more or less than average may be more due to characteristics of that person than characteristics of the rater.

<!--
The estimated model can be written as
$$\begin{aligned}
Y_{i,j} &= \beta_0 + \gamma_{0,i} + \gamma_{0,j} + (\beta_1 + \gamma_{1,i} + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
&= `r format(fixef(modg$full_model)[1],digits=3, nsmall=2)` + \gamma_{0,i} + \gamma_{0,j} + (`r format(fixef(modg$full_model)[2],digits=3, nsmall=2)` + \gamma_{1,i} + \gamma_{1,j}) \times X_{1,i,j} + \epsilon_{i,j} \\
\left( \begin{matrix} \gamma_{0,i} \\ \gamma_{1,i} \\ \gamma_{0,j} \\ \gamma_{1,j} \end{matrix} \right)
&\sim \sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \\ 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} 
\sigma^2_{\gamma_{0,r}} & \rho_{\gamma_{0,r},\gamma_{1,r}} \sigma_{\gamma_{0,r}}\sigma_{\gamma_{1,r}} & 0 & 0 \\
\rho_{\gamma_{0,r},\gamma_{1,r}} \sigma_{\gamma_{0,r}} \sigma_{\gamma_{1,r}} & \sigma^2_{\gamma_{1,r}} & 0 & 0 \\
0 & 0 & \sigma^2_{\gamma_{0,o}} & \rho_{\gamma_{0,o},\gamma_{1,o}} \sigma_{\gamma_{0,o}} \sigma_{\gamma_{1,o}} \\ 
0 & 0 & \rho_{\gamma_{0,o},\gamma_{1,o}} \sigma_{\gamma_{0,o}} \sigma_{\gamma_{1,o}} & \sigma^2_{\gamma_{1,o}}
\end{matrix} \right] \right) \\
& \sim \mathbf{Normal}\left( \left[ \begin{matrix} 0 \\ 0 \\ 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} 
(`r attr(VarCorr(moda$full_model)$iid,"stddev")[1]`)^2 &`r attr(VarCorr(moda$full_model)$iid,"correlation")[2,1]` \times `r attr(VarCorr(moda$full_model)$iid,"stddev")[1]` \times `r attr(VarCorr(moda$full_model)$iid,"stddev")[2]` & 0 & 0 \\
`r attr(VarCorr(moda$full_model)$iid,"correlation")[2,1]` \times `r attr(VarCorr(moda$full_model)$iid,"stddev")[1]` \times `r attr(VarCorr(moda$full_model)$iid,"stddev")[2]` & (`r attr(VarCorr(moda$full_model)$iid,"stddev")[1]`)^2 & 0 & 0 \\
0 & 0 & (`r attr(VarCorr(moda$full_model)$pid,"stddev")[1]`)^2 & \rho_{\gamma_{0,o},\gamma_{1,o}} \sigma_{\gamma_{0,o}} \sigma_{\gamma_{1,o}} \\ 
0 & 0 & `r attr(VarCorr(moda$full_model)$pid,"correlation")[2,1]` \times `r attr(VarCorr(moda$full_model)$pid,"stddev")[1]` \times `r attr(VarCorr(moda$full_model)$pid,"stddev")[2]` & (`r attr(VarCorr(moda$full_model)$pid,"stddev")[1]`)^2
\end{matrix} \right] \right)
\\
\epsilon_{i,j} &\sim \mathbf{Normal}(0,`r format(attr(VarCorr(moda$full_model),"sc"), digits=3, nsmall=2)`)
\end{aligned}$$
-->


## Choosing the random effects structure

The correct specification of the random-effects structure in the model is very important. Omitting a random effect when there is in fact variability in this effect across the levels of a grouping factor can dramatically increase Type I errors well beyond the desired significance level [@judd_treating_2012; @barr_random_2013-1]. As such, it has been recommended that one should initially start with the __maximal random effects structure__ [@barr_random_2013-1; @Singmann2019-mixed]. The maximal model is the model that includes random effects for all grouping factors in a study. Specifically, for each grouping factor, the maximal model contains random intercepts as well as random slopes for all fixed effects, plus all correlations among the random effects. 

By using a maximal structure, it is unlikely that you would miss a potentially important source of variability in the data. Inclusion of random effects which are not really there might, given sufficient data, not be too much of an issue, as the model would estimate the variance of such random effects to be low. However, if the sample size is limited, a common problem is that the maximal model is not fully identified [@bates_parsimonious_2015], especially for mixed models with complicated random effects structures. An unidentified model means that not all parameters can be estimated properly. This is often evident by the estimated variance-covariance matrix of the random effects being degenerate or singular. In such cases, you may obtain estimated variances of 0, near 0, or even negative estimated variances,^[A variance can never be negative, so this is clearly an indication of issues in the estimation!] and correlations between random effects of $\pm 1$. The occurrence of such situations is due to the fact the parameters associated to random effects (e.g., $\sigma^2_{\gamma_1}$) are more difficult to estimate than fixed effects (e.g., $\beta_1$). 

<!-- https://www.sciencedirect.com/science/article/pii/S0749596X17300013 for Matcuschek -->

When the maximal structure can not be used because of these estimation issues, it is generally recommended to reduce the complexity of the random effects structure, because degenerate or overparameterised models can reduce the statistical power of any tests conducted with them [@matuschek_balancing_2017]. A first step is generally to remove correlations among random slopes, as these tend to contribute the largest number of random effects parameters that need to be estimated. They are usually also more difficult to estimate than the variance parameters. If a model still shows problems after removing correlations, other random-effects parameters could be removed, starting with the highest-order random effects parameter with the lowest estimated variance. Iterative approaches to obtain the __optimal random effects structure__, rather than the maximal one, are provided by @matuschek_balancing_2017 and @bates_parsimonious_2015. Applying these iterative procedures can be complicated. They are also data-driven, and hence the results may be influenced by random noise in the data. Sometimes, it might not be possible to reduce the random-effects structure such that all problematic random effects parameters are removed (e.g., in cases when there is random variability in higher-order effects, but not in lower-order effects). In those cases, one might accept a few problematic or degenerate parameters (e.g., variances of zero). This would provide more conservative tests for the fixed effects than simply removing justifiable random effects and inflating Type I error rates to an unknown degree. It is also advisable to check the robustness of results by comparing the fixed-effects estimates and associated hypothesis tests across all estimated models with a different random effects structure. It is often the case that the testing of fixed effects in highly overparameterised models diverge from the corresponding tests in reduced models. In these cases, the tests for the simpler models may be preferred.

<!--chapter:end:09-LMER.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Introduction to Bayesian hypothesis testing {#ch-Bayes-factors}

In this chapter, we will introduce an alternative to the Frequentist null-hypothesis significance testing procedure employed up to now, namely a Bayesian hypothesis testing procedure. This also consists of comparing statistical models. What is new here is that Bayesian models contain a prior distribution over the values of the model parameters. In doing so for both the null and the alternative model, Bayesian model comparisons provide a more direct measure of the relative evidence of the null model compared to the alternative. We will start with an introduction to the fundamental aspects of Bayesian inference, which concerns revising beliefs in light of observed data. We then go on to discuss "default priors", which can be useful in a Bayesian testing procedure. We end with an overview of some objections to the traditional Frequentist method of hypothesis testing, and a comparison between the two approaches.  

## Fundamentals of Bayesian inference

### Probability in times of Covid

Let's start with a classic example in a topical guise. With the aim to move to a new phase in the Covid-19 pandemic, the UK government  planned to employ mass testing as part of a wider strategy also involving general vaccination ([Guardian, 1 December 2020](https://www.theguardian.com/world/2020/dec/01/funds-for-mass-covid-testing-offered-to-local-authorities-in-tier-3)).^[I originally wrote most of this chapter in the first week of December 2020. Revising the chapter in the first week of December 2021, not too much has changed.] The testing involves lateral flow tests, which are relatively inexpensive and give a result in about 20 minutes. At the time, estimates were that these tests give a positive test result in 76.8% of cases of Covid-19, and a negative test result in 99.68% of cases ([Department of Health and Social Care, 11 November 2020](https://www.gov.uk/government/news/oxford-university-and-phe-confirm-high-sensitivity-of-lateral-flow-tests)).^[Later evidence indicates that the tests may be more accurate than thought at first ([BBC, 14 October 2021](https://www.bbc.co.uk/news/health-58899612))]. The true positive rate (76.8%) is also called the __sensitivity__ of a test, and the true negative rate (99.68%) the __specificity__. On 26 November 2020, the [Office of National Statistics ](https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/conditionsanddiseases/bulletins/coronaviruscovid19infectionsurveypilot/26november2020) estimated the rate of Covid-19 cases in the general population of England to be 1.17%. That implies that about 1 in 85 people carried the virus, which is also called the __base rate__. Suppose someone is tested and the test result is positive. What is the probability that they are actually infected with Covid-19? Perhaps surprisingly, this probability does _not_ equal .768. This is the conditional probability $P(\text{positive test}|\text{Covid-19})$, i.e. the sensitivity. What we would like to know however is a different conditional probability, namely $P(\text{Covid-19}|\text{positive test})$. And these are _not_ the same!

We can work out the desired probability using the rules of probability discussed in Section \@ref(sec:02-rules-of-probability). 
$$P(\text{Covid-19}|\text{positive test}) = \frac{P(\text{Covid-19 and positive test})}{P(\text{positive test})}$$
From the multiplication rule, we know that
$$P(\text{Covid-19 and positive test}) = P(\text{positive test}|\text{Covid-19}) \times P(\text{Covid-19})$$ 
To work out $P(\text{positive test})$, we need to consider all the ways in which someone can obtain a positive test result. In this situation, there are two: the person can carry Covid-19 and have a positive test result, or the person can not carry Covid-19 and obtain a positive test result. Thus 
$$P(\text{positive test}) = P(\text{Covid-19 and positive test}) + P(\text{no Covid-19 and positive test})$$
We have already specified how to calculate $P(\text{Covid-19 and positive test})$. Similarly, we can compute $P(\text{no Covid-19 and positive test})$ as
$$P(\text{no Covid-19 and positive test}) = P(\text{positive test}|\text{no Covid-19}) \times P(\text{no Covid-19})$$
Now we are in a position to calculate $P(\text{Covid-19}|\text{positive test})$ from the sensitivity and specificity of the test, and the base rate of Covid-19 infection. We know that $P(\text{Covid-19}) = .0117$, hence $P(\text{no Covid-19}) = 1 - P(\text{Covid-19}) = 1 - .0117 = .9883$. Putting all the numbers in a table:
```{r covid-mass-test-sensitivy-specificty}
tab <- data.frame(c("76.8%","23.2%"),c("0.32%","99.68%"))
rownames(tab) <- c("positive test","negative test")
knitr::kable(tab, col.names = c("Covid-19 (1.17%)","no Covid-19 (98.83%)"), align=c("r","r"), booktabs=TRUE, linesep="")
```

$$\begin{aligned}
P(\text{Covid-19}|\text{positive test}) &= \frac{P(\text{Covid-19 and positive test})}{P(\text{positive test})} \\
&= \frac{P(\text{positive test}|\text{Covid-19}) P(\text{Covid-19})}{P(\text{pos. test}|\text{Covid-19}) P(\text{Covid-19}) + P(\text{pos. test}|\text{no Covid-19}) P(\text{no Covid-19})} \\
&= \frac{.768 \times .0117}{.768 \times .0117 + .0032 \times .9883} \\
&= 0.7396
\end{aligned}$$
So just under 3 out of all 4 people that test positive in this scenario would actually carry Covid-19. As a result, 1 out of 4 people might be asked to quarantine without really needing to do so. Although of course very unfortunate for those people, that does not seem like a too-high price to pay to me. But the base-rate is very important here. If the rate of Covid-19 infections is lowered to $P(\text{Covid-19}) = .001$ (i.e. 0.1%), then the result would be $P(\text{Covid-19}|\text{positive test}) = 0.1937$, which means that only about one in five people who test positive are actually infected by Covid-19! When the base-rate is lowered, massive testing seems like a much less useful procedure.

Perhaps the equations seem a little abstract. Another way to explain the resulting conditional probability is through the tree diagram of Figure \@ref(fig:covid-19-test-tree). The tree represents a group of 100,000 people from the general population, of which 1,176 would have Covid-19, and 903 of these would also get a positive test result. Of the 98,824 people without Covid-19, 316 would receive a positive test result. While the change of a false positive is very low, because so many people do not have Covid-19, the actual frequency of people without Covid-19 who obtain a positive test result is not that much smaller than the number of people with Covid-19 who obtain a positive test result. The conditional probability can then be computed simply as 
$$P(\text{Covid-19}|\text{positive test}) = \frac{903}{903 + 316} = .74$$
which is equal to the value computed earlier (up to rounding error resulting from converting probabilities to whole people).

```{r covid-19-test-tree, fig.cap="Outcome tree representing mass testing for Covid-19."}
library(DiagrammeR)

treePlot <- "
  digraph cvid_tree_plot {

    graph [layout = dot,
           rankdir = LR]

    node [shape = box] 
    S [label = '100,000 people']
    
    node [shape = oval, style = filled]
    H [label = 'Covid-19', fillcolor = '#cc0000']
    T [label = 'no Covid-19', fillcolor = '#00cc00']
    HH [label = 'positive test', fillcolor = '#ff0000' ]
    HT [label = 'negative test', fillcolor = '#990000']
    TH [label = 'positive-test', fillcolor =  '#00ff00']
    TT [label = 'negative-test', fillcolor = '#009900']
    
    edge [label = '1,176']
    S -> {H}
    
    edge [label = '98,824', labelloc=b]
    S -> {T}
    
    edge [label = '903']
    H -> {HH}
    
    edge [label = '273', labelloc=b]
    H -> {HT}
    
    edge [label = '316' ]
    T -> {TH}
    
    edge [label = '98,508', labelloc=b]
    T -> {TT}
}
"
DiagrammeR::grViz(treePlot)
```

### Bayes' rule

In calculating a conditional probability from other conditional probabilities and base-rates, we have just applied the general rules of probability. That's nothing special, really. In abstract notation, the formula known as Bayes' rule is

\begin{equation}
P(A|B) = \frac{P(B|A) P(A)}{P(B|A)P(A) + P(B|\neg A) P(\neg A)}
(\#eq:bayes-rule)
\end{equation}

Again, there is nothing special about this formula itself, it follows from the rules of probability. These rules were however not clearly specified when Reverend Thomas Bayes defined the rule in an essay which was posthumously published [@bayes1763]. More importantly, he used the rule to infer an unknown parameter of a statistical model. According to the Frequentist View (see Section \@ref(sec:02-probability-definition)), a parameter has a true value, but you cannot assign a probability to it, because it is not a random event that has a long-run frequency. It just has one value: the true value.

In assigning probabilities to parameters, Thomas Bayes can be seen as the founding father of the Subjectivist View of probability. There has been quite a lot of philosophical discussion about probability interpretations. The subjectivist view is that a probability represents a rational degree of belief. This belief can be about anything, whether actual events in the world, or more abstract concepts such as hypotheses or model parameters. Bayesian inference concerns adjusting prior beliefs in light of evidence. The resulting adjusted belief is called the posterior belief. In the previous example, the base-rate of Covid-19 infections can be seen as a rational prior belief that a randomly chosen person from the general population in England has Covid-19. Upon observing a positive test result, this __prior probability__ $P(\text{Covid-19})$ can be adjusted to become the __posterior probability__  $P(\text{Covid-19}|\text{positive test})$.

### We missed you Paul!

In Bayesian statistics, we can apply the principles of Bayesian inference to anything we can assign degrees of belief to. For instance, our belief that Paul the Octopus had psychic abilities. In our general model of Paul's predictions (Section \@ref(sec:02-binomial-model), Equation \@ref(eq:definition-binomial-distribution)), we assumed there was a probability that he made a correct prediction, which we denoted by $\theta$. This parameter probability could in principle take any value $0 \leq \theta \leq 1$. The idea of a prior distribution for such a parameter is to assign to each possible value of $\theta$ a "degree of belief" that this is the true value. These degrees of belief should obey the rules of probability. In the coin-flipping model, which assumed Paul was randomly guessing, there was only one possible value, namely $\theta=.5$. That means that, if we were to believe this model is true, we would consequently  believe that any other value is impossible: $P(\theta \neq .5) = 0$, which implies $P(\theta = .5) = 1$. If we don't believe that Paul is necessarily randomly guessing, then the parameter could have other values as well. Figure \@ref(fig:prior-distributions-Paul) shows two possible prior distributions. In the plot on the left, the prior assigns an equal probability to any possible value of $\theta$. This is also called a uniform distribution, and reflects the beliefs of someone who considers that "anything goes" when it comes to Paul's ability to predict the outcome of football matches. In the plot on the right, the prior distribution reflects the beliefs of someone who quite strongly considers Paul a good predictor of the outcome of football matches.  

```{r prior-distributions-Paul, fig.cap="Two different prior distributions for the probability that Paul makes a correct prediction, the normalised likelihood function ($P(Y|\\theta)/P(Y)$) and the resulting posterior distributions after observing that Paul made $Y=12$ out of $n=14$ correct predictions. Wherever the normalised likelihood is larger than 1, the posterior probability is larger than the prior probability, while the posterior probability is lower than the prior probability wherever the normalised likelihood is lower than 1. Note that in MODEL 1, the normalised likelihood and posterior distribution are identical and therefore overlapping on the plot.", fig.width=6,fig.height=3.5, out.width="90%"}
library(dplyr)
library(ggplot2)

data.frame(model=rep(c("MODEL 1","MODEL 2"),each=3*100),type=rep(c("prior","likelihood","posterior"),each=100),x=seq(0,1,length=100)) %>%
  mutate(type=factor(type,levels=c("prior","likelihood","posterior")), y=if_else(model == "MODEL 1",case_when(type=="prior" ~ dbeta(x,1,1), type=="likelihood" ~ dbinom(12,14,x)/(choose(14,12)*(beta(1+12,1+2)/beta(1,1))), type == "posterior" ~ dbeta(x,1+12,1+2)),case_when(type=="prior" ~ dbeta(x,12,2), type=="likelihood" ~ dbinom(12,14,x)/(choose(14,12)*(beta(12+12,2+2)/beta(12,2))), type == "posterior" ~ dbeta(x,12+12,2+2)))) %>%
  ggplot(aes(x=x,y=y,colour=type, group=type)) + geom_line(lwd=1.5,alpha=.5) + facet_wrap(~model) + xlab(expression(theta)) + ylab(expression(p(theta))) + theme(legend.position="bottom", legend.title = element_blank())
```

We can write the posterior distribution for the parameter $\theta$, conditional upon observed data $Y$ (e.g., $Y=12$ out of $n=14$ correct predictions) as:
\begin{align}
p(\theta|Y) &= \frac{P(Y|\theta) \times p(\theta)}{P(Y)} \\
&= \frac{P(Y|\theta)}{P(Y)} \times p(\theta)
(\#eq:posterior-parameter-specification)
\end{align}
Here, $P(Y|\theta)$ is the likelihood function (the probability of the observed data $Y$ given a particular value of $\theta$), $p(\theta)$ is the prior distribution of the parameter $\theta$, and $P(Y)$ is the probability of the data over all possible values of $\theta$ (weighted by their prior probability). This is also called the __marginal likelihood__, and technically is defined as
$$P(Y) = \int P(Y|\theta) p(\theta) d \theta$$
i.e. as the integral of the product of the likelihood function and prior over all possible values of $\theta$, but you don't need to worry about this. The formulation on the second line of Equation \@ref(eq:posterior-parameter-specification) is just a rearrangement of the terms, but is meant to show that you can think of the posterior probability as the product of the prior probability and the __normalised likelihood__ (the likelihood of the data for a particular value of $\theta$ compared to the marginal likelihood of $Y$ over all possible values of $\theta$).  

In words, we can describe Equation \@ref(eq:posterior-parameter-specification) as:
$$\begin{aligned}
\text{posterior} &= \frac{\text{likelihood} \times \text{prior}}{\text{marginal likelihood}} \\
&= \frac{\text{likelihood}}{\text{marginal likelihood}} \times \text{prior} \\
&= \text{normalised likelihood} \times \text{prior}
\end{aligned}$$
The values of the normalized likelihood and the resulting posterior distribution are also shown in Figure \@ref(fig:prior-distributions-Paul). Because the posterior probabilities are calculated by multiplying the prior probability by the normalised likelihood, the posterior probability will be _higher_ than the prior probability when the normalised likelihood is _larger_ than 1. You can think of the normalised likelihood as an average likelihood over all possible values of $\theta$ (we will discuss this in a little more detail soon). So when a particular value of $\theta$ assigns a higher likelihood to the data than the average likelihood of the data, i.e. $\frac{P(Y|\theta)}{P(Y)} > 1$, the evidence that this value of $\theta$ is the true value increases. Conversely, the posterior probability will be _lower_ than than the prior probability when the normalised likelihood is _smaller_ than 1. So when a particular value of $\theta$ assigns a lower likelihood to the data than average, the evidence that this value of $\theta$ is the true value decreases. This, in a nutshell, is how Bayesian inference of parameters works.

Inspecting the posterior distributions resulting from the two models in Figure \@ref(fig:prior-distributions-Paul), you can see that the posterior distribution is mostly located at the higher values of $\theta$, because small values of $\theta$ are not very likely given Paul's success rate of 12 out of 14 correct predictions. Comparing the posterior distributions between the two models, you may also see that the posterior distribution for MODEL 2 is less dispersed (i.e. "narrower") than for MODEL 1. This is a direct consequence of the difference in the prior distributions. The uniform distribution of MODEL 1 does not make very precise predictions regarding the likely values of $\theta$ ("anything goes"). The more dispersed the prior is, the more dispersed the posterior will be. The prior distribution of MODEL 2 is, in some sense, much more daring, indicating that low values of $\theta$ are very unlikely _a priori_. Because these values are not believed to be true in the first place, they are also relatively less likely after observing the data. 

### The marginal likelihood and prior predictive distribution

A Bayesian statistical model consists of both the prior distribution and the likelihood function. Both are integral parts of a Bayesian model. This is different from statistical models in the Frequentist tradition, which only focus on the likelihood. One way to view a Bayesian model is as a _hierarchical model_, similar how linear mixed-effects models can be viewed as hierarchical models. With this formulation, it is straightforward to simulate a Bayesian model, by first sampling a random parameter value from the prior distribution $p(\theta)$, and then using this to sample a value of the dependent variable from the conditional distribution $P(Y|\theta)$. The resulting marginal distribution of the dependent variable, $P(Y)$, is also called the __prior predictive__ distribution. The sampling scheme is
\begin{align}
Y_j &\sim P(Y|\theta_j) \\
\theta_j &\sim p(\theta)
\end{align}
Not only can we use each sampled parameter value $\theta_j$ to sample a value of the dependent variable, we can also use each to compute the likelihood of the data actually observed, e.g. $P(Y=12|\theta_j)$. Figure \@ref(fig:prior-predictive-samples-binomial-bayesian-models) shows 1000 samples parameter values (prior samples), corresponding samples of the dependent variable (prior predictive samples), and the likelihood value for Paul's predictions (sampled likelihood), for both models in Figure \@ref(fig:prior-distributions-Paul). Note that all these are based on the same set of sampled parameter values $\theta$. 

```{r prior-predictive-samples-binomial-bayesian-models, fig.width=6,fig.height=3.5, fig.cap="Number of correct predictions (out of 14) for 1000 simulations from both Bayesian models of Figure \\@ref(fig:prior-distributions-Paul).", fig.show='hold'}
set.seed(91237898)
tdat <- rbind(
  data.frame(model = "MODEL 1", theta = rbeta(1000,1,1)),
  data.frame(model = "MODEL 2", theta = rbeta(1000,12,2))
) %>% mutate(likelihood=dbinom(12,14,prob=theta))
tdat$Y = rbinom(2000,size=14,prob=tdat$theta)
tdat %>%
  ggplot(aes(x=theta)) + geom_histogram(colour="black", binwidth=.025) + facet_wrap(~model) + ggtitle("Prior samples") + xlab(expression(theta))

tdat %>%
  ggplot(aes(x=Y)) + geom_histogram(colour="black", binwidth=1) + facet_wrap(~model) + ggtitle("Prior predictive samples")

tdat %>%
  ggplot(aes(x=likelihood)) + geom_histogram(colour="black", binwidth=.01) + facet_wrap(~model) + ggtitle("Sampled likelihood") + xlab("Likelihood (k=12, n=14)")

```

You can see that the prior predictive distribution looks quite similar in shape to the prior distribution, which is because for this model, there is a close link between the probability of a correct prediction ($\theta$), and the total number of correct predictions ($Y$). You can also see that the likelihood value is relatively low for many sampled parameters from MODEL 1. That is because there are many relatively low sampled values of $\theta$ in MODEL 1, for which the likelihood of 12 out of 14 correct predictions is low. Model 2 does better in accounting for the observed data, with generally higher likelihood values for the sampled parameters. This better fit to the observed data can also be seen in the prior predictive distribution of MODEL 2, where 12 correct predictions is a common occurrence. In MODEL 1, all numbers of correct predictions are sampled roughly equally often.

The sampled likelihoods can be used to compute an average likelihood for each model. This average is an estimate of the marginal likelihood, and computed simply as
$$\hat{P}(Y=12) = \sum_{j=1}^n P(Y|\theta_j)$$
Because most of the sampled likelihoods are low for MODEL 1, the average is relatively low as well: $\hat{P}(Y=12) = `r format(mean(tdat$likelihood[tdat$model == "MODEL 1"]),digits=3)`$. For MODEL 3, most of the sampled likelihoods are relatively high, and hence the average is higher than for MODEL 1:  $\hat{P}(Y=12) = `r format(mean(tdat$likelihood[tdat$model == "MODEL 2"]),digits=3)`$. Because MODEL 2 on average assigns a higher probability to the observed data, it offers a better account of the observed data than MODEL 1. As we will see, a Bayesian version of the likelihood ratio called the Bayes factor, which reflects the ratio of marginal likelihoods for models, is central in the Bayesian version of hypothesis testing.

Before we go on to look at the Bayes factor in more detail, I would like to point out a close correspondence between the marginal likelihood and the prior predictive distribution. In fact, the marginal likelihood is just the prior predictive distribution evaluated at the observed data point. The prior predictive distribution is the marginal distribution of the data, whilst the marginal likelihood is the value of this distribution for a particular value of the dependent variable. So we can also estimate the marginal likelihood by computing the relative frequency of $Y=12$ in the prior predictive distribution. For MODEL 1, this gives $\hat{P}(Y=12) = `r sum(tdat$Y[tdat$model == "MODEL 1"] == 12)/1000`$, and for MODEL 2, it is $\hat{P}(Y=12) = `r sum(tdat$Y[tdat$model == "MODEL 2"] == 12)/1000`$. These two ways of estimating the marginal likelihood are not exactly the same, because they approach the same quantity via a different route.^[The first method is generally more reliable.] And both are subject to random sampling variation due to simulating a limited number of parameter values. Accurately computing marginal likelihoods is, for most models, a rather complicated thing, but we won't go into the details here. 

### Hypothesis testing, relative evidence, and the Bayes factor

In the Frequentist null-hypothesis significance testing procedure, we defined a hypothesis test in terms of comparing two nested models, a general MODEL G and a restricted MODEL R which is a special case of MODEL G. Moreover, we defined the testing procedure in terms of determining the probability of a test result, or one more extreme, given that the simpler MODEL R is the true model. This was necessary because MODEL G is too vague to determine the sampling distribution of the test statistic.

By supplying a prior distribution to parameters, Bayesian models can be "vague" whilst not suffering from the problem that they effectively make no predictions. As we saw for the prior predictive distributions in Figure \@ref(fig:prior-predictive-samples-binomial-bayesian-models), even MODEL 1, which assumes all possible values of the parameter $\theta$ are equally likely, still provides a valid predicted distribution of the data. Because any Bayesian model with a valid prior distribution provides a valid prior predictive distribution, which then also provides a valid value for the marginal likelihood, we do not have to worry about complications that arise when comparing models in the Frequentist tradition, such as that the likelihood of one model will always be higher than the other because we need to estimate an additional parameter by maximum likelihood. The relative marginal likelihood of the data assigned by each model, which can be stated as a marginal likelihood ratio analogous to the likelihood ratio of Chapter \@ref(ch-modeling), provides a direct measure of the relative evidence for both models. The marginal likelihood ratio is also called the Bayes factor, and can be defined for two general Bayesian models as:
\begin{equation}
\text{BF}_{12} = \text{BayesFactor}(\text{MODEL 1}, \text{MODEL 2}) = \frac{P(Y_1,\ldots,Y_n|\text{MODEL 1})}{P(Y_1,\ldots,Y_n|\text{MODEL 2})}
(\#eq:Bayesfactor-definition)
\end{equation}
where $P(Y_1,\ldots,Y_n|\text{MODEL } j)$ denotes the marginal likelihood of observed data $Y_1,\ldots,Y_n$ according to MODEL $j$.

The Bayes factor is a central statistic of interest in Bayesian hypothesis testing. It is a direct measure of the relative evidence for two models. Its importance can also be seen when we consider the ratio of the posterior probabilities for two models, which is also called the __posterior odds__. In a Bayesian framework, we can assign probabilities not just to data and parameters, but also to whole models. These probabilities reflect our belief that a model is "true" in the sense that it provides a better account of the data than other models. Before observing data, we can assign a prior probability $P(\text{model } j)$ to a model, and we can update this to a posterior probability $P(\text{model } j|Y_1,\ldots,Y_n)$ after observing data $Y_1,\ldots,Y_n$. If the marginal likelihood $P(\text{MODEL 2}|Y_1,\ldots,Y_n)$ is larger than 1, the posterior probability is higher than the prior probability, and hence our belief in the model would increase. If the marginal likelihood is smaller than 1, the posterior probability is lower than the prior probability, and hence our belief in the model would decrease. We can compare the relative change in our belief for two models by considering the posterior odds ratio, which is just the ratio of the posterior probability of two models, and computed by multiplying the ratio of the prior probabilities of the models (the prior odds ratio) by the marginal likelihood ratio:
$$\begin{aligned}
\frac{P(\text{MODEL 1}|Y_1,\ldots,Y_n)}{P(\text{MODEL 2}|Y_1,\ldots,Y_n)} &= \frac{P(Y_1,\ldots,Y_n|\text{MODEL 1})}{P(Y_1,\ldots,Y_n|\text{MODEL 2})} \times \frac{P(\text{MODEL 1})}{P(\text{MODEL 2})} \\
\text{posterior odds} &= \text{Bayes factor} \times \text{prior odds}
\end{aligned}$$

In terms of the relative evidence that the data provides for the two models, the Bayes factor is all that matters, as the prior probabilities do not depend on the data. Moreover, if we assign an equal prior probability to each model, then the prior odds ratio would equal 1, and hence the posterior odds ratio is identical to the Bayes factor.

In a Frequentist framework, we would evaluate the magnitude of the likelihood ratio by considering its  place within the sampling distribution under the assumption that one of the models is true. Although in principle we might be able to determine the sampling distribution of the Bayes factor in a similar manner, there is no need. A main reason for going through all this work in the Frequentist procedure was that the models are on unequal footing, with the likelihood ratio always favouring a model with additional parameters. The Bayes Factor does not inherently favour a more general model compared to a restricted one. Hence, we can interpret its value "as is". The Bayes factor is a continuous measure of relative evidential support, and there is no real need for classifications such as "significant" and "non-significant". Nevertheless, some guidance in interpreting the magnitude might be useful. One convention is the classification provided by @jeffreys1939theory in Table \@ref(tab:Bayes-Factor-values-interpretation). Because small values below 1, when the Bayes factor favours the second model, can be difficult to discern, the table also provides the corresponding values of the logarithm of the Bayes factor ($\log \text{BF}_{1,2}$). On a logarithmic scale, any value above 0 favours the first model, and any value below 0 the second one. Moreover, magnitudes above and below 0 can be assigned a similar meaning. 

Table: (\#tab:Bayes-Factor-values-interpretation) Interpretation of the values of the Bayes factor [after @jeffreys1939theory].

$\text{BF}_{1,2}$ | $\log \text{BF}_{1,2}$ | Interpretation 
--------:|--------------:|---------------
&gt; 100  | &gt; `r round(log(100),2)`  | Extreme evidence for MODEL 1 
30 -- 100 | `r round(log(30),2)` --  `r round(log(100),2)`  | Very strong evidence for MODEL 1 
10 -- 30 | `r round(log(10),2)` -- `r round(log(30),2)` | Strong evidence for MODEL 1
3 -- 10 | `r round(log(3),2)` -- `r round(log(10),2)` | Moderate evidence for MODEL 1
1 -- 3 | `r round(log(1),2)` -- `r round(log(3),2)` | Anecdotal evidence for MODEL 1
1 | 0 | No evidence 
1/3 -- 1 | `r round(log(1/3),2)` -- `r round(log(1),2)` | Anecdotal evidence for MODEL 2
1/10 -- 1/3 | `r round(log(1/10),2)` -- `r round(log(1/3),2)` | Moderate evidence for MODEL 2
1/30 -- 1/10 | `r round(log(1/30),2)` -- `r round(log(1/10),2)` | Strong evidence for MODEL 2
1/100 -- 1/30 | `r round(log(1/100),2)` -- `r round(log(1/30),2)` | Very strong evidence for MODEL 2
&lt; 1/100 | &lt; `r round(log(1/100),2)`  | Extreme evidence for MODEL 2

The Bayes factor is a general measure that can be used to compare any Bayesian models. We do not have to focus on nested models, as we did with null-hypothesis significance testing. But such nested model comparisons are often of interest. For instance, when considering Paul's psychic abilities, fixing $\theta = .5$ is a useful model of an octopus without psychic abilities, while a model that allows $\theta$ to take other values is a useful model of a (somewhat) psychic octopus. For the first model, assigning prior probability to $\theta$ is simple: the prior probability of $\theta = .5$ is $P(\theta = .5) = 1$, and $P(\theta \neq .5) = 0$. For the second model, we need to consider how likely each possible value of $\theta$ is. Figure \@ref(fig:prior-predictive-samples-binomial-bayesian-models) shows two choices for this prior distribution, which are both valid representations of the belief that $\theta$ can be different from .5. These choices will give a different marginal likelihood, and hence a different value of the Bayes factor when comparing them to the restricted null-model

$$\text{MODEL 0}: \theta = .5$$
The Bayes factor comparing MODEL 1 to MODEL 0 is

$$\text{BF}_{1,0} = `r ((choose(14,12)*(beta(12+1,2+1)/beta(1,1)))/dbinom(12,14,.5))`$$
which indicates that the data (12 out of 14 correct predictions) is roughly 12 times as likely under MODEL 1 compared to MODEL 0, which in the classification of Table \@ref(tab:Bayes-Factor-values-interpretation) means strong evidence for MODEL 1. For MODEL 2, the Bayes factor is

$$\text{BF}_{2,0} = `r ((choose(14,12)*(beta(12+12,2+2)/beta(12,2)))/dbinom(12,14,.5))`$$
which indicates that the data is roughly 36 times as likely under MODEL 2 compared to MODEL 0, which would be classified as very strong evidence for MODEL 2. In both cases, the data favours the alternative model to the null model and may be taken as sufficient to reject MODEL 0. However, the strength of the evidence varies with the choice of prior distribution of the alternative model. This is as it should be. A model such as such as MODEL 2, which places stronger belief on higher values of $\theta$, is more consistent with Paul's high number of correct predictions.

Bayesian hypothesis testing with Bayes factors is, at it's heart, a model comparison procedure. Bayesian models consist of a likelihood function _and_ a prior distribution. A different prior distribution means a different model, and therefore a different result of the model comparison. Because there are an infinite number of alternative prior distributions to the one of the null model, there really isn't a single test of the null hypothesis $H_0: \theta = .5$. The prior distribution of MODEL 1, where each possible value of $\theta$ is equally likely, is the Bayesian equivalent of the alternative hypothesis in a null-hypothesis significance testing, and as such might seem a natural default against which to compare the null hypothesis. But there is nothing to force this choice, and other priors are in principle equally valid, as long as they reflect your _a priori_ beliefs about likely values of the parameter. Notice the "a priori" specification in the last sentence: it is vital that the prior distribution is chosen _before_ observing the data. If you choose the a prior distribution to match the data _after_ having looked at it, the procedure loses some of its meaning as a hypothesis test, even if the Bayes factor is still an accurate reflection of the evidential support of the models.

### Parameter estimates and credible intervals

Maximum likelihood estimation provides a single point-estimate for each parameter. In a Bayesian framework, estimation involves updating prior beliefs to posterior beliefs. What we end up with is a posterior distribution over the parameter values. If you want to report a single estimate, you could chose one of the measures of location: the mean, median, or mode of the posterior distribution. Unless the posterior is symmetric, these will have different values (see Figure \@ref(fig:posterior-distribution-Paul)), and one is not necessarily better than the other. I would usually choose the posterior mean, but if the posterior is very skewed, a measure such as the mode or median might provide a better reflection of the location of the distribution. 

In addition to reporting an estimate, it is generally also a good idea to consider the uncertainty in the posterior distribution. A Bayesian version of a confidence interval (with a more straightforward interpretation!) is a __credible interval__. A credible interval is an interval in the posterior distribution which contains a given proportion of the probability mass. A common interval is the __Highest Density Interval__ (HDI), which is the narrowest interval which contains a given proportion of the probability mass. Figure \@ref(fig:posterior-distribution-Paul) shows the 95% HDI of the posterior probability that Paul makes a correct prediction, where the prior distribution was the uniform distribution of MODEL 1 in Figure \@ref(fig:prior-distributions-Paul). 

```{r posterior-distribution-Paul, fig.cap="Posterior distribution for the probability that Paul makes a correct prediction, for MODEL 1 in Figure \\@ref(fig:prior-distributions-Paul).  ", fig.width=6,fig.height=3.5, out.width="90%"}
library(dplyr)
library(ggplot2)

HDI <- HDInterval::hdi(qbeta, .95, shape1=13, shape2=3)


data.frame(model=rep(c("MODEL 1","MODEL 2"),each=3*100),type=rep(c("prior","likelihood","posterior"),each=100),x=seq(0,1,length=100)) %>%
  mutate(type=factor(type,levels=c("prior","likelihood","posterior")), y=if_else(model == "MODEL 1",case_when(type=="prior" ~ dbeta(x,1,1), type=="likelihood" ~ dbinom(12,14,x)/(choose(14,12)*(beta(1+12,1+2)/beta(1,1))), type == "posterior" ~ dbeta(x,1+12,1+2)),case_when(type=="prior" ~ dbeta(x,12,2), type=="likelihood" ~ dbinom(12,14,x)/(choose(14,12)*(beta(12+12,2+2)/beta(12,2))), type == "posterior" ~ dbeta(x,12+12,2+2)))) %>%
  filter(type=="posterior", model == "MODEL 1") %>%
  ggplot(aes(x=x,y=y)) + geom_line(lwd=1) + xlab(expression(theta)) + ylab(expression(p(theta))) + theme(legend.position="bottom", legend.title = element_blank()) + geom_segment(data=data.frame(x=13/16, xend=13/16, y=0, yend = dbeta(13/16, 13,3)), mapping=aes(x=x,y=y,xend=xend, yend=yend, linetype="mean")) + geom_segment(data=data.frame(x=(13-1/3)/(16 - 2/3), xend=(13-1/3)/(16 - 2/3), y=0, yend = dbeta((13-1/3)/(16 - 2/3), 13,3)), mapping=aes(x=x,y=y,xend=xend, yend=yend, linetype="median")) + geom_segment(data=data.frame(x=(13-1)/(16 - 2), xend=(13-1)/(16 - 2), y=0, yend = dbeta((13-1)/(16 - 2), 13,3)), mapping=aes(x=x,y=y,xend=xend, yend=yend, linetype="mode")) + geom_polygon(data=data.frame(x=c(HDI[1], seq(HDI[1],HDI[2],length=100), HDI[2]), y=c(0,dbeta(seq(HDI[1],HDI[2],length=100), shape1=13,shape2=3),0)), mapping=aes(x=x,y=y, fill="95% HDI"), alpha=.5) #+ scale_linetype_manual(values=c("mean" = "solid", "median" = "broken", "mode" = "dotted"))
```

A slightly different way to compute a credible interval is as the __central credible interval__. For such an interval, the excluded left and right tail of the distribution each contain $\tfrac{\alpha}{2}$ of the probability mass (where e.g. $\alpha = .05$ for a 95% credible interval). Unlike the HDI, the central credible interval is not generally the most narrow interval which contains a given proportion of the posterior probability. But it is generally more straightforward to compute. Nevertheless, the HDI is more often reported than the central credible interval.

A nice thing about credible intervals is that they have a straightforward interpretation: the (subjective) probability that the true value of a parameter lies within an $x$% credible interval is $x$%. Compare this to the correct interpretation of an $x$% Frequentist confidence interval, which is that for infinite samples from the DGP, and computing an infinite number of corresponding confidence intervals, $x$% of those intervals would contain the true value of the parameter. 

## A Bayesian t-test

As discussed above, Bayesian hypothesis testing concerns comparing models with different prior distributions for model parameters. If one model, the "null model", restricts a parameter to take a specific value, such as $\theta = .5$, or $\mu = 0$, while another model allows the parameter to take different values, we compare a restricted model to a more general one, and hence we can think of the model comparison as a Bayesian equivalent to a null-hypothesis significance test. The prior distribution assigned to the parameter in the more general alternative model will determine the outcome of the test, and hence it is of the utmost importance to choose this sensibly. This, however, is not always easy. Therefore, much work has been conducted to derive sensible __default priors__ to enable researchers to conduct Bayesian hypothesis tests without requiring them to define prior distributions which reflect their own subjective beliefs.

@rouder2009bayesian developed a default prior distribution to test whether two groups have a different mean. The test is based on the two-group version of the General Linear Model (e.g. Section \@ref(sec:06-two-groups)):

$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \epsilon_i \quad \quad \epsilon_i \sim \textbf{Normal}(0, \sigma_\epsilon)$$
where $X_{1,i}$ is a contrast-coded predictor with the values $X_{1i} = \pm \tfrac{1}{2}$ for the different groups. Remember that with this contrast code, the slope $\beta_1$ reflects the difference between the group means, e.g. $\beta_1 = \mu_1 - \mu_2$, and the intercept represents the grand mean $\beta_0 = \frac{\mu_1 + \mu_2}{2}$. Testing for group differences involves a test of the following hypotheses: 

$$\begin{aligned}
H_0\!: & \quad \beta_1 = 0 \\
H_1\!: & \quad \beta_1 \neq 0 \\
\end{aligned}$$

To do this in a Bayesian framework, we need prior distributions for all the model parameters ($\beta_0$, $\beta_1$, and $\sigma_\epsilon$). @rouder2009bayesian propose to use so-called uninformative priors for $\beta_0$ and $\sigma_\epsilon$ (effectively meaning that for these parameters, "anything goes"). The main consideration is then the prior distribution for $\beta_1$. Rather than defining a prior distribution for $\beta_1$ directly, they propose to define a prior distribution for $\frac{\beta_1}{\sigma_\epsilon}$, which is the difference between the group means divided by the variance of the dependent variable within each group. This is a measure of effect-size and is also known as Cohen's $d$:

$$\text{Cohen's } d = \frac{\mu_1 - \mu_2}{\sigma_\epsilon} \quad \left(= \frac{\beta_1}{\sigma_\epsilon}\right)$$
Defining the prior distribution for the effect-size is more convenient than defining the prior distribution for the difference between the means, as the latter difference is dependent on the scale of the dependent variable, which makes it difficult to define a general prior distribution suitable for all two-group comparisons. The "default" prior distribution they propose is a so-called scaled Cauchy distribution:

$$\frac{\beta_1}{\sigma_\epsilon} \sim \mathbf{Cauchy}(r)$$
The Cauchy distribution is identical to a $t$-distribution with one degree of freedom ($\text{df} = 1$). The scaling factor $r$ can be used to change the width of the distribution, so that either smaller or larger effect sizes become more probable. Examples of the distribution, with three common values for the scaling factor $r$ ("medium": $r = \frac{\sqrt{2}}{2}$, "wide": $r = 1$, and "ultrawide": $r = \sqrt{2}$), are depicted in Figure \@ref(fig:scaled-cauchy-distribution).

```{r scaled-cauchy-distribution, fig.cap="Scaled Cauchy prior distributions on the effect size $\\frac{\\beta_1}{\\sigma_\\epsilon}$"}
library(dplyr)
library(ggplot2)
 x <- seq(-8,8,length=500)
rbind(data.frame(r=sqrt(2)/2, x = x, y = dcauchy(x/(sqrt(2)/2))),
      data.frame(r=1, x = x, y = dcauchy(x)),
      data.frame(r=sqrt(2), x = x, y = dcauchy(x/(sqrt(2))))
) %>% mutate(r = factor(r,labels=c(expression(sqrt(2) %/% 2),expression(1),expression(sqrt(2))))) %>%
  ggplot(aes(x=x,y=y,colour=r, group=r)) + geom_line() + theme(legend.position="bottom") + scale_colour_discrete(labels = expression(over(sqrt(2),2), 1, sqrt(2))) + xlab(expression(over(beta[1],sigma[epsilon]))) + ylab("")
```

@rouder2009bayesian call the combination of the priors for the effect size and error variance the Jeffreys-Zellner-Siow prior (JZS prior). The "default" Bayesian t-test is to compare the model with these priors to one which assumes $\beta_1 = 0$, i.e. a model with a prior $P(\beta_1 = 0) = 1$ and   $P(\beta_1 \neq 0) = 0$, whilst using the same prior distributions for the other parameters ($\beta_0$ and $\sigma_\epsilon$).

```{r compute-bayesian-t-test}
library(BayesFactor)
library(sdamr)
data("tetris2015")
dat <- subset(tetris2015, Condition %in% c("Tetris_Reactivation","Reactivation"))
dat$Condition <- droplevels(dat$Condition)
dat$intrusions <- dat$Days_One_to_Seven_Number_of_Intrusions
tmp <- ttestBF(dat$intrusions[dat$Condition == "Reactivation"], dat$intrusions[dat$Condition == "Tetris_Reactivation"], r=1)
```

As an example, we can apply the Bayesian t-test to the data from the Tetris study analysed in Chapter \@ref(ch-ANOVA). Comparing the Tetris+Reactivation condition to the Reactivation-Only condition, and setting the scale of the prior distribution for the effects size in the alternative MODEL 1 to $r=1$, provides a Bayes factor comparing the alternative hypothesis $H_1$ ($\beta \neq 0$) to the null-hypothesis $H_0$ ($\beta_1 = 0$) of $\text{BF}_{1,0} = `r exp(tmp@bayesFactor$bf)`$, which can be interpreted as strong evidence against the null hypothesis.

As we indicated earlier, the value of the Bayes factor depends on the prior distribution for the tested parameter in the model representing the alternative hypothesis. This dependence is shown in Figure \@ref(fig:bayesian-t-tests-different-rscale) by varying the scaling factor $r$.  

```{r bayesian-t-tests-different-rscale, cache=TRUE, fig.cap="Bayes factor $\\text{BF}_{1,0}$ testing equivalence of the means of the Tetris+Reactivation and Reactivation-Only conditions for different values of the scaling factor $r$ of the scaled Cauchy distribution."}
rscales <- exp(seq(log(.001),log(4),length=100))
BFS <- vector("double",length=length(rscales))
for(i in 1:length(rscales)) {
  BFS[i] <- exp(ttestBF(dat$intrusions[dat$Condition == "Reactivation"], dat$intrusions[dat$Condition == "Tetris_Reactivation"], r=rscales[i])@bayesFactor$bf)
}
ggplot(data.frame(r=rscales,BF=BFS),aes(x=r,y=BF)) + geom_line() + geom_hline(yintercept=1, lty=3) + geom_hline(yintercept=3, lty=3) + geom_hline(yintercept=10, lty=3) + ylim(c(0,max(BFS))) + ylab(expression(BF[group("",list(1,0),"")])) + xlab(expression(italic(r)))
```

As this figure shows, the Bayes factor is small for values of $r$ close to 0. The lower the value of $r$, the less wide the resulting Cauchy distribution becomes. In the limit, as $r$ reaches 0, the prior distribution in the alternative model becomes the same as that of the null model (i.e., assigning only probability to the value $\beta_1 = 0$). This makes the models indistinguishable, and the Bayes factor would be 1, regardless of the data. As $r$ increases in value, we see that the Bayes factor quickly rises, showing support for the alternative model. For this data, the Bayes factor is largest for a scaling factor just below $r=1$. When the prior distribution becomes wider than this, the Bayes factor decreases again. This is because the prior distribution then effectively assigns too much probability to high values of the effect size, and as a result lower probability to small and medium values of the effect size. At some point, the probability assigned to the effect size in the data becomes so low, that the null model will provide a better account of the data than the alternative model. A plot like the one in Figure \@ref(fig:bayesian-t-tests-different-rscale) is useful to inspect the robustness of a test result to the specification of the prior distribution. In this case, the Bayes factor shows strong evidence ($\text{BF}_{1,0} > 10$) for a wide range of sensible values of $r$, and hence one might consider the test result quite robust. You should not use a plot like this to determine the "optimal" choice of the prior distribution (i.e. the one with the highest Bayes factor). If you did this, then the prior distribution would depend on the data, which is sometimes referred to as "double-dipping". You would then end up with similar issues as in Frequentist hypothesis testing, where substituting an unknown parameter with a maximum likelihood estimate biases the likelihood ratio to favour the alternative hypothesis, which we then needed to correct for by considering the sampling distribution of the likelihood ratio statistic under the assumption that the null hypothesis is true. A nice thing about Bayes factors is that we do not need to worry about such complications. But that changes if you try to "optimise" a prior distribution by looking at the data.

## Bayes factors for General Linear Models

The suggested default prior distributions can be generalized straightforwardly to more complex versions of the General Linear Model, such as multiple regression [@liang2008mixtures] and ANOVA models [@rouder2012default], by specifying analogous JZS prior distributions over all parameters. This provides a means to test each parameter in a model individually, as well as computing omnibus tests by comparing a general model to one where the prior distribution allows only a single value (i.e. $\beta_j = 0$) for multiple parameters.

Table \@ref(tab:bayesian-lm-speedate) shows the results of a Bayesian equivalent to the moderated regression model discussed in Section \@ref(sec:05-dont-forget-about-the-fun). The results generally confirm the results of the frequenist tests employed there, although evidence for the interaction between fun and intelligence can be classified as "anecdotal". 
```{r bayesian-lm-speedate, cache=TRUE}
set.seed(239472493)
data("speeddate")
dat <- subset(speeddate,!is.na(other_like) & !is.na(other_attr) & !is.na(other_intel) & !is.na(other_fun))
dat$other_attr <-  scale(dat$other_attr, scale=FALSE)
dat$other_intel <- scale(dat$other_intel, scale=FALSE)
dat$other_fun <- scale(dat$other_fun,scale=FALSE)
dat$attrintel <- dat$other_attr*dat$other_intel
dat$funintel <- dat$other_fun*dat$other_intel
BFreg <- regressionBF(other_like ~ other_attr + other_intel + other_fun + attrintel + funintel, data=dat, whichModels = "top", progress=FALSE)
tmp <- data.frame(effect = c("$\\texttt{attr}$","$\\texttt{intel}$","$\\texttt{fun}$","$\\texttt{attr} \\times \\texttt{intel}$","$\\texttt{fun} \\times \\texttt{intel}$"), BF =  exp(-BFreg@bayesFactor$bf)[c(5:1)])
tmp$BF <- round(tmp$BF,2)
if(knitr::is_latex_output()) {
  tmp$BF[tmp$BF > 1000] <- "$> 1000$"
} else {
  tmp$BF[tmp$BF > 1000] <- "&gt; 1000"
}
knitr::kable(tmp, escape=FALSE, booktabs=TRUE, linesep="", caption = "Results of a Bayesian regression analysis for the Speed Dating data (cf Table \\@ref(tab:full-model-like)) with a default JZS prior with 'medium' scaling factor $r = \\sqrt{2}/4$ (for regression models, default scaling factors are  $\\sqrt{2}/4$, $1/2$, and $\\sqrt{2}/2$ for medium, wide, and ultrawide, respectively). The test of each effect compares the full model to one with that effect excluded.", format.args = list(scientific = FALSE))

```

Table \@ref(tab:bayesian-anova-expBelief) shows the Bayesian equivalent of the factorial ANOVA reported in Section \@ref(sec:06-main-effects-and-interactions). The results show "extreme" evidence for an effect of experimenter belief, and no evidence for an effect of power prime, nor for an interaction between power prime and experimenter belief. In the Frequentist null-hypothesis significance test, the absence of a significant test result can not be taken as direct evidence for the null hypothesis. There is actually no straightforward way to quantify the evidence for the null hypothesis in a Frequentist framework. This is not so for the Bayesian hypothesis tests. Indeed, the Bayes factor directly quantifies the relative evidence for either the alternative or null hypothesis. Hence, we find "moderate" evidence that the null hypothesis is true for power prime, and for the interaction between power prime and experimenter belief. This ability to quantify evidence both for and against the null hypothesis is one of the major benefits of a Bayesian hypothesis testing procedure. 

```{r bayesian-anova-expBelief, cache=TRUE}
set.seed(23987423)
options(contrasts = c("contr.sum", "contr.poly"))
data("expBelief")
BFanova <- expBelief %>%
  mutate(experimenterBelief = factor(experimenterBelief), primeCond = factor(primeCond)) %>%
    anovaBF(ApproachAdvantage ~ experimenterBelief * primeCond, whichModels = "top", data=., iterations = 500000, progress=FALSE)
knitr::kable(data.frame(effect = c("$\\texttt{P}$","$\\texttt{B}$","$\\texttt{P} \\times \\texttt{B}$"), BF =  exp(-BFanova@bayesFactor$bf)[c(2,3,1)]), escape=FALSE, booktabs=TRUE, linesep="", caption = "Results of a Bayesian factorial ANOVA analysis for the social priming data (cf Table \\@ref(tab:expBelief-factorial-ANOVA-results)) with a default JZS prior with a 'medium' scaling factor of $r = 1/2$ (for ANOVA models, default scaling factors are $1/2$, $\\sqrt{2}/2$, and $1$ for medium, wide, and ultrawide, respectively; this assumes standard effect coding for the contrast-coded predictors, which then matches the priors to those set for the linear regression model). The test of each effect compares the full model to one with that effect excluded.")
```

## Some objections to null-hypothesis significance testing

Above, we have presented a Bayesian alternative to the traditional Frequentist null-hypothesis significance testing (NHST) procedure. While still the dominant method of statistical inference in psychology, the appropriateness of the NHST has been hotly debated almost since its inception [@Cohen94; @Nickerson00; @Wagenmakers07]. One issue is that a significant test result is not the same as a "theoretical" or "practical" significance. For a given true effect not equal to 0, the (expected) $p$-value becomes smaller and smaller as the sample size increases, because of the increased power in detecting that effect. As a result, even the smallest effect size will become significant for a sufficiently large sample size. For example, a medicine might result in a significant decrease of a symptom compared to a placebo, even if the effect is hardly noticeable to the patient. I should point out that this is more an issue with testing a "point" null hypothesis (e.g. the hypothesis that the effect is exactly equal to 0), rather than an issue with the Frequentist procedure per se. It is an important limitation of null hypothesis testing procedures in general. A similar objection to these hypotheses is that the null hypothesis is unlikely to ever be exactly true. @thompson1992two states the potential issues strongly as: 

> Statistical significance testing can involve a tautological logic in which tired researchers, having collected data on hundreds of subjects, then, conduct a statistical test to evaluate whether there were a lot of subjects, which the researchers already know, because they collected the data and know they are tired. This tautology has created considerable damage as regards the cumulation of knowledge. 
> [@thompson1992two, p. 436]

There are other objections, which I will go into in the following sections.

### The $p$-value is not a proper measure of evidential support

<!-- For a given true effect size not equal to 0, the (expected) $p$-value becomes smaller and smaller as the sample size increases. As a result, very small effect sizes will be significant for a sufficient sample size.--> 

It is common practice to interpret the magnitude of the $p$-value as an indication of the strength of the evidence against the null hypothesis. That is, a smaller $p$-value is taken to indicate stronger evidence against the null hypothesis than a larger $p$-value. Indeed, Fisher himself seems to have subscribed to this view [@Wagenmakers07]. While it is true that the magnitude is often correlated with the strength of evidence, there are some tricky issues regarding this. If a $p$-value were a "proper" measure of evidential support, then if two experiments provide the same $p$-value, they should provide the same support against the null hypothesis. But what if the first experiment had a sample size of 10, and the second a sample size of 10,000? Would a $p$-value of say $p=.04$ indicate the same evidence against the null-hypothesis? The general consensus is that sample size is an important consideration in the interpretation of the $p$-value, although not always for the same reason. On the one hand, many researchers argue that the $p$-value of the larger study provides stronger evidence, possibly because the significant result in the larger study might be less likely due to random sample variability [see e.g. @rosenthal1963interpretation]. On the other hand, it can be argued that the smaller study actually provides stronger evidence, because to obtain the same $p$-value, the effect size must be larger in the smaller study. Bayesian analysis suggests the latter interpretation is the correct one [@Wagenmakers07]. That the same $p$-value can indicate a different strength of evidence means that the $p$-value does not directly reflect evidential support (at least not without considering the sample size). 

Another thing worth pointing out is that, if the null hypothesis is true, any $p$-value is equally likely. This is by definition. Remember that the $p$-value is defined as the probability of obtaining the same test statistic, or one more extreme, assuming the null-hypothesis is true. A $p$-value of say $p=.04$ indicates that you would expect to find an equal or more extreme value of the test statistic in 4% of all possible replications of the experiment. Conversely, in 4% of all replications would you obtain a $p$-value of $p \leq .04$. For a $p$-value of $p=.1$, you would expect to find a similar or smaller $p$-value in 10% of all replications of the experiment. The only distribution for which this relation between the value ($p$) and the probability of obtaining a value equal-or-smaller than it $P(p-\text{value} \leq p)$, is the uniform distribution. So, when the null hypothesis is true, there is no reason to expect a large $p$-value, because every $p$-value is equally likely. When the null hypothesis is false, smaller $p$-values are more likely than higher $p$-values, especially as the sample size increases. This is show by simulation for a one-sample t-test in Figure \@ref(fig:simulation-p-value-distribution). Under the null hypothesis (left plot), the distribution of the $p$-values is uniform.   

```{r simulation-p-value-distribution,fig.width=8,fig.height=3, cache=TRUE, fig.cap="Distribution of $p$-values for 10,000 simulations of a one-sample $t$-test. $\\delta = \\frac{\\mu - \\mu_0}{\\sigma}$ refers to the effect size. Under the null hypothesis (left plot; $\\delta = 0$) the distribution of the $p$-values is uniform. When the null-hypothesis is false ($\\delta = .3$), the distribution is skewed, with smaller $p$-values being more probable, especially when the sample size is larger (compare the middle plot with $n=10$ to the right-hand plot with $n=50$)."}
set.seed(234876234)
tmp <- rbind(data.frame(situation="Î´ = 0 (n=50)", p = apply(matrix(rnorm(500000),nrow=50),2,function(x) t.test(x)$p.value)),
      data.frame(situation="Î´; = .3 (n=10)", p = apply(matrix(rnorm(100000, mean=.3),nrow=10),2,function(x) t.test(x)$p.value)),
      data.frame(situation="Î´ = .3 (n=50)", p = apply(matrix(rnorm(500000, mean=.3),nrow=50),2,function(x) t.test(x)$p.value)))
tmp$situation <- factor(tmp$situation, levels=unique(tmp$situation))
ggplot(tmp,aes(x=p)) + geom_histogram(aes(y=..density..),breaks=seq(0,1,length=21), colour="black") + facet_wrap(~situation) + xlab("p-value")
```

### The $p$-value depends on researcher intentions

The sampling distribution of a test statistic is the distribution of the values of the statistic calculated for an infinite number of datasets produced by the same Data Generating Process (DGP). The DGP includes all the relevant factors that affect the data, including not only characteristics of the population under study, but also characteristics of the study, such as whether participants were randomly sampled, how many participants were included, which measurement tools were used, etc. Choices such as when to stop collecting data are part of the study design. That means that the same data can have a different $p$-value, depending on whether the sample size was fixed a priori, or whether sampling continued until some criterion was reached. The following story, paraphrased from [@berger1988likelihood, p. 30â€“33], may highlight the issue:

A scientist has obtained 100 independent observations that are assumed be Normal-distributed with mean $\mu$ and standard deviation $\sigma$. In order to test the null hypothesis that $\mu=0$, the scientist consults a Frequentist statistician. The mean of the observations is $\overline{Y} = 0.2$, and the sample standard deviation is $S_Y=1$, hence the $p$-value is $p = `r pvalue(2*(1-pt((.2-0)/(1/sqrt(100)),df=99)))`$, which is a little lower than than the adopted significance level of $\alpha.05$. This leads to a rejection of the null hypothesis, and a happy scientist. However, the statistician decides to probe deeper and asks the scientist what he would have done in case that the experiment had not yielded a significant result after 100 observations. The scientist replies he would have collected another 100 observations. As such, the implicit sampling plan was not to collect $n=100$ observation and stop, but rather to first take 100 observations and check whether $p <.05$, and collect another 100 observations (resulting in $n=200$) if not. This is a so-called sequential testing procedure, and requires a different treatment than a fixed-sampling procedure. In controlling the Type 1 error of the procedure as a whole, one would need to consider the possible results after $n=100$ observations, but also after $n=200$ observations, which is possible, but not straightforward, as the results of after $n=100$ are dependent on the results after $n=100$ observations. But the clever statistician works it out and then convinces the scientist that the appropriate p-value for this sequential testing procedure is no longer significant. The puzzled and disappointed scientist leaves to collect another 100 observations. After lots of hard work, the scientist returns, and the statistician computes a $p$-value for the new data, which is now significant. Just to make sure the sampling plan is appropriately reflected in the calculation, the statistician asks what the scientist would have done if the result would not have been significant at this point. The scientist answers "This would depend on the status of my funding; If my grant is renewed, I would test another 100 observations. If my grant is not renewed, I would have had to stop the experiment. Not that this matters, of course, because the data were significant anywayâ€. The statistician then explains that the correct inference depends on the grant renewal; if the grant is not renewed, the sampling plan stands and no correction is necessary. But if the grant is renewed, the scientist could have collected more data, which calls for a further correction, similar to the first one. The annoyed scientist then leaves and resolves to never again share with the statistician her hypothetical research intentions.

What this story shows is that in considering infinite possible repetitions of a study, everything about the study that might lead to variations in the results should be taken into account. This includes a scientists' decisions made during each hypothetical replication of the study. As such, the interpretation of the data at hand (i.e., whether the hypothesis test is significant or not significant) depends on hypothetical decisions in situations that did not actually occur. If exactly the same data had been collected by a scientist who would have not have collected more observations, regardless of the outcome of the first test, then the result would have been judged significant. So the same data can provide different evidence. This does not mean the Frequentist NHST is inconsistent. The procedure "does what it says on the tin", namely providing a bound on the rate of Type 2 errors _in decisions_, when the null hypothesis is true. In considering the accuracy of the decision procedure, we need to consider all situations in which a decision might be made in the context of a given study. This means considering the full design of the study, including the sampling plan, as well as, to some extent, the analysis plan. For instance, if you were to "explore" the data, trying out different ways to analyse the data, by e.g. including or excluding potential covariates and applying different criteria to excluding participants or their responses until you obtain a significant test result for an effect of interest, then the significance level $\alpha$ for that test needs to be adjusted to account for such a fishing expedition. This fishing expedition is also called p-hacking [@simmons2011false] and there really isn't a suitable correction for it. Although corrections for multiple comparisons exist, which allow you to test all possible comparisons within a single model (e.g. the ScheffÃ© correction), when you go on to consider different models, and different subsets of the data to apply that model to, all bets are off. This, simply put, is just really bad scientific practice. And it would render the $p$-value meaningless. 

### Results of a NHST are often misinterpreted

I have said it before, and I will say it again: the $p$-value is the probability of observing a particular value of a test statistic, or one more extreme, given that the null-hypothesis is true. This is the proper, and only, interpretation of the $p$-value. It is a tricky one, to be sure, and the meaning of the $p$-value is often misunderstood. Some common misconceptions [see e.g., @Nickerson00] are:

1. The $p$-value is the probability that the null-hypothesis is true, given the data, i.e. $p = P(H_0|\text{data})$. This posterior probability can be calculated in a Bayesian framework, but not in a Frequentist one. 
2. One minus the $p$-value is the probability that the alternative hypothesis is true, given the data, i.e. $1-p = P(H_1|\text{data})$. Again, the posterior probability of the alternative hypothesis can be obtained in a Bayesian framework, when the alternative hypothesis is properly defined by a suitable prior distribution. In the conventional Frequentist NHST, the alternative hypothesis is so poorly defined, that it can't be assigned any probability (apart from perhaps $P(H_1) = P(H_1|\text{data}) = 1$, which does not depend on the data, and just reflects that e.g. $-\infty \leq \mu - \mu_0 \leq \infty$ will have _some_ value).  
3. The $p$-value is the probability that the results were due to random chance. If you take a statistical model seriously, then all results are, to some extent, due to random chance. Trying to work out the probability that something is a probability seems a rather pointless exercise (if you want to know the answer, it is 1. It would have been more fun if the answer was 42, but alas, the scale of probabilities does not allow this particular answer).

Misinterpretations of $p$-values are mistakes by practitioners, and do not indicate a problem with NHST itself. However, it does point to a mismatch between what the procedure provides, and what the practitioner would like the procedure to provide. If one desires to know the probability that the null hypothesis is true, or the probability that the alternative hypothesis is true, than one has to use a Bayesian procedure. Unless you consider a wider context, where the truth of hypotheses can be sampled from a distribution, then there is no "long-run frequency" for the truth of hypotheses, and hence no Frequentist definition of that probability.

## To Bayes or not to Bayes? A pragmatic view

At this point, you might feel slightly annoyed. Perhaps even _very_ annoyed. We have spent all the preceding chapters focusing on the Frequentist null hypothesis significance testing procedure, and after all that work I'm informing you of these issues. Why? Was all that work for nothing?

No, obviously not. Although much of the criticism regarding the NHST is appropriate, as long as you understand what it does and apply the procedure properly, there is no need to abandon it. The NHST is designed to limit the rate of Type 1 errors (rejecting the null hypothesis when it is true). It does this well. And, when using the appropriate test statistic, in the most powerful way possible. Limiting Type 1 errors is, whilst modest, a reasonable concern in scientific practice. The Bayesian alternative allows you to do more, such as evaluate the relative evidence for and against the null hypothesis, and even calculate the posterior probability of both (as long as you are willing to assign a prior probability to both as well). 

An advantage of the NHST is its "objectiveness": once you have determined a suitable distribution of the data, and decided on a particular value for a parameter to test, there are no other decisions to make apart from setting the significance level of the test. In the Bayesian hypothesis testing procedure, you also need to specify a prior distribution for the parameter of interest in the alternative hypothesis. Although considering what parameter values you would expect if the null hypothesis were false is an inherently important consideration, it is often not straightforward when you start a research project, or rely on measures you have not used before in a particular context. Although much work has been devoted to deriving sensible "default priors", I don't believe there is a sensible objective prior applicable to all situations. Given a freedom to choose a prior distribution for the alternative hypothesis, this makes the Bayesian testing procedure inherently subjective. This is perfectly in keeping with the subjectivist interpretation of probability as the rational belief of an agent endowed with (subjective) prior beliefs. Moreover, at some point, if you were to accumulate all data, the effect of prior beliefs "washes out" (as long as you don't assign a probability of zero to the true parameter value).

My pragmatic answer to the question whether you should use a Bayesian test or a Frequentist one is then the following: if you can define a suitable prior distribution to reflect what you expect to observe in a study, before you actually conduct that study, then use a Bayesian testing procedure. This will allow you to do what you most likely would like to do, namely quantify the evidence for your hypotheses against alternative hypotheses. If you are unable to form any expectations regarding the effects within your study, you probably should consider a traditional NHST to assess whether there is an indication of any effect, and limiting your Type 1 error rate in doing so. In some sense, this is a "last resort", but in psychology, where quantitative predictions are inherently difficult, something I reluctantly have to rely on quite frequently. Instead of a hypothesis test, you could also consider simply estimating the effect-size in that case, with a suitable credible interval. 

## In practice

The steps involved in conducting a Bayesian hypothesis test are not too different from the steps involved in conducting a Frequentist hypothesis test, with the additional step of choosing prior distributions over the values of the model parameters. 

1. Explore the data. Plot distributions of the data within the conditions of the experiment (if any), pairwise scatterplots between numerical predictors and the dependent variable, etc. Consider what model you might use to analyse the data, and assess the validity of the underlying assumptions.

2. Choose an appropriate general statistical model. In many cases, this will be a version of the GLM or an extension such as a linear mixed-effects model, perhaps using suitably transformed dependent and independent variables.

3. Choose appropriate prior distributions for the model parameters. This is generally the most difficult part. If you have prior data, then you could base the prior distributions on this. If not, then ideally, formulate prior distributions which reflect your beliefs about the data. You can check whether the prior distributions lead to sensible predictions by simulating data from the resulting model (i.e., computing prior predictive distributions). Otherwise, you can resort to "default" prior distributions.

4. Conduct the analysis. To test null-hypotheses, compare the general model to a set of restricted models which fix a parameter to a particular value (e.g. 0), and compute the Bayes Factor for each of these comparisons. To help you interpret the magnitude of the Bayes Factor, you can consult Table \@ref(tab:Bayes-Factor-values-interpretation). Where possible, consider conducting a robustness analysis, by e.g. varying the scaling factor of the prior distributions. This will inform you about the extent to which the results hinge on a particular choice of prior, or whether they hold for a range of prior distributions. 

```{r bayesian-lm-speedate-report, cache=TRUE}
set.seed(239472493)
data("speeddate")
dat <- subset(speeddate,!is.na(other_like) & !is.na(other_attr) & !is.na(other_intel) & !is.na(other_fun))
dat$other_attr <-  scale(dat$other_attr, scale=FALSE)
dat$other_intel <- scale(dat$other_intel, scale=FALSE)
dat$other_fun <- scale(dat$other_fun,scale=FALSE)
dat$attrintel <- dat$other_attr*dat$other_intel
dat$funintel <- dat$other_fun*dat$other_intel
BFreg <- regressionBF(other_like ~ other_attr + other_intel + other_fun + attrintel + funintel, data=dat, whichModels = "top", progress=FALSE)
tmp <- data.frame(effect = c("$\\texttt{attr}$","$\\texttt{intel}$","$\\texttt{fun}$","$\\texttt{attr} \\times \\texttt{intel}$","$\\texttt{fun} \\times \\texttt{intel}$"), BF =  exp(-BFreg@bayesFactor$bf)[c(5:1)])
BFlm <- lmBF(other_like ~ other_attr + other_intel + other_fun + attrintel + funintel, data=dat, progress=FALSE)
chains <- posterior(BFlm, iterations=10000, progress=FALSE)
```

5. Report the results. Make sure that you describe the statistical model, as well as the prior distributions chosen. The latter is crucial, as Bayes Factors are not interpretable without knowing the prior distributions. For example, the results of the analysis in Table \@ref(tab:bayesian-lm-speedate), with additional results from the posterior parameter distributions, may be reported as:

> To analyse the effect of rated attractiveness, intelligence, and fun on the liking of dating partners, we used a Bayesian linear regression analysis [@rouder2012default2]. In the model, we allowed the effect of attractiveness and fun to be moderated by intelligence. All predictors were mean-centered before entering the analysis. We used a default JZS-prior for all parameters, with a medium scaling factor of $r = \sqrt{2}/4$, as recommended by @R-BayesFactor. The analysis showed "extreme" evidence for effects of attractiveness, intelligence, and fun ($\text{BF}_{1,0} > 1000$; comparing the model to one with a point-prior at 0 for each effect). All effects were positive, with the posterior means of the slopes equalling $\hat{\beta}_\text{attr} = `r round(colMeans(chains)["other_attr"],3)`$, 95% HDI [`r paste(round(HDInterval::hdi(chains[,"other_attr"]),3),collapse="; ")`], $\hat{\beta}_\text{intel} = `r round(colMeans(chains)["other_intel"],3)`$, 95% HDI [`r paste(round(HDInterval::hdi(chains[,"other_intel"]),3),collapse="; ")`], and $\hat{\beta}_\text{fun} = `r round(colMeans(chains)["other_fun"],3)`$, 95% HDI [`r paste(round(HDInterval::hdi(chains[,"other_fun"]),3),collapse="; ")`]. In addition, we found "very strong" evidence for a moderation of the effect of attractiveness by intelligence ($BF_{0,1} = `r tmp[4,2]`$). For every one-unit increase in rated intelligence, the effect of attraciveness reduced by $\hat{\beta}_{\text{attr} \times \text{intel}} = `r abs(round(colMeans(chains)["attrintel"],3))`$, 95% HDI [`r paste(round(HDInterval::hdi(chains[,"attrintel"]),3),collapse="; ")`]. There was only "anecdotal" evidence for a moderation of the effect of fun by intelligence ($BF_{0,1} = `r tmp[5,2]`$). Although we don't place too much confidence in this result, it indicates that for every one-unit increase in rated intelligence, the effect of fun increased by $\hat{\beta}_{\text{fun} \times \text{intel}} = `r round(colMeans(chains)["funintel"],3)`$, 95% HDI [`r paste(round(HDInterval::hdi(chains[,"funintel"]),3),collapse="; ")`].

## "Summary"

```{r, fig.cap="'Piled Higher and Deeper' by Jorge Cham www.phdcomics.com. Source: https://phdcomics.com/comics/archive.php?comicid=905"}
if(knitr::is_html_output()) {
  knitr::include_graphics("https://phdcomics.com/comics/archive/phd082707s.gif")
} else {
  #download.file("https://phdcomics.com/comics/archive/phd082707s.gif", "phd082707s.gif", mode = 'wb')
  # knitr::include_graphics("phd082707s.gif")
  knitr::include_graphics("phd082707s.png")
}
```

<!--chapter:end:10-Bayes-testing.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
# Being a responsible data analyst {#ch-responsible-data-analyst}

In this chapter, we will consider some general principles which will hopefully allow you to be a good and responsible "data science citizen". In the preceding chapters, we have covered just a small set of the possible statistical models which you can apply to data, and which allow you to test hypotheses about important characteristics of the Data Generating Process. My aim has been -- and is -- to provide you with the knowledge and tools to be flexible in your approach to data analysis problems, and to choose the model of the Data Generating Process that is suitable for your goals and the characteristics of the process that provided the data. I have not given you a set of rules to follow blindly, because this would be restrictive and pointless. In the end, it is up to you to decide how to approach a problem. That freedom is perhaps both a gift and a curse. Just like coming up with a useful and explanatory theory, or designing an informative study, coming up with a useful way to analyse the resulting data is often not an easy task. It will require a good knowledge of the possibilities and limitations of various statistical analyses. Such knowledge generally comes with practice. Keep an open mind and keep learning. You will undoubtedly make mistakes along the way. We all do!

In the following, I aim to provide some general suggestions and tips which may help to avoid some of these mistakes. And if not avoided, they may help you document and learn from your mistakes. This chapter is really about the ethics of data analysis, and my suggestions can be roughly summarized as: "be honest to yourself and your audience". 

## Consider analysis before data collection

Before you embark on a study, you should give consideration to how you will analyse the data. How will you test the main hypotheses of interest? Will the data you collect be sufficiently informative for your goals? When designing a study, there are many choices to make: How many participants will you test? Will experimental manipulations be "within" participants, or "between" participants? What will you manipulate? Will you use a double-blind procedure? What are the outcome measures? These choices will ultimately decide what you can conclude from a study. But that also depends, at least partly, on the analysis you will use. 

Ideally, experimental design and statistical analysis go hand in hand. If you can make reasonably precise predictions regarding participants' behaviour from your theory, you can use simulation to generate possible datasets, and assess to what extent your proposed analyses will reflect your theoretical predictions. When possible, you can and should use this to conduct an __a priori power analysis__ for your hypothesis tests. Unfortunately, when you're not conducting a replication of an earlier study, this is often difficult, to say the least. Without prior research or precise process models linking sensory input to observable output (i.e. behavioural responses), making precise numerical predictions about effects of manipulations on behavioural/cognitive/perceptual tasks or self-report measures is pretty much impossible. Personally, I don't think making predictions on standardized effect-size measures, as is generally done in a priori power analysis, is any easier. Dividing a predicted effect by the expected noise is not any easier than considering both separately. That said, it is useful to determine which effect size will provide you with e.g. a power of $P(\text{reject } H_0| H_0 \text{ false}) = .8$ with a planned number of participants. If that effect size seems unreasonably large, you may want to reconsider running the study as planned, and recruit more participants or simplify the design. If at all possible, I would advise on running a small pilot study, so you can get a sense of the data that you might get. You can use this pilot data as a basis to simulate larger datasets to assess the possible results of a full study.

In the absence of pilot data, there are some general considerations which can increase the power of hypothesis tests for a study. Initially, try to keep analyses __as simple as possible__. When resources are limited, it may be better to choose reliability over generalizability. Focus on a single main effect, rather than including all possible moderators in a complicated design. Those moderators can be looked at later, after you have assessed that a main effect is actually relevant. Generally, a two-group comparison is more powerful than a $k$-group comparison. And when considering possible measurements of that effect, choose the most precise possible. Generally, (almost) continuous dependent variables are more powerful than discrete ones. 

## Explore the data

Before viewing your data through the lens of a statistical model, get a sense of what the "raw data" looks like. Visual exploration, whether through raincloud plots, histograms, or boxplots, is very useful in this respect. Outliers and otherwise unusual data points are of particular concern, as they may have a dramatic impact on the results of further analyses. Removal of outliers should be done carefully and with reason. There is always a danger of "fitting the data to your hypothesis". I tend to always run analyses with all data and with the "cleaned" data after removal of outliers. If the results are robust against the outliers, I tend to report the results with the full dataset. If the results are substantially affected by outliers, I mainly focus on the results of the analyses after removal of outliers, but report the results for the full data in e.g. an appendix or supporting materials. This will allow your audience to make up their own mind. Another important consideration is homogeneity of variance, although this is often best assessed within the context of a particular statistical model. Missing data is important too. If some conditions in an experiment lead to substantially more missing data than others, this can indicate that there is a problem in the design, where data is not "missing at random". For instance, if you were to assess the effects of a new medicine, and all participants for whom the medicine had an adverse effect on their health were to drop out of a study, the results of the study would be obviously biased. 

## Evaluate the assumptions underlying your analyses

In most of the analyses covered here, which were (extensions of) the General Linear Model, we assumed the residuals were independent and Normal-distributed with a mean of 0 and a constant standard deviation $\sigma_\epsilon$. A Normal distribution concerns continuous variables, and in reality, we do not have infinite precision in measurement. Hence, the assumption of continuity is strictly false. That doesn't necessarily mean that the General Linear Model is invalid. As long as the assumptions are approximately true, we can quite safely use the GLM without worrying that the assumed Type 1 error (if staying within a Frequentist viewpoint) is far from the actual Type 1 error rate. "Approximately true" is obviously a vague statement. The general consensus is that ANOVAs are reasonably robust against deviations from Normal-distributed errors, and when sample sizes are equal, also to violations of homoscedasticity [see @lix1996consequences; @maxwell2017designing]. Regarding potential heteroscedasticity, @maxwell2017designing offer the following advice: in a one-way ANOVA model with equal sample sizes, you may stick to the standard $F$ test if the ratio of the largest sample variance $S^2_\text{max}$ to the smallest sample variance $S^2_\text{min}$ is smaller than 4, i.e. $\frac{S^2_\text{max}}{S^2_\text{min}} < 4$. When sample sizes are unequal, things become a little more complicated. The suggestion is to also take the ratio of the sample size in the largest group, $n_\text{max}$ to the sample size of the smallest group, $n_\text{min}$ into account, and stick to the standard $F$ test whenever
$$\frac{n_\text{max}}{n_\text{min}} \times \frac{S^2_\text{max}}{S^2_\text{min}} < 4$$
When the ratio is larger than 4, you could decide to use an alternative test, such as the Brown-Forsythe or the Welch test [see e.g. @tomarken1986comparison]. When the variance of a dependent variable increases with the mean, it may also be possible to transform the dependent variable using e.g. a logarithmic or square-root transformation. Such a relation between the mean value and the variance is often found for response times. Tasks which require relatively little time tend to show less variation between people or between occasions than tasks that require more time to complete. It is quite standard to analyse $\log(\text{RT})$ rather than "raw" RTs. But a transformation is not always necessary. One issue with transforming the dependent variable is that it will change the relation between predictors and the DV. If a relation is linear between a predictor and the dependent variable, the relation between the predictor and the transformed dependent variable will be nonlinear. This is not a concern in pure ANOVA type models, but may prove a thorny issue in linear regression models. Transformation of variables is therefore something that should be considered carefully. Determining the correct base of the logarithm to achieve the desired result of homogenising the  variance can also be tricky. Some common transformations are provided in Table \@ref(tab:common-data-transformations). A general technique which aims to help you determine the appropriate transformation function is the so-called Box-Cox transformation [see e.g. @sakia1992box].  

Table: (\#tab:common-data-transformations) Some common transformations for the dependent variable in linear models.

Transformation | Name | Usage
:--------------|:-----|:--------------
$Y^2$          | square | useful for negatively-skewed data, as it will increase the spread among higher scores compared to lower ones
$\sqrt{Y}$     | square-root | useful for positively skewed data, as it will decrease the spread among higher scores compared to lower one
$\log_e{(Y)}$  | natural logarithm | useful for positively skewed data, as it will decrease the spread among higher scores compared to lower one. If there are values equal to or smaller than 1, you can use an alternative form $\log_e{(Y + c)}$, with e.g. $c = \min(1,|\min(Y)|)$
$\log_{10}{(Y)}$ | base 10 logarithm | useful for highly positively skewed data, as it will decrease the spread among higher scores compared to lower one. If there are values equal to or smaller than 1, you can use a similar adjustment as above.

Violation of the assumption of independent errors is much more problematic than a violation of Normality or homoscedasticity. This requires the use of an appropriate model (e.g. a linear mixed-effects model) that properly accounts for the dependence in the data.

While there are many formal statistical tests to assess whether the assumption of Normality (e.g. the Shapiro-Wilk test, or the Kolmogorov-Smirnov test) or homoscedasticity (e.g. the Levene test) holds, I don't advice the judicious use of these tests. A main issue here is that if you have a sufficient sample size, such tests can become very powerful, and will often result in a rejection of the null hypothesis of a Normal distribution or homoscedasticity, even if the departure from these assumptions is only slight and hence not anything to really worry about. Relying on graphical procedures to detect substantial departures is generally more sensible.

## Distinguish between confirmatory and exploratory analyses

When embarking on a data analysis journey, you will often take a planned route, but also take some unplanned paths. It is important to distinguish between these, as unplanned routes may have been led by random noise or other idiosyncrasies in the dataset collected. Andrew Gelman quite nicely describes the various choices made during an analysis as a "garden of forking paths" [@gelman2013garden]. Other authors talk of "researcher degrees of freedom" [@simmons2011false]. Consider the following hypothetical example:

>  A researcher is interested in differences between Democrats and Republicans in how they perform in a short mathematics test when it is expressed in two different contexts, either involving health care or the military. The research hypothesis is that context matters, and one would expect Democrats to do better in the health-care context and Republicans in the military context. Party identification measured on a standard 7-point  scale  and  various demographic information also available. At this point there is a huge number of possible comparisons that can be performed--all consistent with the data. For example, the pattern could be found (with statistical significance) among men and not among women--explicable under the theory that men  are more ideological than women. Or the pattern could be found among women but not among men--explicable under the theory that women are more sensitive to context, compared to men. Or the pattern could be statistically significant for neither group, but the difference could be significant (still fitting the theory, as described above). Or the effect might only appear among men who are being asked the questions by  female interviewers. We might see a difference between sexes in the health-care context but not the military context; this would make sense given that health care is currently a highly politically salient issue and the military is not. There are degrees of freedom in the classification of respondents into Democrats and Republicans from a 7-point scale. And how are independents and nonpartisans handled? They could be excluded  entirely. Or perhaps the key pattern is between partisans and nonpartisans? And so on.  
> [@gelman2013garden, p. 3]

As this example illustrates, there are many potential ways to show evidence for a seemingly straightforward hypothesis regarding an interaction between political affiliation and context, each which can be justified by theory. An unscrupulous analyst hunting for a significant result might try all these, and choose to report just the comparison with the strongest test result. This is also called "p-hacking" [@simmons2011false], and will result in a proliferation of Type 1 errors in the scientific literature. But even highly conscientious analysts can be led astray when allowing hypothesis tests to be inspired by patterns in the data. The results of unplanned analyses that were inspired by other analyses should be treated with caution. Data is inherently noisy, and patterns which may seem obvious and easy to provide a post-hoc explanation for are noisy too. Whilst Bayesian hypothesis testing is sometimes portrayed as being immune to researcher intentions, this holds really just for confirmatory analyses where the priors are determined before data collection. Bayesian analyses, like Frequentist ones, can be "hacked" too [@simonsohn2014posterior].

To diminish the likelihood of being led astray, it is important to clearly distinguish between confirmatory and exploratory analyses. Confirmatory analyses are those that are planned before seeing the data. To be truly confirmatory, you should not only decide exactly which particular model to use, and which parameters to test, but also what exclusion criteria (if any) to apply to data points (e.g., what are considered to be outliers). If all is decided before seeing the data, these confirmatory analyses can be preregistered. Such preregistration is useful because by committing to a particular analysis, you avoid the temptation to deviate from your initial plans in seemingly innocuous ways, which nevertheless will increase the chance of Type 1 errors. Preregistration of analyses does not preclude further exploratory analyses. But these can then be clearly identified as such, and their results treated with the additional caution they deserve. When you don't have pilot data, preregistration is difficult however, as there are often unforeseen issues in the data (e.g., heteroscedasticity which may require a transformation of the data, or unforeseen outliers). In responding to such unforeseen issues, it may be necessary to change the planned analyses which in principle would render them "exploratory". I don't think it is always necessary to be this strict. But it will be up to the analyst to provide a strong justification for the deviation from the analysis plan. Simply providing such a justification is already a substantial advance over not clearly identifying data-driven choices which may have affected the results. 

## Aim for openness and reproducibility

In clearly distinguishing between confirmatory and exploratory analyses, you will be honest to yourself and to your audience. You should be open about the choices you have made when analysing the data, and why you made them. This will allow your audience to determine whether they agree with your choices, or not, and hence whether they would likely reach the same conclusions as you. Transparency and openness are extremely important principles in the scientific process. Where possible, you should make the data of studies publicly accessible, so that other scientists have the opportunity to perform their own analyses or to combine data to perform meta-analyses. When sharing the data, it is also very useful to share your analysis scripts (or e.g. JASP output file), so that other researchers can exactly replicate your results. By sharing your full analysis scripts, you provide a clear document of the process that transformed the raw data as collected to the results presented in a scientific manuscript. This is not only of benefit to other scientists, but also to yourself. In some months or years after performing your analyses, you will often forget some of the details. And scripts tend to get lost if not stored safely and permanently on an external server. There are useful and freely accessible platforms such as the Open Science Framework (https://osf.iof), figshare (https://figshare.com/), and Zenodo (https://zenodo.org/), which are specifically designed to openly share research outputs such as data and analysis scripts. Another popular choice is github (https://github.com/). 

## Communicate clearly and concisely

When writing up the results of statistical analyses, you should aim for clarity and conciseness. Provide all the relevant details of statistical analyses. The American Psychological Association (APA) provides extensive guidelines for reporting results of psychological studies in the APA publication manual [@apa2020publication]. Some of the APA rules can be a little tedious, but they are meant to provide standards to make scientific communication effective. A useful summary of the main APA style guide can be found [here](https://owl.purdue.edu/owl/research_and_citation/apa_style/apa_formatting_and_style_guide/apa_sample_paper.html) and brief guidelines on reporting numbers and statistics can be found [here](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf). Andy Fields has also written a useful [guide to writing research reports](https://www.discoveringstatistics.com/docs/writinglabreports.pdf). 

Here are some of my recommendations for describing analyses in the results section of a paper. They are mostly consistent with the APA guidelines, although I must admit the last version of the APA publication manual I read was the 5th edition from 2001.

I will start by listing some general guidelines, and then provide two example write-ups, one for the multiple regression analysis that we focused on in Chapter \@ref(ch-multiple-regression), and another for the factorial ANOVA that we focused on in Chapter \@ref(ch-factorial-ANOVA).

1. Start by describing the main objectives of an analysis.
2. Identify the model underlying the analysis (e.g. a multiple regression model, an ANOVA model, or a linear mixed-effects model).
3. Present the data on which the analysis is based. This is generally best done in a graph.
4. Report the results of statistical tests. Provide the key determinants of the distribution of the test statistic, such as the degrees of freedom for $t$-tests and $F$ tests, as well as the actual statistic (rounded to two decimals), and the precise $p$-value (rounded to three decimals), unless the $p$-value is smaller than .001. For example, $t(47) = 2.58$, $p = .013$, and $F(3, 396) = 6.31$, $p < .001$. For ANOVA results, it is also common to provide the Mean Squared Error (MSE) value, which is the unbiased estimate of the error variance, i.e. $\text{MSE} = \text{SSE}(\text{MODEL G})/(n - \text{npar}(G))$.
5. Where possible and relevant, provide estimated effect sizes and/or confidence intervals in addition to test results. For a multiple regression model, you can provide confidence intervals for the slopes, as well as the $R^2$ of the full model. For effects in an ANOVA model, a common effect size measure is the coefficient of partial determination, i.e. partial $\eta^2$ (Equation \@ref(eq:semi-partial-determination)).
6. Describe what the effects mean in terms of the dependent variable. For instance, in a multiple regression model, you should indicate what a significant slope indicates in terms of increasing or decreasing the (predicted value of the) dependent variable. For main effects in an ANOVA model, you should describe what a significant effect indicates about the (marginal) means of the dependent variable.
7. When the interpretation of a test result is not immediately clear (e.g. for an omnibus test in an ANOVA), describe the appropriate follow-up tests which provide clarity about the interpretation of that test result.

### Example of reporting a multiple regression analysis

```{r example-multiple-regression-write-up}
library(sdamr)
data("trump2016")
dat <- subset(trump2016,state != "District of Columbia")
mod <- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat)
```

An example of how I might communicate the results of the multiple regression analysis conducted to determine the effect of hate groups on votes for Donald Trumpm is as follows: 

> To determine the effect of hate groups on votes for Donald Trump in the 2016 US elections, we conducted a multiple regression analysis. The dependent variable was the percentage of votes for Donald Trump in each of the $n=50$ US states, and to account for variations in the size of the population of each state, the number of hate groups was measured per million citizens. As education level is expected to be related to voting behaviour, the regression model also included a measure of education level (as the percentage of citizens with a Bachelors degree or higher) as an additional predictor. This allowed us to determine the unique effect of hate groups on voting behaviour whilst controlling for the possibly confounding effect of education. Pairwise plots depicting the pairwise relations between percentage Trump votes, number of hate groups, and education level, are shown in Figure \@ref(fig:example-multiple-regression-write-up-plot). The model accounted for a significant proportion of the variance of voting behaviour (`r papaja::apa_print(mod)$full_result$modelfit$r2`). The analysis showed a significant unique effect of hate groups on votes for Donald Trump (`r papaja::apa_print(mod)$full_result$hate_groups_per_million`). For every additional hate group per million citizens, votes for Donald Trump are predicted to increase by `r round(coefficients(mod)[2],2)`%. In addition, the analysis showed a significant effect of education level (`r papaja::apa_print(mod)$full_result$percent_bachelors_degree_or_higher`). For every additional one percent of citizens with at least a Bachelor's degree, votes for Donald Trump are predicted to decrease by `r round(abs(coefficients(mod)[3]),2)`%.

```{r example-multiple-regression-write-up-plot, fig.cap="Pairwise scatterplots, nonparametric density plots, and pairwise correlations for the percentage of votes for Donald Trump, hate groups (per million citizens) and education level ($\\text{\\%}$ of citizens with a Bachelors degree or higher)."}
library(GGally)
ggpairs(dat[,c(7,4,5)], upper=list(continuous = wrap(ggally_cor, family="sans")), columnLabels = c("% Trump votes", "hate groups (per million)", "education level"))
```

### Example of reporting a factorial ANOVA

```{r example-factorial-ANOVA-write-up}
library(sdamr)
data("expBelief")
dat <- expBelief
dat$primeCond <- factor(dat$primeCond, labels=c("High power","Low power"))
contrasts(dat$primeCond) <- c(1/2, -1/2)
dat$experimenterBelief <- factor(dat$experimenterBelief, labels=c("High power","Low power"))
contrasts(dat$experimenterBelief) <- c(1/2, -1/2)
modglm <- lm(ApproachAdvantage ~ primeCond*experimenterBelief, data=dat)
modg <- car::Anova(modglm,type=3)
```

An example of how I might communicate the results of the factorial ANOVA assessing the effects of power prime and experimenter belief on approach advantage scores, is as follows: 

> To assess the effect of power prime and experimenter belief on participants' speed in approaching and avoiding virtual targets, a 2 (power prime: low power vs high power) by 2 (experimenter belief: low power vs high power) factorial ANOVA was conducted. The dependent variable was the "approach advantage" score, computed as the difference in the average time (in milliseconds) between to making an approach vs an avoid response. The approach advantage scores in the four conditions are depicted in Figure \@ref(fig:example-factorial-ANOVA-write-up-plot). The analysis showed a significant main effect of experimenter belief (`r papaja::apa_print(modg, es= "pes")$full_result$experimenterBelief`). The approach advantage score was higher when the experimenter was made to believe participants were provided with a high power prime ($M = `r round(coefficients(modglm)[1] + .5*coefficients(modglm)[3],2)`$) compared to when they were made to believe participants were provided with a low power prime ($M = `r round(coefficients(modglm)[1] - .5*coefficients(modglm)[3],2)`$). The main effect of power prime was not significant (`r papaja::apa_print(modg, es= "pes")$full_result$primeCond`). The interaction between experimenter belief and power prime was also not significant (`r papaja::apa_print(modg, es= "pes")$full_result$primeCond_experimenterBelief`).

```{r example-factorial-ANOVA-write-up-plot, fig.cap="Approach advantage scores by Experimenter belief and Power prime. Means and $\\text{95\\%}$ confidence intervals are shown as black dots and bands respectively. Raw data points are shown in grey."}
afex::afex_plot(afex::aov_car(ApproachAdvantage ~ primeCond*experimenterBelief + Error(pid), data=dat, type=3),x="experimenterBelief", trace = "primeCond", legend_title = "Power prime", error="between") + xlab("Experimenter belief") + ylab("Approach advantage") + theme(legend.position = "bottom")
```

The APA guidelines indicate that the results section should detail the results of statistical analyses in an "objective" manner, without directly noting what they imply in terms of support for your theory. While you should avoid statements such as "This clearly shows our theory is true", it is good practice to provide some guidance on how to interpret the results. In the above, I tried to clearly indicate what a significant effect indicates in terms of the marginal means of the the dependent variable. This is good practice, and does not mention anything about a theory. In addition, it is fine to add qualifiers like "As predicted, ..." in the results section. More general evaluation of the evidence for your theory is generally left to the Discussion section of manuscripts, however. So in the discussion, we might interpret the results of this analysis:

> The results of our analysis clearly show that experimenter beliefs have an effect on participants' behaviour. When experimenters believed participants were primed to take a high- or low-power role, those participants behaved according to the expectations from social priming theory. However, the actual prime provided to participants appeared to have little effect on their behaviour. This suggests that previous results might be due to experimenter expectations, rather than a direct effect of the power priming manipulations on participants' behaviour. The precise mechanism by which experimenter expectations influence participants' behaviour may be subtle and were not directly addressed in this study. If anything, our results indicate that future investigations of power priming should adopt a double-blind procedure, where neither participants nor experimenters are aware of the condition assigned to participants.

<!--chapter:end:11-responsible.Rmd-->

```{r include=FALSE, cache=FALSE}
#options(knitr.kable.NA = "")

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

knitr::opts_hooks$set(echo = function(options) {
  if (options$fig.width < options$fig.height) {
    options$fig.width <- options$fig.height
  }
  options
})

knitr::knit_hooks$set(webgl = rgl::hook_webgl)

options(digits=3)
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

library(sdamr)
library(ggplot2)

m_scale_colour_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "colour") {
  scale_colour_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

m_scale_fill_viridis_d <- function(..., alpha = 1, begin = 0, end = 0.9, direction = 1, 
                                     option = "D", aesthetics = "fill") {
  scale_fill_viridis_d(..., alpha = alpha, begin = begin, end = end, direction = direction, 
                         option = option, aesthetics = aesthetics)
}

scale_colour_discrete <- m_scale_colour_viridis_d
scale_fill_discrete <- m_scale_fill_viridis_d
scale_colour_continuous <- scale_colour_viridis_c
scale_fill_continuous <- scale_fill_viridis_c
scale_colour_binned <- scale_colour_viridis_b
scale_fill_binned <- scale_fill_viridis_b

my_theme <- ggplot2::theme_get()

options(digits=3)

### Functions
pretNum <- function(x) {
  prettyNum(x, big.mark=",")
}

pvalue <- function(x, limit = FALSE, limit_below = 1e-3) {
  if(x > 1 | x < 0) stop("that's not a p-value matey")
  if(limit & x < limit_below) return(paste0("<", pvalue(limit_below, limit=FALSE)))
  x <- as.character(prettyNum(x,scientific=FALSE, digits=options()$digits))
  return(sub("0.",".", x))
}

apa_pvalue <- function(x, digits = 3L, na_string = "", add_equals = FALSE) {
  x <- papaja::printp(x = x, digits = digits, na_string = na_string, add_equals = add_equals)
  if(substr(x,1,1) == ".") x <- paste0("= ",x)
  x
}

write_GLM_equation <- function(mod, digits=NULL, include_sde = TRUE, dv_name, iv_names) {
  if(missing(dv_name)) dv_name <- attr(mod$terms,"variables")[[2]]
  if(!missing(dv_name) & knitr::is_latex_output()) {
    dv_name <- stringr::str_replace_all(dv_name,"(?<!\\\\)_","\\\\_")
  }
  coefs <- coefficients(mod)
  intercept_included <- names(coefs)[1] == "(Intercept)"
  if(missing(iv_names) & length(coefs) > as.numeric(intercept_included)) {
    iv_names <- names(coefs)
    if(intercept_included) iv_names <- iv_names[-1]
    if(knitr::is_latex_output()) {
      iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
    }
  }
  if(knitr::is_latex_output() && !missing(iv_names)) {
    iv_names <- stringr::str_replace_all(iv_names,"(?<!\\\\)_","\\\\_")
  }
  sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(mod$model) - length(coefs)))
  if(is.null(digits)) digits <- options("digits")$digits
  out <- paste0("\\texttt{",dv_name,"}_i = ")
  idx <- 1
  if(intercept_included) {
    out <- paste0(out, format(coefs[idx],digits=digits))
    idx <- idx + 1
  }
  if(intercept_included & length(coefs) > 1 | !intercept_included & length(coefs) > 0) {
    for(i in idx:length(coefs)) {
      if(i == 1) {
        out <- paste0(out, ifelse(coefs[i] > 0,""," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      } else {
        out <- paste0(out, ifelse(coefs[i] > 0," + "," - "),format(abs(coefs[i]),digits=digits), " \\times \\texttt{", iv_names[ifelse(intercept_included, i - 1, i)], "}_i ")
      }
    }
  }
  
  out <- paste0(out," + \\hat{\\epsilon}_i ")
  if(include_sde) {
    out <- paste0(out, "\\quad \\quad \\hat{\\epsilon}_i \\sim \\mathbf{Normal}(0, ", format(sd_e,digits=digits), ")")
  }
  out
}

#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
numbers2words <- function(x){
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

write_anova_results <- function(mod) {
  out <- paste0("$F(",mod$Df[2],",",mod$Res.Df[2],") = ", round(mod$F[2],2),"$, $p",apa_pvalue(mod[2,"Pr(>F)"]),"$")
  out
}

```
`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:999-references.Rmd-->

