# Simple linear regression

In this chapter, we continue our journey into the General Linear Model by extending the very simple model of the previous chapter to include a predictor. The resulting model assumes there is a linear relation between the dependent variable and the predictor, and is also known as a simple linear regression model. We will look at the parameters of the model in detail, and discuss their estimation, as well as testing whether their values are equal to a priori ones of interest.

```{r load-trump-data}
library(sdamr)
data("trump2016")
dat <- subset(trump2016,state != "District of Columbia")
```

## Trump, votes, and hate groups

Donald Trump is perhaps the most divisive president in American history. The 2016 US elections were mired in controversy. [Some reports](https://www.theguardian.com/world/2020/mar/18/white-nationalist-hate-groups-southern-poverty-law-center) indicate the number of white nationalist hate groups have gone up by 55% in the Trump era. One study found a strong relation between Trump support and anti-immigrant sentiment, racism, and sexism [@schaffner_understanding_2018]. 

[Paul Ricci](https://www.csiwithoutdeadbodies.com/2017/02/hate-groups-and-trumps-vote-predictive.html) collated data about the number of hate groups in the different US states and votes for Trump. A scatterplot of these two variables is provided in Figure \@ref(fig:scatterplot-trump-votes-hate-groups). You can see that states with relatively little hate groups appear to also have relatively little votes for Trump, while states with a relatively large number of hate groups have relatively many votes for Trump.

```{r scatterplot-trump-votes-hate-groups,fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens"}
library(ggplot2)

if(knitr::is_html_output()) {
  plotly::ggplotly(ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes,name=state)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump"), tooltip=c("name","x","y"))
} else {
  ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump")
}
```

To assess the relation between Trump votes and the number of hate groups, we will use a linear regression model. As the name suggests, this model involves a line, a straight one in fact. This straight line represents the mean value of the dependent variable $Y$ (i.e. the percentage of Trump votes) for cases which have a particular value for the predictor (or independent) variable $X$ (i.e. the number of hate groups). The model concerns e.g. the mean percentage of Trump votes in all states with 5.55 hate groups per 1 million citizens, or all states with 0 hate groups per 1 million citizens. The model allows for variation around the mean of the actual percentages of Trump votes in all states with that particular number of hate groups per 1 million citizens. In particular, it assumes that these actual percentages are Normal-distributed. 


## The model

In the previous chapter, we used a simple statistical model:
$$Y_i \sim \mathbf{Normal}(\mu,\sigma)$$
This model assumes that each observation is independently drawn from a Normal distribution, with a mean $\mu$ and a standard deviation $\sigma$. We can state this model in an equivalent way as:
\begin{equation}
Y_i = \mu + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma)
(\#eq:simple-model-as-glm)
\end{equation}
In this two-part formulation, the formula on the left side decomposes observation in a _structural part_ (here the mean, $\mu$), and a _random part_ that we usually call the error ($\epsilon_i$). We don't know much about this error. We just assume that is is drawn from a Normal distribution with a mean of 0 and standard deviation $\sigma_\epsilon$. This is stated in the formula on the right.^[If you are wondering why this formation provides the same model as in the previous chapter, you can show this using the properties of the Normal distribution. In the second formulation, you can view $Y$ as a linear transformation of the error $\epsilon$: $Y_i = a + b \epsilon$, with $a = \mu$ and $b=1$. The mean of $Y$ is then $a + \mu_\epsilon = \mu + 0 = \mu$, and the standard deviation of $Y$ equals $\sqrt{|b| \sigma} = 1 \times \sigma = \sigma$, which are the same parameters as when directly specifying the Normal distribution for $Y$.]

A model like this doesn't allow for very precise predictions. As the error term is assumed to be completely random, it is unpredictable. So if you were asked to predict the value of $Y$, all you could really do is to use $\mu$ as your prediction. If, however, the dependent variable (e.g. votes for Trump) is related to an independent or predictor variable (e.g. number of hate groups), then we would like to be able to use the predictor to adjust our prediction. __Linear regression__ allows us to to this in a straightforward way. 

The bivariate regression model is depicted in Figure \@ref(fig:simple-regression-explanation-plot). More formally, the model can be defined as follows:

\begin{equation}
Y_i = \beta_0 + \beta_1 \times X_{i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
(\#eq:bivariate-regression-model-definition)
\end{equation}

Here, $Y_i$ is the value of the $i$-th observation of the dependent variable (with $i = 1, \ldots, n$) and $X_i$ the value of the $i$-th observation of the predictor variable. These are the observable parts of the data. The model contains three important parameters:

* The __intercept__ $\beta_0$, which is the mean of $Y$ when $X=0$. 
* The __slope__ $\beta_1$, which reflects the increase or decrease in the mean of $Y$ for every _1-unit increase_ in the predictor $X$. By a 1-unit increase, we simply mean that the value of $X$ goes up by 1, e.g. from $X=2$ to $X=3$, or from $X=12.63$ to $X=13.63$.
* The __standard deviation__ $\sigma_\epsilon$ of the __error__ or __residual__ terms $\epsilon_i$. The errors are assumed to be drawn independently from the same Normal distribution, with a mean of 0 and standard deviation $\sigma_\epsilon$.

```{r simple-regression-explanation-plot,fig.cap="The simple regression model. $\\textbf{A}$: The intercept is the mean of $Y$ when $X=0$ and is the point at which the regression line crosses the y-axis. The slope determines the steepness of the regression line and represents the increase (or decrease) in the mean of $Y$ for every 1-unit increase in the predictor $X$. $\\textbf{B}$: the error terms or residuals are the vertical distances of each observed Y-value from the regression line. $\\textbf{C}$: The errors in a regression model are assumed to follow a Normal distribution around the regression line.",fig.show="hold",fig.width=6,fig.height=6,out.width="100%"}

layout(matrix(c(1,4,1,3,2,3,2,5),nrow=2))
par(mar=c(4,4,2,2))
x <- seq(-1,5)
y <- seq(-1,5)
xp <- c(-.5,.7,1.4,2.5,3.5,4)
yp <- c(1.2,1,3,4,4.5,3)

plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5), main="A")
abline(a=2,b=.5)
abline(v=0,lty=2)
abline(h=0,lty=2)
arrows(0,0,0,2,lwd=2,code=2,length=0.1)
text(0,.5*2,labels=expression(beta[0]),pos=4)
lines(c(0,1),c(2,2),lty=3)
arrows(1,2,1,.5 + 2,lwd=2,code=2,length=0.1)
text(1,2 + .5*.5,labels=expression(beta[1]),pos=4)
lines(c(1,2),c(2.5,2.5),lty=3)
arrows(2,2.5,2,.5 + 2.5,lwd=2,code=2,length=0.1)
text(2,2.5 + .5*.5,labels=expression(beta[1]),pos=4)
points(xp,yp)

plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5), main="B")
abline(a=2,b=.5)
#abline(v=0,lty=2)
#abline(h=0,lty=2)
for(i in 1:length(xp)) {
  lines(c(xp[i],xp[i]),c(yp[i],.5*xp[i]+2))
  text(xp[i],2 + .5*xp[i] + .5*(yp[i] - (2 + .5*xp[i])),labels=expression(epsilon[i]),pos=4)
}
points(xp,yp)

plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5), main="C")
abline(a=2,b=.5)
#abline(v=0,lty=2)
#abline(h=0,lty=2)
#points(xp,yp)
yt <- seq(0,6,length=100)
xt <- c(0,.75*dnorm(yt,mean=3,sd=.5),0)
yt <- c(0,yt,6)
polygon(-1+xt,-1.5+yt,col=rgb(0,0,0,alpha=.5))
polygon(0+xt,-1+yt,col=rgb(0,0,0,alpha=.5))
polygon(1+xt,-.5+yt,col=rgb(0,0,0,alpha=.5))
polygon(2+xt,yt,col=rgb(0,0,0,alpha=.5))
polygon(3+xt,.5+yt,col=rgb(0,0,0,alpha=.5))
polygon(4+xt,1+yt,col=rgb(0,0,0,alpha=.5))
polygon(5+xt,1.5+yt,col=rgb(0,0,0,alpha=.5))
```

Remember, unlike variables, parameters are _not_ observable. We can infer their value from the data by estimation and/or performing hypothesis tests. But we can never be completely sure that such inferences are correct. Let's consider the main model formula again:
$$Y_i = \beta_0 + \beta_1 \times X_{i} + \epsilon_i$$
This formula involves both observable variables ($Y$ and $X$) and unobservable parameters ($\beta_0$, $\beta_1$, and $\epsilon_i$). Indeed, the errors $\epsilon_i$ are really parameters as well, because they cannot be directly observed. There are many of these (one for each observation $i = 1, \ldots, n$), and they are not of primary concern. The other part, $\beta_0 + \beta_1 \times X_i$, determines the regression line, representing the mean value of $Y$ for each possible value of $X$. These mean values are __conditional means__, they are the mean of $Y$, conditional upon a particular value of the predictor $X$. We can use $\mu_{Y|X_{i}}$ to denote this conditional mean, so
\begin{equation}
\mu_{Y|X_{i}} = \beta_0 + \beta_1 \times X_{i}
(\#eq:conditional-mean-simple-regression)
\end{equation}

Now, plugging the conditional mean into Equation \@ref(eq:bivariate-regression-model-definition) we get
$$Y_i = \mu_{Y|X_{i}} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$
which, in many respects, is very similar to the simple model of Equation \@ref(eq:simple-model-as-glm). The key difference is the use of a conditional mean $\mu_{Y|X_{i}}$ instead of a constant mean $\mu$. So we can view our new model as an extension of the simple one of the previous chapter, where we now use a straight line to represent the relation between the mean and a predictor variable.

<!--
If you were asked to predict the percentage of Trump votes for a state with for example 5.55 hate groups, the conditional mean of the percentage Trump votes for all states with 5.55 hate groups would be a good prediction, because without knowing anything else, it will be impossible to predict whether actual the votes for Trump will be above or below the mean for a given state. Using $\hat{Y}$ to denote the predicted value of the dependent variable, we can thus write:
$$\hat{Y}_i = \beta_0 + \beta_1 \times X_{i}$$

$$Y_i = \hat{Y}_i + \epsilon_i$$
This last equation can be written in words as 
$$\text{data} = \text{prediction} + \text{error}$$
This is just a mathematical truism, you can _always_ make this statement, because the error is simply the difference between the observed values (the data) and the predicted values. But perhaps it helps in thinking about what error is. Moreover, if you look at Equation \@(eq:bivariate-regression-model-definition) again, you might notice that the probabilistic part of the model (i.e., the part involving probability distributions) really only concerns the errors $\epsilon_i$. These are, as already indicated, assumed to be independently drawn from a Normal distribution with a mean of 0 and a standard deviation $\sigma$:
$$\epsilon_i \sim \mathbf{Normal}(0,\sigma)$$
-->

## Estimation {#sec:04-estimation}

The maximum likelihood estimates of the model parameters are
\begin{equation} 
\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \overline{X}
(\#eq:simple-regression-intercept-estimate)
\end{equation}
for the intercept, and
\begin{equation}
\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}
(\#eq:simple-regression-slope-estimate)
\end{equation}
for the slope. Note that to estimate the intercept, we first need the estimate of the slope. So let's focus on
this one first. The top part of the division (the numerator) contains a sum of 
deviations of the predictor values ($X_i$) from its average ($\overline{X}$) multiplied by deviations
of the values $Y_i$ of the dependent variable from its average ($\overline{Y}$). Let's consider these
multiplied deviations a little further. Each deviation is positive (larger than 0) when the value is 
higher than the average, and negative (smaller than 0), when the value is lower than the average.
So the multiplied deviations are positive whenever both values are larger than their average, and 
whenever both values are below their average (a negative value multiplied by another negative value 
is positive). If we were to divide the sum of the multiplied deviations by $n$ (the number of observations), 
we'd get the average of these multiplied deviations. This average is also called the __covariance__ between 
$X$ and $Y$: 
$$\text{Cov}(X,Y) =  \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{n}$$
Note that, as an estimator, this provides biased estimates of the true covariance. An unbiased estimator
of the true covariance is obtained by dividing by $n-1$ instead of $n$ (just as for the variance). Going back to our example, the covariance between Trump votes and hate groups would be positive whenever states with higher-than-average Trump votes are generally also states with higher-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally also states with lower-than-average hate groups. 
The covariance would be negative, on the other hand, whenever states with higher-than-average Trump votes are generally states with lower-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally states with higher-than-average hate groups. A positive or negative covariance is indicative of a relation between $X$ and $Y$. Indeed, the well-known
Pearson correlation coefficient is a standardized covariance, where the standardization scales the correlation
to always be between -1 and 1 and involves dividing the covariance by the product of the standard deviations
of both variables:
\begin{align}
r_{X,Y} &= \frac{\text{Cov}(X,Y)}{S_X \times S_Y} \\
&= \frac{\frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{n}}{\sqrt{\frac{\sum_{i=1}^n (X_i - \overline{X})^2}{n}} \times \sqrt{\frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}}}
\end{align}

Going back to the estimate of the slope (Equation \@ref(eq:simple-regression-slope-estimate)), we can view this as a different way of standardizing the covariance. Looking at the bottom part of the division (the denominator), we can see it consists of the sum of squared deviations of the predictor values from its mean. If we were to divide this sum by the number of observations, we'd get the variance of $X$. As dividing both the top part (numerator) and bottom part (denominator) in a division by the same value does not affect the outcome of the division (i.e. $\frac{a}{b} = \frac{a/c}{b/c}$), we can choose to divide both by $n$ so the numerator becomes the covariance and the denominator the variance (we could also divide both by $n-1$ so they become unbiased estimators of the covariance and variance). So an alternative way of computing the estimate of the slope is
$$\hat{\beta}_1 = \frac{\text{Cov}(X,Y)}{S^2_X}$$
Note that the variance of $X$ equals the product of the standard deviation of $X$ and itself, as $S^2_X = S_X \times S_X$. So the slope estimate looks quite a bit like the correlation coefficient, where instead of the standard deviation of $Y$, we use the standard deviation of $X$ twice. With a little algebraic manipulation, we can also state the slope estimate in terms of the correlation as 
$$\hat{\beta}_1 = \frac{S_Y}{S_X} r_{X,Y}$$
The reason for going into these alternative formulations is not to pain or confuse you with lots of equations. What these alternative formulations show is that the slope tells us something about the relation between $X$ and $Y$, just like the covariance and correlation do. If the sample correlation is 0, then so is the estimated slope. It is important to realise that we have been discussing the _estimate_ of the slope, not the true value itself. But the same relations hold for the true values. If we denote the true correlation as $\rho_{X,Y}$, then the true value of the slope can be defined as
$$\beta_1 = \frac{\sigma_Y}{\sigma_X} \rho_{X,Y}$$
The true value of the slope is 0 when the true correlation between $X$ and $Y$ equals $\rho_{X,Y} = 0$. It would also be 0 if the true standard deviation of $Y$ equals $\sigma_Y = 0$, but this implies that $Y$ is a constant and that is not a very interesting situation.

That was perhaps a little tortuous, and we haven't even discussed the estimate of the intercept! Remember that the intercept represents the mean value of $Y$ for all cases where $X=0$. This average is often not so interesting itself, although in our example, we might be interested in what the average percentage of votes for Trump would be in the absence of any hate groups. Equation \@ref(eq:simple-regression-intercept-estimate) shows that we can estimate this value by adjusting the sample average $\overline{Y}$ by subtracting $\hat{\beta}_1 \times \overline{X}$ from it. How come? Well, it can be shown that __the regression line always passes through the point $(\overline{X},\overline{Y})$__. The derivation for showing that this has to be the case is not that interesting, so you'll just have to trust me, or look it up elsewhere. But this implies that $$\overline{Y} = \beta_0 + \beta_1 \times \overline{X}$$
and then we can simply subtract $\beta_1 \times \overline{X}$ from both sides to get $\overline{Y} - \beta_1 \times \overline{X} = \beta_0$.

Finally, we can also estimate $\sigma_\epsilon$, the standard deviation of the error. As usual for variances and standard deviations, the maximum likelihood estimate is biased, so we'll focus on an unbiased estimator. Like before, an estimate of the variance is computed from a sum of squared deviations from an estimated mean. In this case, we need to use the estimated _conditional_ means
$$\hat{\mu}_{Y|X_i} = \hat{\beta}_0 + \hat{\beta}_1 \times X_i$$
to compute the following estimate of the error variance:
\begin{equation}
\hat{\sigma}^2_\epsilon = \frac{\sum_{i=1}^n (Y_i - \hat{\mu}_{Y|X_i})^2}{n-2}
(\#eq:simple-regression-sd-error-variance-estimate)
\end{equation}
Note that we are dividing by $n-2$ here, rather than by $n-1$ as we did when estimating the variance of $Y$ in a model without predictors. The reason for this is that we are now using two noisy parameter estimates (i.e. $\hat{\beta}_0$ and $\hat{\beta}_1$), rather than just one. As usual, to get the estimate of $\hat{\sigma}_\epsilon$, we can just take the square root of $\hat{\sigma}^2_\epsilon$.

### Estimating the relation between Trump votes and hate groups

So, what are the estimates of the model predicting Trump votes by hate groups? We can calculate the estimates relatively easily by first computing the following sample quantities of location, scale, and covariation

```{r}
tab <- data.frame(a=mean(dat$hate_groups_per_million), b=mean(dat$percent_Trump_votes), c=(nrow(dat)-1)/nrow(dat)*var(dat$hate_groups_per_million), d = (nrow(dat)-1)/nrow(dat)*var(dat$percent_Trump_votes), e = (nrow(dat)-1)/nrow(dat)*cov(dat$hate_groups_per_million, dat$percent_Trump_votes))
colnames(tab) <- c("$\\overline{\\mathtt{hate}}$", "$\\overline{\\mathtt{votes}}$", "$S^2_\\mathtt{hate}$", "$S^2_\\mathtt{votes}$", "$\\text{Cov}(\\mathtt{hate},\\mathtt{votes})$")
knitr::kable(tab, row.names = FALSE, escape = FALSE, digits=6, booktabs = TRUE, linesep="")
```

The estimate of the slope is then
$$\hat{\beta}_1 = \frac{`r tab[,5]`}{`r tab[,3]`} = `r tab[,5]/tab[,3]`$$
and the estimate of the intercept is 
$$\hat{\beta}_0 = `r tab[,2]` - `r tab[,5]/tab[,3]` \times `r tab[1]` = `r tab[,2] - tab[,5]/tab[,3] * tab[,1]`$$
```{r estimate-simple-regression-trump2016}
mod <- lm(percent_Trump_votes ~ hate_groups_per_million,data=dat)
```

Of course, we would not normally bother with calculating these estimates "hy hand", but rather rely on statistical software such as R or JASP to calculate these for us. In any case, the estimated model becomes:

$$\texttt{votes}_i = `r round(coef(mod)[1],3)` + `r round(coef(mod)[2],3)` \times \texttt{hate}_i + e_i$$
According to this model, the average percentage of Trump votes in states without any hate groups is `r round(coef(mod)[1],3)`. For every one additional hate group (per million citizens), the percentage of Trump votes increases by `r round(coef(mod)[2],3)`. You can view the resulting regression line (the conditional means of votes for Trump as predicted by the model) in Figure \@ref(fig:scatterplot-trump-votes-hate-groups-with-regression).

```{r scatterplot-trump-votes-hate-groups-with-regression, fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens with the estimated regression line."}
library(ggplot2)

if(knitr::is_html_output()) {
  plotly::ggplotly(ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes,name=state)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1]), tooltip=c("name","x","y"))
} else {
  ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1])
}
```

## Hypothesis testing

The estimated model indicates a positive relation between the number of hate groups and votes for Donald Trump. But the slope is an _estimated_ parameter. It might be that, in reality, there is no relation between hate groups and Trump votes. But because we have limited and noisy data, sometimes the estimated slope might be positive, and other times it might be negative. So the question is whether we have enough evidence to reject the null hypothesis that $\beta_1 = 0$.

As before, there are two main ways in which to look at testing whether the parameters of the model are equal to a priori values. The first is to consider how variable the parameter estimates are under the assumption that the true parameter is identical to the a priori value. The second way is to compare two models, one in which the parameter is fixed to the a priori value, and one where it is freely estimated. Both of these ways will provide us with the same outcome. The model comparison approach is more flexible, however, as it also allows you to test multiple parameters simultaneously. We'll start with discussing the first method, but it will be the last time until a while. After that, we'll focus on model comparison throughout. 

### Sampling distribution of estimates

Remember, an estimate (whether of the mean or another parameter) is a noisy reflection of the true value of that parameter. The noise comes from having access only to limited data, not _all_ the data the data generating process can produce. Suppose in reality there is no relation between hate groups and Trump votes, so that the true slope is $\beta_1 = 0$. In that case, the model becomes
\begin{align}
Y_i &= \beta_0 + 0 \times X_i + \epsilon \\
&= \beta_0 + \epsilon_i
\end{align}
which is identical to the simple model in Equation, renaming $\beta_0 = \mu$ and $\sigma_\epsilon = \sigma$. Then, the true value of the intercept would be $\beta_0 = \mu$. If we'd know the true value of the standard deviation $\sigma_\epsilon$, we'd have a fully specified model (a Normal distribution) which we can use to generate as many alternative data sets as we'd like. By generating these data sets, and estimating the slope of our model for each, we can get an overview of the variability of the estimates when the true slope equals $\beta_1 = 0$. An unbiased estimator ensures that on average, these estimates equal the true value. The main thing of interest is then the variability around this value. A consistent estimator ensures that this variability becomes smaller with the larger data sets. But to understand how well we can estimate the parameter for the present data, we would simulate data sets with the same number of observations (so $n=`r nrow(dat)`$). Unfortunately, we don't know the true value of $\sigma$. Our data provides an estimate of $\sigma$, but we know this estimate is noisy itself. Thinking in the same way about the sampling distribution of $\hat{\sigma}$, we could first sample a values of $\sigma$, and then use each of these to generate a data set for which to estimate $\beta_1$. Doing this many (many many!) times would give us a good overview of the variability of the estimates. As before, we don't have to actually simulate the data sets. If the model corresponding to the null hypothesis is true, then we can derive that the sampling distribution of the estimates follows a t-distribution. Thus, we should look for the $t$ value our data provides, and evaluate this value within the context of the sampling distribution derived under the model where $\beta_1 = 0$. For both parameters (intercept and slope), the same logic applies. In general then, for parameters $\beta_j$ (where $j = 0$ or 1), the $t$-value of our data is computed as 
\begin{equation}
t = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)} \quad \quad \quad t \sim \mathbf{T}(n-2)
\end{equation}
where $\mathbf{T}(n-2)$ denotes a standard Student t-distribution with $n-2$ degrees of freedom, and $\text{SE}(\hat{\beta}_j)$ is the standard error of the estimate, which you should remember is the standard deviation of the sampling distribution of the estimates. I won't bore you with how to compute this standard error; statistical software does a good job at this. One thing to realise though is that _the computed standard error is valid for datasets with exactly the same values of the predictor_. In other words, it assumes the predictor values are fixed (a part of the Data Generating Process). If you'd collect a different dataset with different values for e.g. the number of hate groups, the standard error would also be different.

R would for instance provide the following results for this regression model:
```{r simple-regression-trump2016-model-summary}
knitr::kable(summary(mod)$coefficients, digits=c(3,3,3,3), booktabs=TRUE, linesep="")
```
The values listed for `(intercept)` concern $\beta_0$, and the values listed for `hate_groups_per_million` concern the slope $\beta_1$. You can see the estimates are identical to those computed earlier (phew, no mistakes there! :-). You can see that the standard errors for the two parameters are quite different in magnitude. This is not so surprising, as they reflect quite different things, the intercept being a particular value of the dependent variable, while the slope represents an increase in the dependent variable for an increase in the predictor. If you'd change the scale of the predictor (e.g., from hate groups per million to hate groups per 10,000 citizens), the slope would change, as well as the corresponding standard error. Changing the scale in this way would not affect the standard error of the intercept. If you divide each estimate by the standard error, you obtain the value of the $t$-statistic (just like we did earlier in Equation \@ref(eq:definition-one-sample-t)). In a manner analogous to that depicted in Figure \@ref(fig:critical-values-t-distribution), you then determine critical values for the t-statistic (based on the degrees of freedom and the significance level $\alpha$), and determine whether the value you computed for the parameters lies within the critical range. If it does, the test result is called significant, and you reject the null hypothesis. If not, then the result is called not significant, and you don't reject the null hypothesis. Now that these are easy to compute with statistical software, it is more common to check the $p$-values. Remember, the $p$-value is the probability of the computed $t$-statistic _or a more extreme value_, assuming the null hypothesis is true. If the $p$-value is smaller than the chosen significance level (e.g. $\alpha = .05$), that means that the test is significant, and the null hypothesis rejected. In the results above, you can see that both p-values are below $\alpha = .05$. Hence, we can reject the null hypothesis that $\beta_0 = 0$ and the null hypothesis that $\beta_1 = 0$. In other words, there is good evidence that, in the absence of hate groups, the percentage of people voting for Trump is not equal to 0.^[It's probably unwise to mix statistics and politics, but wouldn't that be an utopia?] Also, there is good evidence that there is a relation between the number of hate groups and Trump votes.

Personally, I find the $t$ statistic quite intuitive in the context of a one-sample t-test. Generalizing the concept to a standardized estimate (dividing the estimate by its standard error) is also reasonably intuitive. However, in a multi-parameter model such as here, the sampling distribution of a single parameter is dependent on the estimation of all the other parameters. For instance, the test of the null hypothesis $H_0: \beta_1 = 0$ (i.e., no effect of hate groups on Trump votes) is based on deriving the sampling distribution of $\hat{\beta}_1$ in a model where $\beta_1 = 0$, but all the other parameters (i.e. $\beta_0$ and $\sigma_\epsilon$) cannot be assumed known). This uncertainty is dealt with similarly as before, resulting in a t-distribution, but now there are two sources of uncertainty (two parameters to estimate). Hence, the degrees of freedom are $n-2$ here.

### Model comparison

The fact that, in a multi-parameter model, a test of one parameter is _not_ conducted in isolation, but rather in the context of all the other parameters in the model, is more explicit in the model comparison approach. When we compare two models, we have to make clear what the parameters are in each: what are the unknown quantities which we will have to estimate, and what are quantities which we can assume a precise value for? 

As before, we will consider comparing _nested models_, in which a restricted MODEL R is a _special case_ of a more general MODEL G. Sticking to simple linear regression models, the most general model we have is that of Equation \@ref(eq:bivariate-regression-model-definition). 

As for the simple model of the previous chapter, it turns out we can write the likelihood for a model as a function of a _sum of squared deviations_. We will not go through the derivation of this again, as it is rather similar and equally tedious. But, because we are now calling these deviations _errors_, will start to refer to them as a __Sum of Squared Errors__ (SSE).

Let's call the model of Equation \@ref(eq:bivariate-regression-model-definition) MODEL G. Before we go on, I want to warn you that what we call MODEL G, and what we call MODEL R, can change from situation to situation. Basically, the identity of MODEL G and MODEL R are "local" to the particular model comparison. You can think of MODEL G as a parent, and MODEL R as a child. While the relation between them is similar, within an extended family, someone can be both a parent to one family member, and be the child of another. This is the form of flexibility that you will need when thinking about nested models. A model can be both more general than one model, and more restricted than another. I will come back to this soon.

First, let's consider what the Sum of Squared Error of a model is. The easiest way to define this is in terms of the __predictions__ of each model. Recall that a regression model has a structural and random part. The structural part defines the conditional mean of the dependent variable, while the random part concerns the random variation of the actual observations around the conditional means. As the random part is by definition unpredictable, there is not much we can do with that in terms of forming predictions. So we're stuck with the structural part. Although not a universal principle, there are many situations in which it makes sense to predict outcomes by the (conditional) mean. If we do this, then we can predict the outcomes with MODEL G as

$$\hat{Y}_{G,i} = \hat{\beta}_{G,0} + \hat{\beta}_{G,1} \times X_i$$
where $\hat{Y}$ stands for a predicted value. We're using the same "hat" for this as an _estimated value_, because it is really also a good estimate of what the value of $Y_i$ might have been if it was another observation with the same value for $X$. We are also assigning the subscript "G" to all the estimates, to distinguish them from those of a different model. Now let's consider a MODEL R in which we assume that there is no relation between Trump votes and hate groups, so that $\beta_1 = 0$. The predictions for this model would be

$$\hat{Y}_{R,i} = \hat{\beta}_{R,0} + 0 \times X_i = \hat{\beta}_{R,0}$$
Now we have two models to make predictions, we can write the corresponding Sum of Squared Error of each as:
\begin{equation}
\text{SSE}(M) = \sum_{i=1}^n \left(Y_i - \hat{Y}_{M,i} \right)^2
(\#eq:SSE-GLM-general)
\end{equation}
where we can replace the general letter $M$ (for Model) with either $G$ of $R$, to get $\text{SSE}(G)$ or $\text{SSE}(R)$, respectively. So the Sum of Squares is based on a difference between each observation and the model prediction for that observation. These are thus the _prediction errors_. If the prediction was equal to the the prediction of the true model (e.g. $\beta_0 + \beta_1 X_{i}$), then these would be equal to the true error terms $\epsilon_i$. But because we only have an estimated model, they are effectively estimates of the true errors. You can see these (unsquared) errors for the two models in Figure \@ref(fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison). 

```{r scatterplot-trump-votes-hate-groups-with-regression-model-comparison, fig.cap="Estimated regression lines for MODEL G (left) and MODEL R (right) with $\\beta_1 = 0$ and the errors.", fig.width=8, fig.height=4, out.width="100%"}
library(ggplot2)
library(dplyr)

mod0 <- lm(percent_Trump_votes ~ 1, data=dat)

ldat <- data.frame(model = rep(c("MODEL G","MODEL R"), each = 2), hate_groups_per_million = rep(c(min(dat$hate_groups_per_million),max(dat$hate_groups_per_million)),2), percent_Trump_votes = c(coef(mod)[1] + coef(mod)[2]*min(dat$hate_groups_per_million), coef(mod)[1] + coef(mod)[2]*max(dat$hate_groups_per_million), coef(mod0)[1], coef(mod0)[1]), ymin=50,ymax=50)

rbind(dat %>% mutate(model = "MODEL G", ymin = if_else(percent_Trump_votes > predict(mod), predict(mod), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod), predict(mod), percent_Trump_votes)),
      dat %>% mutate(model = "MODEL R", ymin = if_else(percent_Trump_votes > predict(mod0), predict(mod0), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod0), predict(mod0), percent_Trump_votes))) %>%
        ggplot(aes(x=hate_groups_per_million,y=percent_Trump_votes, ymin = ymin, ymax = ymax)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_line(data=ldat) + geom_linerange(alpha=.5) + facet_wrap(~model)

# 
# dat %>% mutate(ymin = if_else(percent_Trump_votes > predict(mod), predict(mod), percent_Trump_votes),
#                ymax = if_else(percent_Trump_votes < predict(mod), predict(mod), percent_Trump_votes)) %>%
#   ggplot(aes(x=hate_groups_per_million,y=percent_Trump_votes, ymin = ymin, ymax = ymax)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1]) + geom_linerange(alpha=.5) + ggtitle("MODEL G")
#            
# 
# mod0 <- lm(percent_Trump_votes ~ 1, data=dat)
# dat %>% mutate(ymin = if_else(percent_Trump_votes > predict(mod0), predict(mod0), percent_Trump_votes),
#                ymax = if_else(percent_Trump_votes < predict(mod0), predict(mod0), percent_Trump_votes)) %>%
#   ggplot(aes(x=hate_groups_per_million,y=percent_Trump_votes, ymin = ymin, ymax = ymax)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_abline(slope = 0, intercept = coef(mod0)[1]) + geom_linerange(alpha=.5) + ggtitle("MODEL R")

```

When you compare the errors between the models, you can see that MODEL G does not provide a better prediction for _each_ observation. Sometimes the distance from an observation to the regression line is larger for MODEL G than for MODEL R. However, MODEL G does seem to provide a better prediction for most observations. This is unsurprising, as MODEL G, by maximising the likelihood, should be at least as good as MODEL R. This means that it is _always the case that_
$$\text{SSE}(R) \geq \text{SSE}(G)$$
The Sum of Squared Errors of MODEL G will never be higher than that of MODEL R. So we cannot just select the model with the lowest SSE, as this would mean we'd always select MODEL G. We need to find a way to determine whether the SSE of MODEL G is sufficiently lower than that of MODEL R to make us believe that MODEL G is indeed superior to MODEL R.

In the previous chapter, we discussed that the $t$ statistic can be used to perform a test which is equivalent to the likelihood ratio test, and that this was useful because the sampling distribution of the $t$ statistic is known, while the sampling distribution of the likelihood ratio is difficult to determine. For the case of linear models, there is a similar argument that leads to a new statistic, which can be viewed as a generalization of the $t$ statistic. This is the $F$ statistic. If was given the letter in honour of Sir Ronald A. Fisher, a rather brilliant and very influential statistician, who developed it in the 1920s.

For comparing two linear models, where MODEL R is a special case of MODEL G, we can define the $F$ statistic as:
\begin{equation}
F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar(G)}}}
(\#eq:glm-f-definition)
\end{equation}
Here, $\text{npar}(G)$ stands for the number of parameters which are estimated in MODEL G, and $\text{npar}(R)$ for the number of parameters which are estimated in MODEL R. In counting the number of estimated parameters, we are excluding the error variance $\sigma^2_\epsilon$. Although this is really also a parameter of the models, it is generally not of direct interest. It is a so-called "nuisance parameter", something that we need to take into account, but we'd rather forget about. In the present example, MODEL G then has two estimated parameters: $\hat{\beta}_{G,0}$ and $\hat{\beta}_{G,1}$, while MODEL R has just one: $\hat{\beta}_{R,0}$. If we compute the SSE for each model (i.e. taking the vertical lines to the regression line in Figure \@ref(fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison) and squaring them), we get $\text{SSE}(G) = `r sum(residuals(mod)^2)`$ and $\text{SSE}(R) = `r sum(residuals(mod0)^2)`$. 

$$\begin{aligned}
F &= \frac{\frac{`r sum(residuals(mod0)^2)` -  `r sum(residuals(mod)^2)`}{`r length(coef(mod))` - `r length(coef(mod0))`}}{\frac{`r sum(residuals(mod)^2)`}{`r nrow(dat)` - `r length(coef(mod))`}} \\
&= \frac{`r  (sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0)))`}{`r sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))`} \\
&= `r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))`
\end{aligned}$$

If MODEL R is true, the sampling distribution of the $F$ statistic follows an $F$ distribution. This distribution has two parameters, both degrees of freedom, which we'll refer to as $\text{df}_1$ and $\text{df}_2$. The first one reflects the difference in the number of estimated parameters between the models
\begin{equation}
\text{df}_1 = \text{npar}(G) - \text{npar}(R)
(\#eq:glm-df1-general)
\end{equation}
and the second one the number of observations minus the number of estimated parameters of the more general model
\begin{equation}
\text{df}_2 = n - \text{npar}(G)
(\#eq:glm-df2-general)
\end{equation}
```{r f-distribution-plot, fig.cap="The F distribution with $\\text{df}_1 = 1$ and $\\text{df}_2 = 48$, and the critical value for $\\alpha = .05$", out.width="70%", fig.width=6, fig.height=.75*6}
xlim <- c(0,5)
data.frame(interval=1,x=c(qf(.95,1,48), seq(qf(.95,1,48),xlim[2],100),xlim[2]), y = c(0,df( seq(qf(.95,1,48),xlim[2],100),1,48),0)) %>%
  ggplot(aes(x=x,y=y,fill=factor(interval,labels=""))) + stat_function(aes(fill=NULL),fun=function(x) df(x,1,48), n=500) + xlim(xlim) + ylab("p(F)") + xlab("F") + geom_polygon(alpha=.8) + theme(legend.position="none") + geom_vline(xintercept = qf(.95, 1, 48), lty=2) + annotate("text", x=xlim[2]-.5, xmax=xlim[2], xmin=qf(.95,1,48),y=2,hjust = .5, vjust=1,label="upper \n critical region") + ylim(c(0,3))
#ggplot() + stat_function(fun=function(x) df(x,1,48), n=500) + xlim(c(0,5)) + xlab("F") + ylab("p(F)") + geom_vline(xintercept = qf(.95, 1, 48), lty=2)
```

Note that the value of the $F$ statistic can never be negative; the distribution is thus defined only over positive values of $F$. High values of $F$ indicate that the MODEL G has substantially less error than MODEL R. Another thing to note is that, __whenever $\text{df}_1 = 1$, there is a direct relation between the $F$ statistic and the $t$ statistic__: $F = t^2$, or conversely, $\sqrt{F} = t$. It is easy to check that this is indeed the case here: $\sqrt{`r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))`} = `r  sqrt(((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))))`$, which is the value of the $t$ statistic we computed earlier.

The critical value of the $F$ statistic, with $\alpha = .05$ and $\text{df}_1 = 2-1 = 1$ and $\text{df}_2 = 50 - 2 = 48$, is `r qf(.95,1,48)`, and any value of $F$ above this critical value would result in a rejection of the null hypothesis. So, because the $F$ value we computed was `r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))`, which is larger than the critical value, we reject the null hypothesis $H_0: \beta_1 = 0$, and conclude that there is evidence of a relation between the number of hate groups and Trump votes. Instead of checking whether the $F$ value is larger than a critical value, we can also compute the p-value, which is the probability of an $F$ value equal to or larger than `r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))` in the distribution depicted in Figure , More formally, the p-value can be defined as $P(F \geq `r  ((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))` | \text{df}_1 = 1, \text{df}_2 = 48) =  `r format(1-pf(((sum(residuals(mod0)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod0))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))),1,48),digits=3)`$. As this probability is smaller than the significance level, this again implies we reject the null hypothesis.

```{r}
mod00 <- lm(percent_Trump_votes ~ hate_groups_per_million - 1, data=dat)
```
A nice thing about the $F$ statistic is that it is general, and can be used to compare any nested linear models. For instance, we can also compare MODEL G to a different MODEL R in which we assume the intercept equals $\beta_0 = 0$, whilst allowing the slope to take any value. This model forces the regression line to go through the (0,0) point. Estimating this MODEL R gives^[Note that we cannot use Equation \@ref(eq:simple-regression-slope-estimate) to estimate the slope now. There is no nice formula for the slope estimate when you fix the intercept to 0, and so you will have to rely on statistical software to do this.]
$$\begin{aligned}
Y_i &= 0 + \hat{\beta}_{R,1} \times X_i \\
&= `r coef(mod00)[1]` \times X_i
\end{aligned}$$
You can see the resulting regression line and error terms in Figure \@ref(fig:scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2). 
```{r scatterplot-trump-votes-hate-groups-with-regression-model-comparison-2, fig.cap="Estimated regression lines for MODEL G (left) and MODEL R (right) with $\\beta_1 = 0$ and the errors.", fig.width=8, fig.height=4, out.width="100%"}
library(ggplot2)
library(dplyr)

ldat <- data.frame(model = rep(c("MODEL G","MODEL R"), each = 2), hate_groups_per_million = rep(c(min(dat$hate_groups_per_million),max(dat$hate_groups_per_million)),2), percent_Trump_votes = c(coef(mod)[1] + coef(mod)[2]*min(dat$hate_groups_per_million), coef(mod)[1] + coef(mod)[2]*max(dat$hate_groups_per_million), coef(mod00)[1]*min(dat$hate_groups_per_million), coef(mod00)[1]*max(dat$hate_groups_per_million)), ymin=50,ymax=50)

rbind(dat %>% mutate(model = "MODEL G", ymin = if_else(percent_Trump_votes > predict(mod), predict(mod), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod), predict(mod), percent_Trump_votes)),
      dat %>% mutate(model = "MODEL R", ymin = if_else(percent_Trump_votes > predict(mod00), predict(mod00), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod00), predict(mod00), percent_Trump_votes))) %>%
        ggplot(aes(x=hate_groups_per_million,y=percent_Trump_votes, ymin = ymin, ymax = ymax)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes for Trump") + geom_line(data=ldat) + geom_linerange(alpha=.5) + facet_wrap(~model)
```
This alternative MODEL R seems clearly inferior to MODEL G. The SSE of this model is $\text{SSE}(R) = `r sum(residuals(mod00)^2)`$. Computing the $F$ statistic gives
$$
\begin{aligned}
F &= \frac{\frac{`r sum(residuals(mod00)^2)` -  `r sum(residuals(mod)^2)`}{`r length(coef(mod))` - `r length(coef(mod00))`}}{\frac{`r sum(residuals(mod)^2)`}{`r nrow(dat)` - `r length(coef(mod))`}} \\
&= \frac{`r  (sum(residuals(mod00)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod00)))`}{`r sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))`} \\
&= `r  ((sum(residuals(mod00)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod00))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))`
\end{aligned}
$$
Because the test involves the same degrees of freedom, the critical value is the same as before. So we reject the null hypothesis again, which here is $H_0: \beta_0 = 0$. The p-value now is $P(F \geq `r  ((sum(residuals(mod00)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod00))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod))))` | \text{df}_1 = 1, \text{df}_2 = 48) `r pvalue(1-pf(((sum(residuals(mod00)^2) -  sum(residuals(mod)^2))/(length(coef(mod)) - length(coef(mod00))))/(sum(residuals(mod)^2)/(nrow(dat) - length(coef(mod)))),1,48),limit=TRUE,limit_below = 1e-4)`.$

<!--
I would like to point out that the discrepancy $Y_i - \hat{Y}_{M,i}$ is  a reasonable estimate of the error term $\epsilon_i$, as it is straightforward to rewrite the model as 
$$\begin{align}
\epsilon_i &= Y_i - (\beta_0 + \beta_1 \times X_i) \\
&= Y_i - \mu_{Y|X{_{i}}}
\end{align}$$
and we have just indicated that $\hat{Y}_i = \hat{\mu}_{Y|X{_{i}}}$. Replacing the conditional mean by its estimate, and realizing that ny using an estimate for this, we than also can't be certain about the true value of $\epsilon$, which would also turn that into an estimate, we then write this formally as:  
$$\hat{\epsilon}_{M,i} = Y_i - \hat{Y}_{M,i}$$
This in turn is a somewhat convoluted way to say that the prediction errors would be a reasonable way to estimate the error variance or standard deviation $\sigma_\epsilon$.
-->


<!--


```{r simple-regression-trump2016-anova}
knitr::kable(anova(mod))
```

-->

<!--
### Confidence interval

The formula to compute confidence intervals for the two parameters can be written as:
$$\hat{\beta}_j \pm t_{1-\alpha/2,n-2} \times \text{SE}(\hat{\beta}_j)$$
where $t_{1-\alpha/2,n-2}$ is the right-critical value in a t-distribution with $n-2$ degrees of freedom and a significance level of $\alpha$. Using $\alpha=.05$ gives us the conventional 95%-confidence interval. 
-->
<!-- 
## Assumptions and outliers

```{r boxplots-trump-votes-hate-groups, fig.show='hold', fig.cap="Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens", out.width="50%",fig.width=4}
plot_raincloud(dat,percent_Trump_votes, point_size = 1) + ylab("Percentage of votes for Trump")
plot_raincloud(dat,hate_groups_per_million, point_size = 1) + ylab("Hate groups per million citizens")
#ggplot(dat,aes(y=percent_Trump_votes, x="")) + geom_violin() + geom_boxplot(width=.1) + ylab("Percentage of votes for Trump") + xlab("") + stat_summary(fun=mean, geom="point", shape=16, size=2)
#ggplot(dat,aes(y=hate_groups_per_million, x="")) + geom_violin() + geom_boxplot(width=.1) + ylab("Hate groups per million citizens") + xlab("") + stat_summary(fun=mean, geom="point", shape=1, size=2)
```




```{r simple-regression-trump2016-residuals-plots,fig.show='hold',out.width='50%',fig.width=4,fig.cap="Predicted vs residual plot and a QQ plot of the residuals"}
res <- residuals(mod)
pred <- predict(mod)
ggplot(data.frame(predicted=pred,residual=res),aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept=0)

plot_qq_marginals(res, breaks=20)
```

## Summary

-->



