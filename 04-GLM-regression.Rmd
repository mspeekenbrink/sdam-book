# Linear regression

```{r load-trump-data}
library(sdamr)
data("trump2016")
dat <- subset(trump2016,state != "District of Columbia")
```

Donald Trump is perhaps the most divisive president in American history.

```{r scatterplot-trump-votes-hate-groups,fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens"}
library(ggplot2)

if(knitr::is_html_output()) {
  plotly::ggplotly(ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes,name=state)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes fpr Trump"), tooltip=c("name","x","y"))
} else {
  ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes fpr Trump")
}
```

```{r boxplots-trump-votes-hate-groups, fig.show='hold', fig.cap="Violin-boxplots for percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens", out.width="50%",fig.width=4}
plot_raincloud(dat,percent_Trump_votes) + ylab("Percentage of votes for Trump")
plot_raincloud(dat,hate_groups_per_million) + ylab("Hate groups per million citizens")
#ggplot(dat,aes(y=percent_Trump_votes, x="")) + geom_violin() + geom_boxplot(width=.1) + ylab("Percentage of votes for Trump") + xlab("") + stat_summary(fun=mean, geom="point", shape=16, size=2)
#ggplot(dat,aes(y=hate_groups_per_million, x="")) + geom_violin() + geom_boxplot(width=.1) + ylab("Hate groups per million citizens") + xlab("") + stat_summary(fun=mean, geom="point", shape=1, size=2)
```


To assess the relation between Trump votes and hate groups, we will use a linear regression model. As the name suggests, this model involves a line, a straight one in fact. This straight line represents the predicted value of the dependent variable $Y$ (i.e. the percentage of Trump votes) for each value of the predictor (or independent) variable $X$ (i.e. the number of hate groups). The model allows the predictions to be wrong by incorporating an error term,   

## The model

The bivariate regression model can be defined as follows:
\begin{equation}
Y_i = \beta_0 + \beta_1 X_{i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathcal{N}(0,\sigma)
(\#eq:bivariate-regression-model-definition)
\end{equation}

* The __intercept__ $\beta_0$ is 
* The __slope__ $\beta_1$ is
* The __error__ or __residual__ $\epsilon_i$

```{r simple-regression-explanation-plot,fig.cap="The simple regression model. Left: The intercept is the predicted value of Y when X=0 and is point at which the regression line crosses the y-axis. The slope determines the steepness of the regression line and represents the increase (or decrease) in the predicted Y-value for every 1-unit increase in X. Right: the error terms or residuals are the vertical distances of each observed Y-value from the regression line",fig.show="hold",fig.width=4,out.width="50%"}
x <- seq(-1,5)
y <- seq(-1,5)
xp <- c(-.5,.7,1.4,2.5,3.5,4)
yp <- c(1.2,1,3,4,4.5,3)

plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5))
abline(a=2,b=.5)
abline(v=0,lty=2)
abline(h=0,lty=2)
arrows(0,0,0,2,lwd=2,code=2,length=0.1)
text(0,.5*2,labels=expression(beta[0]),pos=4)
lines(c(0,1),c(2,2),lty=3)
arrows(1,2,1,.5 + 2,lwd=2,code=2,length=0.1)
text(1,2 + .5*.5,labels=expression(beta[1]),pos=4)
lines(c(1,2),c(2.5,2.5),lty=3)
arrows(2,2.5,2,.5 + 2.5,lwd=2,code=2,length=0.1)
text(2,2.5 + .5*.5,labels=expression(beta[1]),pos=4)
points(xp,yp)

plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5))
abline(a=2,b=.5)
abline(v=0,lty=2)
abline(h=0,lty=2)
for(i in 1:length(xp)) {
  lines(c(xp[i],xp[i]),c(yp[i],.5*xp[i]+2))
  text(xp[i],2 + .5*xp[i] + .5*(yp[i] - (2 + .5*xp[i])),labels=expression(epsilon[i]),pos=4)
}
points(xp,yp)

```


```{r simple-regression-distribution-error-plot, fig.cap="The errors in a regression model are assumed to follow a Normal distribution around the regression line." ,fig.width=4,out.width="50%"}
plot(x,.5*x + 2,type="n",xlab="X",ylab="Y",xlim=c(-1,5),ylim=c(-1,5),fig.width=4,out.width="50%")
abline(a=2,b=.5)
abline(v=0,lty=2)
abline(h=0,lty=2)
#points(xp,yp)
yt <- seq(0,6,length=100)
xt <- c(0,.75*dnorm(yt,mean=3,sd=.5),0)
yt <- c(0,yt,6)
polygon(-1+xt,-1.5+yt,col=rgb(0,0,0,alpha=.5))
polygon(0+xt,-1+yt,col=rgb(0,0,0,alpha=.5))
polygon(1+xt,-.5+yt,col=rgb(0,0,0,alpha=.5))
polygon(2+xt,yt,col=rgb(0,0,0,alpha=.5))
polygon(3+xt,.5+yt,col=rgb(0,0,0,alpha=.5))
polygon(4+xt,1+yt,col=rgb(0,0,0,alpha=.5))
polygon(5+xt,1.5+yt,col=rgb(0,0,0,alpha=.5))

```

## Estimation

The maximum likelihood estimates of the model parameters are
\begin{equation} 
\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \overline{X} ,
(\#eq:simple-regression-intercept-estimate)
\end{equation}
for the intercept, and
\begin{equation}
\hat{\beta}_1 = \frac{\sum_{i=1}^N (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^N (X_i - \overline{X})^2} .
(\#eq:simple-regression-slope-estimate)
\end{equation}
for the slope. Note that to estimate the intercept, we need the estimate of the slope. Let's focus on
the estimate of the slope first. The top part of the division (the numerator) contains a sum of 
deviations of the predictor values ($X_i$) from its average ($\overline{X}$) multiplied by deviations
of the values $Y_i$ of the dependent variable from its average ($\overline{Y}$). Let's consider these
multiplied deviations a little further. Each deviation is positive (larger than 0) when the value is 
higher than the average, and negative (smaller than 0), when the value is lower than the average.
So the multiplied deviations are positive whenever both values are larger than their average, and 
whenever both values are below their average (a negative value multiplied by another negative value 
is positive). If we were to divide the sum of the multiplied deviations by $N$ (the number of observations), 
we'd get the average of these multiplied deviations. This average is also called the __covariance__ between 
$X$ and $Y$: 
$$\text{Cov}(X,Y) =  \frac{\sum_{i=1}^N (X_i - \overline{X})(Y_i - \overline{Y})}{N}$$
Note that, as an estimator, this provides biased estimates of the true covariance. An unbiased estimator
of the true covariance is obtained by dividing by $N-1$ instead of $N$ (just as for the variance). Going back to our example, the covariance between trump votes and hate groups would be positive whenever states with higher-than-average Trump votes are generally also states with higher-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally also states with lower-than-average hate groups. 
The covariance would be negative, on the other hand, whenever states with higher-than-average Trump votes are generally states with lower-than-average hate groups, and whenever
states with lower-than-average Trump votes are generally states with higher-than-average hate groups. A positive or negative covariance is indicative of a relation between $X$ and $Y$. Indeed, the well-known
Pearson correlation coefficient is a standardized covariance, where the standardization scales the correlation
to always be between -1 and 1 and involves dividing the covariance by the product of the standard deviations
of both variables:
\begin{align}
r_{X,Y} &= \frac{\text{Cov}(X,Y)}{S_X \times S_Y} \\
&= \frac{\frac{\sum_{i=1}^N (X_i - \overline{X})(Y_i - \overline{Y})}{N}}{\sqrt{\frac{\sum_{i=1}^N (X_i - \overline{X})^2}{N}} \times \sqrt{\frac{\sum_{i=1}^N (Y_i - \overline{Y})^2}{N}}}
\end{align}

Going back to the estimate of the slope (Equation \@ref(eq:simple-regression-slope-estimate)), we can view this as a different way of standardizing the covariance. Looking at the bottom part of the division (the denominator), we can see it consists of the sum of squared deviations of the predictor values from its mean. If we were to divide this sum by the number of observations, we'd get the variance of $X$. As dividing both the top part (numerator) and bottom part (denominator) in a division by the same value does not affect the outcome of the division (i.e. $\frac{a}{b} = \frac{a/c}{b/c}$), we can choose to divide both by $N$ so the numerator becomes the covariance and the denominator the variance (we could also divide both by $N-1$ so they become unbiased estimators of the covariance and variance). So an alternative way of computing the estimate of the slope is
$$\hat{\beta}_1 = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}$$
Note that the variance of $X$ equals the product of the standard deviation of $X$ and itself, as $S^2_X = S_X \times S_X$. So the slope estimate looks quite a bit like the correlation coefficient, where instead of the standard deviation of $Y$, we use the standard deviation of $X$ twice. With a little algebraic manipulation, we can also state the slope estimate in terms of the correlation as 
$$\hat{\beta}_1 = \frac{S_Y}{S_X} r_{X,Y}$$
The reason for going into these alternative formulations is not to pain or confuse you with lots of equations. What these alternative formulations show is that the slope tells us something about the relation between $X$ and $Y$, just like the covariance and correlation do. If the sample correlation is 0, then so is the estimated slope. It is important to realise that we have been discussing the _estimate_ of the slope, not the true value itself. But the same relations hold for the true values. If we denote the true correlation as $\rho_{X,Y}$, then the true value of the slope can be defined as
$$\beta_1 = \frac{\sigma_Y}{\sigma_X} \rho_{X,Y}$$
The true value of the slope is 0 when the true correlation between $X$ and $Y$ equals $\rho_{X,Y} = 0$. It would also be 0 if the true standard deviation of $Y$ equals $\sigma_Y = 0$, but this implies that $Y$ is a constant and that is not a very interesting situation.

That was perhaps a little tortuous, and we haven't even discussed the estimate of the intercept! Remember that the intercept represents the predicted value of $Y$ at the point where $X=0$. It is the average value of $Y$ for all those cases where $X=0$. This is often not so interesting, although in our example, we might be interested in what the average percentage of votes for Trump would be in places where there are no hate groups. Equation \@ref(eq:simple-regression-intercept-estimate) shows that we can estimate this value by adjusting the average of $Y$ by subtracting $\hat{\beta}_1 \times \overline{X}$ from it. How come? [TODO]

So, what are the estimates of the model predicting Trump votes by hate groups?

```{r estimate-simple-regression-trump2016}
mod <- lm(percent_Trump_votes ~ hate_groups_per_million,data=dat)
```

$$\texttt{trump_votes}_i = `r round(coef(mod)[1],3)` + `r round(coef(mod)[2],3)` \times \texttt{hate_groups} + e_i$$

```{r scatterplot-trump-votes-hate-groups-with-regression,fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens with the estimated regression line."}
library(ggplot2)

if(knitr::is_html_output()) {
  plotly::ggplotly(ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes,name=state)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes fpr Trump") + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1]), tooltip=c("name","x","y"))
} else {
  ggplot(dat,aes(x=hate_groups_per_million,y=percent_Trump_votes)) + geom_point() + xlab("Hate groups per million citizens") + ylab("% votes fpr Trump") + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1])
}
```

## Hypothesis testing

As before, there are two main ways in which too look at testing whether the parameters of the simple regression model are different from a priori values. The first is to consider how variable the parameter estimates are under the assumption that the true parameter is identical to the a priori value. The second way is to compare two models, one in which the parameter is fixed to the a priori value, and one where it is freely estimated. Both of these ways will provide us with the same outcome. Personally, I find the model comparison way easier to think about.

### Sampling distribution of estimates

Remember, an estimate (whether of the mean or of another parameter) is a noisy reflection of the true value of the parameter, and the noise comes from having access only to limited data and not all the data that the data generating process can produce. Suppose in reality there is no relation between hate groups and Trump votes, so that the true slope is $\beta_1 = 0$. In that case, the model becomes
\begin{align}
Y_i &= \beta_0 + 0 \times X_i + \epsilon \\
&= \beta_0 + \epsilon_i
\end{align}
which is identical to the simple model of the previous chapter. Then, the true value of the intercept would be $\beta_0 = \mu$. If we'd know the true value of the standard deviation $\sigma$, we'd have a fully specified model (a Normal distribution) which we can use to generate as many alternative data sets as we'd like. By generating these data sets, and estimating the slope of our model for each, we can get an overview of the variability of the estimates when the true slope equals $\beta_1 = 0$. An unbiased estimator ensures that on average, these estimates equal the true value. The main thing of interest is then the variability around this value. A consistent estimator ensures that this variability becomes smaller with the larger data sets. But to understand how well we can estimate the parameter for the present data, we would simulate data sets with the same number of observations (so $N=`r nrow(dat)`$). Unfortunately, we don't know the true value of $\sigma$. Our data provides an estimate of $\sigma$, but we know this estimate is noisy itself. Thinking first in the same way about the sampling distribution of $\hat{\sigma}$, we could first sample a values of $\sigma$, and then use each of these to generate a data set for which to estimate $\beta_1$. Doing this many (many many!) times would give us a good overview of the variability of the estimates. As before, we don't have to actually simulate the data sets. If the null model is true, then we can derive that the sampling distribution of the estimates follows a t-distribution. Thus, we should look for the $t$ value our data provides, and evaluate this value within the context of the sampling distribution derived under the model where $\beta_1 = 0$. For both paraneters (intercept and slope), the same logic applies. In general then, for parameters $\beta_j$ (where $j = 0$ or 1), the $t$-value of our data is computed as 
\begin{equation}
t_{N-2} = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)} \quad \quad \quad t_{N-1} \sim \mathcal{T}(N-2)
\end{equation}
where $\text{SE}(\hat{\beta}_j)$ is the standard error of the estimate, which you should remember is the standard deviation of the sampling distribution of the estimates. I won't bore you with how to compute this standard error; statistical software does a good job at this. One thing to realise though is that the computation assumes that the data always have the same values for the predictor. 

```{r simple-regression-trump2016-model-summary}
knitr::kable(summary(mod)$coefficients)
```

### Model comparison

```{r simple-regression-trump2016-anova}
knitr::kable(anova(mod))
```

## Confidence intervals

The formula to compute confidence intervals for the two parameters can be written as:
$$\hat{\beta}_j \pm t_{1-\alpha/2,N-2} \times \text{SE}(\hat{\beta}_j)$$
where $t_{1-\alpha/2,N-2}$ is the right-critical value in a t-distribution with $N-2$ degrees of freedom and a significance level of $\alpha$. Using $\alpha=.05$ gives us the conventional 95%-confidence interval. 

## Assumptions and outliers



```{r simple-regression-trump2016-residuals-plots,fig.show='hold',out.width='50%',fig.width=4,fig.cap="Predicted vs residual plot and a QQ plot of the residuals"}
res <- residuals(mod)
pred <- predict(mod)
ggplot(data.frame(predicted=pred,residual=res),aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept=0)

plot_qq_marginals(res, breaks=20)
```

## Summary




