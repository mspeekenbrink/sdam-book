# A model with a mean: Introduction to the General Linear Model

In this chapter, we will start our journey into the General Linear Model with the most basic example within the GLM: a model concerning a single Normal distribution, where we are interested in statements about the mean of that distribution. We will look at the Normal distribution in detail, and cover estimation of its parameters, before focusing on testing hypotheses about the mean. We end the chapter with a look ahead towards the other chapters, by providing a brief overview of the General Linear Model. but first, let's discuss the research example used throughout the chapter.

## Numeric judgment and anchoring

In 1906, Sir Francis Galton attended The West of England Fat Stock and Poultry Exhibition in Plymouth.  One of the attractions at the fair was a contest to guess the weight of a "dressed" ox on display. The nearest guess to the actual weight of the ox would win a prize, and a total of 800 attendees participated for a small fee. Not only was Francis Galton a now-viled eugenicist (founder of the Galton Laboratory of National Eugenics at UCL), he was also a statistician, and bought the used tickets off the stall holder. Back home he analysed the 787 usable tickets and published his findings in @galton1907vox. What surprised Galton was that while the individual guesses were often far from the true weight of the ox, the average guess of the group as a whole was highly accurate. Such findings are now often referred to as the "Wisdom of the crowds".

Although the wisdom of the crowds has been demonstrated in various domains, from decades of research, we also know that human judgment is subject to a long list of biases. One such bias is called "anchoring", and it pertains to the influence of prior exposure to a more or less arbitrary numerical value on the subsequent numerical judgment. For instance, if I ask you "to estimate the height of"How tall do you think Mount Everest is?", but first state that Mount Everest is taller than 2000 feet, you are likely to give a smaller number than if I first state that Mount Everest is shorter than 45,500 feet. 

In this example, we will focus on data obtained from a large scale replication study, covering the above described anchoring manipulation, as well as many other experiments [@klein2014investigating]. This "Many Labs" study contained data collected by a large number of labs from different countries, and here we will use data collected by Konrad Bocian and Natalia Frankowska from the University of Social Sciences
and Humanities Campus in Sopot, Poland. In particular, we will focus on the judged height of Mount Everest (in meters) after a low anchor. From the wisdow of the crowds idea, we might expect the average judgment to be identical (or at least very close) to the true height of Mount Everest, which is 8848 meters above sea level. On the other hand, if the low anchor biased the judgments, we might expect the average judgment to deviate from the true height.

```{r load-anchoring, echo = FALSE}
library(sdamr)
library(ggplot2)
data(anchoring)
dat <- subset(anchoring,referrer %in% c("swps","swpson") & anchor == "low")
```

### Exploring the data

Before diving into statistical modeling, as discussed, it is always good to explore the data. A graphical overview in the form of a histogram and combined violin and boxplot, is given in Figure \@ref(fig:anchoring-eda-plots). The minimum was judged height was `r min(dat$everest_meters)` meters (so quite close to the anchor), and the maximum was `r pretNum(max(dat$everest_meters))` meters. The median judgment was `r median(dat$everest_meters)` and the mean `r mean(dat$everest_meters)`. 

```{r anchoring-eda-plots, echo=FALSE, fig.cap="Histogram and boxplot of participants' judgments", fig.show="hold", out.width="48%", fig.width=3, fig.height=3}
ggplot(dat,aes(x=everest_meters)) + geom_histogram(binwidth=500) + xlab("Height of Mount Everest in meters")

ggplot(dat,aes(y=everest_meters, x="")) + geom_violin() + geom_boxplot(width=.1) + ylab("Height of Mount Everest in meters") + xlab("") + stat_summary(fun=mean, geom="point", shape=16, size=2)
```

## The Normal distribution

The Normal distribution has the well-known bell-shaped curve depicted in Figure \@ref(fig:normal-density-plot).

```{r normal-density-plot, fig.cap="The Normal density function for $\\mu = 0$ and $\\sigma = 1$", fig.height=4,out.width="80%"}
ggplot() + stat_function(fun=function(x) dnorm(x,mean=0,sd=1)) + xlim(-4,4) + ylab("f(y)") + xlab("y")
```

The curve is given by the following function:

\begin{equation}
p(y) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(y-\mu)^2}{2\sigma^2}}
(\#eq:normal-density-function)
\end{equation}

If that looks complicated, don't worry. First, let me point out that $\pi$ refers to the mathematical constant pi, i.e. $\pi= 3.141593\ldots$, and $e$ refers to the mathematical constant also known as Euler's number, i.e. $e =  2.718282\ldots$. They are known constants and not parameters. The Normal distribution does have two parameters: $\mu$ ("mu"), the mean, and $\sigma$ ("sigma"), also called the standard deviation. Confusingly, the formula above is not actually the formula for the Normal probability distribution. Rather, it is the formula of the Normal __density function__. The Normal distribution applies to continuous variables. Technically, you can't assign a probability to a particular value of a continuous variable, you can only assign probabilities to ranges of values. This is because a continuous variable has an infinite number of values (even if we restrict ourselves to a range). If we gave each unique value a probability of anything larger than 0, the total probability would not be 1, but infinite. If you add up an infinite amount of values, no matter how tiny they are, at some point you would reach the sum of 1, and then you'd have to keep on adding more. No matter how large the sum becomes, you would have to keep on going. Adding up an infinite number of values is not light work... 

$$P(a \leq Y \leq b) = \int_a^b \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(y-\mu)^2}{2\sigma^2}} d y$$
If you are unfamiliar with calculus and have never seen an integration sign ($\int$), don't worry, there won't be much more of this. It is a mathematical way of referring to the "area under the curve" between point $a$ and $b$. A proper probability density function respects the rule of total probability, such that $P(-\infty \leq Y \leq \infty) = 1$. Figure \@ref(fig:normal-distribution-plot) shows the probability of three of such ranges, each symmetrical around the mean. The probability of a value falling in the range between $\mu - 3 \times \sigma$ and $\mu + 3 \times \sigma$ is very close to 1. Hence, values more than three standard deviations from the mean are very unlikely.

```{r normal-distribution-plot, fig.cap="Normal distribution and the probability of a value falling in one of three overlapping ranges. Note that the ranges are overlapping, so that the range between $\\mu - 3 \\sigma$ and $\\mu + 3 \\sigma$ also covers the range between $\\mu - 2 \\sigma$ and $\\mu + 2 \\sigma$, but that this isn't shown in the colours.",fig.height=3}
library(scales)
library(dplyr)
library(tidyr)
levels <- c(expression("mu %+-% sigma %~~% .683"),expression("mu %+-% 2 %*% sigma %~~% .954"),expression("mu %+-% 3 %*% sigma %~~% .997"))[3:1]
rbind(data.frame(interval=1,x=c(-3,seq(-3,3,length=100),1),y=c(0,dnorm(seq(-3,3,length=100)),0)),
      data.frame(interval=2,x=c(-2,seq(-2,2,length=100),2),y=c(0,dnorm(seq(-2,2,length=100)),0)),
      data.frame(interval=3,x=c(-1,seq(-1,1,length=100),1),y=c(0,dnorm(seq(-1,1,length=100)),0))) %>%
  ggplot(aes(x=x,y=y,fill=factor(interval,labels=levels))) + stat_function(aes(fill=NULL),fun=function(x) dnorm(x,mean=0,sd=1)) + xlim(-4,4) + ylab("f(y)") + xlab("y") + geom_polygon(alpha=.8) + scale_fill_discrete(name="interval",labels = parse_format()) + theme(legend.title=element_text("interval"))
```

## The Central Limit Theorem

A main reason that the Normal distribution is used so often (and perhaps called "Normal", rather than "Abnormal"), is due to a mathematical fact known as the __Central Limit Theorem__:

```{definition, clt, name="Central Limit Theorem", echo=TRUE} 
The distribution of the sum of $N$ independent variables approaches the Normal distribution as the number of variables approaches infinity ($N \rightarrow \infty$).

```


This is quite an amazing theorem. Although you might wonder why you should care about sums of an infinite number of random variables, there are two things to note. Firstly, the rate at which the distribution "approaches" the normal distribution can be relatively quick. Secondly, the sample mean can be viewed as the (normalized) sum of $N$ variables:

$$\overline{Y} = \frac{1}{N} \sum_{i=1}^N Y_i$$

So the central limit theorem implies that as long as the number of samples is large enough, the sample mean will follow a Normal distribution.

### The Central Limit Theorem in action

To see the central limit theorem in action, let's pick an arbitrary probability distribution over 5 values, as shown in Figure \@ref(fig:weird-prob-dist). This will be the true distribution (i.e. the Data Generating Process) from which we can sample values.   

```{r weird-prob-dist,fig.cap="An aribitrary probability distribution over 5 values, clearly not a Normal distribution!",fig.width=4,fig.height=3,out.width="50%"}
library(dplyr)
library(tidyr)
library(purrr)

values <- c(1,2,3,4,5)
prob <- c(.5,.2,.1,.6,.3)
prob <- prob/sum(prob)

data.frame(value=values,probability=prob) %>% 
  ggplot(aes(x=value,y=probability)) + geom_col() + ylim(c(0,1))
```

When we repeatedly sample 5 values and calculate the mean, we can look at the distribution of these sampled means. This is shown in Figure \@ref(fig:clt-animation-1). Figure \@ref(fig:clt-animation-2) shows the same when we repeatedly sample 20 values and calculate the mean. 

```{r clt-animation-1, dev='png', echo=FALSE, cache=TRUE, fig.cap="The distribution of the mean of 5 samples (from the distribution in Figure XX", out.width='50%', fig.width=4}

library(gganimate)
library(dplyr)
library(tidyr)
library(purrr)

set.seed(345)
values <- c(1,2,3,4,5)
prob <- c(.5,.2,.1,.6,.3)
prob <- prob/sum(prob)

n <- 5
n_sim <- 200
clt_dat <- data.frame(sim=1:n_sim,size=n,outcome=sapply(1:n_sim,function(x) sum(sample(values,size=n,prob=prob,replace=TRUE))/n))

clt_dat %>%
  pull(sim) %>% 
    map_df(~ clt_dat %>% filter(sim %in% 1:.x) %>% mutate(sim = .x)) %>% 
  ggplot(aes(x=outcome)) + geom_histogram(binwidth=.2) + xlim(c(1,5)) + ylim(c(0,30)) + transition_manual(sim) + ease_aes("linear") + enter_fade() + exit_fade()

```

```{r clt-animation-2, dev='png',cache=TRUE, fig.cap="The distribution of the mean of 20 samples from the distribution in Figure XX"}

set.seed(567)
n_sim <- 1000
n <- 20
clt_dat <- data.frame(sim=1:n_sim,size=n,outcome=sapply(1:n_sim,function(x) sum(sample(values,size=n,prob=prob,replace=TRUE))/n))

clt_dat %>%
  pull(sim) %>% 
    map_df(~ clt_dat %>% filter(sim %in% 1:.x) %>% mutate(sim = .x)) %>%
  ggplot(aes(x=outcome)) + geom_histogram(binwidth=.05) + xlim(c(1,5)) + ylim(c(0,75)) + transition_manual(sim) + ease_aes("linear") + enter_fade() + exit_fade()
```

So according to the Central Limit Theorem, if we focus on the mean of a sufficiently large number of independent observations, we can reasonably assume that the sampling distribution of the mean will follow a Normal distribution, even if the distribution from which the actual values were drawn is far from Normal. We might also imagine that if a variable is the outcome of a process in which lots of independent sources of noise are added, that the resulting variable will follow a Normal distribution. [todo: add example]

## Parameter estimation

The maximum likelihood estimate of $\mu$ is the sample mean:

\begin{equation}
\hat{\mu} = \overline{Y} = \frac{\sum_{i=1}^N Y_i}{N}
(\#eq:ml-estimator-mean-simple-model)
\end{equation}

The maximum likelihood estimate of the variance is the sample variance $\hat{\sigma}^2_\text{ML} = \frac{\sum_{i=1}^N (Y_i - \overline{Y})}{N}$. However, this estimator of the variance is biased. It is therefore common to estimate the variance with the unbiased estimator:

\begin{equation}
\hat{\sigma}^2 = S^2 = \frac{\sum_{i=1}^N (Y_i - \overline{Y})}{N-1}
(\#eq:unbiased-estimator-variance-simple-model)
\end{equation}

The estimator above is for $\sigma^2$ (sigma squared, i.e. sigma raised to the power of 2). If we want an estimator of $\sigma$, we can simply take the square-root to get the following estimator of $\sigma$:

$$\hat{\sigma} = S = \sqrt{S^2}$$

### Sampling distribution of the estimated mean

Remember that the estimator of the mean is an algorithm that provides estimates from data. Different data sets will give different estimates, even when these are generated from the same Data Generating Process. Our model of the Data Generating Process is the Normal distribution, which has two parameters: $\mu$ and $\sigma$. Let's pick some values for these parameters, so that we can use our model to simulate data. For instance, suppose the judgments are on average equal to the true value, so $\mu = 8848$. Individual judgments are quite variable, however, so let's take $\sigma = 2000$. Figure~\@ref{fig:histogram-sampling-mean-simple} shows the distribution of the estimated mean of 10,000 data sets each consisting of $N=109$ observations (just like in the anchoring data). Also shown (as a dotted line) is the Normal distribution of the model we used to generate the data. Clearly, the estimated means are much less variable than the simulated judgments.

```{r histogram-sampling-mean-simple, fig.cap="Estimated means for 10000 simulated data sets of N = 109, drawn from a Normal distribution with mean 8848 and standard deviation 2000. The dotted line is represents the Normal distribution from which the data sets were generated, and the solid line the theoretical distribution of the estimated means."}
set.seed(145)
mu <- 8848
sigma <- 2000
n_sim <- 10000
n <- 109
sim <- matrix(rnorm(n*n_sim, mean=mu, sd = sigma),ncol=n)
estimates <- rowMeans(sim)
ggplot(data.frame(estimates=estimates),aes(x=estimates)) + geom_histogram(binwidth=20) + xlim(8848-+ 2000,8848 + 2000) + stat_function(fun = function(x) 
    dnorm(x, mean = 8848, sd = 2000) * 20 * 10000, linetype=2) + stat_function(fun = function(x) 
    dnorm(x, mean = 8848, sd = 2000/sqrt(109)) * 20 * 10000, linetype=1)

```

## Testing whether $\mu$ has an specific value

### The classical way

\begin{equation}
t = \frac{\hat{\mu} - \underline{\mu}}{\hat{\sigma}/\sqrt{N}} = \frac{\overline{Y} - \underset{\sim}{\mu}}{S/\sqrt{N}}
\end{equation}

This is also called a one-sample t-test. 

#### Two-tailed and one-tailed tests

### The model comparison way

```
http://www.stats.ox.ac.uk/~dlunn/b8_02/b8pdf_8.pdf
http://www.stat.ucla.edu/~hqxu/stat105/pdf/ch09.pdf
https://www.math.arizona.edu/~jwatkins/v-anova.pdf
```

There is an equivalent way to perform the hypothesis test that $\mu = \underline{\mu}$, by comparing two versions of a statistical model, one in which we assume we know the value of $\mu$ to be $\underline{\mu}$ (Model R), and one in which we don't (Model G). As we did for Paul the Octopus, we then look at the likelihood ratio to see whether Model R is tenable. Note that in both models, the standard deviation is considered unknown and will have to be estimated. 

## The General Linear Model

$$Y_i = \beta_0 + \beta_1 X_{1i} + \ldots + \beta_K X_{Ki} + \epsilon_i \quad \quad \epsilon_i \sim N(0,\sigma^2)$$
