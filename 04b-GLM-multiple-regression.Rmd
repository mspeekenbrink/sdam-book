# Multiple regression

In this chapter, we extend the simple regression model to include multiple predictor variables. The slopes of the predictors in the resulting multiple regression model reflect the unique effect of each predictor on the dependent variable, after removing the effect of all other predictors from both the dependent and predictor variable. We will look at what that means in some detail. We then go on to discuss parameter estimation and testing. 

```{r load-trump-data-again}
library(papaja)
library(sdamr)
data("trump2016")
dat <- subset(trump2016,state != "District of Columbia")
```

## Trump, votes, and hate groups (again)

In the previous chapter, we found evidence for a relation between the number of hate groups and votes for Trump. This relation was found with an observational study, not an experimental one. Because the number of hate groups was not randomly assigned to states, there are possible confounding factors that could account for the result. The well-known phrase "correlation does not imply causation" should be taken seriously. For instance, it could be that hate groups are especially prevalent in places with relatively low levels of education. And it might be that those with a relatively low level of education are also more likely to vote for Trump. In other words, education level might be a __common cause__ of both hate groups and Trump votes, whilst there is no direct relation between hate groups and Trump votes. The difference between a direct relation, and a spurious one through a common cause, is depicted in Figure \@ref(fig:direct-relation-vs-common-cause). You can see more, often rather amusing examples of spurious relations on the [spurious correlation](https://www.tylervigen.com/spurious-correlations) website.

```{tikz direct-relation-vs-common-cause, device='svg', fig.cap="A direct relation between hate groups and Trump votes vs a spurious relation through a common cause (education)."}
  \usetikzlibrary{arrows}
	\begin{tikzpicture}
	\pgfsetxvec{\pgfpoint{3cm}{0cm}}
	\pgfsetyvec{\pgfpoint{0cm}{3cm}}
  \tikzstyle{every node}=[minimum size=2cm]

  \draw node at (0,1.5) {\textbf{direct relation}};
	\draw node[circle,draw, align=center] (x1) at (0,0.5) {\texttt{hate} \\ \texttt{groups}};
	\draw node[circle,draw, align=center] (y) at (1.5,0.5) {\texttt{Trump} \\ \texttt{votes}};

  \draw node at (3,1.5) {\textbf{common cause}};
	\draw node[circle,draw, align=center] (x1b) at (0+3,0) {\texttt{hate} \\ \texttt{groups}};
	\draw node[circle,draw, align=center] (yb) at (1.5+3,0) {\texttt{Trump} \\ \texttt{votes}};
	\draw node[circle,draw, align=center] (x2b) at (.75+3,1) {\texttt{education}};

	\draw[->, line width=.5mm] (x1) -- (y);

	\draw[->, line width=.5mm] (x2b) -- (yb);
	\draw[->, line width=.5mm] (x2b) -- (x1b);

	\end{tikzpicture}
```

To assess the relation between hate groups and Trump votes in the possible presence of common causes and other confounds, ideally, we would like to "remove" the effect of such confounds from both the "hate groups" predictor, and the "Trump votes" dependent variable. If, after statistically controlling for confounds in such a way, we still find evidence for a relation between hate groups and Trump votes, that would strengthen our belief that the relation is real, rather than spurious. Multiple regression is a way in which to determine _unique effects_ of predictor variables on the dependent variable. Before we move on to define the multiple regression model, we will work through an indirect way to control for a third variable (e.g. a possible common cause), which will provide insight into what is meant by unique effects in a multiple regression model.

### Controlling for education level

We have indicated that, potentially, the level of education might be a common cause for both the prevalence of hate groups, and voter support for Trump. What if we could assess the relation between hate groups and Trump votes after removing the effect of education level from both? One reasonable way to do this, using the tools we already have, is by means of simple regression models. The idea is reasonably straightforward. In a regression model predicting Trump votes from education level, the error terms (residuals) of that model reflect that part of the variation in Trump votes which can not be predicted from education level. Similarly, in a regression model predicting hate groups from education level, the error terms (residuals) of that model reflect the variation in hate groups which can not be predicted from education level. So, if there is still a relation between the _residual_ Trump vote, and the _residual_ hate groups, then the relation can not be due to education level as a common cause.

```{r}
mod1 <- lm(percent_Trump_votes ~ percent_bachelors_degree_or_higher, data=dat)
tab1 <- summary(mod1)$coefficients
mod2 <- lm(hate_groups_per_million ~ percent_bachelors_degree_or_higher, data=dat)
tab2 <- summary(mod2)$coefficients
```

When we estimate a regression model predicting Trump votes from education level (defined as the percentage of citizens who obtained a Bachelors degree or higher), we obtain the following estimated model
$$\texttt{Trump_votes}_i = `r coefficients(mod1)[1]` `r ifelse(coefficients(mod1)[2] >= 0, "+", "-")` `r abs(coefficients(mod1)[2])` \times \texttt{education}_i + \hat{\epsilon}_{\texttt{votes},i}$$
and for the hate groups we obtain
$$\texttt{hate_groups}_i = `r coefficients(mod2)[1]` `r ifelse(coefficients(mod1)[2] >= 0, "+", "-")` `r abs(coefficients(mod2)[2])` \times \texttt{education}_i + \hat{\epsilon}_{\texttt{hate},i}$$
Note that I'm labelling the error terms for both models with a subscript (either $\texttt{votes}$ or $\texttt{hate}$) to denote these are the residual terms for a model predicting Trump votes and hate groups from education levels respectively. Also, I'm adding the hat above them to indicate that these are not the "true errors" (from a model with the true values of the slope and intercept) but rather estimates of these resulting from using estimated parameters. The residuals, as well as the estimated regression lines, are depicted in in Figure \@ref(fig:regressions-controlling-for-education). The slope of $\texttt{education}_i$ is significant in both models. In the model for Trump votes, $\hat{\beta}_\texttt{education} = `r tab1[2,1]`$, `r apa_print(mod1)$statistic$percent_bachelors_degree_or_higher`. In the model for hate groups, $\hat{\beta}_\texttt{education} = `r tab2[2,1]`$, `r apa_print(mod2)$statistic$percent_bachelors_degree_or_higher`.
This thus shows a reliable relation between education level and Trump votes, and between education level and hate groups, which is supportive of the idea of a common cause. However, significance is not a prerequisite to statistically control for a third variable.

```{r regressions-controlling-for-education, fig.show="hold", out.width="50%", fig.width=5, fig.height=4, fig.cap="Predicting Trump votes and hate groups by linear regression models with education level as predictor. The residuals of each model are depicted as vertical lines from the observations to the regression line."}
library(ggplot2)
library(dplyr)
dat %>% mutate(ymin = if_else(percent_Trump_votes > predict(mod1), predict(mod1), percent_Trump_votes),
               ymax = if_else(percent_Trump_votes < predict(mod1), predict(mod1), percent_Trump_votes)) %>%
  ggplot(aes(x=percent_bachelors_degree_or_higher, y=percent_Trump_votes, ymin=ymin, ymax=ymax)) + geom_point() + xlab("% bachelors degree or higher") + ylab("% votes for Trump") + geom_abline(intercept = coefficients(mod1)[1], slope=coefficients(mod1)[2]) + geom_linerange(alpha=.5)

dat %>% mutate(ymin = if_else(hate_groups_per_million > predict(mod2), predict(mod2), hate_groups_per_million),
               ymax = if_else(hate_groups_per_million < predict(mod2), predict(mod2), hate_groups_per_million)) %>%
  ggplot(aes(x=percent_bachelors_degree_or_higher, y=hate_groups_per_million, ymin=ymin, ymax=ymax)) + geom_point() + xlab("% bachelors degree or higher") + ylab("Hate groups per million citizens") + geom_abline(intercept = coefficients(mod2)[1], slope=coefficients(mod2)[2]) + geom_linerange(alpha=.5)
```

```{r}
tdat <- data.frame(res_trump_vote=residuals(mod1), res_hate_groups = residuals(mod2))
mod3 <- lm(res_trump_vote ~ res_hate_groups, data=tdat)
```

The regression line in both models represents the variation in the dependent variable (Trump votes and hate groups) that is dependent on education level. In both models, the error term $\epsilon$ is assumed to be completely independent from the predictor. Hence, these are the aspects of Trump votes and hate groups that cannot be explained by education level. To assess whether there is a relation between these residual terms, which would indicate a relation between Trump votes and hate groups which is independent from education level, we can estimate another linear regression model
$$
\begin{equation}
\hat{\epsilon}_{\texttt{votes},i}  = 0 + `r coefficients(mod3)[2]` \times \hat{\epsilon}_{\texttt{hate},i} + \hat{\epsilon}_i
(\#eq:slope-hate-groups-from-residuals)
\end{equation}
$$
Note that the intercept is equal to 0. This is necessarily the case, because __the residuals of estimated regression models always have a mean of 0__. If you go back to the estimate of the intercept in Equation \@ref(eq:simple-regression-intercept-estimate), you will see that if the means are both 0, the result has to be 0 as well:
$$\begin{align}
\hat{\beta}_0 &= \overline{Y} - \hat{\beta}_1 \overline{X} \\
&= 0 - \hat{\beta}_1 \times 0 \\
&= 0
\end{align}$$

A scatterplot of the dependent variable (the _residual_ Trump votes $\hat{\epsilon}_{\texttt{votes},i}$) and the predictor (the _residual_ hate groups $\hat{\epsilon}_{\texttt{hate},i}$), together with the estimated regression line, is provided in Figure \@ref(fig:scatterplot-residual-votes-and-hate).

```{r scatterplot-residual-votes-and-hate, fig.cap = "Residual Trump votes and hate groups, after statistically removing the effect of education level from both, and the estimated regression line.", out.width="80%", fig.width=6, fig.height=4}
tdat %>%
  ggplot(aes(x=res_hate_groups, y=res_trump_vote)) + geom_point() + geom_abline(intercept=coefficients(mod3)[1], slope = coefficients(mod3)[2]) + xlab("Residual hate groups per million citizens") + ylab("Residual % votes for Trump")
```

We can see that there appears to be a positive relation between residual Trump votes and residual hate groups. If we perform a test of the null hypothesis $H_0: \beta_1 = 0$ in this model, we find that the result is significant, `r apa_print(mod3)$statistic$res_hate_groups`. This thus indicates that there is a relation between Trump votes and hate groups that can not be attributed to education level. While the procedure of removing the effect of education level from both Trump votes and hate groups is appropriate, there however is a slight issue with the hypothesis test we just performed. To remove the effect of education level, we first had to estimate two new models. These estimates are noisy, and this noise may affect the estimates of the third model in which we predicted the residual Trump votes from the residual hate groups. The hypothesis test does not take this additional source of noise into account.

It turns out that we do not need to estimate separate models to remove the effect of education level. By including both education level and hate groups as predictors of Trump votes in a multiple regression model, the estimated slope of hate groups will be _exactly equal_ to the one that we just computed from the residuals. Multiple regression models thus concern the unique effects of each predictor on the dependent variable, removing the effect of all other predictors from that relation.  

## The multiple regression model

The multiple regression model is a straightforward extension of the simple linear regression model, including more than one predictor $X$:
\begin{equation}
Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_k \times X_{m,i} + \epsilon_i \quad \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
(\#eq:multiple-regression-model-definition)
\end{equation}
Note that we are using $m$ to reflect the total number of predictors $X$ in the model. As for the simple regression model, this model consists of a structural part which reflects the conditional mean of the dependent variable $Y$, conditional upon all predictors:
$$\beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_k \times X_{m,i} = \mu_{Y|X_1,\ldots,X_m}$$
and a random part $\epsilon_i$. 

```{r}
mod <- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat)
```

For a model with $m=2$ predictors, such as
$$\begin{equation}
\texttt{trump_votes}_i = \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)
(\#eq:multiple-regression-votes-by-hate-education)
\end{equation}$$
the data can be represented in a three-dimensional space. In this space, the conditional means $\mu_{Y | X_{1,i}, X_{2,i}}$ can be represented as a __regression plane__. A visual representation is given below.

```{r 3d-regression-plane, fig.cap='Three-dimensional representation of a regression model predicting Trump votes from hate groups and education level. By clicking on the image and moving your mouse, you should be able to rotate the image to explore it further. You can also click on the 'Play' button for an animation of such a rotation.", fig.align='center', cache=TRUE}
library(rgl)
#library(rglwidget)
plot3d(x=dat$hate_groups_per_million, y=dat$percent_bachelors_degree_or_higher, z=dat$percent_Trump_votes, xlab="Hate groups", ylab="% bachelors degree or higher", zlab="% Trump votes", type = "s", col = "purple", size = 1)
coefs <- coef(mod)
a <- coefs["hate_groups_per_million"]
b <- coefs["percent_bachelors_degree_or_higher"]
c <- -1
d <- coefs["(Intercept)"]
planes3d(a, b, c, d, alpha = 0.5)
userMat <- matrix(c(0.04618797,-0.998904467,-0.005153618,0,0.02906432,-0.003813314,0.999554276,0,-0.99849236,-0.046314929,0.028857738,0,0.00000000,0.000000000,0.000000000,1), ncol=4, byrow = TRUE)
rgl.viewpoint(userMatrix = userMat)

fn <- rgl::par3dinterp(time = (0:3)*0.75, userMatrix = list(userMat,
                                      rotate3d(userMat, pi/2, 0, 0, -1),
                                      rotate3d(userMat, pi/2, 0, 0, 1),
                                      rotate3d(userMat, pi/2, 1, 0, 0))) 
widget <- rglwidget(elementId = "interactive-3dplot-multiple-regression") %>%
  playwidget(par3dinterpControl(fn, 0, 2*3*.75, steps=30),
       step = 0.01, loop = TRUE, rate = 0.5)
widget

```

Keeping all predictors constant apart from one, the model becomes a simple linear regression model for the non-constant variable. If you pick a value of "% bachelors degree or higher", you can think of this as slicing the regression plane at that point. The slice of the plane is a single straight line, which is then a simple regression model. For example, if we focus on states with an education level of `r min(dat$percent_bachelors_degree_or_higher)`% with a bachelors degree or higher, then the multiple regression model can be written as
$$\begin{align}
\texttt{trump_votes}_i &= \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times `r min(dat$percent_bachelors_degree_or_higher)` + \epsilon_i  \\
&= (\beta_0 + \beta_2 \times `r min(dat$percent_bachelors_degree_or_higher)`) + \beta_1 \times \texttt{hate_groups}_i + \epsilon_i \\
&= \beta_0' + \beta_1 \times \texttt{hate_groups}_i + \epsilon_i
\end{align}
$$
which is a simple regression model with a new intercept $\beta_0' = \beta_0 + \beta_2 \times `r min(dat$percent_bachelors_degree_or_higher)`$ composed as the sum of the original intercept and `r min(dat$percent_bachelors_degree_or_higher)` times the slope of $\texttt{education_level}$, and the same slope $\beta_1$ for $\texttt{hate_groups}$. You can see these simple regression lines, for three different values of education level, in Figure \@ref(fig:trump-multiple-regression-scatter-with-multiple-regression-lines). Notice that the regression lines for different education levels are parallel, because they have the same slope. However, the regression lines are at different heights, because they have a different intercept.

```{r trump-multiple-regression-scatter-with-multiple-regression-lines, fig.cap="Percentage of votes for Trump in the 2016 elections for 50 US states and the number of hate groups per 1 million citizens, with regression lines for three different values of education level.", fig.width=5, fig.height=5, out.width='60%'}
ggplot(dat,aes(x=hate_groups_per_million, y=percent_Trump_votes, colour=percent_bachelors_degree_or_higher)) + geom_point() + geom_abline(intercept = coefs[1] + min(dat$percent_bachelors_degree_or_higher)*coefs[3], slope = coefs[2], colour=viridis::viridis_pal()(3)[1]) + geom_abline(intercept = coefs[1] + (min(dat$percent_bachelors_degree_or_higher) + (max(dat$percent_bachelors_degree_or_higher) - min(dat$percent_bachelors_degree_or_higher))/2) *coefs[3], slope = coefs[2], colour=viridis::viridis_pal()(3)[2]) + geom_abline(intercept = coefs[1] + max(dat$percent_bachelors_degree_or_higher)*coefs[3], slope = coefs[2], colour = viridis::viridis_pal()(3)[3]) + theme(legend.position = "bottom") + labs(colour = "% bachelors degree or higher") + xlab("Hate groups per million citizens") + ylab("% votes for Trump")
```

## Estimation {#sec:04b-estimation}

The estimate of the intercept is a direct generalization of that for a simple regression model: 
\begin{equation} 
\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \times \overline{X}_1 -  \hat{\beta}_2 \times \overline{X}_2 - \ldots -  \hat{\beta}_m \times \overline{X}_m
(\#eq:multiple-regression-intercept-estimate)
\end{equation}
Again, it is an adjustment of the mean of the dependent variable, subtracting means multiplied by slopes of each predictor variable. 

The estimates of the slopes are straightforward to compute using matrix algebra.^[A vector with the intercept and slopes can be computed by first constructing a so-called design matrix $\mathbf{X}$, where the first column contains only 1's, and the remaining columns contain values for each predictor. The vector with estimates is then computed as $\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}$, where $\mathbf{Y}$ is the vector with the values of the dependent variable, and $^\top$ stands for the matrix transpose, and $\mathbf{X}^{-1}$ for the inverse of matrix $\mathbf{X}$.] However, we won't cover matrix algebra here, so we will leave the estimation of the slopes to statistical software. 

The (unbiased) estimate of the error variance is also a direct generalization of the estimate for a simple regression model:
\begin{equation}
\hat{\sigma}^2_\epsilon = \frac{\sum_{i=1}^n (Y_i - \hat{\mu}_{Y|X_1, \ldots, X_m})^2}{n-\text{npar}(M)}
(\#eq:multiple-regression-sd-error-variance-estimate)
\end{equation}
Here, $\text{npar}(M)$ is the number of parameters in the model, excluding the error variance (i.e., it is the number of $\beta_j$ parameter, where $j=0,\ldots,m$. So $\text{npar}(M) = m + 1$. 

```{r}
coefs <- coefficients(mod)
sd_e <- sqrt(sum(residuals(mod)^2)/(nrow(dat) - length(coefficients(mod))))
```
Estimating the model in which we predict Trump votes from hate groups and education level gives the following estimates:
$$`r write_GLM_equation(mod,dv_name = "trump_votes", iv_names = c("hate_groups","education"), digits=4)`$$
A main thing to note here is that the estimated slope of $\texttt{hate_groups}$ is exactly the same as in Equation \@ref(eq:slope-hate-groups-from-residuals) when first computing the residual Trump votes and hate groups! This is because the slopes in a multiple regression model are __unique effects__ of a predictor on the dependent variable, after removing the effect of all other predictors from both that predictor and the dependent variable. So, equivalently, the slope of $\texttt{education}$ in the model above is identical to the slope we would obtain by first computing the residual Trump votes from
$$\texttt{trump_votes}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \texttt{hate_groups}_i + \hat{\epsilon}_{\text{vote},i}$$

and the residual education from 
$$\texttt{education}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \texttt{hate_groups}_i + \hat{\epsilon}_{\text{education},i}$$
and then determining the slope of the residual $\texttt{education}$ in the model
$$\hat{\epsilon}_{\text{vote},i} + \beta_0 + \beta_1 \times \hat{\epsilon}_{\text{education},i} + \epsilon_i$$

## Inference {#sec:04b-inference}

Testing hypotheses regarding the parameters of multiple regression models is analogous to testing the parameters of simple regression models. So one approach us to look at the sampling distribution of the estimates under the null hypothesis and compute the appropriate $t$ statistic. Testing, for each parameter the null hypothesis $H_0: \beta_j = 0$, where $j = 0,\ldots,2$, we obtain the results given in \@ref(tab:t-test-results-multiple-regression-table).
```{r t-test-results-multiple-regression-table}
tab <- summary(mod)$coefficients
rownames(tab) <- c("Intercept","Hate groups per million", "% bachelors degree or higher")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SE}(\\hat{\\beta})$", "$t$", "$P(\\geq \\lvert t \\rvert)$")
knitr::kable(tab, caption = "Null-hypothesis significance tests using the t statistic.", escape = FALSE)
```

Alternatively, we can compare models in which we either fix the parameter to the value assumed in the null hypothesis or not. As discussed in the previous chapter, this is a more general approach. When comparing two multiple regression models, we use the $F$-statistic defined in Equation \@ref(eq:glm-f-definition):
$$F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar(G)}}}$$
For instance, to test the null hypothesis $\beta_1 = 0$ in the model defined in Equation \@ref(eq:multiple-regression-votes-by-hate-education), we compare this general model
$$\text{MODEL G: } \quad \texttt{trump_votes}_i = \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i$$  
to a restricted model where we fix $\beta_1 = 0$:
$$\begin{align}
\text{MODEL R: } \quad \texttt{trump_votes}_i &= \beta_0 + 0 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \\
&= \beta_0 + \beta_1 \times \texttt{education}_i + \epsilon_i
\end{align}
$$  
In other words, we would compare the multiple regression model to a simple regression model where we don't include $\texttt{hate_groups}$, as the null hypothesis assumes that this predictor has no _unique_ effect on the dependent variable. If the Sum of Squared Error of MODEL R is not substantially higher than that of MODEL G, then there would be no good evidence that $\beta_1 \neq 0$. Remember that the Sum of Squared Error of MODEL R can never be lower than that of MODEL G, i.e.  $\text{SSE}(R) \geq \text{SSE}(G)$. This is because __estimating the parameters of MODEL G by maximum likelihood is equivalent to minimizing the Sum of Squared Error__. If $\hat{\beta}_1 = 0$ in MODEL G, that would mean that the SSE of both models is exactly the same. If $\hat{\beta}_1 \neq 0$ in MODEL G, then $\text{SSE}(R) > \text{SSE}(G)$, because otherwise the estimated MODEL G would not have maximised the likelihood (or equivalently minimised the SSE).

We have already shown results for MODEL G. The Sum of Squared Error for that model is $\text{SSE}(G) = `r sum(residuals(mod)^2)`$. Estimating MODEL R gives 
$$`r modg <- mod; modr <- lm(percent_Trump_votes ~ percent_bachelors_degree_or_higher, data=dat); write_GLM_equation(modr,dv_name = "trump_votes", iv_names = c("education"), digits=4)`$$
with $\text{SSE}(R) = `r sum(residuals(modr)^2)`$. The $F$ statistic then becomes:
\begin{align}
F &= \frac{\frac{`r sum(residuals(modr)^2)` -  `r sum(residuals(modg)^2)`}{`r length(coef(modg))` - `r length(coef(modr))`}}{\frac{`r sum(residuals(modg)^2)`}{`r nrow(dat)` - `r length(coef(modg))`}} \\
&= \frac{`r  (sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr)))`}{`r sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg)))`} \\
&= `r  ((sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr))))/(sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg))))`
\end{align}
To determine whether this value is significant, we need to compare it to an $F$-distribution with $\text{df}_1 = 3-2 = 1$ and $\text{df}_2 = `r nrow(dat)` - 3 = `r nrow(dat) - 3`$ degrees of freedom. For this distribution and a significance level of $\alpha = .05$, the critical value is `r qf(.95, df1 =  1, df2=nrow(dat) - 3)`, which is lower than the computed $F$ value, and hence the result is significant and we reject the null hypothesis.

Similarly, to test the null-hypothesis $\beta_2 = 0$ in MODEL G as defined above, we compare this model to a _new_ restricted MODEL R where we fix $\beta_2 = 0$:
$$
\begin{align}
\text{MODEL R: } \quad \texttt{trump_votes}_i &= \beta_0 + \beta_1 \times \texttt{hate_groups}_i + 0 \times \texttt{education}_i + \epsilon_i \\
&= \beta_0 + \beta_1 \times \texttt{hate_groups}_i + \epsilon_i
\end{align}
$$  
This is the model we estimated in the previous chapter, where we determined the estimates as 
$$`r modr <- lm(percent_Trump_votes ~ hate_groups_per_million, data=dat); write_GLM_equation(modr,dv_name = "trump_votes", iv_names = c("hate_groups"), digits=4)`$$ 
with an associated $\text{SSE}(R) = `r sum(residuals(modr)^2)`$. The $F$ statistic is then computed as 
\begin{align}
F &= \frac{\frac{`r sum(residuals(modr)^2)` -  `r sum(residuals(modg)^2)`}{`r length(coef(modg))` - `r length(coef(modr))`}}{\frac{`r sum(residuals(modg)^2)`}{`r nrow(dat)` - `r length(coef(modg))`}} \\
&= \frac{`r  (sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr)))`}{`r sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg)))`} \\
&= `r  ((sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr))))/(sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg))))`
\end{align}
The critical value is the same as before, and hence we also reject this null hypothesis.

To test whether the intercept $\beta_0 = 0$, we would compare MDOEL G to yet another MODEL R:
$$
\begin{align}
\text{MODEL R: } \quad \texttt{trump_votes}_i &= 0 + \beta_1 \times \texttt{hate_groups}_i + \beta_2 \times \texttt{education}_i + \epsilon_i \\
&=\beta_1 \times \texttt{hate_groups}_i + \beta_1 \times \texttt{education}_i + \epsilon_i
\end{align}
$$  
which is estimated as 
$$`r modr <- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher - 1, data=dat); write_GLM_equation(modr,dv_name = "trump_votes", iv_names = c("hate_groups"), digits=4)`$$ 
with $\text{SSE}(R) = `r sum(coefficients(modr)^2)`$. The $F$ statistic is computed in the same way as before:
\begin{align}
F &= \frac{\frac{`r sum(residuals(modr)^2)` -  `r sum(residuals(modg)^2)`}{`r length(coef(modg))` - `r length(coef(modr))`}}{\frac{`r sum(residuals(modg)^2)`}{`r nrow(dat)` - `r length(coef(modg))`}} \\
&= \frac{`r  (sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr)))`}{`r sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg)))`} \\
&= `r  ((sum(residuals(modr)^2) -  sum(residuals(modg)^2))/(length(coef(modg)) - length(coef(modr))))/(sum(residuals(modg)^2)/(nrow(dat) - length(coef(modg))))`
\end{align}

We have collect all the test results in Table \@ref(tab:trump-multiple-regression-ANOVA-table).
```{r trump-multiple-regression-ANOVA-table}
opts <- options()
options(knitr.kable.NA = "")
tmp <- cbind(estimate = c(coefficients(mod),Error=NA),car::Anova(mod, type=3))
rownames(tab) <- c("Intercept","Hate groups per million", "% bachelors degree or higher")
colnames(tab) <- c("$\\hat{\\beta}$","$\\text{SS}$", "$F$", "$P(\\geq \\lvert F \\rvert)$")
knitr::kable(tab, caption = "Null-hypothesis significance tests by model comparisons and the F statistic.", escape = FALSE)
options(opts)
```


When performing such model comparisons, it is very important to remember that what we call MODEL G and MODEL R will depend on the null hypothesis tested. Generally, MODEL G will remain the same within the context of an analysis, but MODEL R will vary from test to test.

## Partitioning variance and Sums of Squares

Let's have another look at the formula for the $F$ statistic:

$$F = \frac{\frac{\text{SSE}(R) -  \text{SSE}(G)}{\text{npar}(G) - \text{npar}(R)}}{\frac{\text{SSE}(G)}{n-\text{npar(G)}}}$$
The numerator (top part) consists of a difference between two SSE terms, divided by the difference in the number of estimated parameters. Because the comparison is between nested models, and MODEL G contains more estimated parameters than MODEL R, we can view this part as the average reduction of the SSE (per parameter) due to the additional parameters of MODEL G. The denominator (bottom part) consists of the SSE of MODEL G divided by $n-\text{npar}(G)$. This is an unbiased estimate of the error variance of MODEL G, and also referred to as the Mean Squared Error (MSE). 

Each SSE term is itself related to an error variance, as the SSE is the sum of squared deviations between the observations and the model predictions (the conditional means $\mu_{Y|X_1,\ldots,X_m})$). 

```{tikz sse-partition, device='svg', fig.cap="Partitioning the variance in a multiple regression model. Each circle represents the variance of a variable. Overlapping regions represent shared variability (e.g. covariance) between variables. ", out.width="50%"}
\def\firstcircle{(1,1) circle (1.75cm)}
\def\secondcircle{(2,1) circle (1.75cm)}
\def\thirdcircle{(1.5,2) circle (1.75cm)}
\begin{tikzpicture}
	\pgfsetxvec{\pgfpoint{2cm}{0cm}}
	\pgfsetyvec{\pgfpoint{0cm}{2cm}}
	\tikzstyle{every node}=[minimum size=.7cm]

	\fill[white!50] (-1,0) -- (-1,3.5) -- (3.5,3.5) -- (3.5,0) -- (-1,0);

	\draw \firstcircle;
	\draw \secondcircle;
	\draw \thirdcircle;
	
	\draw (0.2,0.2) node {$X_1$};
	\draw (2.8,0.2) node {$X_2$};
	\draw (1.5,3) node {$Y$};
	\draw (1.5,2.3) node {$A$};
	\draw (1.1,1.6) node {$B$};
	\draw (1.5,1.4) node {$C$};
	\draw (1.9,1.6) node {$D$};

\end{tikzpicture}	
```




<!--


## Controlling for education level and poverty

```{r}
#mod <- lm(percent_Trump_votes ~ hate_groups_per_million + #percent_bachelors_degree_or_higher + percent_in_poverty,data=dat)
```

```{r}
#opts <- options()
#options(knitr.kable.NA = "")
#tmp <- cbind(estimate = c(coefficients(mod),Error=NA),car::Anova(mod, type=3))
#knitr::kable(tmp)
#options(opts)
```
-->

## Assumptions {#sec:04b-assumptions}

The assumptions of the multiple regression model, and with that any of the versions of the General Linear Model we discuss, are all really about the errors, which are assumed to be independently and identically distributed:
$$\epsilon_i \sim \mathbf{Normal}(0, \sigma)$$

In words, this implies the following assumptions for a multiple regression and/or GLM:

1. __Normality__: the errors $\epsilon_i$ are Normal-distributed
2. __Unbiasedness__: the mean of $\epsilon_i$ is 0. This means that the conditional means are indeed a linear function of the predictors, and that the model predictions are unbiased.
3. __Homoscedasticity__: the errors $\epsilon_i$ have a constant variance $\sigma^2$ and thus also a constant standard deviation $\sigma$.
4. __Independence__: any error term $\epsilon_i$ is independent of any other $\epsilon_j$ (for all $i \text{ and } j \neq i$). Independence here means that $p(\epsilon_j|\epsilon_i) = p(epsilon_j)$ for all $i \text{ and } j \neq i$

It is important to realise that the assumption of Normal-distributed errors does not necessarily translate in assuming that the dependent variable $Y$ is itself Normal-distributed. This will often to be the case. Not does the model require the predictors to be Normal-distributed. No assumption is made at all about the distribution of the predictors. 

In Section \@ref(sec:02-assumptions) we discussed some methods to assess the assumption of Normality (i.e. histograms, QQ-plots and statistical tests such as the Shapiro-Wilk test). The assumption of unbiasedness and homoscedasticity is generally assessed visually with a so-called predicted-ny-residual plot. This plot (see Figure \@ref(fig:trump-predicted-residual)) depicts the residuals as a function of the model predictions (i.e. the estimated conditional means). The unbiasedness assumption implies that the for each predicted value $\hat{Y}$ (i.e. each conditional mean), the residual or error terms are scattered around 0. For instance, it should _not_ be the case that for relatively low and high values of the predictions, the errors are generally above 0, while for medium values of the predictions, the errors are generally below 0. Such a pattern would be indicative of biased predictions and a likely non-linear relation between predictors and the dependent variable. The homoscedasticity assumption implies that the spread of the residuals is equal for each predicted value $\hat{Y}$. Looking at the predicted vs residual plot in Figure \@ref(fig:trump-predicted-residual), there is no clear indication that either assumption is violated. In addition, the QQ-plot indicates that the expected and sample quantiles do not differ radically, at least for the middle quantiles. Note that at the extremes (low and high quantiles), you will generally find more variable results, as quantiles in the tails of a distribution are less reliable.

```{r trump-predicted-residual, fig.cap="Predicted vs residual plot, and a QQ plot, for MODEL G predicting Trump votes from hate groups and education level.", fig.show="hold", out.width="50%", fig.width=4,fig.height=4}
data.frame(predicted=predict(modg), residual=residuals(modg)) %>%
  ggplot(aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept = 0, lty=3)
data.frame(predicted=predict(modg), residual=residuals(modg)) %>%
  ggplot(aes(sample = residual)) + stat_qq() + stat_qq_line()
```

For comparison, Figure shows two examples of predicted vs residual plots where the assumption of unbiasedness and homoscedasticity do not hold. 
```{r trump-predicted-residual, fig.cap="Predicted vs residual plot, and a QQ plot, for MODEL G predicting Trump votes from hate groups and education level.", fig.show="hold", out.width="50%", fig.width=4,fig.height=4}
tmod <- data.frame() 
data.frame(predicted=predict(modg), residual=residuals(modg)) %>%
  ggplot(aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept = 0, lty=3)
data.frame(predicted=predict(modg), residual=residuals(modg)) %>%
  ggplot(aes(sample = residual)) + stat_qq() + stat_qq_line()
```


## Multicollinearity 

## Outliers

<!-- 
## Polynomial regression


## Summary

-->
